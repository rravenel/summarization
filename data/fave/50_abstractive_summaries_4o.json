[{"papers": [{"metadata": {"year": 1976}, "authors": ["G. Panayi"], "summary": "Auto-immune disease may result from the interaction of the genetic load of the individual, modification of self-tissue antigens by environmental agents such as virus or drugs and abnormalities of the immunological system itself such as the loss of controlling or suppressor T cells with age. In the majority of people the outcome is tolerance, maintenance of normal tissue architecture and function. In the unfortunate few the outcome is auto-immune disease, that is, failure to recognize \"self\".", "title": "Auto-immune disease.", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Angela  Richard-Eaglin", "Benjamin A Smallheer"], "summary": "Autoimmune disorders are a category of diseases in which the immune system attacks healthy cells as a result of a dysfunction of the acquired immune system. Clinical presentation and diagnosis are disease specific and often correspond with the degree of inflammation, as well as the systems involved. Treatment varies based on the specific disease, its stage of presentation, and patient symptoms. The primary goal of treatment is to decrease inflammation, minimize symptoms, and lessen the potential for relapse. Graves disease, Hashimoto thyroiditis, rheumatoid arthritis, Crohn disease, ulcerative colitis, systemic lupus erythematosus, and multiple sclerosis are discussed in this article.", "title": "Immunosuppressive/Autoimmune Disorders.", "claims": null}, {"metadata": {"year": 2016}, "authors": ["A. M\u00e9kinian", "O. Fain"], "summary": "\n        \n      ", "title": "Autoimmune diseases. Epidemiological, diagnostic and treatment principles", "claims": null}, {"metadata": {"year": 2001}, "authors": ["Sue R. Beers"], "summary": "An autoimmune disorder is the body\u2019s reaction to an unknown stimulus causing the overproduction of antibodies that are directed against body tissue (Wallace, 1995). Although most autoimmune diseases are associated with significant and chronic morbidity and disability, epidemiological studies are generally limited to a few of the more prevalent disorders such as rheumatoid arthritis or diabetes mellitus. Recent estimates indicate that 1 in 31 individuals in the United States currently are diagnosed with an autoimmune disease, with women at higher risk than men (Jacobson, Gange, Rose, & Graham, 1997).", "title": "Systemic Autoimmune Disease", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Rukhshanda Nosheen", "Dr. Muhammad Bilal", "Dr Madiha Afzal"], "summary": ": Immune system disorders cause abnormally low activity or over activity of the immune system. In cases of immune system over activity, the body attacks and damages its own tissues (autoimmune diseases). Immune deficiency diseases decrease the body's ability to fight invaders, causing vulnerability to infections. Some of the autoimmune disease are as follows rheumatoid arthritis, systemic lupus erythematosus (lupus), inflammatory bowel disease (IBD),multiple sclerosis (MS),Type 1 diabetes mellitus, Guillain-Barre syndrome, chronic inflammatory demyelinating polyneuropathy, psoriasis. Human", "title": "STUDY OF CURRENT AND EMERGING HUMAN AUTOIMMUNE DISEASES", "claims": null}, {"metadata": {"year": 2008}, "authors": ["J. B. Soep", "J. R. Hollister"], "summary": "Autoimmune diseases occur when the immune system attacks the body\u2019s own organs and tissues. Rheumatologic diseases encompass conditions in which this autoimmune response causes inflammation of the musculoskeletal system, skin, and blood vessels. Rheumatologic conditions often present with arthritis, fever, and rash. Due to the frequent joint and muscle symptoms, patients may experience gross motor and fine motor delays.", "title": "Illnesses: Autoimmune Rheumatological Diseases", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Shradha  Agarwal", "Charlotte  Cunningham-Rundles"], "summary": "Common variable immunodeficiency (CVID) is the most common clinically significant primary immune defect. Although the hallmark of CVID is hypogammaglobulinemia, the intrinsic dysregulation of the immune system leads to defective T-cell activation and proliferation, as well as dendritic cell and cytokine defects. Although 70% to 80% of patients have had recurrent sinopulmonary infections, autoimmunity and inflammatory complications are also common. The most common autoimmune conditions are immune thrombocytopenic purpura and hemolytic anemia, but other autoimmune complications arise, including rheumatoid arthritis, pernicious anemia, primary biliary cirrhosis, thyroiditis, sicca syndrome, systemic lupus, and inflammatory bowel disease. Treatment of autoimmunity includes highdose immunoglobulins, corticosteroids, selected immu no suppressants, and other immune modulators. This review focuses on autoimmune conditions associated with CVID, potential mechanisms of immune dysregulation, and therapeutic strategies.", "title": "Autoimmunity in common variable immunodeficiency", "claims": null}, {"metadata": {"year": 2012}, "authors": ["E. Choi"], "summary": "The immune system is the body\u2019s main line of defense against invasion by infectious organisms, such as bacteria, virus, and fungi. In normal immune systems, an immune response does not occur against the self-antigen, and is called self-tolerance. Autoimmune diseases occur when body tissues are attacked by the body\u2019s own immune system due to loss of tolerance to self-antigens (Dejaco et al., 2006). Under these conditions, body tissues are destroyed by antigen-specific cytotoxic T cells or auto-antibodies, and the accompanying inflammation can cause functional disability and morbidity. Autoimmune diseases are a heterogenous group of diseases with a wide spectrum of symptoms that affect approximately 6% of the population (Siatskas et al., 2006). They can be broadly classified as organ-specific or systemic depending on the location of the target antigen and clinical features (Sakaguchi, 2000). Common examples of systemic autoimmune diseases include systemic lupus erythematosus (SLE), rheumatoid arthritis, systemic sclerosis, ankylosing spondylitis, and polymyositis; examples of organ-specific autoimmune diseases include type 1 diabetes, Addison\u2019s disease, Hashimoto thyroiditis, Graves\u2019 disease, Sjogren's syndrome, vitiligo, pernicious anemia, glomerulonephritis, myasthenia gravis, Goodpasture\u2019s syndrome, autoimmune hemolytic anemia, idiopathic thrombocytopenia purpura, and pulmonary fibrosis. The clinical features of autoimmune diseases are very different, but immunemediated mechanisms are associated with the generation of an adaptive immune response toward the target antigen (Kuby, 1994; Siatskas et al., 2006).", "title": "New Therapeutic Challenges in Autoimmune Diseases", "claims": null}], "query": "Autoimmune disease", "summary_abstract": "The collection of abstracts provides a comprehensive overview of autoimmune diseases, highlighting their complex nature and the underlying mechanisms. Autoimmune diseases arise when the immune system mistakenly attacks the body's own tissues, a process often linked to genetic predispositions, environmental factors, and immune system dysregulation (Panayi, 1976; Choi, 2012). These diseases are characterized by a loss of self-tolerance, leading to the production of auto-antibodies and cytotoxic T cells that target self-antigens, resulting in tissue damage and inflammation (Choi, 2012; Beers, 2001).\n\nAutoimmune diseases can be classified into organ-specific and systemic categories, with examples including type 1 diabetes, Hashimoto thyroiditis, rheumatoid arthritis, and systemic lupus erythematosus (Choi, 2012; Richard-Eaglin & Smallheer, 2018). The clinical presentation of these diseases varies widely, often correlating with the degree of inflammation and the systems involved (Richard-Eaglin & Smallheer, 2018). Treatment strategies focus on reducing inflammation, managing symptoms, and preventing relapses, utilizing approaches such as immunosuppressants and immune modulators (Agarwal & Cunningham-Rundles, 2009).\n\nEpidemiological data suggest that autoimmune diseases affect a significant portion of the population, with women being at a higher risk than men (Beers, 2001). The prevalence and impact of these diseases underscore the importance of understanding their pathogenesis and developing effective therapeutic interventions.", "summary_extract": null}, {"papers": [{"metadata": {"year": 1990}, "authors": ["J. Hall"], "summary": "Genomic imprinting is the modification of gene expression depending on whether the genetic material is inherited from the mother or the father. The article reviews the concept and its implications in various human disorders such as the chromosomal deletion syndromes, human cancers, endocrine disorders, and various other diseases which show parent-of-origin effects.", "title": "Genomic imprinting.", "claims": null}, {"metadata": {"year": 1990}, "authors": ["J. Casades\u00fas", "R. Maldonado"], "summary": "Genomic imprinting is an epigenetic mark introduced on a DNA molecule without alteration of the base sequence. Upon replication, the primary mark is propagated to the daughter DNA molecules. Epigenetic DNA modification often serves as a regulatory signal and may play a crucial role in many developmental processes. Although this mode of gene regulation was first discovered in multicellular eukaryotes, cases of imprinting have been recently found in lower eukaryotes, bacteria and phage. Thus it may be reasonable to list DNA modification among the major mechanisms that regulate gene expression.", "title": "Genomic imprinting in microorganisms.", "claims": null}, {"metadata": {"year": 2007}, "authors": ["\u0410. \u0410. \u041f\u0435\u043d\u0434\u0438\u043d\u0430", "\u041e. \u0410. \u0415\u0444\u0438\u043c\u043e\u0432\u0430", "\u0422. \u0412. \u041a\u0443\u0437\u043d\u0435\u0446\u043e\u0432\u0430", "\u0412\u043b\u0430\u0434\u0438\u0441\u043b\u0430\u0432 \u0421\u0435\u0440\u0433\u0435\u0435\u0432\u0438\u0447 \u0411\u0430\u0440\u0430\u043d\u043e\u0432"], "summary": "Genomic imprinting is an epigenetic mechanism of homologous gene expression regulation, depending on their parent-of-origin. The present review is focused on pathologies, heritable diseases and syndromes, caused by disruption of imprinting resulted from hemizygosity of imprinted genes, uniparental disomies and epigenetic disorders.", "title": "\u0411\u043e\u043b\u0435\u0437\u043d\u0438 \u0433\u0435\u043d\u043e\u043c\u043d\u043e\u0433\u043e \u0438\u043c\u043f\u0440\u0438\u043d\u0442\u0438\u043d\u0433\u0430", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Lara K Abramowitz", "Marisa S Bartolomei"], "summary": "Genomic imprinting is an epigenetic process resulting in the monoallelic parent-of-origin-specific expression of a subset of genes in the mammalian genome. The parental alleles are differentially marked by DNA methylation during gametogenesis when the genomes are in separate compartments. How methylation machinery recognizes and differentially modifies these imprinted regions in germ cells remains a key question in the field. While studies have focused on determining a sequence signature that alone could distinguish imprinted regions from the rest of the genome, recent reports do not support such a hypothesis. Rather, it is becoming clear that features such as transcription, histone modifications and higher order chromatin are employed either individually or in combination to set up parental imprints.", "title": "Genomic imprinting: recognition and marking of imprinted loci.", "claims": null}, {"metadata": {"year": 1997}, "authors": ["N. Niikawa"], "summary": "Genomic imprinting is a new concept proposed to explain unusual observations in early mammalian development, the occurrence of certain genetic diseases, genetic anticipation or incomplete penetrance, and tumorigenesis. The basic mechanism of the imprinting has remained obscure, although DNA-methylation, chromatin structure, and/or DNA replication may have a role. Genomic imprinting is a biological phenomenon determined by an evolutionally acquired, underlying system that may control harmonious development and growth in mammals. It is also relevant to the occurrence of some genetic disorders in man.", "title": "Genomic imprinting relevant to genetic diseases.", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Parul Gupta", "D. Chakraborty", "R. Taggar", "Dhirendra Kumar", "R. Sharma", "V. P. Singh"], "summary": "Genomic imprinting refers to an epigenetic mark that distinguishes parental alleles and results in a monoallelic, parental-specific expression pattern in mammals. The alleles of imprinted genes are marked epigenetically as discrete elements termed imprinting control regions with their parental origin in gametes through the use of DNA methylation, at the very least. Imprinted genes are normally involved in foetal growth and behavioural development. Consequently, aberrant imprinting disturbs development and is the cause of numerous well-known imprinting disorders, including Beckwith-Wiedemann syndrome, Prader-Willi syndrome, Cancer and Angelman syndrome.", "title": "Genomic imprinting in mammals - a review.", "claims": null}, {"metadata": {"year": 2011}, "authors": ["J. Killian"], "summary": "Genomic imprinting is the biological process whereby a gene or genomic domain exists in a state of epigenetic differentiation that depends upon its parent of origin. Importantly, the establishment and propagation of these parent-specific genomic conformations does not alter the primary DNA sequence comprised of A, C, G, and T nucleotides. Genomic imprints may be covalent (DNA methylation) or non-covalent (DNA-protein and DNA-RNA interactions, genomic localization in nuclear space), and the process of imprinting encompasses the specialized nuclear enzymatic machinery that maintains parental epigenetic markings throughout the cell cycle. Because of genomic imprinting, the parent of origin of homologous genetic alleles in diploid individuals can be determined in the absence of DNA sequence polymorphisms and without recourse to parental DNA samples. As illustrated in Figure 1, alleles of imprinted genes look and behave differently, as determined by parent of origin.", "title": "Genomic Imprinting: Parental differentiation of the genome", "claims": null}, {"metadata": {"year": 2021}, "authors": ["E. Sazhenova", "I. Lebedev"], "summary": "Genomic imprinting is an epigenetic phenomenon that differentiates maternal and paternal copies of genes in the genome and causes monoallelic expression depending on parental origin. Imprinting is an evolutionary puzzle, as it bears the costs of diploidization without its advantages, namely, protection from recessive mutations. The aim of this review is to answer the question of why genomic imprinting arose and became fixed in the evolution of angiosperms, insects, marsupials, and placental mammals.", "title": "[Evolutionary Aspects of Genomic Imprinting].", "claims": null}], "query": "genomic imprinting", "summary_abstract": "The collection of papers provides a comprehensive overview of genomic imprinting, an epigenetic phenomenon that results in the parent-of-origin-specific expression of certain genes. Hall (1990) and Gupta et al. (2014) highlight the implications of genomic imprinting in various human disorders, including chromosomal deletion syndromes, cancers, and specific imprinting disorders like Beckwith-Wiedemann and Prader-Willi syndromes. Casades\u00fas and Maldonado (1990) and Abramowitz and Bartolomei (2012) discuss the mechanisms of imprinting, emphasizing the role of DNA methylation and other epigenetic marks in regulating gene expression without altering the DNA sequence. Niikawa (1997) and Killian (2011) further explore the biological significance of imprinting, suggesting its role in development and growth, as well as its evolutionary implications. Sazhenova and Lebedev (2021) delve into the evolutionary aspects, questioning why imprinting persists despite its costs. Collectively, these studies underscore the complexity and significance of genomic imprinting in both health and disease, as well as its evolutionary and developmental roles.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2004}, "authors": ["A. Christe", "P. Vock"], "summary": "The most common causes of pulmonary edema are cardiac failure, renal failure and injury edema (diffuse alveolar damage). The injury edema typically shows airspace consolidation due to exsudation of fluid in the periphery of the lung with air bronchograms, no interstitial fluid accumulation can be found and only rarely pleural effusions are present. Cardiac and renal edemas often show a mixed interstitial and alveolar transudation without air bronchograms. Pleural effusions are often present. Both usually have an increased heart-size and an increased vascular pedicle width. To distinguish them better one has to look at the distribution of the pulmonary edema: The cardiac edema typically shows a gravitational and the renal edema a central distribution.", "title": "[Radiologic criteria to differentiate pulmonary edema].", "claims": null}, {"metadata": {"year": 1980}, "authors": ["Hans  Ewerbeck"], "summary": "In patients with nonspecific bronchopneumonias due to viral diseases such as influenza and measles or to mycoplasmal and rickettsial diseases, both hila are enlarged and frequently have indistinct borders. The perihilar markings are increased and consist of streaky densities extending outward, with peribronchial and interstitial infiltrates. Unilateral hilar reactions are seen with unilateral pulmonary lesions (pneumonia, abscess).", "title": "Significant Radiologic Findings in Pulmonary Diagnosis", "claims": null}, {"metadata": {"year": 1962}, "authors": ["J. P. Lavender", "J. Doppman"], "summary": "The anatomy of the normal hilar shadow as shown on tomograms of 48 normal patients has been reviewed and the importance of the superior pulmonary vein emphasised. The changes occurring in the hilar appearance have been described in 33 patients with mitral valve disease and 45 patients with left ventricular failure. The pulmonary artery was shown to be enlarged in those cases of mitral stenosis having pulmonary arterial hypertension and a similar enlargement in over 50 per cent of the cases of left ventricular failure suggests the existence of pulmonary arterial hypertension in these patients. In the majority of both mitral and left ventricular failure cases, the lateral border of the hilum was straightened or convex, a change shown to be due to enlarged upper lobe veins. This sign, together with a clearly defined, oblique lower border of the superior pulmonary vein, are of value in analysing the hilum in states of pulmonary venous hypertension.", "title": "The hilum in pulmonary venous hypertension.", "claims": null}, {"metadata": {"year": 1987}, "authors": ["J. F. Nunn"], "summary": "Publisher Summary \nThe pulmonary edema is defined as an increase in pulmonary extravascular water, which occurs when transudation or exudation exceeds the capacity of the lymphatic drainage. The pulmonary capillary endothelial cells abut against one another at fairly loose junctions that are of the order of 5 nm wide. These junctions permit the passage of quite large molecules and the pulmonary lymph contains albumin at about half the concentration in plasma. The epithelial cells meet at tight junctions with a gap of only about 1 nm. The tightness of these junctions is crucial for the prevention of the escape of large molecules, such as albumin, from the interstitial fluid into the alveoli. The lung has a well developed lymphatic system draining the interstitial tissue through a network of channels around the bronchi and pulmonary vessels towards the hilum. The lymphatic vessels cannot be identified at alveolar level but can be seen in association with bronchioles.", "title": "Chapter 23 \u2013 Pulmonary oedema", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Maria  Barile"], "summary": "Pulmonary edema is a common clinical entity caused by the extravascular movement of fluid into the pulmonary interstitium and alveoli. The four physiologic categories of edema include hydrostatic pressure edema, permeability edema with and without diffuse alveolar damage (DAD), and mixed edema where there is both an increase in hydrostatic pressure and membrane permeability. As radiographic manifestations and etiologies are varied, an appreciation for both the common and uncommon manifestations and causes of pulmonary edema is essential for accurate diagnosis.", "title": "Pulmonary Edema: A Pictorial Review of Imaging Manifestations and Current Understanding of Mechanisms of Disease", "claims": null}, {"metadata": {"year": 1989}, "authors": ["J. Gurney", "L. Goodman"], "summary": "Focal patterns of pulmonary edema are confusing and often mistaken for the more common causes of focal lung disease, pneumonia, infarction, or aspiration. The authors report four cases of right upper lobe edema secondary to mitral regurgitation. The pathogenesis believed to be responsible for this condition is the vector of blood flow from the left ventricle to left atrium, which may be targeted at the right superior pulmonary vein, locally accentuating the forces for edema formation in the right upper lobe. Pulmonary edema accompanying mitral regurgitation should be suspected whenever right upper lobe consolidation develops in a patient with known or suspected mitral valve disease. The presence of interstitial edema in the remainder of the lungs can help in the differentiation of this condition from pneumonia and other disorders.", "title": "Pulmonary edema localized in the right upper lobe accompanying mitral regurgitation.", "claims": null}, {"metadata": {"year": 1971}, "authors": [], "summary": "obstruction of the superior vena cava as its main clinical characteristic. When the fibrosis develops at the hilar level, typically in the age group 15 to 35, a mainly unilateral mass may cause only mild respiratory disability. If both hila are affected, increasing incapacity may lead to death. The diagnosis of hilar fibrosis is often difficult, especially when the mass is not obvious radiologically. The clinical findings vary according to whether the arteries or veins are predominantly affected.3 When the arteries bear the brunt, recurrent pulmonary and pleural infections are the rule; haemoptyses may result from widespread bronchopulmonary anastomoses. Radiographs show a shift of the mediastinum to the affected side, a small pulmonary artery, and a thickened pleura. Angiograms and studies of lung function confirm the diminished arterial blood flow. Pulmonary hypertension develops when both pulmonary arteries are involved. When the veins are mainly infiltrated, severe haemoptyses may arise from capillary congestion. Pulmonary hypertension may lead to right heart failure. Radiographs usually show a small lung with venous congestion and pleural thickening. Cardiac catheterization may show an exceptionally high pulmonary artery pressure. Additional diagnostic evidence of idiopathic hilar fibrosis may come from bronchography or bronchoscopy, when a long smooth bronchial stricture may be evident owing to compression of the associated large airways by fibrous tissue. The main differential diagnosis is from primary lung cancer, and it may be resolved only at thoracotomy. Primary pulmonary venous sclerosis is not associated with a hilar mass, nor are the larger bronchi affected. Medical treatment has nothing effective to offer. Surgical correction of obstruction is rarely feasible and attempts to dilate bronchial strictures may lead to dangerous haemorrhage. Perhaps these unfortunate patients, few as they are, will be afforded relief when more is known of the cause of this obscure malady.", "title": "Bites and blains.", "claims": null}, {"metadata": {"year": 1971}, "authors": ["R. Singer", "C. Ogilvy", "G. Rordorf", "J. Biller", "J. Wilterdink"], "summary": "obstruction of the superior vena cava as its main clinical characteristic. When the fibrosis develops at the hilar level, typically in the age group 15 to 35, a mainly unilateral mass may cause only mild respiratory disability. If both hila are affected, increasing incapacity may lead to death. The diagnosis of hilar fibrosis is often difficult, especially when the mass is not obvious radiologically. The clinical findings vary according to whether the arteries or veins are predominantly affected.3 When the arteries bear the brunt, recurrent pulmonary and pleural infections are the rule; haemoptyses may result from widespread bronchopulmonary anastomoses. Radiographs show a shift of the mediastinum to the affected side, a small pulmonary artery, and a thickened pleura. Angiograms and studies of lung function confirm the diminished arterial blood flow. Pulmonary hypertension develops when both pulmonary arteries are involved. When the veins are mainly infiltrated, severe haemoptyses may arise from capillary congestion. Pulmonary hypertension may lead to right heart failure. Radiographs usually show a small lung with venous congestion and pleural thickening. Cardiac catheterization may show an exceptionally high pulmonary artery pressure. Additional diagnostic evidence of idiopathic hilar fibrosis may come from bronchography or bronchoscopy, when a long smooth bronchial stricture may be evident owing to compression of the associated large airways by fibrous tissue. The main differential diagnosis is from primary lung cancer, and it may be resolved only at thoracotomy. Primary pulmonary venous sclerosis is not associated with a hilar mass, nor are the larger bronchi affected. Medical treatment has nothing effective to offer. Surgical correction of obstruction is rarely feasible and attempts to dilate bronchial strictures may lead to dangerous haemorrhage. Perhaps these unfortunate patients, few as they are, will be afforded relief when more is known of the cause of this obscure malady.", "title": "Treatment of aneurysmal subarachnoid haemorrhage.", "claims": null}], "query": "why does pulmonary edema tend to occur in hilar area rather than peripheral?", "summary_abstract": "The tendency for pulmonary edema to occur in the hilar area rather than the periphery can be attributed to several factors related to the anatomy and physiology of the lungs. Nunn (1987) highlights the role of the lung's lymphatic system, which is well-developed around the bronchi and pulmonary vessels, directing fluid towards the hilum. This anatomical feature facilitates the accumulation of fluid in the hilar region. Lavender and Doppman (1962) emphasize the significance of pulmonary venous hypertension, particularly in conditions like mitral valve disease and left ventricular failure, which often result in changes in the hilar appearance due to enlarged upper lobe veins. This suggests that the hilar region is more susceptible to fluid accumulation due to increased venous pressure. Additionally, Christe and Vock (2004) note that cardiac edema, a common cause of pulmonary edema, typically shows a central distribution, which aligns with the hilar region. Collectively, these studies suggest that the anatomical and physiological characteristics of the hilar area, combined with the effects of certain cardiovascular conditions, contribute to the predisposition for pulmonary edema to manifest in this region.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2010}, "authors": ["Chunyan  He", "Abrar A.  Qureshi", "Jiali  Han"], "summary": "Ultraviolet (UV)-induced oxidative stress has been implicated in skin carcinogenesis [1]. Several antioxidant enzymes, such as glutathione peroxidase (GPX) and catalase (CAT), counteract oxidative damage and constitute a primary defense against oxidative stress. GPX is a soluble selenoprotein that reduces H2O2 and organic hydroperoxides to H2O, and GPX1 is the most abundant and ubiquitous intracellular isoform [1]. GPX1 activity is not strongly affected by UV and is considered to be the most important antioxidant enzyme defense mechanism in the skin [2]. CAT is a heme enzyme that neutralizes reactive oxygen species by converting H2O2 to H2O and O2. CAT activity in the skin is significantly reduced after exposure to UV [2], which suggests its effect may be prone to effect modification by environmental factors. \n \nInherited variants in the encoding genes that affect the activity or expression of these antioxidant enzymes are hypothesized to modulate oxidative stress and thus influence skin cancer risk. A polymorphism in the GPX1 gene (Pro198Leu, rs1050450) and a polymorphism in the promoter region of the CAT gene (C-262T, rs1001179) have been shown to be associated with lower enzyme activities of their encoded enzymes [3, 4]. To test our main hypothesis that these two genetic polymorphisms are associated with skin cancer risk, we conducted a nested case-control study of Caucasians (218 melanoma, 285 squamous cell carcinoma (SCC), and 300 basal cell carcinoma (BCC) cases, and 870 age-matched controls) within the Nurses\u2019 Health Study. We further investigated potential gene-environment interactions between these polymorphisms and lifestyle factors such as dietary antioxidant intake and sun exposure related risk factors. A detailed description of the characteristics of cases and controls was published previously [5]. Information on dietary intake was collected prospectively by food-frequency questionnaires, and total-energy-adjusted cumulative average of dietary intake was used to reduce within-person variation and represent long-term dietary intake [6]. \n \nWe genotyped the two single nuclear polymorphisms (SNPs) (rs1050450 and rs1001179) by the 5\u2032 nuclease assay (TaqMan\u00ae) in 384-well format, using the ABI PRISM 7900HT Sequence Detection System (Applied Biosystems, Foster City, CA). The distributions of genotypes for the two SNPs were in Hardy-Weinberg equilibrium among controls (p=0.94, 0.83, respectively). We compared the cases of each type of skin cancer to the common control series. We used unconditional multivariate logistic regression to model the association between genetic polymorphisms and skin cancer risk and to estimate multivariate Odds Ratios (ORs) and 95% Confidence Intervals (CIs). To test statistical significance of gene-environmental interactions, we used dominant model for genotypes and dichotomized environmental exposures as low versus high based on median values among controls. We tested the statistical significance of a single multiplicative interaction term. \n \nIn the main effect analysis (Table 1), we observed that the GPX1 198 Leu/Leu genotype was significantly associated with a two-fold increased risk of melanoma (OR, 2.14; 95% CI, 1.22\u20133.72), after adjustment for age and other covariates. No association was found between this polymorphism and SCC or BCC risk, which was consistent with one previous study [7]. This polymorphism has been shown to be associated with lung cancer [8] and breast cancer [3] previously. We did not observe significant association between the CAT C-262T polymorphism and the risk of any type of skin cancer. \n \n \n \nTable 1 \n \nAssociation between GPX Pro198Leu and CAT C-262T genetic polymorphisms and skin cancer risk a \n \n \n \nAs exploratory analyses, we further tested gene-environment interactions between the genetic variants and lifestyle factors that modulate oxidative stress. We found the association between the CAT C-262T polymorphism and melanoma risk was significantly modified by history of severe sunburns (p for interaction, 0.008, Table 2), a variable combining exposure intensity and biological response to sun exposure. The positive association between history of severe sunburns and melanoma risk was restricted to T carriers (OR, 1.73; 95% CI, 1.02\u20132.92), compared to women with CC genotype (OR, 1.03; 95% CI, 0.63\u20131.69). We also observed a significant gene-diet interaction between the CAT C-262T polymorphism and total carotenoid intake on melanoma risk (p for interaction, 0.01). The inverse association of total carotenoid intake with melanoma risk was limited among women with CC genotype (OR, 0.63; 95% CI, 0.41\u20130.97), whereas no association was observed among T carriers (OR, 1.23; 95% CI, 0.77\u20131.97). Inconsistent results were reported on the relationship between dietary carotenoid intake and melanoma risk in several previous case-control studies. An inverse association between the intake and the risk of melanoma was observed in some studies [9], but not in other studies [10]. Our results suggest that the inconsistency in the literature may reflect a potential gene-diet interaction. As we tested different genetic polymorphisms, multiple environmental exposures and dietary factors, and three types of skin cancer, multiple testing in our study may lead to false positive results. Replications in independent studies are needed to validate these results. No significant interactions were observed between the GPX1 polymorphism and these lifestyle factors on melanoma risk. We did not observe any significant interaction between these genetic variants and environmental exposures on the risk of SCC or BCC. \n \n \n \nTable 2 \n \nInteraction between the CAT C-262T genetic polymorphism and history of severe sunburns and total carotenoid intake on melanoma risk \n \n \n \nIn summary, we first observed the GPX1 198 Leu/Leu genotype was significantly associated with a two-fold increased risk of melanoma, and the association between the CAT C-262T polymorphism and melanoma risk was significantly modified by history of severe sunburns and total carotenoid intake. Further research is needed to confirm these possible associations and illustrate the underlying molecular mechanisms.", "title": "Polymorphisms in genes involved in oxidative stress and their interactions with lifestyle factors on skin cancer risk.", "claims": null}, {"metadata": {"year": 2011}, "authors": ["D. Robbins", "Yunfeng Zhao"], "summary": "Recent studies have shown that antioxidant enzyme expression and activity are drastically reduced in most human skin diseases, leading to propagation of oxidative stress and continuous disease progression. However, antioxidants, an endogenous defense system against reactive oxygen species (ROS), can be induced by exogenous sources, resulting in protective effects against associated oxidative injury. Many studies have shown that the induction of antioxidants is an effective strategy to combat various disease states. In one approach, a SOD mimetic was applied topically to mouse skin in the two-stage skin carcinogenesis model. This method effectively reduced oxidative injury and proliferation without interfering with apoptosis. In another approach, Protandim, a combination of 5 well-studied medicinal plants, was given via dietary administration and significantly decreased tumor incidence and multiplicity by 33% and 57%, respectively. These studies suggest that alterations in antioxidant response may be a novel approach to chemoprevention. This paper focuses on how regulation of antioxidant expression and activity can be modulated in skin disease and the potential clinical implications of antioxidant-based therapies.", "title": "The Role of Manganese Superoxide Dismutase in Skin Cancer", "claims": null}, {"metadata": {"year": 2017}, "authors": ["W. Chowdhury", "Shahida Arbee", "S. Debnath", "Shah Mehedi Bin Zahur", "Sharmim Akter", "A. Karim", "Md. Mohabbulla Mohib", "A. Tisha", "Md Abu Taher Sagor", "Sarif Mohiuddin"], "summary": "Evidence reported that cancers are spreading every nook and corner of the world at an alarming rate. Skin diseases like chronic skin inflammations, psoriasis and skin cancers have also been burning topic in today. Protections of a biological system are often hampered while skin gets damaged. Factors like UV, radiation, viruses, chronic diseases, genetic predispositions, food habits and environmental exposures might lead to skin cancers. In addition to these, urbanization and globalization may also contaminate the environment that may eventually modify several biological and genetic functions. USA, Europe and Australia are in the most dangerous zone to be exposed. We basically performed detailed search of PubMed, Google Scholar and Science Direct for literature search and collecting related information. On the other hand, experiments suggested that antioxidant components such as phenolic acid derivatives, flavonoids and flavonol found to be preventive against cancer cell proliferations. Moreover, antioxidants have been also evaluated as a protective agent against chronic inflammatory diseases as well. These molecules may participate as an additional therapy which could exert synergistic effects while applying with other chemotherapeutic agents. Our literature findings and hypothetical figure may establish a good correlation between skin cancer and antioxidant therapy. Therefore this study will be focused on skin cancer biology and some possible management strategies using antioxidant phyto-nutrients.", "title": "Potent Role of Antioxidant Molecules in Prevention and Management of SkinCancer", "claims": null}, {"metadata": {"year": 2014}, "authors": ["A. Godic", "B. Polj\u0161ak", "M. Adami\u010d", "R. Dahmane"], "summary": "Skin cells are constantly exposed to reactive oxygen species (ROS) and oxidative stress from exogenous and endogenous sources. UV radiation is the most important environmental factor in the development of skin cancer and skin aging. The primary products caused by UV exposure are generally direct DNA oxidation or generation of free radicals which form and decompose extremely quickly but can produce effects that can last for hours, days, or even years. UV-induced generation of ROS in the skin develops oxidative stress when their formation exceeds the antioxidant defense ability. The reduction of oxidative stress can be achieved on two levels: by lowering exposure to UVR and/or by increasing levels of antioxidant defense in order to scavenge ROS. The only endogenous protection of our skin is melanin and enzymatic antioxidants. Melanin, the pigment deposited by melanocytes, is the first line of defense against DNA damage at the surface of the skin, but it cannot totally prevent skin damage. A second category of defense is repair processes, which remove the damaged biomolecules before they can accumulate and before their presence results in altered cell metabolism. Additional UV protection includes avoidance of sun exposure, usage of sunscreens, protective clothes, and antioxidant supplements.", "title": "The Role of Antioxidants in Skin Cancer Prevention and Treatment", "claims": null}, {"metadata": {"year": 2003}, "authors": ["S. Briganti", "M. Picardo"], "summary": "Due to its interface function between the body and the environment, the skin is chronically exposed to both endogenous and environmental pro\u2010oxidant agents, leading to the harmful generation of reactive oxygen species (ROS). There is compelling evidence that oxidative stress is involved in the damage of cellular constituents, such as DNA, cell membrane lipids or proteins. To protect the skin against the over\u2010load of oxidant species, it contains a well\u2010organised system of both chemical and enzymatic antioxidant which are able to work in a synergistic manner. Skin antioxidant network protects cells against oxidative injury and prevent the production of oxidation products, such as 4\u2010hydroxy\u20102\u2010nonenal or malonaldehyde, which are able to induce protein damage, apoptosis or release of pro\u2010inflammatory mediators, such as cytokines. When oxidative stress overwhelms the skin antioxidant capacity the subsequent modification of cellular redox apparatus leads to an alteration of cell homeostasis and a generation of degenerative processes. Topical application or oral administration of antioxidants has been recently suggested as preventive therapy for skin photoaging and UV\u2010induced cancer. The recognition that ROS can act as second messengers in the induction of several biological responses, such as the activation of NF\u2010kB or AP\u20101, the generation of cytokines, the modulation of signalling pathways, etc., has led many researchers to focus on the possible effects of antioxidants in many pathological processes. The recent demonstration that the peroxisome proliferators\u2010activated receptors, whose natural ligands are polyunsaturated fatty acids and theirs oxidation products, have a central role in the induction of some skin diseases, such as psoriasis or acne, has indicated new links between free radicals and skin inflammation. Based on these findings, the review summarises the possible correlations between antioxidant imbalance, lipid oxidative breakage and skin diseases, from both a pathological and therapeutic points of view.", "title": "Antioxidant activity, lipid peroxidation and skin diseases. What's new", "claims": null}, {"metadata": {"year": 2023}, "authors": ["K. Sable", "B. Shields"], "summary": "Dietary supplements, including vitamins and their derivatives, have been utilized within the field of dermatology to treat a variety of skin conditions. Antioxidants inhibit oxidation and decrease cellular damage caused by free radicals, potentially preventing DNA damage due to UV radiation. Laboratory studies have demonstrated promising results supporting the possible role of antioxidants for prevention of skin cancer related to UV exposure. We review the effects of frequently encountered antioxidants and vitamins suggested for the chemoprevention of melanoma and nonmelanoma skin cancer (NMSC) in humans.", "title": "The Role of Dietary Antioxidants in Melanoma and Nonmelanoma Skin Cancer.", "claims": null}, {"metadata": {"year": 2017}, "authors": ["M. Furue", "H. Uchi", "C. Mitoma", "A. Hashimoto-Hachiya", "T. Chiba", "Takamichi Ito", "T. Nakahara", "G. Tsuji"], "summary": "Skin is the outermost part of the body and is, thus, inevitably exposed to UV rays and environmental pollutants. Oxidative stress by these hazardous factors accelerates skin aging and induces skin inflammation and carcinogenesis. Aryl hydrocarbon receptors (AHRs) are chemical sensors that are abundantly expressed in epidermal keratinocytes and mediate the production of reactive oxygen species. To neutralize or minimize oxidative stress, the keratinocytes also express nuclear factor-erythroid 2-related factor-2 (NRF2), which is a master switch for antioxidant signaling. Notably, there is fine-tuned crosstalk between AHR and NRF2, which mutually increase or decrease their activation states. Many NRF2-mediated antioxidant phytochemicals are capable of up- and downmodulating AHR signaling. The precise mechanisms by which these phytochemicals differentially affect the AHR and NRF2 system remain largely unknown and warrant future investigation.", "title": "Antioxidants for Healthy Skin: The Emerging Role of Aryl Hydrocarbon Receptors and Nuclear Factor-Erythroid 2-Related Factor-2", "claims": null}, {"metadata": {"year": 2004}, "authors": ["C. Sander", "Hong Chang", "F. Hamm", "P. Elsner", "J. Thiele"], "summary": "Melanoma and nonmelanoma skin cancers are among the most prevalent cancers in the human population. Solar ultraviolet radiation is considered a major etiological factor but the relationship between dose, timing, and nature of exposure to tumor development is still unclear. Free radicals are generated by normal physiologic processes, including aerobic metabolism and inflammatory response, but may inflict cellular damage when generation is increased and antioxidant defense mechanisms are overwhelmed. Important findings supporting the free radical hypothesis in skin carcinogenesis are: (1) Reactive oxygen species (ROS) are generated in UVA\u2010 and UVB\u2010irradiated skin in excessive doses, (2) the natural cutaneous antioxidant defense is impaired upon UV\u2010exposure, (3) free radicals are involved in all steps of carcinogenesis, (4) supplementation with antioxidants can inhibit skin carcinogenesis, and (5) conditions that increase ROS generation enhance photocarcinogenesis. These findings provide a promising rationale for the development of powerful new antioxidant strategies in the prevention and therapy of skin cancer.", "title": "Role of oxidative stress and the antioxidant network in cutaneous carcinogenesis", "claims": null}], "query": "Antioxidant enzymes and Skin Cancer risk", "summary_abstract": "The collection of papers provides a comprehensive overview of the role of antioxidant enzymes in modulating skin cancer risk. He et al. (2010) highlight the significance of genetic polymorphisms in antioxidant enzymes, such as GPX1 and CAT, in influencing skin cancer risk. They found that the GPX1 198 Leu/Leu genotype is associated with a two-fold increased risk of melanoma, while the CAT C-262T polymorphism's association with melanoma risk is significantly modified by factors like severe sunburns and carotenoid intake. This suggests a complex interplay between genetic factors and environmental exposures in skin cancer risk.\n\nRobbins and Zhao (2011) emphasize the potential of antioxidant induction as a chemopreventive strategy, noting that exogenous sources can enhance antioxidant defenses and reduce oxidative injury in skin carcinogenesis models. Similarly, Chowdhury et al. (2017) discuss the protective role of antioxidant components, such as phenolic acids and flavonoids, against cancer cell proliferation, suggesting their potential as adjunctive therapies in skin cancer management.\n\nGodic et al. (2014) and Briganti and Picardo (2003) both underscore the importance of antioxidants in counteracting UV-induced oxidative stress, which is a major factor in skin cancer development. They advocate for strategies that enhance antioxidant defenses, such as the use of supplements and topical applications, to mitigate oxidative damage.\n\nSable and Shields (2023) review the role of dietary antioxidants in preventing skin cancer, supporting the notion that antioxidants can inhibit DNA damage from UV radiation. Furue et al. (2017) explore the interaction between AHR and NRF2 pathways in keratinocytes, highlighting the complex regulatory mechanisms of antioxidant signaling in response to oxidative stress.\n\nFinally, Sander et al. (2004) provide evidence that free radicals play a crucial role in skin carcinogenesis and that antioxidant supplementation can inhibit this process, reinforcing the potential of antioxidants in skin cancer prevention and therapy.\n\nCollectively, these studies suggest that antioxidant enzymes and their genetic variants, along with dietary and topical antioxidants, play a significant role in modulating skin cancer risk, offering promising avenues for prevention and treatment strategies.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2022}, "authors": ["Madeline E. McCarthy", "William B. Dodd", "Xiaoming Lu", "Nishi Patel", "Charlotte V. Haskell", "H. Sanabria", "M. Blenner", "M. Birtwistle"], "summary": "Systematic, genome-scale genetic screens have been instrumental for elucidating genotype-phenotype relationships, but approaches for probing genetic interactions have been limited to at most \u223c100 pre-selected gene combinations in mammalian cells. Here, we introduce a theory for high-throughput genetic interaction screens. The theory extends our recently developed Multiplexing using Spectral Imaging and Combinatorics (MuSIC) approach to propose \u223c105 spectrally unique, genetically-encoded MuSIC barcodes from 18 currently available fluorescent proteins. Simulation studies based on constraints imposed by spectral flow cytometry equipment suggest that genetic interaction screens at the human genome-scale may be possible if MuSIC barcodes can be paired to guide RNAs. While experimental testing of this theory awaits, it offers transformative potential for genetic perturbation technology and knowledge of genetic function. More broadly, the availability of a genome-scale spectral barcode library for non-destructive identification of single-cells could find more widespread applications such as traditional genetic screening and high-dimensional lineage tracing.", "title": "A Theory for High-Throughput Genetic Interaction Screening", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Dorothea  Emig", "Hagen  Blankenburg", "Fidel  Ram\u00edrez", "Mario  Albrecht"], "summary": "Complex biological systems comprise a large number of interacting molecules. The identification and detailed characterization of the functions of the involved genes and proteins are crucial for modeling and understanding such systems. To interrogate the various cellular processes, high-throughput techniques such as the Affymetrix Exon Array or RNA interference (RNAi) screens are powerful experimental approaches for functional genomics. However, they typically yield long gene lists that require computational methods to further analyze and functionally annotate the experimental results and to gain more insight into important molecular interactions. Here, we focus on bioinformatics software tools for the functional interpretation of exon expression data to discover alternative splicing events and their impact on gene and protein architecture, molecular networks, and pathways. We additionally demonstrate how to explore large lists of candidate genes as they also result from RNAi screens. In particular, our exemplary application studies show how to analyze the function of human genes that play a major role in human stem cells or viral infections.", "title": "Functional characterization of human genes from exon expression and RNA interference results.", "claims": null}, {"metadata": {"year": 2004}, "authors": ["Adam  Friedman", "Norbert  Perrimon"], "summary": "The availability of complete genome sequences from many organisms has yielded the ability to perform high-throughput, genome-wide screens of gene function. Within the past year, rapid advances have been made towards this goal in many major model systems, including yeast, worms, flies, and mammals. Yeast genome-wide screens have taken advantage of libraries of deletion strains, but RNA-interference has been used in other organisms to knockdown gene function. Examples of recent large-scale functional genetic screens include drug-target identification in yeast, regulators of fat accumulation in worms, growth and viability in flies, and proteasome-mediated degradation in mammalian cells. Within the next five years, such screens are likely to lead to annotation of function of most genes across multiple organisms. Integration of such data with other genomic approaches will extend our understanding of cellular networks.", "title": "Genome-wide high-throughput screens in functional genomics.", "claims": null}, {"metadata": {"year": 2004}, "authors": ["Anne E. Carpenter", "David M. Sabatini"], "summary": "By using genome information to create tools for perturbing gene function, it is now possible to undertake systematic genome-wide functional screens that examine the contribution of every gene to a biological process. The directed nature of these experiments contrasts with traditional methods, in which random mutations are induced and the resulting mutants are screened for various phenotypes. The first genome-wide functional screens in Caenorhabditis elegans and Drosophila melanogaster have recently been published, and screens in human cells will soon follow. These high-throughput techniques promise the rapid annotation of genomes with high-quality information about the biological function of each gene.", "title": "Systematic genome-wide screens of gene function", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Maximilian  Billmann", "Michael  Boutros"], "summary": "Genetic screens have identified many novel components of various biological processes, such as components required for cell cycle and cell division. While forward genetic screens typically generate unstructured \u2018hit\u2019 lists, genetic interaction mapping approaches can identify functional relations in a systematic fashion. Here, we discuss a recent study by our group demonstrating a two-step approach to first screen for regulators of the mitotic cell cycle, and subsequently guide hypothesis generation by using genetic interaction analysis. The screen used a high-content microscopy assay and automated image analysis to capture defects during mitotic progression and cytokinesis. Genetic interaction networks derived from process-specific features generate a snapshot of functional gene relations in those processes, which follow a temporal order during the cell cycle. This complements a recently published approach, which inferred directional genetic interactions reconstructing hierarchical relationships between genes across different phases during mitotic progression. In conclusion, this strategy leverages unbiased, genome-wide, yet highly sensitive and process-focused functional screening in cells.", "title": "Systematic epistatic mapping of cellular processes", "claims": null}, {"metadata": {"year": 2013}, "authors": ["J. Millstein"], "summary": "Next-generation sequencing and other high-throughput technologies have made it feasible to characterize millions of sequence variations on large numbers of study participants. But when it comes to identifying a small number of these genetic features (or feature sets) that are associated with a disease trait, the investigator is faced with a formidable multiple-testing challenge. It can be thought of as a signal-to-noise problem, where the large number of unrelated genetic features tends to drown out the faint signal of the small number of biologically relevant features.", "title": "Screening-testing approaches for gene-gene and gene-environment interactions using independent statistics", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Alina Guna", "K. R. Page", "Joseph R. Replogle", "Theodore K. Esantsi", "Maxine L. Wang", "J. Weissman", "R. M. Voorhees"], "summary": "The ability to map genetic interactions has been essential for determining gene function and defining biological pathways. Therefore, a system to readily perform genome-wide genetic modifier screens in human cells is a powerful platform for dissecting complex processes in mammalian cells, where redundancy and adaptation commonly mask the phenotype of a single genetic perturbation. Here, we report a CRISPR interference (CRISPRi) based platform, compatible with Fluorescence Activated Cell Sorting (FACS)-based reporter screens, that can be used to query epistatic relationships at scale. This is enabled by a flexible dual-sgRNA library design that allows for the simultaneous delivery and selection of a fixed sgRNA and a second randomized guide, comprised of a genome-wide library, with a single transduction. As a proof of principle, we apply our approach to study the pathways that mediate tail-anchored (TA) protein insertion at the endoplasmic reticulum (ER). We show that this dual-guide library approach can be successfully coupled with FACS-based reporter screening, to identify genetic epistasis and thereby place TA biogenesis factors in their respective parallel pathways. We demonstrate that this dual-guide approach is both more sensitive and specific than traditional growth screening approaches, and is ideally suited for dissecting the complex interplay between factors in human cells.", "title": "A dual sgRNA library design to probe genetic modifiers using genome-wide CRISPRi screens", "claims": null}, {"metadata": {"year": 2005}, "authors": ["F. Al-Shahrour", "R. D\u00edaz-Uriarte", "J. Dopazo"], "summary": "MOTIVATION\nThe analysis of genome-scale data from different high throughput techniques can be used to obtain lists of genes ordered according to their different behaviours under distinct experimental conditions corresponding to different phenotypes (e.g. differential gene expression between diseased samples and controls, different response to a drug, etc.). The order in which the genes appear in the list is a consequence of the biological roles that the genes play within the cell, which account, at molecular scale, for the macroscopic differences observed between the phenotypes studied. Typically, two steps are followed for understanding the biological processes that differentiate phenotypes at molecular level: first, genes with significant differential expression are selected on the basis of their experimental values and subsequently, the functional properties of these genes are analysed. Instead, we present a simple procedure which combines experimental measurements with available biological information in a way that genes are simultaneously tested in groups related by common functional properties. The method proposed constitutes a very sensitive tool for selecting genes with significant differential behaviour in the experimental conditions tested.\n\n\nRESULTS\nWe propose the use of a method to scan ordered lists of genes. The method allows the understanding of the biological processes operating at molecular level behind the macroscopic experiment from which the list was generated. This procedure can be useful in situations where it is not possible to obtain statistically significant differences based on the experimental measurements (e.g. low prevalence diseases, etc.). Two examples demonstrate its application in two microarray experiments and the type of information that can be extracted.", "title": "Discovering molecular functions significantly related to phenotypes by combining gene expression data and biological information", "claims": null}], "query": "high throughput genetic screen are a powerful tool in identifying individual gene function and interaction", "summary_abstract": "The collection of papers collectively underscores the transformative potential of high-throughput genetic screens in elucidating gene function and interactions. McCarthy et al. (2022) introduce a theoretical framework for high-throughput genetic interaction screens using MuSIC barcodes, which could enable genome-scale genetic interaction studies, thereby enhancing our understanding of genetic functions. Emig et al. (2012) highlight the power of high-throughput techniques like RNAi screens in functional genomics, emphasizing the need for computational tools to interpret the vast data these screens generate. Friedman and Perrimon (2004) discuss the rapid advancements in genome-wide screens across various model organisms, which are pivotal for annotating gene functions and understanding cellular networks. Carpenter and Sabatini (2004) emphasize the systematic nature of genome-wide functional screens, which allow for the detailed examination of each gene's contribution to biological processes. Billmann and Boutros (2016) demonstrate the utility of genetic interaction mapping in identifying functional relationships, particularly in the context of the cell cycle. Guna et al. (2023) present a CRISPRi-based platform for genome-wide genetic modifier screens, which is particularly adept at uncovering complex genetic interactions in human cells. Collectively, these studies illustrate the significant advancements and applications of high-throughput genetic screens in mapping gene functions and interactions, offering insights into complex biological processes and pathways.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2019}, "authors": ["Boer  Cui", "Genevi\u00e8ve  Boisjoly", "Ahmed M El-Geneidy", "David M Levinson"], "summary": "Abstract Inequality in transport provision is an area of growing concern among transport professionals, as it results in low-income individuals travelling at lower speeds while covering smaller distances. Accessibility, the ease of reaching destinations, may hold the key in correcting these inequalities through providing a means to evaluate land use and transport interventions. This article examines the relationship between accessibility and commute duration for low-income individuals compared to the higher-income, in three major Canadian metropolitan regions, Toronto, Montreal, and Vancouver using separate multilevel mixed effects statistical models for car and public transport commuters. Accessibility measures are generated for jobs and workers both at the origin (home) and the destination (place of work) to account for the impact of competing labor and firms. Our models show that the impacts of accessibility on commute duration are present and in many cases stronger for low-income individuals than for higher income groups. The results suggest that low-income individuals have more to gain (in terms of reduced commute time) from increased accessibility to low-income jobs at the origin and to workers at the destination. Similarly, they also have more to lose from increased accessibility to low-income workers at the origin and to low-income jobs at the destination, which are proxies for increased competition. Policies targeting improvements in accessibility to jobs, especially low-income ones, by car and public transport while managing the presence of competition can serve to bridge the inequality gap that exists in commuting behavior.", "title": "Accessibility and the journey to work through the lens of equity", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Boer Cui", "G. Boisjoly", "A. El-geneidy", "D. Levinson"], "summary": "Inequality in transport provision is an area of growing concern among transport professionals, as it results in low-income individuals travelling at lower speeds while covering smaller distances. Accessibility, the ease of reaching destinations, may hold the key in correcting these inequalities through providing a means to evaluate land use and transport interventions. This article examines the relationship between accessibility and commuting duration for low-income individuals, compared to the general population, in three major Canadian metropolitan regions, Toronto, Montreal, and Vancouver using multilevel mixed effects statistical models for car and public transport commuters separately. Accessibility measures are generated for jobs and workers both at the origin (home) and the destination (place of work) to account for the impact of competing labor and firms. Our models show that the impacts of accessibility on commuting duration are present and stronger for low-income individuals than for the general population, and the differences in impact are more visible for public transport commuters. The results suggest that low-income individuals have more to gain (in terms of reduced commute time) from increased accessibility to low-income jobs at the origin and to workers at the destination. Similarly, they also have more to lose from increased accessibility to low-income workers at the origin and to low- income jobs at the destination, which are proxies for increased competition. Policies targeting improvements in accessibility to jobs, especially low-income ones, by car and public transport while managing the presence of competition can serve to bridge the inequality gap that exists in commuting behavior.", "title": "Accessibility, equity, and the journey to work", "claims": null}, {"metadata": {"year": 2011}, "authors": ["A. Soltani", "Yousef Esmaeili Ivaki"], "summary": "Neo-classical economic doctrine dominating governmental policies shows its impact on recent transport policies, causing these policies; tend to base on demand and efficiency criteria instead of equity concerns. Public transit operating for remote areas is less cost-effective eventually leading to have a low level of service quality. In metropolitan areas of developing countries, a large part of socially disadvantaged and vulnerable groups live in outer suburban locations not in the inner-city. Transit equity evaluation is required by in order to consider the requirements of more vulnerable populations, as well as to foster equal benefits. The evaluation approach is based on highlighting the spatial distribution and clustering patterns of bus network and service as well as some disadvantaged social groups including unemployed, migrated, less educated, elderly, young, and disabled. The hypothesis is that vulnerable groups and economically disadvantaged communities receive a less than equal share of public bus services. The findings show that poor accessibility is associated both with low-income neighborhoods and with neighborhoods with disproportionately high populations of migrated, less-educated, unemployed and low-income groups. Modifications need to make in transport planning and policy system to achieve a better distribution of public transport services in hope of increasing level of service for minority groups and economically disadvantaged communities.", "title": "Inequity in the Provision of Public Bus Service for Socially Disadvantaged Groups", "claims": null}, {"metadata": {"year": 2004}, "authors": ["D. Mignot"], "summary": "The current context of increasing social and spatial disparities raises the issue of universal accessibility to the city and its services, in particular for low incomes persons. The issue of inequality with regard to travel, which we have considered first of all at an aggregate level using the usual travel indicators (number of trips, distance covered, travel time budget), essentially comes down to inequality in access to the car. An analysis of recent changes in urban public transport pricing policy and a survey conducted within welfare and social integration agencies in the conurbations of Lyon, Nancy and Nantes have been used to obtain a more accurate qualitative and quantitative picture of transport difficulties.", "title": "Transport et justice sociale", "claims": null}, {"metadata": {"year": 2011}, "authors": ["D. Lewis"], "summary": "Poverty, inequality and social exclusion are closely tied to personal mobility and the accessibility of goods and services. Evidence of the economic role of transport in promoting better living standards and greater wellbeing can be seen in the effects of both overall public investment in transport infrastructure, and in the impacts of specific transport policies, projects and multi-project plans.", "title": "Economic perspectives on transport and equality", "claims": null}, {"metadata": {"year": 2005}, "authors": ["David Caubel"], "summary": "The social dimension of urban transport policies becomes a paramount issue of the public action. This may result from the observed growth of social inequalities, and the persistence or even the worsening of segregation processes. In particular, this issue leads to questions about the access to activities and services that individuals need everyday. It also relates to the characterization of an equal access for all to the city.In this context, the aim of the paper is to validate, by the measure in the case of the Lyons conurbation, the existence of urban accessibility disparities between the richest and the deprived territories residents, by accounting for the private car and the public transport. We show that there are equal accesses to amenities by car, but the travel modes unequal access implies inequities in accessing urban activities. Then, our paper analyses the social impacts of the public transport policy contents aiming at reducing the urban accessibility disparities or the gaps to the \"standard society\". The assessment made, based on the interpretation of the Lyons Urban Travel Plan, accounts for concrete answers of the access to amenities improvement for everyone, and in particular for the underprivileged populations. Nevertheless, these answers prove to be limited. A public transport policy cannot claim to reduce the unequal access between territories and to fight against the exclusion of a minority of the population, without accounting for the local and overall contexts of urban morphology, without being in line with regional planning policies \u2013 in particular the activities location control policies.", "title": "Public transport development and equal access for all to urban amenities", "claims": null}, {"metadata": {"year": 1980}, "authors": ["E. Boer"], "summary": "In practice there are not equal chances for everybody to use the transport system. The national government aims to lessen the inequality in this respect as regards certain groups of the population. Such groups are for instance the poor, children, old people, the disabled, the people without a driving licence, and housewives. The question of when there can be said to be a large inequality in the use of transport is difficult to answer. This report describes what is known about the phenomenon of unequal accessibility to the transport system and discusses how far this phenomenon can be studied. Social scientific studies in different countries of the choice behaviour of travellers are reviewed. (TRRL)", "title": "INEQUALITY IN THE USE OF TRANSPORT, ITS RELEVANCE AND THE POSSIBILITY OF STUDYING IT. AN INTERNATIONAL LITERATURE REVIEW", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Anik Pramanik", "Pan Xu", "Yifang Xu"], "summary": "There are many news articles reporting the obstacles con- fronting poverty-stricken households in access to public transits. These barriers create a great deal of inconveniences for these impoverished families and more importantly, they con-tribute a lot of social inequalities. A typical approach ad- dressing the issue is to build more transport infrastructure to o\ufb00er more opportunities to access the public transits es- pecially for those deprived communities. Examples include adding more bus lines connecting needy residents to railways systems and extending existing bus lines to areas with low socioeconomic status. Recently, a new strategy is proposed, which is to harness the ubiquitous ride-hailing services to con-nect disadvantaged households with the nearest public trans- portations. Compared with the former infrastructure-based solution, the ride-hailing-based strategy enjoys a few exclusive bene\ufb01ts such as higher e\ufb00ectiveness and more \ufb02exibility. In this paper, we propose an optimization model to study how to integrate the two approaches together for equity-promotion purposes. Speci\ufb01cally, we aim to design a strategy of allo- cating a given limited budget to di\ufb00erent candidate programs such that the overall social equity is maximized, which is de- \ufb01ned as the minimum covering ratio among all pre-speci\ufb01ed protected groups of households (based on race, income, etc.). We have designed a linear-programming (LP) based rounding algorithm, which proves to achieve an optimal approximation ratio of 1 \u2212 1 / \ud835\udc52 . Additionally, we test our algorithm against a few baselines on real data assembled by outsourcing multiple public datasets collected in the city of Chicago. Experimental results con\ufb01rm our theoretical predictions and demonstrate the e\ufb00ectiveness of our LP-based strategy in promoting social equity, especially when the budget is insu\ufb03cient.", "title": "Equity Promotion in Public Transportation", "claims": null}], "query": "competition public transport accessibility inequality poverty", "summary_abstract": "The collection of papers collectively highlights the significant role of public transport accessibility in addressing inequality and poverty. Boer Cui et al. (2019) and Boer Cui et al. (2018) emphasize that low-income individuals experience more pronounced benefits from improved accessibility to jobs via public transport, as it can significantly reduce their commute times. However, increased competition for low-income jobs can also exacerbate inequalities. Soltani and Ivaki (2011) point out that economically disadvantaged communities often receive a lower share of public bus services, which necessitates modifications in transport planning to ensure equitable service distribution. Mignot (2004) and Caubel (2005) discuss the disparities in urban accessibility, noting that public transport policies need to be aligned with regional planning to effectively reduce these inequalities. Pramanik et al. (2022) propose integrating traditional infrastructure improvements with ride-hailing services to enhance access for impoverished communities, suggesting that such strategies can be more flexible and effective in promoting social equity. Collectively, these studies underscore the importance of targeted transport policies and infrastructure investments in bridging accessibility gaps and reducing poverty-related inequalities.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2016}, "authors": ["Florian Dumpert", "Katja von Eschwege", "M. Beck"], "summary": "This article shows the motivation for, approach to, and results of applying support vector machines in official statistics concerning the business register for test purposes. A support vector machine is a universally applicable machine learning and classification method. Based on a mathematical optimisation approach, objects can be classified by specific variables and be allocated to corresponding classes. The non-parametric statistical method succeeded in classifying enterprises with respect to the so-called third sector and is therefore suitable to improve and complement the method used up to now.", "title": "Einsatz von Support Vector Machines bei der Sektorzuordnung von Unternehmen", "claims": null}, {"metadata": {"year": 2001}, "authors": ["D. Sebald", "J. Bucklew"], "summary": "Two enhancements are proposed to the application and theory of support vector machines. The first is a method of multicategory classification based on the binary classification version of the support vector machine (SVM). The method, which is called the M-ary SVM, represents each category in binary format, and to each bit of that representation is assigned a conventional SVM. This approach requires only [log/sub 2/(K)] SVMs, where K is the number of classes. We give an example of classification on an octaphase-shift-keying (8-PSK) pattern space to illustrate the main concepts. The second enhancement is that of adding equality constraints to the conventional binary classification SVM. This allows pinning the classification boundary to points that are known a priori to lie on the boundary. Applications of this method often arise in problems having some type of symmetry, We present one such example where the M-ary SVM is used to classify symbols of a CDMA two-user, multiuser detection pattern space.", "title": "Support vector machines and the multiple hypothesis test problem", "claims": null}, {"metadata": {"year": 2011}, "authors": ["R. Gilbert", "T. Trafalis", "I. Adrianto"], "summary": "In this article, we introduce the subject of support vector machines (SVMs), describing their applications to binary and multiclass classification as well as different SVM formulations that are used in such supervised learning problems. This article is not exhaustive and many approaches have not been considered. More detailed treatments are discussed in the books by Abe, Cristianini and Shawe-Taylor, Scholkopf, and Smola and Vapnik. \n \n \nKeywords: \n \nsupport vector machines; \nstatistical learning; \nmulticlass classification; \noptimization", "title": "Support Vector Machines for Classification", "claims": null}, {"metadata": {"year": 1999}, "authors": ["M. Niranjan"], "summary": "Summary form only given. There has been much interest in the use of support vector machines (SVM) as an approach to high performance pattern classification. In the linearly separable case, SVMs attempt to position a class boundary so that the margin from the nearest example is maximised. This criterion can be implemented by solving a quadratic programming problem, and the solution turns out to be one in which the class boundary may be expressed as a linear combination of a subset of the training data (the support vectors). The elegance of the QP formulation, and the relationship between control of complexity in this formulation and Vapnik-Chervonenkis dimensions are seen as prime attractions of the SVM method. A related idea in high performance pattern classification is that of boosting multiple classifiers. The author shows that the standard SVM formulation is not robust to noise and explains the performance of boosting algorithms by reference to receiver operating characteristics curves.", "title": "Support vector machines: a tutorial overview and critical appraisal", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Yoonkyung  Lee"], "summary": "The support vector machine is a supervised learning technique for classification increasingly used in many applications of data mining, engineering, and bioinformatics. This chapter aims to provide an introduction to the method, covering from the basic concept of the optimal separating hyperplane to its nonlinear generalization through kernels. A general framework of kernel methods that encompass the support vector machine as a special case is outlined. In addition, statistical properties that illuminate both advantage and limitation of the method due to its specific mechanism for classification are briefly discussed. For illustration of the method and related practical issues, an application to real data with high-dimensional features is presented.", "title": "Support vector machines for classification: a statistical portrait.", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Shahab  Araghinejad"], "summary": "Classifying data is a common task in data-driven modeling. Using support vector machines, we can separate classes of data by a hyperplane. A support vector machine (SVM) is a concept for a set of related supervised learning methods that analyze data and recognize patterns, used for classification and regression analysis. The formulation of SVM uses the structural risk minimization principle, which has been shown to be superior to the traditional empirical risk minimization principle used by conventional neural networks. This chapter presents principles of classification and regression analysis by support vector machines, briefly. Also related MATLAB programs are presented.", "title": "Support Vector Machines", "claims": null}, {"metadata": {"year": 2002}, "authors": ["J. Currall"], "summary": "In the last decade Support Vector Machines (SVMs) have emerged as an important learning technique for solving classification and regression problems in various fields, most notably in computational biology, finance and text categorization. This is due in part to built-in mechanisms to ensure good generalization which leads to accurate prediction, the use of kernel functions to model non-linear distributions, the ability to train relatively quickly on large data sets using novel mathematical optimization techniques and most significantly the possibility of theoretical analysis using computational learning theory. In this thesis, we discuss the theoretical basis and computational approaches to Support Vector Machines.", "title": "Support Vector Machines for Classification and Regression", "claims": null}, {"metadata": {"year": 2012}, "authors": ["H. Bhavsar", "Mahesh Panchal"], "summary": "185 All Rights Reserved \u00a9 2012 IJARCET Abstract-With increasing amounts of data being generated by businesses and researchers there is a need for fast, accurate and robust algorithms for data analysis. Improvements in databases technology, computing performance and artificial intelligence have contributed to the development of intelligent data analysis. Support vector machines are a specific type of machine learning algorithm that are among the most widelyused for many statistical learning problems, such as spam filtering, text classification, handwriting analysis, face and object recognition, and countless others. Support vector machines have also come into widespread use in practically every area of bioinformatics within the last ten years, and their area of influence continues to expand today. The support vector machine has been developed as robust tool for classification and regression in noisy, complex domains. The two key features of support vector machines are generalization theory, which leads to a principled way to choose an hypothesis; and, kernel functions, which introduce nonlinearity in the hypothesis space without explicitly requiring a non-linear algorithm.", "title": "A Review on Support Vector Machine for Data Classification", "claims": null}], "query": "which classifications are suitable for support vector machines?", "summary_abstract": "Support vector machines (SVMs) are a versatile and powerful tool for classification tasks across various domains. They are particularly suitable for both binary and multiclass classification problems. Sebald and Bucklew (2001) discuss the M-ary SVM, an extension of the binary SVM, which efficiently handles multicategory classification by representing each category in binary format. This approach is further supported by Gilbert et al. (2011), who highlight the application of SVMs in both binary and multiclass classification scenarios.\n\nSVMs are also effective in high-dimensional feature spaces, as noted by Lee (2010), who emphasizes their use in data mining, engineering, and bioinformatics. The ability of SVMs to handle non-linear distributions through kernel functions is a significant advantage, as described by Currall (2002) and Bhavsar and Panchal (2012). These kernel functions allow SVMs to model complex, non-linear relationships without explicitly requiring a non-linear algorithm, making them suitable for a wide range of applications, including text classification, handwriting analysis, and bioinformatics.\n\nMoreover, SVMs are robust to noise and can generalize well, which is crucial for accurate predictions in noisy, complex domains (Niranjan, 1999; Araghinejad, 2014). The structural risk minimization principle used in SVMs, as highlighted by Araghinejad (2014), further enhances their classification capabilities by providing a superior alternative to traditional empirical risk minimization methods.\n\nIn summary, support vector machines are suitable for a wide array of classification tasks, including binary and multiclass problems, and are particularly effective in high-dimensional and non-linear feature spaces. Their robustness to noise and strong generalization capabilities make them a preferred choice in various fields, from bioinformatics to text categorization.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2021}, "authors": ["Antonio  Cano", "Paul  Ar\u00e9valo", "Dar\u00edo  Benavides", "Francisco  Jurado"], "summary": "Abstract The problem of climate change because greenhouse gas emissions is aggravating, especially in public transport, which encourages the development of new technologies and clean energy control methods for the propulsion of vehicles such as tramways. A new energy control for a real tramway has been proposed in this paper, combining renewable sources, supercapacitors and lithium ion batteries, both components will absorb the energy from the regenerative braking of the tramway. The system has been modeled in Matlab considering certain restrictions in each component in order to supply the load on the round trip. Finally, a techno-economic and environmental analysis has been done identifying new patterns with respect to existing tramway systems. The annual energy required by the tramway is 867.62 MWh/year. The power variations are mainly supplied by the supercapacitor and the lithium ion battery functions as a backup. In this regard, the proposed system saves $ 2205,724 by supplying energy to the tramway and selling the excess energy to the grid for 20 years. Finally, the renewable system will have avoided 8,445.4 tCO2/MWh.", "title": "Sustainable tramway, techno-economic analysis and environmental effects in an urban public transport. A comparative study", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Xin  Wang", "Inge  Norstad", "Kjetil  Fagerholt", "Marielle  Christiansen"], "summary": "In this chapter we examine, from a tramp ship operator\u2019s point of view, how potential CO2 emission reduction measures impact the operational decisions and their economic and environmental consequences. Two market-based measures (MBMs) are discussed, the bunker levy scheme and the emission trading scheme, and we show that both can be incorporated in a similar way into a typical tramp ship routing and scheduling model. We also demonstrate with a computational study the environmental benefits of these CO2 reduction schemes.", "title": "Green Tramp Shipping Routing and Scheduling: Effects of Market-Based Measures on CO2 Reduction", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Minje Choia", "Gayoung Kanga", "Juhyeon Kwaka", "Yoonjung Jangb", "S. Leec"], "summary": "The Net-Zero standard was proposed by Science Based Target (SBTi) to bring the net greenhouse gas emissions to zero as global warming intensifies, and this policy is spreading worldwide. Public transportation plays an important role in eco-friendly transportation and establishing a railway-oriented public transportation system is important. Among modes of railway traffic, trams are easy to access compared with subways (a representative modes of railway transportation) and are economical because of their low construction and operation costs. If a priority signal is given to the tram operation, the scheduled speed increases; The efficiency can be further improved. The purpose of this study was to analyse how the conversion of modes to public transportation caused by tram construction can affect the atmosphere and to study how much the increase in physical activity caused by the increase in public transportation affects the reduction of disease. Dongtan New Town in Korea, where trams are scheduled to be introduced, was set as the study area, and the effect of the conversion of modes of transportation resulting from tram construction was analysed through the modal split process of the four-stage transportation demand prediction model. The analysis shows that trams will generate a 54,700 trips/d conversion to public transportation within the affected area. The benefit from air pollution reduction is 25.13", "title": "Calculating the Environmental Benefits of Trams", "claims": null}, {"metadata": {"year": 1998}, "authors": ["C. Pronello"], "summary": "The paper presents the results of a simulation model to calculate the tramsport pollutant emissions in different utirban sites", "title": "A predictive model of atmospheric pollution with environmental capacity constraint: an application in the city of Turin", "claims": null}, {"metadata": {"year": 1999}, "authors": ["R. Soberman", "E. Miller"], "summary": "Certain concerns about sustainable transportation derive from the premise that significant atmospheric concentrations of carbon dioxide (CO2) and other greenhouse gases which contribute to global warming can be attributed to automotive emissions. Among measures considered to curtail automobile dependency and increase transit modal split, \"full cost pricing\" \u2192 a means of extracting both environmental costs and hidden subsidies which road users allegedly impose on society \u2192 has emerged as an increasingly popular suggestion. This paper examines the potential impact of full cost pricing on the \"sustainability\" of urban transportation, using Toronto as a case study. The analysis deals with the direct effects on mode choice and vehicle utilization resulting from increased user costs as might be achieved through fuel taxes or road pricing, as well as the indirect, longer term effects on location decisions. To investigate the land-use impacts, population distributions were altered to reflect more compact developm...", "title": "Impacts of full cost pricing on the sustainability of urban transportation: towards Canada's Kyoto commitments", "claims": null}, {"metadata": {"year": 2020}, "authors": ["J. Niemann", "Julian Bruckmann", "Florian Krautzer"], "summary": "The population is growing constantly in urban areas. This results in an increasing demand for mobility solutions while it is also worldwide aimed to reduce greenhouse gas emissions. This paper summarizes the results of a comparative study concerning the greenhouse gas emissions (based on carbon footprint) caused by alternative urban passenger transportation systems. The emissions for the vehicles and their infrastructure are analyszed over the entire life cycle from manufacturing up to their end of life. An existing cable car sytem in La Paz, Bolivia was analysed and evaluated in comparison to other modes of transportation such as small busses, large busses and a tram. According to the system definitions and the considered balance framework the study shows that beside the use of the systems the materials and the manufacturing as well as the infrastructure have a significant impact on the total emissions over the life cycle. To put focus on the \u201etrue and real\u201c impacts to the society it is preferable to consider the measurement results in total absolute emissions.", "title": "Green City Deals: A study on the global warming potential of alternative urban transportation systems", "claims": null}, {"metadata": {"year": 2009}, "authors": ["T. Litman"], "summary": "This paper describes climate change impacts and costs, presents methods for quantifying and monetizing (measuring in monetary units) these impacts, summarizes published unit cost estimates, and explains the values used in the report, Transportation Cost and Benefit Analysis. Climate change emission valuation depends on many factors including the range of impacts considered, the methods used to quantify impacts, and emission reduction targets. Recent studies predict that damage costs are potentially very high if atmospheric greenhouse gas levels exceed critical thresholds, while emission control costs are $20-50 per tonne of carbon dioxide equivalent (CO2e). Some transportation emission reduction strategies have relatively low costs when co-benefits such as consumer savings, congestion reductions and safety are considered. Climate Change Emission Valuation for Transportation Economic Analysis Victoria Transport Policy Institute", "title": "Climate change emission valuation for transportation economic analysis", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Baek-Jae  Kim", "Kwang-Eui  Yoo"], "summary": "The aviation sector has developed dramatically in recent decades. However, environmental and social concerns are gradually posing limitations on the growth of the air transport industry. Air travel contributes to climate change, and causes significant environmental damage. It is therefore subject to environmental impact charges, which will intensify over time. These environmental charges will influence air travel. This research aims to investigate the impact of environment costs by examining their influence on air passenger demand within the context of two major airline network models: the hub-and-spoke network and the point-to-point network. The additional costs, caused by environmental rehabilitation, will affect both the networks and profit structures of airlines, and these structures will need to be amended in time. This study may contribute to the South Korean aviation industry, especially to the airlines that will be affected by the CO2 constraints imposed by the European Union Emissions Trading System from 2012 onwards.", "title": "The Implication of Environmental Costs on Air Passenger Demand for Airline Networks", "claims": null}], "query": "tramsport fare prices environmental impacts climate change", "summary_abstract": "The collection of papers provides a comprehensive view of the environmental impacts of transport fare prices and their relation to climate change. Cano et al. (2021) highlight the potential of integrating renewable energy sources into tramway systems, which can significantly reduce CO2 emissions by avoiding 8,445.4 tCO2/MWh over 20 years. This approach not only offers environmental benefits but also economic savings by selling excess energy back to the grid. Choi et al. (2022) emphasize the role of trams in promoting eco-friendly public transportation, noting that tram construction can lead to a significant shift from private to public transport, thereby reducing air pollution. Soberman and Miller (1999) discuss the concept of \"full cost pricing\" in urban transportation, which includes environmental costs in fare pricing, potentially reducing automobile dependency and encouraging more sustainable transit options. Niemann et al. (2020) provide a comparative analysis of urban transportation systems, indicating that trams, along with other public transport modes, have a lower carbon footprint over their life cycle compared to private vehicles. These studies collectively suggest that incorporating environmental costs into transport fare pricing and investing in sustainable public transport systems like trams can significantly mitigate climate change impacts.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2006}, "authors": ["L. Troelsen", "S. Jacobsen"], "summary": "Rheumatoid arthritis is associated with increased cardiovascular morbidity and mortality due to atherosclerosis. This cannot be explained by an increased presence of traditional risk factors but seems to depend on inflammatory mechanisms. The association of inflammatory pathways with atherosclerosis is complex, and more research is required to optimise preventative measures against cardiovascular complications in inflammatory rheumatic diseases.", "title": "[Chronic inflammation increases the risk of cardiovascular disease in patients with rheumatoid arthritis].", "claims": null}, {"metadata": {"year": 2014}, "authors": ["E. Choy", "K. Ganeshalingam", "A. Semb", "Z. Szekanecz", "M. Nurmohamed"], "summary": "Risk of cardiovascular (CV) disease is increased among RA patients. High inflammatory burden associated with RA appears to be a key driver of the increased cardiovascular risk. Inflammation is linked with accelerated atherosclerosis and associated with a paradoxical inversion of the relationship between CV risk and lipid levels in patients with untreated RA, recently coined the lipid paradox. Furthermore, the inflammatory burden is also associated with qualitative as well as quantitative changes in lipoproteins, with the anti-inflammatory and atheroprotective roles associated with high-density lipoprotein cholesterol significantly altered. RA therapies can increase lipid levels, which may reflect the normalization of lipids due to their inflammatory-dampening effects. However, these confounding influences of inflammation and RA therapies on lipid profiles pose challenges for assessing CV risk in RA patients and interpretation of traditional CV risk scores. In this review we examine the relationship between the increased inflammatory burden in RA and CV risk, exploring how inflammation influences lipid profiles, the impact of RA therapies and strategies for identifying and monitoring CV risk in RA patients aimed at improving CV outcomes.", "title": "Cardiovascular risk in rheumatoid arthritis: recent advances in the understanding of the pivotal role of inflammation, risk predictors and the impact of treatment", "claims": null}, {"metadata": {"year": 2004}, "authors": ["Lexin  Wang", "Guoqin  Feng"], "summary": "Patients with rheumatoid arthritis (RA) have an increased prevalence of coronary heart disease and a high cardiovascular mortality rate. The causes of increased coronary heart disease in RA patients are poorly understood. Conventional cardiovascular risk factors, such as inactivity, overweight or dyslipidemia may play a role, but they do not seem to be wholly responsible for the increased cardiovascular risk. RA is associated with a high incidence of inflammation and vascular endothelial injuries. Endothelial dysfunction is one of the key steps in the pathogenesis of atherosclerosis in non-RA patients. Therefore, we hypothesized that inflammation-induced vascular endothelial injuries may be responsible for the increased risk of coronary heart disease and high rates of cardiovascular mortality in patients with RA.", "title": "Rheumatoid arthritis increases the risk of coronary heart disease via vascular endothelial injuries.", "claims": null}, {"metadata": {"year": 2010}, "authors": ["J. Rovensk\u00fd", "M. Vlcek", "R. Imrich"], "summary": "Risk of cardiovascular diseases is significantly higher in patients with rheumatoid arthritis (RA) than in normal population, leading to higher mortality of these patients. An accelerated atherosclerosis has been considered a basis for the increased cardiovascular risk in RA. Besides classical atherosclerosis risk factors, systemic inflammation plays a substantial role. Indirect mechanisms such as insulin resistance and dyslipidemia may play a role, however, inflammation probably causes direct damage to blood vessels. Thus, systemic inflammation has a primary role and other factors accelerate this process. An adequate anti-inflammatory therapy can have a positive effect also on cardiovascular diseases in RA.", "title": "[Cardiovascular diseases in rheumatoid arthritis].", "claims": null}, {"metadata": {"year": 2013}, "authors": ["I. V. D. van den Oever", "A. V. van Sijl", "M. Nurmohamed"], "summary": "The risk of cardiovascular morbidity and mortality is increased in rheumatoid arthritis. The classical cardiovascular risk factors, including smoking, hypertension, dyslipidaemia, insulin resistance and diabetes mellitus, obesity and physical inactivity do not appear to explain the excess cardiovascular risk in rheumatoid arthritis, although they do contribute, albeit in a different way or to a lesser extent, to rheumatoid arthritis in comparison with the general population. A very important link between rheumatoid arthritis and cardiovascular disease is inflammation as it plays a key role in all stages of atherosclerosis: from endothelial dysfunction to plaque rupture and thrombosis. It also has an influence on and accentuates some traditional cardiovascular risk factors, such as dyslipidaemia, obesity and insulin resistance. To date, the exact pathophysiologic mechanism by which this relation between cardiovascular disease and rheumatoid arthritis can be explained is not completely clear. Cardiovascular risk management in rheumatoid arthritis is mandatory. Unfortunately, the way this should be done remains a point of discussion. In this review issues regarding cardiovascular risk in rheumatoid arthritis and its management will be addressed, according to evidence presented in the latest studies and our own experience-based opinion.", "title": "Management of cardiovascular risk in patients with rheumatoid arthritis: evidence and expert opinion", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Sarah  Skeoch", "Ian N. Bruce"], "summary": "Rheumatoid arthritis (RA) has long been associated with increased cardiovascular risk, but despite substantial improvements in disease management, mortality remains high. Atherosclerosis is more prevalent in RA than in the general population, and atherosclerotic lesions progress at a faster rate and might be more prone to rupture, causing clinical events. Cells and cytokines implicated in RA pathogenesis are also involved in the development and progression of atherosclerosis, which is generally recognized as an inflammatory condition. The two diseases also share genetic and environmental risk factors, which suggests that patients who develop RA might also be predisposed to developing cardiovascular disease. In RA, inflammation and atherosclerosis are closely linked. Inflammation mediates its effects on atherosclerosis both through modulation of traditional risk factors and by directly affecting the vessel wall. Treatments such as TNF inhibitors might have a beneficial effect on cardiovascular risk. However, whether this benefit is attributable to effective control of inflammation or whether targeting specific cytokines, implicated in atherosclerosis, provides additional risk reduction is unclear. Further knowledge of the predictors of cardiovascular risk, the effects of early control of inflammation and of drug-specific effects are likely to improve the recognition and management of cardiovascular risk in patients with RA.", "title": "Atherosclerosis in rheumatoid arthritis: is it all about inflammation?", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Michael T Nurmohamed"], "summary": "The increased mortality in rheumatoid arthritis (RA) is mainly due to (atherosclerotic) cardiovascular disease. The cardiovascular morbidity is also increased in comparison with the general population. This increased cardiovascular burden could be caused by 1) an enhanced prevalence of cardiovascular risk factors 2) under treatment of cardiovascular risk factors or 3) RA itself, particularly due to its chronic inflammatory component. Cardiovascular risk factors only partially explain the increased cardiovascular risk and it is becoming increasingly acknowledged that the underlying inflammation in RA plays an essential role. This is probably related to the fact that atherosclerosis also has an inflammatory etiology that is accelerated by RA. Similarly, it can be expected that effective suppression of this inflammatory process by disease modifying antirheumatic drugs and/or biologicals lowers the cardiovascular risk. Altogether, there is accumulating evidence that the increased cardiovascular risk in RA is comparable to that of type 2 diabetes and actually RA should be seen as a new, independent, cardiovascular risk factor for which cardiovascular risk management is essential.", "title": "Cardiovascular risk in rheumatoid arthritis.", "claims": null}, {"metadata": {"year": 2010}, "authors": ["G. Kitas", "S. Gabriel"], "summary": "Rheumatoid arthritis is associated with an increased risk for cardiovascular events, such as myocardial infarction and stroke. Epidemiological evidence suggests that classic cardiovascular risk factors, such as hypertension, dyslipidaemia, insulin resistance and body composition alterations are important but not sufficient to explain all of the excess risk. High-grade systemic inflammation and its interplay with classic risk factors may also contribute. Some associations between classic risk factors and cardiovascular risk in people with rheumatoid arthritis appear counterintuitive but may be explained on the basis of biological alterations. More research is necessary to uncover the exact mechanisms responsible for this phenomenon, develop accurate systems used to identify patients at high risk, design and assess prevention strategies specific to this population of patients.", "title": "Cardiovascular disease in rheumatoid arthritis: state of the art and future perspectives", "claims": null}], "query": "How does inflammation increase the risk of cardiovascular disease in rheumatoid arthritis?", "summary_abstract": "The collection of papers collectively highlights the significant role of inflammation in increasing the risk of cardiovascular disease (CVD) in patients with rheumatoid arthritis (RA). Inflammation is identified as a key driver of accelerated atherosclerosis, a major contributor to cardiovascular morbidity and mortality in RA patients (Troelsen & Jacobsen, 2006; Choy et al., 2014; Rovensk\u00fd et al., 2010). The inflammatory burden in RA is linked to endothelial dysfunction, a critical step in atherosclerosis development, and is thought to cause direct damage to blood vessels (Wang & Feng, 2004; van den Oever et al., 2013).\n\nInflammation in RA also affects lipid profiles, leading to the \"lipid paradox,\" where traditional lipid levels do not correlate with cardiovascular risk as expected (Choy et al., 2014). This paradox, along with inflammation-induced changes in lipoproteins, complicates the assessment of cardiovascular risk using traditional metrics (Choy et al., 2014; Kitas & Gabriel, 2010).\n\nFurthermore, inflammation in RA interacts with traditional cardiovascular risk factors, such as dyslipidemia and insulin resistance, exacerbating their effects (van den Oever et al., 2013; Nurmohamed, 2009). The chronic inflammatory state in RA is increasingly recognized as an independent risk factor for cardiovascular disease, comparable to type 2 diabetes (Nurmohamed, 2009).\n\nEffective management of inflammation through RA therapies, such as TNF inhibitors, may reduce cardiovascular risk, although the specific mechanisms and benefits require further investigation (Skeoch & Bruce, 2015). Overall, the papers underscore the necessity of integrating cardiovascular risk management into RA treatment plans to mitigate the heightened risk of cardiovascular events in this patient population (Nurmohamed, 2009; Kitas & Gabriel, 2010).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2001}, "authors": ["Deborah G. . Ancona", "P. Goodman", "B. Lawrence", "M. Tushman"], "summary": "The article discusses the issue of time as it pertains to organizational research. The author believes that looking at research in terms of time is a powerful tool in assessing organizational phenomena. According to the author, temporal research allows researchers to gain more perspective when looking at organizational issues such as decision making, group performance and organizational transformation. The author notes that the field of temporal research is translated into concepts including pacing, timing and sequencing.", "title": "Time: A New Research Lens", "claims": null}, {"metadata": {"year": 1973}, "authors": ["R. Ebert", "D. Piehl"], "summary": "This article provides an operational measure of the time frame within which a decision-maker is operating and empirical data indicating the importance of the time frame to the accomplishment of an organization9s goals. It presents an initial analysis of the organizational climate that may encourage a long or a short time horizon on the part of decision-makers.", "title": "Time Horizon: A Concept for Management", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Andrew Brodsky", "Sanford E. DeVoe"], "summary": "From the time we get up to the time we go to bed, there is no part of the day that does not have a temporal facet. One of the core aspects of temporal research in organizational contexts has been t...", "title": "Management Research on Time: Exploring Temporal Aspects of Work and Organizations", "claims": null}, {"metadata": {"year": 2019}, "authors": ["A. Basu", "M. Maciejewski"], "summary": "When designing a comparative outcomes or a cost-effectiveness analysis, the time horizon defining the duration of time for outcomes assessment must be carefully considered. The time horizon must be long enough to capture the intended and unintended benefits and harms of the intervention(s).1,2 In some instances, the time horizon should extend beyond the duration of a clinical trial when a specific end point is measured, whereas in other instances modeling outcomes over a longer period is unnecessary. Using a longer time horizon than is necessary may add unnecessary cost and complexity to the cost-effectiveness analysis model. In the May 2017 issue of JAMA Ophthalmology, Wittenborn et al3 examined costs and effectiveness of home-based macular degeneration monitoring systems using a lifetime horizon in a cost-effectiveness analysis and a 10-year horizon in a budget impact analysis. The rationale for selection of time horizons and their implications for interpreting the research is reviewed in this JAMA Guide to Statistics and Methods article.", "title": "Choosing a Time Horizon in Cost and Cost-effectiveness Analyses.", "claims": null}, {"metadata": {"year": 1988}, "authors": ["Allen C. Bluedorn", "R. Denhardt"], "summary": "The concept of time is introduced as a major topic for organizational and management research. Including a discussion of differing times and temporalities, macro level research and theory are described that relate time to such substantive areas as organizational culture, strategic planning, and organizational contingency theory. At the micro level, theory and research on time and individual differences, decision making, motivation, and group behavior are reviewed critically. Organizational and management topics of particular salience forfuture temporal research and management practice are identified.", "title": "Time and Organizations", "claims": null}, {"metadata": {"year": 1999}, "authors": ["Heejin Lee", "J. Liebenau"], "summary": "While there is much `time-related research', there is little `research on time'. This is striking since time is a key point in understanding organizations, their actions, culture, efficacy, etc. Most studies of time in management and organizational theory take time for granted. While there are numerous studies that address temporal issues, they are widely dispersed and unsystematic. This paper provides a classification of temporal studies of organizations and management. The scheme is built around two criteria: concepts of time and the role of time in research design. In the former, there are two contrasting concepts of time: clock time and social time. In the latter, time plays the roles of independent or dependent variables. By intersecting the two criteria, four notions of temporality (`deciding', `working', `varying' and `changing' times) are introduced to account for a variety of studies of time. The resulting classification not only reveals the current situation of studies about time, but it also indicates a direction which further research effort should take. We conclude by showing that temporally sensitive approaches will benefit research on organizations.", "title": "Time in Organizational Studies: Towards a New Research Direction", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Pierre-Luc Brunelle"], "summary": "This review depicts the theoretical perspectives on time and their implications for organizational research. It takes the shape of a critical review of time conceptualizations across the social and behavioral sciences. To describe time as duration and to decipher the role of the past, present, and future in organizational life, one must put time as the focal point of its investigation. First we reconsolidate the most widely used conceptual perspectives of time that have been considered in organization studies. Secondly we address time in relation to the research design and approaches in organizational research. As a synthesis, this paper put forth the implications that derive for theory, research and practice. This review logically provides a summary of how time has been, and can be, considered when dealing with organizational issues. Lastly, taking the form of a discussion, we bring forth how time and temporality across organizations may be interpreted, studied and used to deepen our knowledge of organizational research topics.", "title": "Time in Organization Studies: An Overview of the Literature on the State of Theory, Research and Practice", "claims": null}, {"metadata": {"year": 1982}, "authors": ["Clark E. Chastain"], "summary": "AN appropriate time span reflects a balance between near\u2010term and long\u2010term factors. Strong pressures are placed on organizations in our competitive society for quick results and, usually, near\u2010term results are overemphasized.", "title": "An appropriate time horizon: Now vs. then", "claims": null}], "query": "In project management research, time horizon is to the duration of a study's data collection period or the length of time over which data is analyzed (Wong et al., 2021). Different time horizons can be employed when studying employee engagement and organizational performance.", "summary_abstract": "The collection of abstracts provides insights into the role of time horizons in project management research, particularly concerning employee engagement and organizational performance. Ancona et al. (2001) emphasize the importance of temporal research in understanding organizational phenomena, suggesting that different time horizons can offer varied perspectives on issues like decision-making and group performance. Ebert and Piehl (1973) highlight the significance of the time frame in achieving organizational goals, indicating that the organizational climate can influence whether a long or short time horizon is adopted by decision-makers. Basu and Maciejewski (2019) discuss the necessity of selecting an appropriate time horizon in cost-effectiveness analyses, noting that it should be long enough to capture all relevant outcomes without adding unnecessary complexity. Bluedorn and Denhardt (1988) explore how time relates to organizational culture and strategic planning, suggesting that different temporalities can impact decision-making and motivation. Lee and Liebenau (1999) classify temporal studies in organizations, introducing concepts like clock time and social time, which can influence research design and outcomes. Brunelle (2017) provides a critical review of time conceptualizations, emphasizing the need to consider time as a focal point in organizational research to better understand past, present, and future dynamics. Lastly, Chastain (1982) discusses the balance between near-term and long-term factors, noting the pressures for quick results in competitive environments. Collectively, these papers underscore the critical role of time horizons in shaping research outcomes and organizational strategies, highlighting the need for careful consideration of temporal factors in project management research.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2010}, "authors": ["B. Koletzko", "S. Baker", "G. Cleghorn", "U. F. Neto", "S. Gopalan", "O. Hernell", "Q. S. Hock", "P. Jirapinyo", "B. Lonnerdal", "P. Pencharz", "Hildegard Pzyrembel", "J. Ram\u00edrez-mayans", "R. Shamir", "D. Turck", "Y. Yamashiro", "Ding Zongyi"], "summary": "The Codex Alimentarius Commission of the Food and Agriculture Organization of the United Nations (FAO) and the World Health Organization (WHO) develops food standards, guidelines and related texts for protecting consumer health and ensuring fair trade practices globally. The major part of the world's population lives in more than 160 countries that are members of the Codex Alimentarius. The Codex Standard on Infant Formula was adopted in 1981 based on scientific knowledge available in the 1970s and is currently being revised. As part of this process, the Codex Committee on Nutrition and Foods for Special Dietary Uses asked the ESPGHAN Committee on Nutrition to initiate a consultation process with the international scientific community to provide a proposal on nutrient levels in infant formulae, based on scientific analysis and taking into account existing scientific reports on the subject. ESPGHAN accepted the request and, in collaboration with its sister societies in the Federation of International Societies on Pediatric Gastroenterology, Hepatology and Nutrition, invited highly qualified experts in the area of infant nutrition to form an International Expert Group (IEG) to review the issues raised. The group arrived at recommendations on the compositional requirements for a global infant formula standard which are reported here.", "title": "Global standard for the composition of infant formula: recommendations of an ESPGHAN coordinated international expert group.", "claims": null}, {"metadata": {"year": 2010}, "authors": ["S. Zlotkin", "J. Siekmann", "A. Lartey", "Zhenyu Yang"], "summary": "The Codex Alimentarius is a collection of internationally recognized standards, codes of practice, guidelines, and other recommendations relating to foods, food production, and food safety. Among other functions, it is responsible for setting international standards for safety and hygiene. Codex food standards and guidelines directed at foods produced primarily for young infants and children have important implications for maintaining nutritional status and health, especially given the positioning of these products as components of established World Health Organization (WHO)/UNICEF-recommended feeding strategies. Recently, new products targeted at this age group (e.g., lipid-based nutrient supplements and micronutrient powders) have been produced and used, but these are not totally covered under existing Codex guidelines or standards. The objective of this paper is to review the role of the Codex process and specifically to suggest revisions to existing Codex guidelines on formulated complementary foods (Guidelines for Formulated Supplementary Foods for Older Infants and Young Children, CAC/GL 08\u20131991) to encompass this new category of fortified complementary foods and home fortificants. In reviewing the existing guidelines, potential areas for revision included the sections on the recommended nutrients in these foods and their intended use. Updating the Codex guidelines provides the opportunity to encourage production and use of new products for children and help ensure that such foods, when used as directed, do not interfere with breastfeeding. The revised guidelines would help governments develop national regulations covering all forms of formulated complementary foods. They would also lessen impediments to international trade by providing clear guidance for foods used in feeding programs and for young children, particularly in developing countries.", "title": "The Role of the Codex Alimentarius Process in Support of New Products to Enhance the Nutritional Health of Infants and Young Children", "claims": null}, {"metadata": {"year": 2011}, "authors": ["R. Dossa", "E. Ahouandjinou", "F. Houngbe"], "summary": "Infant feeding practices do not always fit with quantity and quality requirements,\u00a0leading to low expression of growth potential. In Benin, 43.1% of children less than6\u00a0months old are exclusively breastfed with 68% of children aged 6-8 months receiving\u00a0complementary food. The study aimed to produce infant flour from raw food\u00a0ingredients available in Bopa district and to test its acceptability by 6-12 months old children. In a first step of the study, formulation and determination of nutritional\u00a0characteristics of the infant flour occurred. A second step concerned acceptability\u00a0tests of gruel made from formulated infant flour. The study sample was composed of\u00a0sixty five mothers and their children. Children\ufffds acceptability test took place in the\u00a0morning for three consecutive days. The gruel was consumed ad libitum. Mothers\ufffd acceptability test consisted of appreciation of organoleptic characteristics of the gruel\u00a0and the infant flour processing.The infant flour was made of maize (65 %), bean (20\u00a0%) and peanut (15 %) and was manually processed. Chemical analysesshowed that it\u00a0contains 4.3% of moisture, 69.3% of carbohydrates, 15.1% of proteins, 10.7% of\u00a0lipids, less than 5% of crude fibres and 1.9% of ash. Its energy density (433.9 kcal/100g) was significantly greater than Codex Alimentarius standards (p<0.05).The\u00a0infant flour contained microbial germs up to 4.8log CFU/g which was closed to\u00a0maximum standard values. Total coliforms (1.7log CFU/g) were significantly lower\u00a0than standard values. The flour was yeast, mould and pathogen (Escherichia coli)\u00a0free. Hundred grams gruel was made from 40g of infant flour, 6g of malted maize and\u00a0250ml of stock of boiled greens leaves (Solanummacrocarpum). Dry matter content of\u00a0gruel was 19.2% and its energy density was 81.5 kcal/100g. Basedon the ratio of\u00a0intake and amount served, 83.3% of children accepted the gruel. However based on\u00a0the ratio of the amount of porridge consumed during the testto the amount usually\u00a0consumed by the children, 65.2% of the childrenaccepted the gruel. Mothers\ufffd appreciation of the gruel ranged from unpleasant to very pleasant with 40% as\u00a0pleasant. Sixty percent of mothers judged the infant flour processing as easy and\u00a0feasible. All mothers expressed their desire to feed their children with the gruel.\u00a0Improving nutritional status of their children motivated their decision. It is concluded\u00a0that integrating this infant flour in nutrition and counselling package targeted to\u00a0mothers may be of a great benefit to the children.", "title": "Evaluation of the suitability and acceptability of a newly designed infant flour for infant feeding in the district of Bopa in south of Benin", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Loba Sonia Euphrasie", "Gbakayoro Jean Brice", "Kouame Kouassi Appolinaire", "Grodji Gbogouri Albarin", "Brou Kouakou"], "summary": "The quality of infant flours used during the infant weaning period is of great importance. The purpose of this study was to formulate infantile flours to cover the nutritional needs of children during weaning to fight against child malnutrition. Two types of flours were formulated, one made from rice enriched with soy, palm oil and sugar and the other made from maize enriched with beans, palm oil and sugar. Amounts and ingredients were combined as recommended. The physico-chemical, microbiological and organoleptic characteristics of these flours were determined according to standard methods. The results obtained showed a satisfaction rate of over 100% for energy and protein compared to nutritional standards. The lipid levels obtained (12.40 \u00b1 0.99 and 9.65 \u00b1 0.01) respectively for the flours FRSHS and FMHHS were in the range of the recommended values and the fiber levels of the flours were less than 5 g, as recommended. The iron contents (11.19 \u00b1 0.12 and 14.27 \u00b1 0.07 mg / 100 g dry matter) and zinc (4.91 \u00b1 0.05 and 6.29 \u00b1 0.35 mg / 100 g of dry matter) flours were largely above the recommendation. The microbiological loads detected in these flours were below the microbiological criteria applicable to infant flours. All the boils prepared from the compound flours were appreciated. The acceptability of the FRSHS boil compared to that of the FMHHS boil by mothers for their children is justified by the color, aroma and taste of the FRSHS boils most appreciated by the mothers. These formulations could then be recommended for infants, thus contributing to the fight against child malnutrition. La qualite des farines infantiles utilisees pendant la periode de sevrage du nourrisson est d\u2019une grande importance. L\u2019objectif de cette etude est de mettre au point deux formulations de farine infantile dont l\u2019une fabriquee a partir de riz enrichi en soja, en huile de palme et en sucre (FRSHS) et l\u2019autre (FMHHS) a partir de mais enrichi en haricot, en huile de palme et en sucre. Pour ce fait, les doses des ingredients pour la formulation des deux farines (FRSHS et FMHHS) ont ete combinees selon lesrecommandations nutritionnelles.Lescaracteristiques physico-chimiques, microbiologiques et organoleptiques de ces farines ont ete determinees selon les methodes standards. Les resultats revelent un taux de satisfaction de plus de 100% pour l\u2019energie et les proteines compare aux normes nutritionnelles. Les teneurs respectives en lipides (12,40 \u00b1 0,99 % et 9,65 \u00b1 0,01%) pour les farines (FRSHS et FMHHS) sont comprises dans l\u2019intervalle des valeurs recommandees et les taux de fibres des farines sont inferieurs a 5 g, tel que recommande. Les teneurs en fer (11,19 \u00b1 0,12 et 14,27\u00b1 0,07 mg /100 g de matieres seche), de zinc (4,91\u00b1 0,05et 6,29 \u00b1 0,35 mg /100 g de matieres seche) et de calcium (358,194 \u00b1 0,09 et 343,87\u00b1 0,44) des farines sont au-dela de la recommandation. Aussi, les farinespresentent-elles de bonnes teneurs en la plupart des mineraux. Les charges microbiologiques detectees dans ces farines sont inferieures aux criteres microbiologiques applicables aux farines infantiles. Les bouillies preparees a partir des farines composees ont ete appreciees par les meres panelistes. Toutefois, la bouillie preparee a partir de la farine FRSHS a ete plus acceptee. L\u2019acceptabilite de la bouillie de FRSHS par rapport a celle de FMHHS par les meres pour leurs enfants se justifie par la couleur, l\u2019arome et le gout de la bouillie de farine FRSHS. Ces farines composees pourraient alors etre recommandees pour les nourrissons, contribuant ainsi a lutter contre la malnutrition infantile.", "title": "Formulations de Farines Compos\u00e9es Dont l\u2019une \u00e0 Base de Riz (Oryza Sativa) et L\u2019autre \u00e0 Base de Ma\u00efs (Zea Mays) Pour Enfants en \u00e2ge de Sevrage", "claims": null}, {"metadata": {"year": 2019}, "authors": ["B. Maxwell", "N. K. Valery", "Combo Agnan Marie-Michel", "Yao N\u2019zu\u00e9 Binjamin", "G. Dago"], "summary": "Aim: This study aimed to evaluate the nutritional quality of the infant flours offered to mothers received in the dietary service of the CHR of Daloa. \nIntroduction: Ivory Coast\u2019s membership in Scaling up Nutrition (SUN) is a momentum in a collective effort to improve the nutrition and nutritional status of the population.\u00a0 \nMethod\u00a0: For this purpose, analyses of biochemical compositions, in particular the levels of protein, fat and minerals in the proposed infant flours, were carried out.\u00a0 \nResults: The formulations of the flours proposed have high nutritional values. The protein content of compound flours increases proportionally with the amount of soy incorporated. Indeed, for FC2 and FC3 formulations, these contents are 17.12 \u00b1 0.19 g / 100 g (FC3) and 17.50 \u00b1 0.56 g / 100 g (FC2) with a rate of incorporation of 25% soy. In addition, the FC1 flour formulation enriched with peanuts is low in protein with a value of 8.69 \u00b1 0.11 g / 100 g. These flours also had mineral contents in accordance with WHO standards of calcium (> 125 mg / kg), iron (> 4 mg / kg) and zinc (> 0.8 mg / kg). In addition these formulations are highly digestible. \nConclusion: However, to use the proposed meal formulations as food for malnutrition, it would necessarily be necessary to supplement them with available local fruits and vegetables, rich in vitamins and minerals.", "title": "Nutritional Quality of Food Supplements for Children from 6 to 59 Months Proposed to the Dietary Service of Regional Hospital of Daloa (Ivory Coast)", "claims": null}, {"metadata": {"year": 2003}, "authors": ["Joint Fao"], "summary": "This publication contains guidance on the development and application of international food hygiene standards, which covers practices from primary production through to final consumption, highlighting key hygiene controls at each stage. It also contains guidance on the use and application of the Hazard Analysis and Critical Control Point (HACCP) system to promote food safety, as well as principles for the establishment and application of microbiological criteria for foods and the conduct of microbiological assessment.", "title": "Codex alimentarius: food hygiene basic texts.", "claims": null}, {"metadata": {"year": 2008}, "authors": ["\u738b\u65ed\u658c"], "summary": "The invention relates to nutrition powder and a preparation method thereof, in particular to infant nutrition corn flour and a preparation method thereof. The main points of the nutrition corn flour are as follows: each 100g of the nutrition corn flour contains 300mg to 600mg of calcium, 6.0mg to 10mg of iron, 2.5mg to 7.0mg of zinc, 300mg to 500mg of sodium, 20mu g to 50mu g of iodine, 10mg to 15mg of DHA, 1000IU to 4000IU of vitamin A, 200IU TO 600IU of vitamin D3, 200mu g to 260mu g of vitamin B1, and 200mu g to 260mu g of vitamin B2. The main points of the preparation method are as follows: sweet corn is taken as the main raw material; dry material prepared with the sweet corn as the main raw material is ground; an appropriate amount of calcium, iron, zinc, sodium, iodine, DHA, vitamin A, vitamin D3, vitamin B1 and vitamin B2 are added into the ground material to receive strengthened nutrition mixing. The infant nutrition corn flour is not only auxiliary food which can improve the nutrition structure of infants, but also the auxiliary food which can promote the infants to grow healthily.", "title": "Infantile nutrition maize flour and preparation thereof", "claims": null}, {"metadata": {"year": 2021}, "authors": ["A. Vasyukova", "K. Krivoshonok", "A. Alekseev", "V. I. Karpov"], "summary": "The article provides basic information about the functional properties of flour culinary products developed on the basis of rice and corn flour with additives of powders of vegetable origin: jerusalem artichoke, sea buckthorn, apples, carrots, tomato, paprika, dill, daminaria, and stevia extract was used as a sweetener. Balanced nutritionally adapted nutrition, corresponding to the physiological needs of schoolchildren, gender and individual characteristics of children aged 7\u201311 years, is necessary for the full development of physical and intellectual abilities. In this connection, the development of flour culinary products nutritionally adapted to the peculiarities of baby food, the optimization of the ingredient composition of flour culinary products, taking into account the technological properties of raw materials and biomedical recommendations for the diet of this category of nutritionists will contribute not only to achieving the best results in their development, physical and mental improvement, but will also prevent a number of alimentary-dependent diseases and conditions of this category of the population. This determines the main task of research and is relevant and promising. The purpose of the development of new specialized products is to obtain enriched gluten-free compositions for the production of high-quality pancake tape by an industrial method. The article presents data on the optimization of the ingredient composition of flour culinary products, considering the technological properties of raw materials and biomedical recommendations for the nutrition of schoolchildren of grades 1\u20134. The list of ingredients with immunomodulatory and antioxidant properties and the possibility of their use in the preparation of specialized flour culinary products for school meals is scientifically substantiated. Optimization was carried out using modern programs providing mathematical modeling of product formulations in the realized range of parameter changes. The qualitative and quantitative prescription composition is modeled in accordance with sanitary norms and requirements of the SanPiN 2.3/2.4.3590\u201320 requirements for children's food products, considering their physical exertion. The relationship between the mass fraction of protein components (rice and corn flour and egg white) and physico-chemical indicators of the quality of flour culinary products for the nutrition of schoolchildren has been established. It is experimentally proved that the use of the developed recipe compositions will contribute to an increase in the content of protein, fats, carbohydrates, vitamins, minerals, increase the biological value and the total content of water-soluble antioxidants in flour culinary products for the nutrition of students aged 7\u201311 years.", "title": "Influence of additions on the structure of the free fares", "claims": null}], "query": "What is the codex alimentarius recommendation for second-age infant flour?", "summary_abstract": "The Codex Alimentarius recommendations for second-age infant flour focus on ensuring nutritional adequacy and safety for infants transitioning from exclusive breastfeeding to complementary foods. Koletzko et al. (2010) highlight the ongoing revision of Codex standards to reflect current scientific knowledge, emphasizing the importance of nutrient levels in infant formulae. Zlotkin et al. (2010) discuss the need for updated guidelines to include new categories of fortified complementary foods, ensuring they support nutritional status without interfering with breastfeeding.\n\nDossa et al. (2011) and Euphrasie et al. (2019) provide insights into the formulation of infant flours that meet or exceed Codex standards for energy, protein, and lipid content, while maintaining low fiber levels as recommended. These studies underscore the importance of microbiological safety, with microbial loads in the flours being below Codex criteria. Maxwell et al. (2019) further support these findings, noting that the mineral content of proposed infant flours aligns with WHO standards, although they suggest supplementation with local fruits and vegetables for enhanced vitamin and mineral intake.\n\nOverall, the collective research emphasizes the Codex Alimentarius' role in setting comprehensive guidelines for the nutritional composition and safety of second-age infant flours, ensuring they meet the dietary needs of infants while supporting global health and trade practices.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2012}, "authors": ["H. Son", "H. J. Kim", "C. K. Kim"], "summary": "To investigate the effect of resistance and endurance training on muscle proteome expression, samples of vastus lateralis from 10 physically active young men were analysed by 2-dimensional electrophoresis (2-DE) and matrix-assisted laser desorption ionization time-of-flight mass spectrometry (MALDI-TOF MS). Differential patterns of protein expression were determined after 4 weeks of endurance or resistance exercise training. Following endurance exercise training, carbonic anhydrase III immunoglobulin heavy chain, myosin heavy chain 1, titin, chromosome 12, and fructose-1,6-bisphosphatase 2 were up-regulated while pyruvate kinase 3 isoform, ubiquitin carboxyl-terminal hydrolase, and phosphoglucomutase were down-regulated. After the 4 weeks of resistance exercise training, five proteins, apolipoprotein A-IV precursor, microtubule-actin cross linking factor 1, myosin light chain, growth hormone inducible transmembrane protein, and an unknown protein were up-regulated and pyruvate kinase 3 isoform, human albumin, and enolase 3 were down-regulated. We conclude that endurance and resistance exercise training differently alter the expression of individual muscle proteins, and that the response of muscle protein expression may be associated with specific myofibre adaptations to exercise training. Proteomic studies represent one of the developing techniques of metabolism which may substantially contribute to new insights into muscle and exercise physiology.", "title": "THE EFFECT OF RESISTANCE AND ENDURANCE EXERCISE TRAINING ON MUSCLE PROTEOME EXPRESSION IN HUMAN SKELETAL MUSCLE", "claims": null}, {"metadata": {"year": 2002}, "authors": ["P. L. Kim"], "summary": "Muscle proteim are in a continuous state of recycling. This process involves a balance between synthesis and breakdown. These opposing processes dictate muscle protein gains and losses. Musck hypertrophy occurs when synthesis exceeds breakdown. In order for the accretion of new muscle proteins, a chronic state of net positive muscle protein balance (synthesis> breakdown) is required. Resistance exerdse is a potent stimulus of protein turnover and the combined effects of exercise and feeding have shown to be necessary for net protein anabolism. Resistance training has been reported to increase muscle strength and induce changes in skeletal muscle morphology. These positive strength adaptations include muscle fibre hypertrophy and a shift in fibre type from IJX to IIA. Previous investigations of resistance training-induced changes in muscle protein synthesis and fibre morphology have utilized cross-sectional or longitudinal, bilateral training designs. Thus, the purpose of this study was to investigate the effects of a progressive eight week unilateral leg resistance training program on skeletal muscle morphology, and resting and exercise-stimulated mixed muscle protein fractional synthesis rate (FSR). Eight young men performed two training sessions each week, and each session consisted of four sets of knee extension (KE) and four sets of leg press (LP) at 80% 1 repetition maximum (1 RM). Needle biopsies from the vastus lateralis muscle of the trained (T) leg were taken before and after training and analyzed for fibre composition, cross\u00ad sectional area (CSA), and myosin heavy chain (MHC) content. Muscle protein FSR was determined using a primed constant stable isotope infusion of [C6]-phenylalanine in both", "title": "Resistance Training-Induced Changes in Human Muscle Protein Synthesis and Fibre Morphology", "claims": null}, {"metadata": {"year": 2017}, "authors": ["B. Petriz", "C. Gomes", "J. Almeida", "G. P. de Oliveira", "Filipe M. Ribeiro", "R. Pereira", "O. L. Franco"], "summary": "Skeletal muscle plasticity and its adaptation to exercise is a topic that is widely discussed and investigated due to its primary role in the field of exercise performance and health promotion. Repetitive muscle contraction through exercise stimuli leads to improved cardiovascular output and the regulation of endothelial dysfunction and metabolic disorders such as insulin resistance and obesity. Considerable improvements in proteomic tools and data analysis have broth some new perspectives in the study of the molecular mechanisms underlying skeletal muscle adaptation in response to physical activity. In this sense, this review updates the main relevant studies concerning muscle proteome adaptation to acute and chronic exercise, from aerobic to resistance training, as well as the proteomic profile of natural inbred high running capacity animal models. Also, some promising prospects in the muscle secretome field are presented, in order to better understand the role of physical activity in the release of extracellular microvesicles and myokines activity. Thus, the present review aims to update the fast\u2010growing exercise\u2010proteomic scenario, leading to some new perspectives about the molecular events under skeletal muscle plasticity in response to physical activity. J. Cell. Physiol. 232: 257\u2013269, 2017. \u00a9 2016 Wiley Periodicals, Inc.", "title": "The Effects of Acute and Chronic Exercise on Skeletal Muscle Proteome", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Seung-Lyul Oh", "S. Oh"], "summary": "The aim was to examine resistance exercise-related genes after 8 weeks of resistance training. Thirty-two male Sprague-Dawley rats were divided into four groups: 4 weeks sedentary (4 wks CON, n=8), 8 weeks sedentary (8 wks CON, n=8), 4 weeks exercise training (4 wks REG, n=8), and 8 weeks exercise training (8 wks REG, n=8). The rats were trained to climb a 1-m vertical incline (85-degree), with weights secured to their tails. They climbed 10 times, 3 days per week, for 8 consecutive weeks. Skeletal muscle was taken from the flexor halucis longus after the exercise training. After separating the total RNA, large-scale gene expression was investigated by beadarray (Illumina RatRef-12 Expression BeadChip) analysis, and qPCR was used to inspect the beadarray data and to analyze the RNA quantitatively. The detection p-value for the genes was p\uff1c0.01, the M-value {M=log\u2082(condition)-log\u2082(reference)} was \uff1e1.0, and the DiffScore was \uff1e20. In total, the expression of 30 genes significantly increased 4 weeks after the exercise training, and the expression of six genes decreased. At 8 weeks, the expression of five genes significantly increased and that of 12 decreased. Several genes are potentially involved in resistance exercise and muscle hypertrophy, including 1) regulation of cell growth (IGFBP1, PLA2G2A, OKL38); 2) myogenesis (CSRP3); 3) tissue regeneration and muscle development (MUSTN1, MYBPH); 4) hypertrophy (CYR61, ATF3, NR4A3); and 5) glucose metabolism (G6PC, PCK1). These results may help to explain previously reported physiological changes of the skeletal muscle and suggest new avenues for further investigation.", "title": "Effect of Resistance Training on Skeletal Muscle Gene Expression in Rats", "claims": null}, {"metadata": {"year": 2016}, "authors": ["A. Padr\u00e3o", "R. Ferreira", "F. Amado", "R. Vitorino", "J. Duarte"], "summary": "Exercise training has been recommended as a nonpharmacological strategy for the prevention and attenuation of skeletal muscle atrophy in distinct pathophysiological conditions. Despite the well\u2010established phenotypic alterations, the molecular mechanisms underlying exercise\u2010induced skeletal muscle remodeling are poorly characterized. Proteomics based on mass spectrometry have been successfully applied for the characterization of skeletal muscle proteome, representing a pivotal approach for the wide characterization of the molecular networks that lead to skeletal muscle remodeling. Nevertheless, few studies were performed to characterize the exercise\u2010induced proteome remodeling of skeletal muscle, with only six research papers focused on the cross\u2010talk between exercise and pathophysiological conditions. In order to add new insights on the impact of distinct exercise programs on skeletal muscle proteome, molecular network analysis was performed with bioinformatics tools. This analysis highlighted an exercise\u2010related proteome signature characterized by the up\u2010regulation of the capacity for ATP generation, oxygen delivery, antioxidant capacity and regulation of mitochondrial protein synthesis. Chronic endurance training up\u2010regulates the tricarboxylic acid cycle and oxidative phosphorylation system, whereas the release of calcium ion into cytosol and amino acid metabolism are the biological processes up\u2010regulated by a single bout of exercise. Other issues as exercise intensity, load, mode and regimen as well as muscle type also influence the exercise\u2010induced proteome signature. The comprehensive analysis of the molecular networks modulated by exercise training in health and disease, taking in consideration all these variables, might not only support the therapeutic effect of exercise but also highlight novel targets for the development of enhanced pharmacological strategies.", "title": "Uncovering the exercise\u2010related proteome signature in skeletal muscle", "claims": null}, {"metadata": {"year": 2017}, "authors": ["T. Groennebaek", "K. Vissing"], "summary": "Skeletal muscle metabolic and contractile properties are reliant on muscle mitochondrial and myofibrillar protein turnover. The turnover of these specific protein pools is compromised during disease, aging, and inactivity. Oppositely, exercise can accentuate muscle protein turnover, thereby counteracting decay in muscle function. According to a traditional consensus, endurance exercise is required to drive mitochondrial adaptations, while resistance exercise is required to drive myofibrillar adaptations. However, concurrent practice of traditional endurance exercise and resistance exercise regimens to achieve both types of muscle adaptations is time-consuming, motivationally demanding, and contended to entail practice at intensity levels, that may not comply with clinical settings. It is therefore of principle interest to identify effective, yet feasible, exercise strategies that may positively affect both mitochondrial and myofibrillar protein turnover. Recently, reports indicate that traditional high-load resistance exercise can stimulate muscle mitochondrial biogenesis and mitochondrial respiratory function. Moreover, fatiguing low-load resistance exercise has been shown capable of promoting muscle hypertrophy and expectedly entails greater metabolic stress to potentially enhance mitochondrial adaptations. Consequently, fatiguing low-load resistance exercise regimens may possess the ability to stimulate muscle mitochondrial adaptations without compromising muscle myofibrillar accretion. However, the exact ability of resistance exercise to drive mitochondrial adaptations is debatable, not least due to some methodological challenges. The current review therefore aims to address the evidence on the effects of resistance exercise on skeletal muscle mitochondrial biogenesis, content and function. In prolongation, a perspective is taken on the specific potential of low-load resistance exercise on promoting mitochondrial adaptations.", "title": "Impact of Resistance Training on Skeletal Muscle Mitochondrial Biogenesis, Content, and Function", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Juha P. Ahtiainen"], "summary": "High muscle contraction forces that lead to gains in muscle function, size, and strength characterize resistance exercise training. The purpose of this chapter is to outline the adaptations in myofiber size and metabolism that occur by stimuli of hormones and local growth factors, mechanical and metabolic stress of muscle tissue, and myofibrillar disruptions induced by a resistance exercise bout. The chapter will highlight the network of intracellular pathways (including mTOR signaling) that ultimately lead to increases in gene expression and protein synthesis. Accumulation of acute exercise responses by systematic training over time modulate the muscle proteome that can be observed as changes in skeletal muscle phenotype.", "title": "Physiological and Molecular Adaptations to Strength Training", "claims": null}, {"metadata": {"year": 2006}, "authors": ["Yifan Yang", "B. Jemio\u0142o", "S. Trappe"], "summary": "The purpose of this study was to characterize changes in mRNA expression of select proteolytic markers in human slow-twitch [myosin heavy chain (MHC) I] and fast-twitch (MHC IIa) single skeletal mu...", "title": "Proteolytic mRNA Expression in Response to Acute Resistance Exercise in Human Single Skeletal Muscle Fibers", "claims": null}], "query": "what are the effects of resistance exercise on the skeletal muscle proteome?", "summary_abstract": "The collection of studies provides a comprehensive overview of the effects of resistance exercise on the skeletal muscle proteome, highlighting various molecular and physiological adaptations. Son et al. (2012) demonstrated that resistance exercise leads to specific changes in protein expression, including the up-regulation of proteins such as apolipoprotein A-IV precursor and myosin light chain, while down-regulating others like pyruvate kinase 3 isoform. This suggests a tailored proteomic response to resistance training, potentially linked to myofiber adaptations.\n\nKim (2002) emphasized that resistance exercise stimulates muscle protein turnover, promoting muscle hypertrophy through a net positive protein balance. This process involves shifts in muscle fiber type and morphology, contributing to increased muscle strength and size.\n\nOh et al. (2013) identified several genes associated with resistance exercise, including those involved in cell growth, myogenesis, and hypertrophy, indicating genetic regulation as a key component of muscle adaptation.\n\nPadr\u00e3o et al. (2016) and Groennebaek and Vissing (2017) highlighted the role of resistance exercise in enhancing mitochondrial function and myofibrillar protein turnover, suggesting that resistance training can stimulate both mitochondrial biogenesis and muscle hypertrophy. This dual effect is particularly relevant for counteracting muscle decay in various conditions.\n\nAhtiainen (2018) discussed the intracellular pathways, such as mTOR signaling, that are activated by resistance exercise, leading to increased gene expression and protein synthesis, which collectively modulate the muscle proteome and phenotype.\n\nOverall, these studies collectively illustrate that resistance exercise induces specific proteomic and genetic changes in skeletal muscle, promoting hypertrophy, enhancing mitochondrial function, and altering protein expression patterns, which contribute to improved muscle function and adaptation.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2015}, "authors": ["Luyddy Pires", "J. Melchior", "E. Ferruzzi"], "summary": "Introduction: Discovered in the 90s by Rizzolati and collaborators in the pre-motor cortex of Rhesus monkeys, mirror neurons are a type of neuron that responds both during a practical and directed action as in the observation of this same action being performed by another individual. Neurology says that this system should not be considered as a separate and self-sufficient system, but as an intrinsic mechanism of the various cortical areas, such as inferior frontal gyrus, temporal and parietal lobes and cingulate cortex. Therefore, it allows immediate and unconscious understanding of the observed activity, allowing even its reproduction. In this way, it is postulated that this neuronal type is directly linked to the autism spectrum disorders. Objective: Analyze the autism by the premise of dysfunction of the mirror neuron system. Materials and methods: Systematic review of current literature from the PubMed database. Discussion: The ability of mirroring actions provided by the mirror neuron system (MNS) is closely linked to the mechanisms of learning. This imitation can also be directly related to the development of empathy, an affective response that allows the emotional processing, giving the ability to sentimental approach to other people. Starting from these premises, these properties are directly correlated to the autism spectrum disorders (ASD), which are difficulties in communication and social interaction, restriction in understanding and mechanical and mental reproduction of new expressions or feelings, as well as limitation in the perception of intent with who they relate. In order to prove this correlation between the MNS and the ASD, comparative experiments were performed in patients with diagnostic confirmation of autism and healthy patients. Exemplifying with the experiments of Schunke et al. and Swoden et al., in the first instance, they have measured the reaction time to imitation of simple, meaningless movements (lifting a finger, a dot or both). As a second experiment, subjects were asked to answer as quickly as possible, to two sound stimuli of different tones (500 or 1000 Hz), and lift the index finger or little finger according to each tone (pattern previously stipulated). The third test blended visual and auditory stimuli so that images were reproduced as the first experiment, but the answer should be given according to the sound stimulus. In all tests, the results proved no significant variation between groups of healthy patients and autistic patients. It was noticed that there was \"interference effects\" in the tests when visual and additives stimuli have been presented together, but present in both groups. Conclusion: Since the emergence of the study of mirror neurons, these were quickly linked to autism spectrum disorders by its deficit in order to, a priori, explain all the symptoms of this disease. However, new standardized and reproducible studies have shown that the mirror neuron system is fully active among the ASD patients. This correlation is consistent with the fact that autism is much more complex than expected after the discovery of mirror neurons.", "title": "UNBREAKING MIRRORS: RELATION BETWEEN AUTISM AND MIRROR NEURONS", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Lieselot Ruysschaert", "P. Warreyn", "J. R. Wiersema", "A. Oostra", "H. Roeyers"], "summary": "Investigating the underlying neural mechanisms of autism spectrum disorder (ASD) has recently been influenced by the discovery of mirror neurons. These neurons, active during both observation and execution of actions, are thought to play a crucial role in imitation and other social\u2010communicative skills that are often impaired in ASD. In the current electroencephalographic study, we investigated mu suppression, indicating neural mirroring in children with ASD between the ages of 24 and 48 months and age\u2010matched typically developing children, during observation of goal\u2010directed actions and non\u2010goal\u2010directed mimicked hand movements, as well as during action execution. Results revealed no significant group differences with significant central mu suppression in the ASD children and control children during both execution and observation of goal\u2010directed actions and during observation of hand movements. Furthermore, no significant correlations between mu suppression on one hand and quality of imitation, age, and social communication questionnaire scores on the other hand were found. These findings challenge the \u201cbroken mirror\u201d hypothesis of ASD, suggesting that impaired neural mirroring is not a distinctive feature of ASD. Autism Res 2014, 7: 197\u2013 206. \u00a9 2014 International Society for Autism Research, Wiley Periodicals, Inc.", "title": "Exploring the Role of Neural Mirroring in Children with Autism Spectrum Disorder", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Bruce K. Dixon"], "summary": "C H I C A G O \u2014 The impaired social interaction and communication characteristic of autistic children is the result of abnormally functioning mirror neurons in the brain, judging from the findings of a novel imaging study. A controlled study of 25 children revealed those with autism have increased gray matter in several areas of the parietal lobes, Manzar Ashtari, Ph.D., said at the annual meeting of the Radiological Society of North America. \u201cWhat we found was that the larger the brain matter, the more restrictive the child\u2019s interest and the more stereotypical his or her behavior, indicating the increased gray matter in autistic children is abnormal,\u201d said Dr. Ashtari, senior neuroscientist at Children\u2019s Hospital of Philadelphia. \u201cThis suggests that the inability of autistic children to relate to people and life situations in an ordinary way may result from an abnormally functioning mirror neuron system,\u201d she said. Mirror neurons are brain cells that are active both when an individual is performing an action and experiencing an emotion or sensation, and when that individual witnesses the same actions, emotions, and sensations in others, Dr. Ashtari explained. \u201cMirror neurons were first discovered in the macaque monkey, and there is a similar system in the human brain,\u201d she said, adding that the mirror neuron system is part of the motor system and plays an essential role in controlling our own actions. The \u201cbroken mirror\u201d theory of autism, which was first proposed about a decade ago, argues that dysfunction of the mirror neuron system is a root cause of social disability in autism. The study led by Dr. Ashtari was conducted at the Fay J. Lindner Center for Autism, North Shore\u2013Long Island Jewish Health System, Bethpage, N.Y., and involved 13 boys diagnosed with high-functioning autism or Asperger syndrome who had IQs greater than 70, and 12 healthy controls. The subjects, average age 11 years, underwent diffusion tensor imaging (DTI), a technique that tracks the movement of water molecules in the brain. Although DTI traditionally is used to study the brain\u2019s white matter and fiber content, Dr. Ashtari\u2019s team applied it to the assessment of gray matter by employing apparent diffusion coefficient based morphometry, which highlights brain regions with changes in gray matter volume. In addition to the gray matter abnormalities linked to the mirror neuron system, the investigators reported that the amount of gray matter in the left parietal area correlated with higher IQs in the control group but not in the autistic children. While this finding was interesting, said Dr. Ashtari, the difference did not reach statistical significance. \u201cHowever, this does suggest that the gray matter in children with autism is dysfunctional.\u201d Dr. Antonia Hamilton doubts the \u201cbroken mirror\u201d theory. \u201cI am skeptical of the mirror neuron\u2013autism link, and the Ashtari study does nothing to change my mind,\u201d she said in an interview. In her own study, Dr. Hamilton reported that children with autism do not suffer general imitation impairment or a global mirror neuron system deficit (Neuropsychologia 2007;45:1859-68). \u201cMirror neurons are active any time you perform an action with your own hand. When you pick up a cup of coffee, or see another person picking up a cup of coffee, the same neurons are involved,\u201d said Dr. Hamilton, a lecturer at the School of Psychology, University of Nottingham (England). \u201cMy experiment found that autistic children do fine when it comes to these practical, goaloriented actions; however, they do not do well with social actions that involve imitation, such as smiling or waving at another person,\u201d she explained. Dr. Hamilton studied 25 children with an independent clinical diagnosis of autism or autism spectrum disorder (ASD). The group had a mean age of 8 years and a mean verbal mental age of just over 4 years and were compared with 29 controls. Children were tested in their ability to copy the experimenter\u2019s hand movement to a target location on a table top, using mirror imitation. The investigators found no evidence for differences in performance between the ASD group and the matched controls. Both showed the typical pattern of hand errors on contralateral trials. \u201cWe can conclude that typical and autistic children have the same tendency to imitate the goal of another person\u2019s action,\u201d the scientists said, noting the concurrency of their results with previous studies. In a second experiment, 23 children with ASD and 31 controls completed a grasp imitation and motor planning task. \u201cMotor planning is known to rely on the frontoparietal circuit which makes up the mirror neuron system, so the [autistic mirror neuron dysfunction, or] AMND predicts poor performance in autism spectrum disorder, which was not found,\u201d they wrote. In another experiment, the children with autism showed no impairment in gesture recognition, and in fact performed better than did the matched controls. The authors concluded that their data are not compatible with the hypothesis of an action representation deficit or mirror neuron deficit in children with autism spectrum disorder. \u201cThe broken-mirror-in-autism idea is a very appealing hypothesis which has received a lot of press in the last few years, despite the fact that there [is a scarcity of] hard data to support it,\u201d Dr. Hamilton said in an interview. \u201cAlso, none of the studies of imitation in autism\u2014 which claim to support the mirror neuron idea\u2014have really shown that the problem is in the mirror neurons themselves, rather than some other social process which controls the mirror neurons,\u201d she added. \u25a0 Yellow areas highlight clusters of increased gray matter in the right and left parietal cortex as seen on DTI. R A D IO L O G IC A L S O C IE T Y O F N O R T H A M E R IC A", "title": "Abnormal Mirror Neurons May Impair Social Skills", "claims": null}, {"metadata": {"year": 2006}, "authors": ["Sarah  Archibald"], "summary": "The social deficits associated with autism, which include difficulty communicating and understanding environmental cues, such as facial expression, have been extensively reported. However, the neural mechanisms behind the disorder remain in question. One proposed culprit is the mirror neuron system, which is thought to be involved in interpreting the emotions of others. Further support for this idea has now been provided by a group of US scientists, who have shown that activation of the mirror neuron system is much reduced in the brains of children with autism. Neuroscientists believe that mirror neurons fire in response to the facial expressions of others, allowing our brains to \u2018mirror\u2019 their actions and so understand their feelings. Mirella Dapretto, who led the research at the University of California, Los Angeles, USA, said, \u201cThe mirroring mechanism may underlie the remarkable ability to read others\u2019 emotional states from a mere glance\u201d (Guardian, 6 December 2005). \u201cOur findings suggest that a dysfunctional mirror neuron system may underlie the social deficits observed in autism. This is exciting because we finally have an account that can explain all core symptoms of this disorder\u201d (Guardian). The researchers observed not only that activation in the brain area containing mirror neurons \u2014 the inferior frontal gyrus pars opercularis \u2014 was reduced in children with autism, but also that the extent of the decrease in activation correlated with the severity of symptoms. Michael Rutter of the Institute of Psychiatry, London, UK, agreed that \u201cThe general notion of linking mirror neurons with the social deficit in autism is quite reasonable\u201d (BBC News Online, 5 December 2005). However, he also observed \u201c...we need more research into the brain systems that might be involved. These might involve mirror neurons, but we need more studies\u201d (BBC News Online). Sarah Archibald R E S E A R C H H I G H L I G H T S", "title": "Mirror image", "claims": null}, {"metadata": {"year": 2013}, "authors": ["C. Andrade", "M. Pond\u00e9"], "summary": "The objective of this study was to conduct a systematic review of experimental studies performed to assess the role of mirror neurons in the pathophysiology of autism. Four papers reported that areas of mirror neurons (MN) were under-active in autistic patients, giving weight to the theory of MN as a cause of ASD. Three papers indicated that MN were activated during the proposed activities, advocating atypical activation of MN, but not necessarily hypoactivation of these areas. One of the articles reported that only part of the MN system or just those areas of interconnected neurons are dysfunctional in autism. Analysis of the selected studies showed a correlation between dysfunction of the mirror neuron system and the main symptoms of autism such as deficits in social cognition, complete absence or a reduction in the individual\u2019s interaction with his/her social environment and a failure in the neural mechanisms of imitation.", "title": "A SYSTEMATIC REVIEW OF THE INFLUENCE OF MIRROR NEURONS IN AUTISM SPECTRUM DISORDERS", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Ruth  Raymaekers", "Jan Roelf Wiersema", "Herbert  Roeyers"], "summary": "Individuals with Autism Spectrum Disorder (ASD) are characterised by an impaired imitation, thought to be critical for early affective, social and communicative development. One neurological system proposed to underlie this function is the mirror neuron system (MNS) and previous research has suggested a dysfunctional MNS in ASD. The EEG mu frequency, more precisely the reduction of the mu power, is considered to be an index for mirror neuron functioning. In this work, EEG registrations are used to evaluate the mirror neuron functioning of twenty children with high functioning autism (HFA) between 8 and 13 years. Their mu suppression to self-executed and observed movement is compared to typically developing peers and related to age, intelligence and symptom severity. Both groups show significant mu suppression to both self and observed hand movements. No group differences are found in either condition. These results do not support the hypothesis that HFA is associated with a dysfunctional MNS. The discrepancy with previous research is discussed in light of the heterogeneity of the ASD population.", "title": "EEG study of the mirror neuron system in children with high functioning autism", "claims": null}, {"metadata": {"year": 2006}, "authors": ["Mirella  Dapretto", "Mari S Davies", "Jennifer H Pfeifer", "Ashley A Scott", "Marian  Sigman", "Susan Y Bookheimer", "Marco  Iacoboni"], "summary": "To examine mirror neuron abnormalities in autism, high-functioning children with autism and matched controls underwent fMRI while imitating and observing emotional expressions. Although both groups performed the tasks equally well, children with autism showed no mirror neuron activity in the inferior frontal gyrus (pars opercularis). Notably, activity in this area was inversely related to symptom severity in the social domain, suggesting that a dysfunctional 'mirror neuron system' may underlie the social deficits observed in autism.", "title": "Understanding emotions in others: mirror neuron dysfunction in children with autism spectrum disorders", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Raphael  Bernier", "Geraldine  Dawson"], "summary": "Autism spectrum disorders are characterized by impairments in the social, communication, and behavioral domains. Impairments in social cognition are considered a core symptom of the disorder. Dysfunction of an observation/execution matching system (the mirror neuron system) has been proposed to serve as a neural mechanism explaining the deficits in social cognition found in autism. Through direct matching of observed and executed behavior an individual can directly experience an internal representation of another\u2019s actions, and therefore, another\u2019s feelings, goals, and intentions. Dysfunction of this system would hinder this process. In this chapter, Bernier and Dawson review findings from behavioral research examining deficits in three areas of social cognition in individuals with autism: imitation, empathy, and theory of mind. They then discuss research implicating mirror neuron dysfunction in autism spectrum disorders; and explore the mirror neuron theory of autism. Autism research utilizing functional and structural brain imaging, electroencephalography, and transcranial magnetic stimulation to study the mirror neuron system is reviewed. Results from these studies suggest dysfunction of the mirror neuron system in both children and adults with autism. The mirror neuron theory of autism proposes that dysfunction of the execution/observation matching system interferes with the acquisition of internal representation of others\u2019 observed behavior, expressions, movements, and emotions. This precludes the individual with autism having an immediate, direct experience of the other through this internal representation. Social impairments, including impairments in imitation, empathy, and theory of mind, are hypothesized to cascade from this lack of immediate, experiential understanding of others in the social world.", "title": "The Role of Mirror Neuron Dysfunction in Autism", "claims": null}], "query": "What is the relationship between mirror neurons and autism in children?", "summary_abstract": "The relationship between mirror neurons and autism in children has been a subject of considerable research, with varying conclusions. Initially, the \"broken mirror\" hypothesis suggested that dysfunction in the mirror neuron system (MNS) could be a root cause of the social deficits observed in autism spectrum disorders (ASD). This hypothesis posits that impaired neural mirroring could explain difficulties in imitation, empathy, and social communication, which are core symptoms of autism (Archibald, 2006; Bernier & Dawson, 2008).\n\nHowever, several studies challenge this hypothesis. For instance, Ruysschaert et al. (2014) found no significant differences in mu suppression, an indicator of neural mirroring, between children with ASD and typically developing children during action observation and execution. Similarly, Raymaekers et al. (2009) reported no differences in mu suppression between high-functioning autistic children and their peers, suggesting that the MNS might not be dysfunctional in ASD.\n\nConversely, some studies have found evidence supporting the \"broken mirror\" theory. Dapretto et al. (2006) observed reduced mirror neuron activity in the inferior frontal gyrus of children with autism, correlating with the severity of social symptoms. This finding suggests a potential link between MNS dysfunction and social deficits in autism.\n\nOther research, such as that by Andrade and Pond\u00e9 (2013), indicates that while some areas of the MNS may be underactive in autistic individuals, the activation is atypical rather than entirely absent. This nuanced view suggests that the MNS may not be uniformly dysfunctional across all individuals with autism.\n\nOverall, the evidence is mixed, with some studies supporting the idea of MNS dysfunction in autism, while others find no significant differences in mirror neuron activity between autistic and typically developing children. This suggests that the relationship between mirror neurons and autism is complex and may vary across individuals, highlighting the need for further research to clarify these findings.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2021}, "authors": ["Juntao  Shao", "Guilian  Shi", "Zhengqin  Qi", "Jingjing  Zheng", "Shigao  Chen"], "summary": "Ultrasound elastography is a modern imaging technique that has developed rapidly in recent years. It enables objective measurement of tissue stiffness, a physical property intuitive to the human sense of touch. This novel technology has become a hotspot and plays a major role in scientific research and academic practice. Presently, ultrasound elastography has been used in the identification of benign and malignant tumors in superficial organs, such as breast and thyroid, providing clinically accurate diagnosis and treatment. The method has also been widely used for the liver, kidney, prostate, lymph nodes, blood vessels, skin and muscle system. In the application of cervical lesions, ultrasound elastography can distinguish normal cervix from abnormal cervix and differentiate benign from malignant lesions. It can significantly improve the diagnostic specificity for cervical cancer and is also useful for assessing infiltration depth and stage of cervical cancer, as well as predicting chemoradiotherapy treatment response. For cervical evaluation during pregnancy, ultrasound elastography is useful for assessing cervical softening and predicting premature delivery and outcome of induced labor. This article reviews the principles of ultrasound elastography as well as the current status and limitations in its application for cervical lesions and the cervix during pregnancy.", "title": "Advancements in the Application of Ultrasound Elastography in the Cervix.", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Weiren Liang"], "summary": "Elastography is a new technique for noninvasive evaluation of tissue resistance to external hardness and deformation, which plays an increasingly important role in the diagnosis and differential diagnosis of thyroid, breast, liver, prostate and other organs in recent years.This paper reviews the application progress of elastography in various organs. \n \nKey words: \nUltrasonography;\u00a0Elasticity Imaging techniques;\u00a0Thyroid diseases;\u00a0Breast diseases;\u00a0Liver diseases;\u00a0Prostatism;\u00a0Genital diseases, female;\u00a0Review", "title": "Progress of application of real-time ultrasound elastography in clinical", "claims": null}, {"metadata": {"year": 2017}, "authors": ["S. Phatak", "G. Marfani", "Nipun Gupta"], "summary": "Elasticity Imaging, Elastography, Sonoelastography, ABSTRACT Ultrasonic elastography (real-time elastography, sonoelastography) is a new improvement of ultrasound technique and one of the various imaging modalities in the last few years which are being used for characterizing soft tissue masses of breast, thyroid, and lymph nodes. Musculoskeletal pathology was one of the early applications of sonoelastography, but the method is not yet standardized. Other new horizons of applications include Liver, Prostate, obstetrics and gynaecology. We are presenting a pictorial essay of elastography cases in Obstetrics and gynaecology seen in our institution. Dr. Suresh Phatak Professor and Head of department, Dept. of Radio-diagnosis, Jawaharlal Nehru Medical College, Sawangi (Meghe), Wardha, Maharashtra, India 442001", "title": "ROLE OF STRAIN ELASTOGRAPHY IN OBSTETRICS AND GYNECOLOGY: PICTORIAL ESSAY OF OUR INITIAL EXPERIENCE", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Liu Qing-qin"], "summary": "Ultrasound elastography is a new technology imaging according to the different elastic coefficient of different tissues,it makes up the shortage of conventional ultrasound,and become a hot research topic in the field of medical imaging. Ultrasound elastography has been widely used in breast diseases,in recent years,reports about other organs increased gradually,such as thyroid,prostate and liver,and shows the growing advantage in the diagnosis and differential diagnosis of various diseases. This paper review the clinical application of ultrasound elastography in these tissues and organs and related development in recent years.", "title": "Research Progress and Clinical Application of Ultrasound Elasticity Imaging Technology", "claims": null}, {"metadata": {"year": 2006}, "authors": ["A. Basarab", "E. Royer", "P. Boissieu", "P. Delachartre"], "summary": "The purpose of elastography is to characterize from mechanical point of view a material (the soft biological tissues in the medical field). Ultrasound elastography is based on the comparison of ultrasound images, when the material is submitted under axial force (compression). This work presents the advances in the instrumentation of an ultrasound scanner, to acquire a sequence of images which will be processed in the aim of elastography. First, motion estimation between two or more images is processed. The estimated displacement gives the possibility to obtain detailed deformation elastograms. Thus, the presence of a hard inclusion (that simulates a tumor a pathological tissue) within a phantom mimicking soft tissues can be more easily detected, which may lead to a better cancer diagnosis.", "title": "Advances in Ultrasound Elastography", "claims": null}, {"metadata": {"year": 2009}, "authors": ["J. Bonner"], "summary": "Elastography is one of the emerging technologies on display at ECR 2009, reflecting the growing importance of imaging techniques that compare the inherent stiffness of healthy and abnormal tissues in advancing the diagnostic value of ultrasound. Elastography\u2019s clinical applications were discussed at two separate satellite symposia on Saturday and Sunday.", "title": "Emergence of elastography gives renewed impetus and vigor to ultrasound market", "claims": null}, {"metadata": {"year": 2014}, "authors": ["S. Dudea"], "summary": "Definitely, contrast and elastography have been the two words prevailing in the ultrasonography research literature of the last decade. Of these, elastography did cast a special charm over the researchers, due to its ever increasing number of technological approaches. A major role was also played by the adaptability of the method(s) to virtually all applications of ultrasonography and even the opening of novel domains, such as diffuse organ structure assessment. No wonder that dedicated radiologists, mostly breast imaging experts, as well as specialists in gastroenterology, endocrinology, internal medicine etc., were all involved in intensive research and publication. Truly, elastography boosted ultrasound related publications and rapidly expanded the field of knowledge. Recently, solid publications compacted available knowledge into guidelines that represent a stage benchmark. The most notorious are the EFSUMB guidelines on the clinical use of ultrasound elastography [1,2]. So, after a decade of clinical usage, has elastography come to the age of ripening? The answer to this question requires a graded approach. Overall, the answer is, definitely , no. Or, at least, not in all domains. Extensive work and widespread experience allow for expert reviews and statements under the form of guidelines for a specific, well documented application of the technique(s). On the other hand, new, potential applications are under permanent scrutiny. Some of the papers published in this issue of the journal richly substantiate these assertions. The knowledge gathered by four groups in a specific application of elastography-this being liver disease-allows for the work up of national guidelines and recommendations [3]. This ambitious project is justified both by recent technological developments and by the sizeable number of publications of the authors. Even in this field, serendipitous findings may occur [4]. The applications of elastography in the study of the musculoskeletal system and even more superficial structures , such as skin, are surprisingly sparse. Some of the publications in this issue highlight potential applications, although reported results are, insofar, negative [5,6]. So why is elastography angel and demon? It must relate with the way we understand clinical medicine and imaging. Not only does elastography translate information addressing our least developed sense (tact) into coherent data addressing our most evolved sense (sight), but it also provides figures. Therefore, not only do we need to understand and interpret this \" visual tact \" and many physical terms we were not aware of but we must also connect this to the figures provided \u2026", "title": "Has Ultrasound Elastography come to the age of ripening?", "claims": null}, {"metadata": {"year": 2018}, "authors": ["B. Yasmeen"], "summary": "Abstract not available Northern International Medical College Journal Vol.9(1) July 2017: 243-244", "title": "Elastography-One Step Ahead", "claims": null}], "query": "summary the current advance of elastography in obgyn field in the past 3 months", "summary_abstract": "Recent advancements in elastography within the obstetrics and gynecology (OBGYN) field have highlighted its growing significance in clinical applications. Shao et al. (2021) emphasize the utility of ultrasound elastography in distinguishing between benign and malignant cervical lesions, improving diagnostic specificity for cervical cancer, and assessing the infiltration depth and stage of the disease. Additionally, elastography is beneficial in evaluating cervical softening during pregnancy, which aids in predicting premature delivery and the outcomes of induced labor. This underscores the technique's potential in enhancing prenatal care and cancer diagnostics within the OBGYN domain.", "summary_extract": null}, {"papers": [{"metadata": {"year": 1993}, "authors": ["H. Brink"], "summary": "Validity and reliability are key aspects of all research. Meticulous attention to these two aspects can make the difference between good research and poor research and can help to assure that fellow scientists accept findings as credible and trustworthy. This is particularly vital in qualitative work, where the researcher\u2019s subjectivity can so readily cloud the interpretation of the data, and where research findings are often questioned or viewed with scepticism by the scientific community.", "title": "Validity and reliability in qualitative research.", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Mohamed Ezzat Khamis Amin", "Lotte Stig N\u00f8rgaard", "Afonso M Cavaco", "Matthew J Witry", "Lisa  Hillman", "Alina  Cernasev", "Shane P Desselle"], "summary": "Spurred by the value it can add, the use of qualitative research methods has been steadily growing by social pharmacy researchers around the globe, either separately or as part of mixed methods research projects. Given this increase, it is important to provide guidance to assist researchers in ensuring quality when employing such methods. This commentary addresses both theoretical fundamentals as well as practical aspects of establishing quality in qualitative social pharmacy research. More specifically, it provides an explanation of each of the criteria of trustworthiness proposed by Lincoln and Guba (credibility, transferability, dependability and confirmability) and different techniques used in establishing them. It also provides a brief overview of authenticity, a more recent and less widely used set of criteria that involve demonstrating fairness, ontological authenticity, educative authenticity, catalytic authenticity, and tactical authenticity. For each of these terms, the commentary provides a definition, how it applies to social pharmacy research, and guidance on when and how to use them. These are accompanied by examples from the pharmacy literature where the criteria have been used. The commentary ends by providing a summary of competing viewpoints of establishing quality in the published literature while inviting the reader to reflect on how the presented criteria would apply to different qualitative research projects.", "title": "Establishing trustworthiness and authenticity in qualitative pharmacy research.", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Susan L. Morrow"], "summary": "This article examines concepts of the trustworthiness, or credibility, of qualitative research. Following a \u201cresearcher-as-instrument,\u201d or self-reflective, statement, the paradigmatic underpinnings of various criteria for judging the quality of qualitative research are explored, setting the stage for a discussion of more transcendent standards (those not associated with specific paradigms) for conducting quality research: social validity, subjectivity and reflexivity, adequacy of data, and adequacy of interpretation. Finally, current guidelines for writing and publishing qualitative research are reviewed, and strategies for conducting and writing qualitative research reports are suggested. Qualitative research, ensuing from a variety of disciplines, paradigms, and epistemologies, embraces multiple standards of quality, known variously as validity, credibility, rigor ,o rtrustworthiness. In addition to some standards that may be thought of as somewhat universal across disciplines and paradigms, the \u201cgoodness\u201d (Morrow & Smith, 2000) of qualitative inquiry is assessed on the basis of the paradigmatic underpinnings of the research and the standards of the discipline. Thus, a grounded theory study or a consensual qualitative research investigation in counseling psychology that is rooted in a postpositivist or constructivist/interpretivist paradigm will look quite different from a critical ethnography in education; and the standards appropriate for evaluating these studies will vary accordingly. I begin this article by addressing the paradigmatic underpinnings of trustworthiness or rigor in qualitative research. Next, I discuss central topics related to trustworthiness or validity that span paradigms and may be thought of as relevant across most research designs. I then provide an overview of guidelines that have been suggested for evaluating qualitative research, particularly in psychology. Finally, I offer recommendations for enhancing the quality of qualitative research in counseling psychology and suggest strategies for writing and publishing. First, however, in keeping with the standard of reflexivity as a way for researchers to inform their audiences about their perspectives as well as to manage their subjectivities, I describe my own assumptions about qualitative research methodology and quality.", "title": "Quality and trustworthiness in qualitative research in counseling psychology.", "claims": null}, {"metadata": {"year": 1999}, "authors": ["J. Cutcliffe", "H. McKenna"], "summary": "Qualitative research is increasingly recognized and valued and its unique place in nursing research is highlighted by many. Despite this, some nurse researchers continue to raise epistemological issues about the problems of objectivity and the validity of qualitative research findings. This paper explores the issues relating to the representativeness or credibility of qualitative research findings. It therefore critiques the existing distinct philosophical and methodological positions concerning the trustworthiness of qualitative research findings, which are described as follows: quantitative studies should be judged using the same criteria and terminology as quantitative studies; it is impossible, in a meaningful way, for any criteria to be used to judge qualitative studies; qualitative studies should be judged using criteria that are developed for and fit the qualitative paradigm; and the credibility of qualitative research findings could be established by testing out the emerging theory by means of conducting a deductive quantitative study. The authors conclude by providing some guidelines for establishing the credibility of qualitative research findings.", "title": "Establishing the credibility of qualitative research findings: the plot thickens.", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Marja Soila-Wadman"], "summary": "The journal is quite a new, international publication, with interests in arts, aesthetics and cultural matters. In this field there have been a lot of creative discussions going on concerning the research methodology. When we authors previously were working with the structure of the article, our belief, at the end of the 2000, was that the scientific status of qualitative research is widely accepted? That the basic arguments concerning the credibility of research would not be required in a specific article in journals which are familiar with qualitative research, and have published research reports where the studies have been conducted by", "title": "The Question of Credibility in Qualitative Research - Once again!", "claims": null}, {"metadata": {"year": 1993}, "authors": ["Cheryl Tatano Beck"], "summary": "The three criteria of credibility, fittingness, and auditability have been focused on in the hope of facilitating the critique of qualitative research. Once criteria that are appropriate to qualitative methodologies are developed, the scientific merit of these research studies can truly be appreciated. If traditional scientific criteria relevant to quantitative studies are used to critique qualitative methods, the development and acceptance of this paradigm-transcending research will be hindered.", "title": "Qualitative Research: The Evaluation of Its Credibility, Fittingness, and Auditability", "claims": null}, {"metadata": {"year": 2008}, "authors": ["W. Kline"], "summary": "The standards used to evaluate qualitative research have been a recurring theme in qualitative research literature for more than 20 years. Included in the discussion of these standards are the qualities of trustworthiness (Y. Lincoln & E. Guba, 1985). More recently, methodological and analytic rigor and coherence have also been emphasized (e.g., M. Poggenpoel & C. P. H. Myburgh, 2005). These factors are discussed and conceptualized as presentational rigor. Implications of presentational rigor for designing and conducting qualitative research and preparing manuscripts that are more likely to be published in Counselor Education and Supervision are discussed.", "title": "Developing and Submitting Credible Qualitative Manuscripts", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Susan Johnson", "Saltanat Rasulova"], "summary": ": Recent developments in impact evaluation recognise the need to go beyond the intense debate over experimental techniques to incorporate theory-based approaches and qualitative research methods. Motivated by an underlying concern that qualitative research in this new wave of qualitative impact evaluation research is appropriately conducted, this paper reviews practical strategies to address rigour deploying Guba and Lincoln\u2019s principles of \u201ctrustworthiness\u201d to do so. In particular we focus on the less discussed principle of \u2018authenticity\u2019 which responds to the demand for research orientations to be more transformative and emancipatory. In development impact evaluation, recent discussion has highlighted the frequent deficit of a transformative orientation and the problematic wider organisational contexts of aid relationships in which commissioned evaluations are conducted. We argue that embracing the authenticity principle offers commissioned researchers with a progressive orientation a rationale for making space for stakeholder interaction and negotiation within the rigour discourse. This in turn creates the scope to incorporate it into checklists of rigour so using the \u2018artefacts\u2019 of evaluation as a means to call commissioning organisations and other stakeholders to be more responsive to concerns for authenticity.", "title": "Qualitative impact evaluation: Incorporating authenticity into the assessment of rigour", "claims": null}], "query": "Write about authenticity and credibility when conducting qualitative research.", "summary_abstract": "The collection of papers collectively underscores the importance of authenticity and credibility in qualitative research, emphasizing the need for rigorous standards to ensure the trustworthiness of findings. Brink (1993) highlights the critical role of validity and reliability in qualitative research, noting that meticulous attention to these aspects can enhance the credibility and trustworthiness of findings, which are often scrutinized due to the subjective nature of qualitative methods. Amin et al. (2020) expand on this by discussing the criteria of trustworthiness proposed by Lincoln and Guba, which include credibility, transferability, dependability, and confirmability, as well as the concept of authenticity. They provide guidance on how these criteria can be applied in social pharmacy research to ensure quality and fairness.\n\nMorrow (2005) explores the paradigmatic underpinnings of trustworthiness in qualitative research, suggesting that credibility is assessed based on the paradigmatic and disciplinary standards. She emphasizes the importance of reflexivity and subjectivity in maintaining the adequacy of data and interpretation. Cutcliffe and McKenna (1999) critique the philosophical and methodological positions on the credibility of qualitative research, advocating for criteria that fit the qualitative paradigm to establish trustworthiness.\n\nBeck (1993) focuses on the criteria of credibility, fittingness, and auditability, arguing that these are essential for appreciating the scientific merit of qualitative research. She warns against using traditional quantitative criteria to evaluate qualitative studies, as this could hinder the development and acceptance of qualitative methodologies. Johnson and Rasulova (2016) discuss the principle of authenticity, which calls for a transformative and emancipatory orientation in research. They argue that incorporating authenticity into the rigour discourse allows for greater stakeholder interaction and responsiveness, enhancing the credibility of qualitative impact evaluations.\n\nOverall, these papers collectively emphasize that authenticity and credibility in qualitative research are achieved through adherence to specific criteria and standards that are tailored to the qualitative paradigm, ensuring that findings are both trustworthy and transformative (Brink, 1993; Amin et al., 2020; Morrow, 2005; Cutcliffe & McKenna, 1999; Beck, 1993; Johnson & Rasulova, 2016).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2015}, "authors": ["Daniel H. Kwak"], "summary": "Microbial secondary metabolites are physiologically significant, exhibiting auxiliary functions for the producer and as scaffolds in the developments of new medicines. Advancements in genome sequencing technologies have enabled researchers to access unprecedented amounts of genomic data that can be used to discover the enzymatic machinery necessary to discover novel and biologically-active molecules. This approach has been termed \u201cgenome mining.\u201d In both of the investigations presented herein, genome mining was utilized to discover and characterize biosynthetic pathways of novel molecules. The findings in one study utilize this approach to discover a small molecule virulence factor from the opportunistic human pathogen Acinetobacter baumannii. This virulence factor has been found to be associated with a number of clinically significant phenotypes, and these findings suggest that this can be a target in the developments of next generation antibiotics. In another study, this approach was implemented to discover and characterize the biosynthetic pathway of anticancer compound hapalosin from the cyanobacterial species Hapalosiphon welwitschii. Cloning and expression of this biosynthetic pathway in the surrogate host Escherichia coli enabled its genetic characterization as well as the generation of a small combinatorial library consisting of analogs incorporating natural and unnatural substrates. Collectively, these investigations demonstrate the utility of genome mining to characterize novel molecules important in pathogenesis or in the biosynthesis of clinically-significant compounds.", "title": "MS Thesis (ETD) Dan Kwak", "claims": null}, {"metadata": {"year": 2016}, "authors": ["N. Ziemert", "T. Weber"], "summary": "The computational mining of genomes has become an important part in the discovery of novel natural products as drug leads. Thousands of bacterial genome sequences are publically available these days containing an even larger number and diversity of secondary metabolite gene clusters that await linkage to their encoded natural products. With the development of high-throughput sequencing methods and the wealth of DNA data available, a variety of genome mining methods and tools have been developed to guide discovery and characterisation of these compounds. This article reviews the development of these computational approaches during the last decade and shows how the revolution of next generation sequencing methods has led to an evolution of various genome mining approaches, techniques and tools. After a short introduction and brief overview of important milestones, this article will focus on the different approaches of mining genomes for secondary metabolites, from detecting biosynthetic genes to resistance based methods and \u201cevo-mining\u201d strategies including a short evaluation of the impact of the development of genome mining methods and tools on the field of natural products and microbial ecology.", "title": "genome mining in microbes \u2013 a review", "claims": null}, {"metadata": {"year": 2016}, "authors": ["N. Ziemert", "Mohammad Alanjary", "T. Weber"], "summary": "Covering: 2006 to 2016The computational mining of genomes has become an important part in the discovery of novel natural products as drug leads. Thousands of bacterial genome sequences are publically available these days containing an even larger number and diversity of secondary metabolite gene clusters that await linkage to their encoded natural products. With the development of high-throughput sequencing methods and the wealth of DNA data available, a variety of genome mining methods and tools have been developed to guide discovery and characterisation of these compounds. This article reviews the development of these computational approaches during the last decade and shows how the revolution of next generation sequencing methods has led to an evolution of various genome mining approaches, techniques and tools. After a short introduction and brief overview of important milestones, this article will focus on the different approaches of mining genomes for secondary metabolites, from detecting biosynthetic genes to resistance based methods and \"evo-mining\" strategies including a short evaluation of the impact of the development of genome mining methods and tools on the field of natural products and microbial ecology.", "title": "The evolution of genome mining in microbes - a review.", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Bikash  Baral", "Amir  Akhgari", "Mikko  Mets\u00e4-Ketel\u00e4"], "summary": "Microbial natural products are a tremendous source of new bioactive chemical entities for drug discovery. Next generation sequencing has revealed an unprecedented genomic potential for production of secondary metabolites by diverse micro-organisms found in the environment and in the microbiota. Genome mining has further led to the discovery of numerous uncharacterized \u2018cryptic\u2019 metabolic pathways in the classical producers of natural products such as Actinobacteria and fungi. These biosynthetic gene clusters may code for improved biologically active metabolites, but harnessing the full genetic potential has been hindered by the observation that many of the pathways are \u2018silent\u2019 under laboratory conditions. Here we provide an overview of the various biotechnological methodologies, which can be divided to pleiotropic, biosynthetic gene cluster specific, and targeted genome-wide approaches that have been developed for the awakening of microbial secondary metabolic pathways.", "title": "Activation of microbial secondary metabolic pathways: Avenues and challenges", "claims": null}, {"metadata": {"year": 2020}, "authors": ["K. Hong", "Changsheng Zhang", "A. Dobson"], "summary": "Bioinformatic tools and their application in genome mining for secondary metabolites Genome mining approaches for the identification of novel secondary metabolites Merging ecology (physical, chemical factors or co-culture) with microbial genome mining for secondary metabolites\u2019 discovery Heterologous systems for the expression of gene clusters to identify novel metabolites Unlocking cryptic pathways, employing genomic based approaches Cutting-edge technology in genome editing and novel metabolites identification", "title": "Genome Mining and Marine Microbial Natural Products", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Anne\u2010Catrin Letzel", "S. Pidot", "C. Hertweck"], "summary": "A total of 211 complete and published genomes from anaerobic bacteria are analysed for the presence of secondary metabolite biosynthesis gene clusters, in particular those tentatively coding for polyketide synthases (PKS) and non-ribosomal peptide synthetases (NRPS). We investigate the distribution of these gene clusters according to bacterial phylogeny and, if known, correlate these to the type of metabolic pathways they encode. The potential of anaerobes as secondary metabolite producers is highlighted.", "title": "A genomic approach to the cryptic secondary metabolome of the anaerobic world.", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Quoc-Thai  Nguyen", "Maria E.  Merlo", "Marnix H.  Medema", "Andris  Jankevics", "Rainer  Breitling", "Eriko  Takano"], "summary": "Many microbial secondary metabolites are of high biotechnological value for medicine, agriculture, and the food industry. Bacterial genome mining has revealed numerous novel secondary metabolite biosynthetic gene clusters, which encode the potential to synthesize a large diversity of compounds that have never been observed before. The stimulation or \u201cawakening\u201d of this cryptic microbial secondary metabolism has naturally attracted the attention of synthetic microbiologists, who exploit recent advances in DNA sequencing and synthesis to achieve unprecedented control over metabolic pathways. One of the indispensable tools in the synthetic biology toolbox is metabolomics, the global quantification of small biomolecules. This review illustrates the pivotal role of metabolomics for the synthetic microbiology of secondary metabolism, including its crucial role in novel compound discovery in microbes, the examination of side products of engineered metabolic pathways, as well as the identification of major bottlenecks for the overproduction of compounds of interest, especially in combination with metabolic modeling. We conclude by highlighting remaining challenges and recent technological advances that will drive metabolomics towards fulfilling its potential as a cornerstone technology of synthetic microbiology.", "title": "Metabolomics methods for the synthetic biology of secondary metabolism", "claims": null}, {"metadata": {"year": 2013}, "authors": ["J. Nov\u00e1kov\u00e1", "M. Farka\u0161ovsk\u00fd"], "summary": "Mining of natural sources for new secondary metabolites has a successful history, which is reflected by the fact that over 50% of all drugs, currently on the market, are derived from natural products. Bacteria are one of the most important sources of bioactive natural products destined for drug discovery. However, less than 1% of the microorganisms observed in different habitats have been cultivated and characterized. To explore the genomic and functional diversity of the vast majority of the microbial world, novel methods were introduced, which are based on analysis of a DNA isolated from environmental communities. Metagenomics represents a strategy offering access to the genetic information present in uncultured bacteria by screening of libraries constructed from DNA isolated from different habitats. Functional- and sequence-driven screens are the major approaches employed to mine metagenomic libraries. This review aims to highlight discoveries in this area and discusses the possible future directions of the field.", "title": "Bioprospecting microbial metagenome for natural products", "claims": null}], "query": "Genome mining for bioactive secondary metabolites from microbes", "summary_abstract": "The collection of abstracts collectively underscores the transformative role of genome mining in the discovery of bioactive secondary metabolites from microbes. Kwak (2015) highlights the utility of genome mining in identifying novel molecules with significant clinical implications, such as a virulence factor from Acinetobacter baumannii and an anticancer compound from cyanobacteria. Ziemert and Weber (2016) emphasize the evolution of computational genome mining approaches, driven by advancements in high-throughput sequencing, which have facilitated the discovery of diverse secondary metabolite gene clusters. Baral et al. (2018) discuss the potential of genome mining to uncover cryptic metabolic pathways in classical natural product producers, although many remain silent under laboratory conditions. Hong et al. (2020) further explore the integration of bioinformatics and heterologous expression systems to unlock these cryptic pathways. Letzel et al. (2013) and Nguyen et al. (2012) both highlight the vast potential of bacterial genomes in producing novel secondary metabolites, with Nguyen et al. emphasizing the role of metabolomics in synthetic biology for novel compound discovery. Nov\u00e1kov\u00e1 and Farka\u0161ovsk\u00fd (2013) introduce metagenomics as a strategy to access genetic information from uncultured bacteria, expanding the scope of genome mining beyond cultivated microorganisms. Collectively, these studies illustrate the significant advancements and methodologies in genome mining, which are pivotal for the discovery and characterization of novel bioactive compounds from microbial sources.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2019}, "authors": ["Charlie Kurth"], "summary": "According to psychological constructivism, emotions result from projecting folk emotion concepts onto felt affective episodes. While constructivists acknowledge there is a biological dimension to emotion, they deny that emotions are (or involve) affect programs. So they also deny emotions are natural kinds. However, the essential role that constructivism gives to felt experience and folk concepts leads to an account that is extensionally inadequate and functionally inaccurate. Moreover, biologically oriented proposals that reject these commitments are not similarly encumbered. Recognizing this has two implications: biological mechanisms are more central to emotion than constructivism allows, and the conclusion that emotions are not natural kinds is premature.", "title": "Are Emotions Psychological Constructions?", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Iris  Berent", "Lisa  Feldman Barrett", "Melanie  Platt"], "summary": "A large literature debates whether emotions are universal and innate. Here, we ask whether reasoning about such matters is shaped by intuitive Essentialist biases that link innateness to the material body. To gauge the perception of innateness, we asked laypeople to evaluate whether emotion categories will be recognized spontaneously by hunter\u2013gatherers who have had no contact with Westerners. Experiment 1 shows that participants believe that emotions are innate and embodied (facially and internally) and these two properties correlate reliably. Experiment 2 demonstrates that the link is causal. When told that emotions are localized in specific brain areas (i.e., embodied), participants concluded that emotions are innate. Experiment 3 shows that this na\u00efve view persists even when participants are explicitly informed that these emotions are acquired. Our results are the first to suggest that laypeople incorrectly believe that, if emotions are embodied, then they must be innate. We suggest that people\u2019s failure to grasp the workings of their psyche arises from the human psyche itself.", "title": "Essentialist Biases in Reasoning About Emotions", "claims": null}, {"metadata": {"year": 2019}, "authors": ["I. Berent", "L. F. Barrett", "Melanie Platt"], "summary": "A large literature debates whether emotions are universal and innate. Here, we ask whether reasoning about such matters is shaped by intuitive Essentialist biases that link innateness to the material body. To gauge the perception of innateness, we asked laypeople to evaluate whether emotion categories will be recognized spontaneously by hunter\u2013gatherers who have had no contact with Westerners. Experiment 1 shows that participants believe that emotions are innate and embodied (facially and internally) and these two properties correlate reliably. Experiment 2 demonstrates that the link is causal. When told that emotions are localized in specific brain areas (i.e., embodied), participants concluded that emotions are innate. Experiment 3 shows that this na\u00efve view persists even when participants are explicitly informed that these emotions are acquired. Our results are the first to suggest that laypeople incorrectly believe that, if emotions are embodied, then they must be innate. We suggest that people\u2019s failure to grasp the workings of their psyche arises from the human psyche itself.", "title": "Essentialist Biases in Reasoning About Emotions", "claims": null}, {"metadata": {"year": 1990}, "authors": ["M. Mascolo", "J. Mancuso"], "summary": "Abstract The theory of emotions outlined in this paper follows a general theory that psychological processes maintain an adaptive equilibrium between one's construction system and sensory input from one's world. Persons build constructions to match any class of sensory data, and also build standards for (construe) varied levels of input which accompany the arousal-related activity associated with standard/input mismatch. Conscious identification of different emotional states reflects the use of constructions assigned to different standard/input relations. A constructivist may regard different emotional states or experiences as idealized configurations of attributes; and these configurations can be treated as prototypes, applying the same analyses which have been developed by cognitive scientists who have offered models for discussions of categorization processes. A model for discussing emotional development is provided, and directions of future constructivist investigations are suggested", "title": "Functioning of Epigenetically Evolved Emotion Systems: A Constructive Analysis", "claims": null}, {"metadata": {"year": 2020}, "authors": ["I. Berent"], "summary": "Can you tell what a stranger feels just by looking at their face? Could you distinguish fear from anger even in a person from an entirely unfamiliar culture (without having the opportunity to learn about it from experience)? Laypeople assume they can, because they believe that emotions are inborn, and they are universally imprinted on the body, both externally, on the face, and internally (I sense anxiety in the rumbling of my gut). In fact, people believe that emotions are innate precisely because they believe that emotions are \u201cin the body.\u201d So strong is their conviction that they will insist on their belief even when told that the emotions in question are in fact acquired. Our tendency to view \u201cwarm\u201d feelings as embodied and innate is the exact mirror image of our tendency to view \u201ccold\u201d concepts as ephemeral and disembodied. A review of the scientific literature reveals that similar presumptions also plague the debate on universal emotions in affective science. Chapter 10 shows how Essentialism (a principle invoked to explain our aversion to innate ideas) also promotes the promiscuous presumption of innate emotions by laypeople and scientists alike.", "title": "Our Big Hearts", "claims": null}, {"metadata": {"year": 2012}, "authors": ["J. de Rivera"], "summary": "Emotions involve transformations of the relationships that exist in the \u201cnatural\u201d world as well as in social worlds. Thus, the emotions that humans construct are based on affective processes that exist independently from human society and are not necessarily constituted by the interplay of culture and genetic or physiological processes.", "title": "Emotions as Transformational Structures", "claims": null}, {"metadata": {"year": 2012}, "authors": ["J. Rivera"], "summary": "Emotions involve transformations of the relationships that exist in the \u201cnatural\u201d world as well as in social worlds. Thus, the emotions that humans construct are based on affective processes that exist independently from human society and are not necessarily constituted by the interplay of culture and genetic or physiological processes.", "title": "Emotions as Transformational Structures", "claims": null}, {"metadata": {"year": 2016}, "authors": ["L. F. Barrett"], "summary": "Abstract The science of emotion has been using folk psychology categories derived from philosophy to search for the brain basis of emotion. The last two decades of neuroscience research have brought us to the brink of a paradigm shift in understanding the workings of the brain, however, setting the stage to revolutionize our understanding of what emotions are and how they work. In this article, we begin with the structure and function of the brain, and from there deduce what the biological basis of emotions might be. The answer is a brain-based, computational account called the theory of constructed emotion.", "title": "The theory of constructed emotion: an active inference account of interoception and categorization", "claims": null}], "query": "Are emotions innate or constructed?", "summary_abstract": "The collection of papers presents a nuanced debate on whether emotions are innate or constructed. Kurth (2019) critiques psychological constructivism, suggesting that while emotions have a biological dimension, they are not merely natural kinds, indicating a more complex interplay between biology and constructivism. Berent et al. (2019, 2020) explore the perception of emotions as innate, revealing that laypeople often mistakenly equate embodied emotions with innateness due to intuitive Essentialist biases. This suggests a common misconception that emotions are inborn because they are physically manifested, despite evidence that they can be acquired. Mascolo and Mancuso (1990) propose a constructivist view, where emotions are seen as idealized configurations of attributes, constructed through cognitive processes. De Rivera (2012) supports the idea that emotions are constructed based on affective processes that exist independently of cultural or genetic influences. Barrett (2016) introduces the theory of constructed emotion, emphasizing a brain-based, computational approach to understanding emotions, which aligns with the constructivist perspective. Collectively, these papers suggest that while there is a biological component to emotions, they are largely constructed through cognitive and social processes, challenging the notion of emotions as purely innate phenomena.", "summary_extract": null}, {"papers": [{"metadata": {"year": 1997}, "authors": ["P. Wetzels", "C. Pfeiffer"], "summary": "Police crime statistics show a significant increase in the rates of violent crimes committed by children during the last two years. However, since in Germany children under the age of 14 are not accountable for criminal offences, these statistics are highly selective. On the other hand, police crime statistics also show a huge increase of violent crimes committed by juveniles and young adults which cannot be explained by changes of police intervention strategies. It seems reasonable to attribute this mainly to changed living conditions of young people, particularly the increase proportion of children and adolescents living below the thresholds of poverty. To put the issue of violence and children in perspective, the victimization of children should not be overlooked. Criminological as well as psychological research show the devastating consequences of physical and sexual abuse experiences during childhood. Results of a representative german survey concerning the prevalence of violent victimization experiences during childhood are presented. 13.5% of the male and 16.1% of the female respondents had been victims of severe physical or sexual abuse during childhood. If repeatedly witnessing parental violence is additionally taken into consideration, these rates are 18.3% for male and 20.5% for female respondents. A comparison of age groups failed to identify significant differences of victimization rates, except the rates of minor violence committed by parents. Thus it can be assumed tentatively, that the proportion of children subjected to severe violent acts committed by closely related adults as well as the rate of those witnessing parental violence has remained constant over time.", "title": "[Childhood and violence: perpetrator and victim perspectives from the viewpoint of criminology].", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Chelsea M. Weaver", "J. Borkowski", "T. Whitman"], "summary": "The relationships between childhood exposure to violence and adolescent conduct problems were investigated in a sample of 88 primiparous adolescent mothers and their children. Regression analyses revealed that witnessing violence and victimization prior to age 10 predicted delinquency and violent behaviors, even after controlling for prenatal maternal and early childhood externalizing problems. Social competency and depression during middle childhood moderated the relationship between victimization and violent behaviors for girls, but not boys: Lower levels of social competency and depression served as risk factors for delinquency among teenage girls who experienced victimization during childhood. These findings have important implications for youth violence prevention programs.", "title": "Violence Breeds Violence: Childhood Exposure and Adolescent Conduct Problems.", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Robert A. Murphy", "Robert A. Rosenheck", "Steven J. Berkowitz", "Steven R. Marans"], "summary": "The Child Development Community Policing Program represents a national model of community-based collaboration between police and mental health professionals for violence-exposed and traumatized children. Administrative data from clinical records of a 24-hour consultation service were examined through stepwise multivariate logistic regression to identify child and event characteristics associated with a direct, in-person response at the time of police contact. Of 2361 children, 809 (34.3%) received a direct, in-person response. Relative to Caucasian children, Hispanic youth were more likely to receive this form of response (OR = 1.36). An acute clinical response was more likely for incidents of gang involvement (OR = 8.12), accidents (OR = 5.21), felony assaults (OR = 2.97), property crimes (OR = 2.30), family violence (OR = 1.53) and psychiatric crises (OR = 1.29). Acute response was less likely when juvenile conduct problems (OR = 0.61), fires (OR = 0.59), child maltreatment (OR = 0.57), and domestic violence (OR = 0.44) were involved. Incidents that were more severe or involved a primary mental health component were related to utilization of intensive CDCP resources.", "title": "Acute Service Delivery in a Police-Mental Health Program for Children Exposed to Violence and Trauma", "claims": null}, {"metadata": {"year": 2015}, "authors": ["D. Finkelhor", "H. Turner"], "summary": "A National Profile of Children Exposed to Family Violence: Police Response, Family Response and Individual Impact provides the first nationally representative data on youth contact with law enforcement and victim services \u2013 including best practices and help-seeking obstacles \u2013 for cases of family violence involving exposure to children. These data come from a nationally representative sample of 517 family violence incidents drawn from the 4503 respondents to the Second National Survey of Children\u2019s Exposure to Violence (NatSCEV II). The NatSCEV study, conducted in 2011, involved telephone interviews with parents of children age 0-9 and with the youths themselves if they were age 10-17. Between 13%-58% of police contacts and between 34%-97% of advocate contacts following domestic violence incidents involving a child witness included actions from one or more of 10 best practices. Most police best practices were associated with increased likelihood of arrest. Almost half of children witnessed an arrest when one occurred, though only 1 in 4 youth were spoken to by police responding to the scene. Youth exposed to domestic violence, as a group, have high rates of other victimizations and adversities. Although this group reports elevated trauma symptoms, the characteristics of a specific domestic violence incident and the response to that incident by police were generally unrelated to youth's current trauma symptoms after controlling for history of victimization and other adversities. However, child current trauma symptoms were lowest when perpetrators left the house after the incident, followed by when no one moved out, and were highest when the victim moved out. Child witnesses to family violence are a highly victimized group, and it is recommended that they systematically receive assessment and services when any member of their family enters the system due to family violence. This document is a research report submitted to the U.S. Department of Justice. This report has not been published by the Department. Opinions or points of view expressed are those of the author(s) and do not necessarily reflect the official position or policies of the U.S. Department of Justice. NIJ 2010-IJ-CX-0021 Final Report 3", "title": "A National profile of children exposed to family violence: Police response, family response, and individual impact.", "claims": null}, {"metadata": {"year": 2012}, "authors": ["H. Richardson-Foster", "N. Stanley", "P. Miller", "G. Thomson"], "summary": "The police represent the front line in the service response to children experiencing domestic violence. This paper examines police intervention in domestic violence incidents involving children, drawing on quantitative and qualitative data from police records and interviews with young people and police officers. The quality of police communication with children and young people emerged as key, and police officers evinced reluctance to engage with children at domestic violence incidents. Providing the police with training and information designed to improve their skills and confidence might promote communication with children in this context. Policy that conceptualised children as victims of domestic violence in policy could focus police attention on the needs of children and young people at such incidents.", "title": "Police intervention in domestic violence incidents where children are present: police and children's perspectives", "claims": null}, {"metadata": {"year": 1990}, "authors": ["Beverly  Rivera", "Cathy Spatz Widom"], "summary": "The relationship between childhood victimization and violent offending is examined using a prospective cohorts design. Official criminal histories for a large sample of substantiated and validated cases of physical and sexual abuse and neglect (N = 908) from the years 1967 through 1971 were compared to those of a matched control group (N = 667) of individuals with no official record of abuse or neglect. Sex-specific and race-specific effects of childhood victimization and other characteristics of violent offending (chronicity, age of onset, temporal patterns, and continuity) are assessed. Childhood victimization increased overall risk for violent offending and particularly increased risk for males and blacks. In comparison to controls, abused and neglected children began delinquent careers earlier. Temporal patterns of violent offending were examined and childhood victims did not differ in age of arrest for first violent offense, nor were they more likely to continue offending. The findings and their limitations are discussed, as well as directions for future research.", "title": "Childhood Victimization and Violent Offending", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Jean Dawson", "M. Wells"], "summary": "ABSTRACT This analysis examines child protective service reports made in 494 cases of child victimization known to police. The data were collected from police reports of assaults involving child victims in two rural northeastern towns from 1990 to 1999. Findings suggest that cases reported to child protective services (CPS) were more likely to involve child maltreatment and involve family/caretaker offenders. Cases in which victims were age 13 to 17 years or the perpetrator was male were less likely to be reported to CPS. The data indicate that child protection agencies failed to learn of approximately 35% of the cases involving parent or caretaker offenders. Law enforcement agency policies and mandated reporting guidelines should streamline collaboration between these two types of agencies.", "title": "Crimes Involving Child Victims: Law Enforcement Reporting to Child Protective Services in Rural Communities", "claims": null}, {"metadata": {"year": 2009}, "authors": ["D. Finkelhor", "H. Turner", "Richard K. Ormrod", "S. Hamby"], "summary": "OBJECTIVE: The objective of this research was to obtain national estimates of exposure to the full spectrum of the childhood violence, abuse, and crime victimizations relevant to both clinical practice and public-policy approaches to the problem. METHODS: The study was based on a cross-sectional national telephone survey that involved a target sample of 4549 children aged 0 to 17 years. RESULTS: A clear majority (60.6%) of the children and youth in this nationally representative sample had experienced at least 1 direct or witnessed victimization in the previous year. Almost half (46.3%) had experienced a physical assault in the study year, 1 in 4 (24.6%) had experienced a property offense, 1 in 10 (10.2%) had experienced a form of child maltreatment, 6.1% had experienced a sexual victimization, and more than 1 in 4 (25.3%) had been a witness to violence or experienced another form of indirect victimization in the year, including 9.8% who had witnessed an intrafamily assault. One in 10 (10.2%) had experienced a victimization-related injury. More than one third (38.7%) had been exposed to 2 or more direct victimizations, 10.9% had 5 or more, and 2.4% had 10 or more during the study year. CONCLUSIONS: The scope and diversity of child exposure to victimization is not well recognized. Clinicians and researchers need to inquire about a larger spectrum of victimization types to identify multiply victimized children and tailor prevention and interventions to the full range of threats that children face.", "title": "Violence, Abuse, and Crime Exposure in a National Sample of Children and Youth", "claims": null}], "query": "police violence and child victimization OR adverse childhood experinces", "summary_abstract": "The collection of papers provides a comprehensive view of the relationship between police violence, child victimization, and adverse childhood experiences. Wetzels and Pfeiffer (1997) highlight the prevalence of severe physical and sexual abuse among children, noting that a significant portion of children witness parental violence, which has remained constant over time. Weaver et al. (2008) find that childhood exposure to violence predicts adolescent delinquency and violent behaviors, with social competency and depression acting as moderating factors for girls. Murphy et al. (2005) discuss the Child Development Community Policing Program, emphasizing the importance of acute responses to severe incidents involving children, although responses are less likely in cases of domestic violence and child maltreatment. Finkelhor and Turner (2015) reveal that children exposed to family violence often experience multiple victimizations and adversities, with police responses having limited impact on trauma symptoms unless the perpetrator leaves the household. Richardson-Foster et al. (2012) stress the need for improved police communication with children during domestic violence incidents, suggesting that training could enhance police engagement. Rivera and Widom (1990) demonstrate that childhood victimization increases the risk of violent offending, particularly among males and blacks, with earlier onset of delinquent behavior. Dawson and Wells (2008) highlight gaps in reporting child victimization to protective services, especially in cases involving family offenders. Finally, Finkelhor et al. (2009) provide national estimates showing that a majority of children experience some form of victimization, underscoring the need for comprehensive approaches to address the wide range of threats faced by children. Collectively, these studies underscore the pervasive nature of child victimization and the complex interplay between exposure to violence and subsequent behavioral outcomes, emphasizing the need for targeted interventions and improved systemic responses.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2004}, "authors": ["L. Valente"], "summary": "The cucurbitacins are highly oxygenated triterpenoid compounds found in several botanical families that show high toxicity and varied biological activities. This review presents the main cucurbitacins so far isolated and their key structural characteristics. It complements and updates the existing reviews on this subject.", "title": "Cucurbitacinas e suas principais caracter\u00edsticas estruturais", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Chao Zhu", "Jieyu Wang", "Zi-hui Meng", "Zhibin Xu"], "summary": "As a new generation of host molecules, Cucurbitins have made remarkable achievements in host-guest chemistry, molecular recognition, molecular assembly, catalysis, separation detection, energy and so on. It has become a hotspot in supramolecular chemistry research. The advance of cucurbit[n]uril analogues, including glycoluril derivatives, hemicucurbit [n] uril, \u201chandle shape\u201d cucurbit[n]uril and heterocucurbit [n] uril is reviewed. Further researches of cucurbit [n] uril analogues are also discussed.", "title": "Research Progress of Cucurbit[n]uril Analogues", "claims": null}, {"metadata": {"year": 1971}, "authors": ["D  Lavie", "E  Glotter"], "summary": "The potent physiological activity of plants belonging to the Cucurbitaceae family has been known since antiquity. They were feared on account of their high toxicity (Elisha\u2019s Miracle)*, and vet valued because of the medicinal properties ascribed to them (40). Greeks and Romans used them, the doctors of the Middle Ages praised their virtues and some were still described in the British Pharmacopoeia of 1914.", "title": "The cucurbitanes, a group of tetracyclic triterpenes.", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Jian Chao Chen", "M. Chiu", "R. Nie", "G. Cordell", "S. Qiu"], "summary": "The natural cucurbitacins constitute a group of triterpenoid substances which are well-known for their bitterness and toxicity. Structurally, they are characterized by the tetracyclic cucurbitane nucleus skeleton, namely, 19-(10-->9beta)-abeo-10alpha-lanost-5-ene (also known as 9beta-methyl-19-norlanosta-5-ene), with a variety of oxygen substitutions at different positions. According to the characteristics of their structures, cucurbitacins are divided into twelve categories. The biological effects of the cucurbitacins are also covered.", "title": "Cucurbitacins and cucurbitane glycosides: structures and biological activities.", "claims": null}, {"metadata": {"year": 1975}, "authors": ["H.  Ripperger", "K.  Seifert"], "summary": "Zusammenfassung Mit Hilfe von Sephadex-LH-20- bzw. Kieselgel-Chromatographie wurden aus Fruchten von Citrullus lanatus var. citroides (Synonym C. colocynthoides) neben Cucurbitacin E (2) zwei Glykoside isoliert, denen die Strukturen 2-o-\u03b2- d -Glucopyranosyl-cucurbitacin I (3) und 2-o-\u03b2- d -Glucopyranosyl-cucurbitacin E (4) zukommen. 3 ist das dritte Cucurbitacin-Glykosid, das kristallin erhalten wurde.", "title": "Die cucurbitacine von Citrullus lanatus var. citroides", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Jos\u00e9 Luis R\u00edos", "Jos\u00e9 M. Escandell", "M. Carmen Recio"], "summary": "The cucurbitacins are a group of tetracyclic triterpenoids derived from the cucurbitane skeleton and found primarily in the Cucurbitaceae family. These triterpenoids, present in free or glycosidic form, are generally responsible for the bitter taste of the plants that contain them and are probably the principal cause of the antifeedant effects observed for such plants. Several plants used in traditional medicine to treat both inflammatory diseases as well as various types of tumors are rich in cucurbitacins, a fact which has given rise to several studies concerning their potential use as anti-inflammatory and anticancer agents. Nevertheless, since many cucurbitacins are extremely toxic, relatively few papers have dealt with their pharmacological activity. Recently, however, the relationship between the toxicity of a compound and its chemical pattern of substitution has been established, thus allowing for a more in-depth understanding of this class of triterpenes. In the present review, we provide a compilation of all the studies published in the last ten years on the pharmacological and biological effects of cucurbitacins, focusing principally on their pharmacological properties, especially their anti-inflammatory and anticancer effects.", "title": "New insights into the bioactivity of cucurbitacins", "claims": null}, {"metadata": {"year": 1985}, "authors": ["Sun Cj", "J. Ry", "Kyi Zy"], "summary": "Cucurbitine is a novel amino acid with schistosomicidal activity. In an attempt to enhance its activity by modification of its chemical structure, some related compounds were synthesized. 1,2-Dicarbethoxy-pyrrolidin-4-one was converted into the corresponding ketoxime. Catalytic hydrogenation of the latter compound under atmospheric pressure and subsequent hydrolysis gave racemic 4-allo-hydroxyproline. However, reduction under high pressure, 4-aminopyrrolidine-2-carboxylic acid (\u2162_a) was obtained. A number of derivatives of cucurbitine, including some esters and acylamino compounds (Table 1) were also prepared. Pharmacological examinations revealed that none of the compounds prepared was more active than cucurbitine.", "title": "[Synthesis of some cucurbitine-related compounds].", "claims": null}, {"metadata": {"year": 1989}, "authors": ["H. Stuppner1", "H. Wagner2"], "summary": "Picrorhiza kurrooa 4 5 6 7 O O O O", "title": "New Cucurbitacin Glycosides from Picrorhiza kurrooa", "claims": null}], "query": "cucurbitins", "summary_abstract": "The collection of abstracts provides insights into the properties and applications of cucurbitins, particularly focusing on cucurbitacins and cucurbitine. Cucurbitacins are highly oxygenated triterpenoid compounds known for their toxicity and biological activities, including potential anti-inflammatory and anticancer effects (R\u00edos et al., 2005). These compounds are primarily found in the Cucurbitaceae family and are characterized by their bitterness and structural complexity (Chen et al., 2005; R\u00edos et al., 2005). Historically, cucurbitacins have been recognized for their medicinal properties despite their toxicity (Lavie & Glotter, 1971).\n\nCucurbitine, on the other hand, is a novel amino acid with schistosomicidal activity. Efforts to enhance its activity through chemical modifications have been explored, although no derivatives have surpassed the activity of cucurbitine itself (Sun et al., 1985).\n\nAdditionally, cucurbitins have gained attention in supramolecular chemistry, particularly in host-guest chemistry and molecular recognition, with advancements in cucurbit[n]uril analogues being a significant area of research (Zhu et al., 2018). Overall, the studies highlight the diverse applications and ongoing research into the chemical and biological properties of cucurbitins.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2005}, "authors": ["A. Branch"], "summary": "Policy for Flexible Work Practices , Outlines the flexible work practices within the NSW Health System.", "title": "Flexible Work Practices - Policy - NSW Department of Health", "claims": null}, {"metadata": {"year": 2012}, "authors": ["M. Grabowska"], "summary": "Flexible employment forms are one of the elements of active labour market policy which is connected with fl exicurity system. Th e new option on labour market in the present times. Results of the global economic crisis and the demographic situation related to ageing societies cause a need to implement solutions on the labour market which shall be both fl exible and protective. Flexible forms of work are forms of work which deviate from the typical form of employment such as a permanent employment contract. Th ese are atypical forms of employment whose characteristics include fl exible working time and place, the form of employment, other relations between the employee and the employer. Th ey are a truly new approach in the labour law as they more and more depart from the classical job under a permanent employment contract.", "title": "Flexible Employment Forms as an Element of Flexicurity", "claims": null}, {"metadata": {"year": 2022}, "authors": ["G. Nazarova", "V. O. Baliasnyi"], "summary": "The article is aimed at studying theoretical approaches to understanding the essence of flexible employment policy in modern enterprises. The article analyzes approaches to understanding the essence of the definition of \u00abemployment of the population\u00bb by domestic and foreign scholars. Modern trends in the field of employment of workers are identified, interpretations of the terms \u00abnon-standard employment\u00bb and \u00abflexible employment\u00bb are studied, their common features and differences are determined. The main trends in the use of labor force are aimed at reducing the costs of structural restructuring of the economy by: the use of various forms of hiring employees and their labor engagement, flexibility in regulating working hours (ensuring a flexible schedule with irregular working hours), flexibility in matters of remuneration (using a differentiated and individual approach), flexibility in the use of forms and methods of social protection of workers. The article analyzes the peculiarities of using the flexibility model in the context of employment problems. Based on the analyzed theoretical approaches, the definition of a flexible employment policy at the State level and the level of individual enterprises is provided. At the State level, flexible employment is defined as a form of labor market regulation, which takes into account and does not oppose the interests of both subjects of social and labor relations, i. e.: employers and hired employees. At the level of individual enterprises, flexible employment policy is the company\u2019s policy towards employees, which gives the latter the opportunity to choose between working and free time \u2013 both in terms of the amount of time and the mode of its use. Thus, the introduction of a flexible employment policy both at the level of individual enterprises and at the State level as a whole will make business more responsible, increase the level of economic activity of the population, make enterprises and organizations more competitive in the labor market in the context of informatization and digitalization of modern society.", "title": "The Theoretical Foundations of Flexible and Non-Standard Employment", "claims": null}, {"metadata": {"year": 1989}, "authors": ["T. Walsh"], "summary": "Drawing on publicly available statistics and data on employment structures, hours and labour utilisation practices from detailed company case\u2010studies in the retail and catering sectors, this paper considers the implications for labour markets of the rise in part\u2010time, temporary and casual working (variously described as \u2018flexible\u2019 labour). Throughout the paper, emphasis will be given to part\u2010time employment and temporary and casual working in their own right, not simply in relation to full\u2010time work.", "title": "\u2018Flexible\u2019 Labour: Some Policy Perspectives", "claims": null}, {"metadata": {"year": 2003}, "authors": ["Li Hui"], "summary": "Flexible employment has been developing rapidly in developed countries and playing an important role in improving labor market and economy since 1970s. There are many reasons for this. An important reason is that governments in the developed countries have made some special policies and measures for the flexible employment, including subsidies for employers and enterprises, assistance to up - starters, support for special groups in poverty as well as increasingly perfect and complete laws and legislations on flexible employment. These policies and measures can definitely give some enlightenment for the flexible employment in Chi-", "title": "Policy of the Flexible Employment in Developed Countries and Its Enlightenment", "claims": null}, {"metadata": {"year": 2001}, "authors": ["S. Ja\u0161arevi\u0107"], "summary": "In today's complex operating conditions, characterized by stricter and stricter competition in the market, there must be a more flexible approach to the organization and continuation of work process, as compared to the years when unemployment and economic crisis did not represent a chronic problem. For our country, with particularly weakened economy and one of the highest rates of actual unemployment in the world, such an approach shall be inevitable. In order to commence more successfully the flexibilization of work, besides reforms leading to originating and open market economy, we shall have to effect significant changes in the area of education (especially expertise), labor legislation and system of social security. However, the experiences of other countries clearly point that amending the regulations shall not be sufficient without reorienting the whole society. especially the state, the unions and employers' associations, towards more active incentive for flexible work in all the operating levels.", "title": "'Flexible work': A necessity of modern economy", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Sla\u0111ana Gligori\u0107", "Sanja \u0160kori\u0107"], "summary": "Global trends in the working world clearly show the changes in the character of labor relations with a significant representation of flexible forms of work. It is an atypical form of work organization that arises as a consequence of strong globalization flows, economic crisis, information and technological revolution, and which stands against the labor law standard of a classical employment, being represented in the form of the employment contract for an indefinite period of time with full time employment. Starting from the concept of flexicurity as the dominant concept in the EU, flexicurity seeks to establish a balance between flexibility in the organization of work in order to preserve stability. Through the flexible organization of work and working hours, there establishes a kind of balance between working hours, the rest time and time for socialization and social activity. However, on the other hand, a flexible organization of work can reduce the rights of the employees and workers outside the employment relationship, especially if this form of work is abused in practice by the contracts concluded contrary to their essence or legal norm. The authors use the historical, comparative-legal and sociological method, as well as the statistical data in the analysis of research subjects with the aim of providing the concrete proposals for the improvement of the existing normative framework.", "title": "Flexible forms of work and work engagement", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Elke J. Jahn", "R. Riphahn", "C. Schnabel"], "summary": "In recent decades, economic policy makers across Europe have sought to increase labour market flexibility by promoting the use of temporary employment. The articles in this Feature provide new results on how fixed\u2010term and agency work contracts affect firm productivity and how the segments of two\u2010tier labour markets interact. This article points to a possible trade\u2010off between efficiency and equity when deregulating labour markets. Taken together, the evidence presented in this Feature suggests that flexible forms of employment can be both a boon and a bane for labour markets and for society as a whole.", "title": "Feature: Flexible Forms of Employment: Boon and Bane", "claims": null}], "query": "what is flexible work policy", "summary_abstract": "Flexible work policy refers to employment practices that deviate from traditional, permanent full-time contracts, offering flexibility in terms of working hours, location, and employment terms. This approach is increasingly adopted to address economic challenges and demographic changes, such as aging populations and economic crises (Grabowska, 2012; Ja\u0161arevi\u0107, 2001). Flexible work policies are characterized by non-standard employment arrangements, including part-time, temporary, and casual work, which allow employees to choose their working hours and balance work with personal time (Nazarova & Baliasnyi, 2022; Walsh, 1989).\n\nAt the state level, flexible employment policies aim to regulate the labor market in a way that aligns the interests of employers and employees, enhancing economic activity and competitiveness (Nazarova & Baliasnyi, 2022). These policies often include government measures such as subsidies and legal frameworks to support flexible employment (Hui, 2003). However, while flexible work arrangements can increase labor market efficiency, they may also pose challenges to employee rights and job security if not properly regulated (Gligori\u0107 & \u0160kori\u0107, 2021; Jahn et al., 2012).\n\nOverall, flexible work policies are seen as a necessary adaptation to modern economic and social conditions, promoting a balance between flexibility and stability in the labor market (Gligori\u0107 & \u0160kori\u0107, 2021).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2011}, "authors": ["Eyal Yaniv"], "summary": "Organizational attention is an underdeveloped construct that can account for a variety of organizational phenomena. Attention is the means by which individuals select and process a limited amount of input from the enormous amount of internal and environmental inputs bombarding the senses, memories and other cognitive processes. This article develops a coherent theory of organizational attention, drawing on Cornelissen\u0219s domain-interactive metaphor model. Topics that form the building blocks of individual attention research, including selective and divided attention, automatic versus controlled processes, attention and memory, attention and learning, are examined in terms of their applicability to the organizational context.", "title": "ORGANIZATIONAL ATTENTION: A METAPHOR FOR A CORE COGNITIVE PROCESS", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Eyal Yaniv", "D. Schwartz"], "summary": "INTRODUCTION Attention is a term commonly used in education, psychiatry, and psychology. Attention can be defined as an internal cognitive process by which one actively selects environmental information (i.e., sensation) or actively processes information from internal sources (i.e., stored memories and thoughts; Sternberg, 1996). In more general terms, attention can be defined as an ability to focus and maintain interest in a given task or idea, including managing distractions. Attention is selective by its nature. According to Pashler (1998, p. 37), \" The process of selecting from among the many potentially available stimuli is the clearest manifestation of selective attention. \" Why do firms respond to certain events or stimuli in their environment while neglecting others? It seems that organizations, just like individuals , have limited attention capacity. Hence, they must select from among the many potentially available stimuli and respond to these selected stimuli only. Organizational attention is defined as the socially structured pattern of attention by decision makers within the organization (Ocasio, 1997). Organizational attention, like human attention , is a limited resource: \" Attentional limits filter or screen incoming information such that a great deal of data pertinent to strategic decision may never get processed \" (2003) show that the extent to which CEOs (chief executive officers) are selective in their attention to sectors of the environment is a significant predictor of performance. Knowledge management (KM) models and process theories, almost without exception, incorporate a stage or phase in which a given knowledge item is brought to bear on a current decision or action. This stage, referred to alternatively as, is of crucial importance in any knowledge-management cycle. The flow of knowledge in and out of an awareness stage is not merely a function of the universe of available organizational memory or the technological tools available to filter and identify such knowledge. It is influenced to a large degree by organizational attention. The second area in which organizational attention is key is knowledge acquisition and creation as discussed by Ocasio (1997), and Yaniv and Elizur (2003). Successful knowledge management requires attention. Davenport and Volpel (2001) argues that attention is the currency of the information age. Knowledge consumers must pay attention to knowledge and become actively involved in the knowledge-transfer processes. This is particularly important when the knowledge to be received is tacit (Nonaka, 1994). Knowledge can be part of the organization's repository, however, if it does not \u2026", "title": "Organizational Attention", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Eyal Yaniv"], "summary": "Organizational attention is an underdeveloped construct that can account for a variety of organizational phenomena. Attention is the means by which individuals select and process a limited amount of input from the enormous amount of internal and environmental inputs bombarding the senses, memories and other cognitive processes. This article develops a coherent theory of organizational attention, drawing on Cornelissen's domain-interactive metaphor model. Topics that form the building blocks of individual attention research, including selective and divided attention, automatic versus controlled processes, attention and memory, attention and learning, are examined in terms of their applicability to the organizational context.", "title": "A METAPHOR FOR A CORE COGNITIVE PROCESS", "claims": null}, {"metadata": {"year": 2010}, "authors": ["L. C. D. M. Ferreira"], "summary": "textabstractOrganizational studies emphasizing the role of attention in organizational behavior depart from the idea that organizations, like individuals, have limited capacity to attend to environmental stimuli. The bounded capacity of the organizations to respond to stimuli is conditioned by the limited cognitions of individuals and by the limited capability of organizations to distribute, coordinate and integrate those cognitions. The cross-level nature of organizational attention, its dual character as both a process and an output, means that theories of attention afford interesting insights to explain organizational behavior.\nThis dissertation presents one conceptual and two empirical studies about organizational attention. In the conceptual study entitled \u201cAttention span: expanding the attention-based view to team, organizational and social movements levels\u201d, it is argued that attentional processes have functional equivalence at the team, organizational and social movements level. The study entitled \u201cWhen a thousand words are (not) enough: an empirical study of the relationship between firm performance and attention to shareholders\u201d, tests the power of the attention-based view combined with resource dependence theory to explain the relationship between financial performance and attention to shareholders. Finally, the study \u201cSense and sensibility: testing the effects of attention structures and organizational attention on financial performance\u201d tests the process model of situated attention by examining the effects of attention structures and the allocation of attention on organizational social responses and performance/\nTogether, these studies deepen and expand attentional perspectives on organizational behavior. Moreover, they renew scholars\u2019 interest in organizational attention, indicating some of the strengths and limitations of theories of attention and also revealing a prolific research stream.", "title": "Attention Mosaics: Studies of Organizational Attention", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Rex Wang Renjie", "P. Verwijmeren"], "summary": "This paper shows that exogenous director distraction affects board monitoring intensity and leads to a higher level of inactivity by management. We construct a firm-level director \"distraction'' measure by exploiting shocks to unrelated industries in which directors have additional directorships. Directors attend significantly fewer board meetings when they are distracted. \nFirms with distracted board members tend to be inactive and experience a significant decline in firm value. Overall, this paper highlights the impact of limited director attention on the effectiveness of corporate governance and the importance of directors in keeping management active.", "title": "Director Attention and Firm Value", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Rex Wang  Renjie", "Patrick  Verwijmeren"], "summary": "This paper shows that exogenous director distraction affects board monitoring intensity and leads to a higher level of inactivity by management. We construct a firm-level director \"distraction'' measure by exploiting shocks to unrelated industries in which directors have additional directorships. Directors attend significantly fewer board meetings when they are distracted. \nFirms with distracted board members tend to be inactive and experience a significant decline in firm value. Overall, this paper highlights the impact of limited director attention on the effectiveness of corporate governance and the importance of directors in keeping management active.", "title": "Director Attention and Firm Value", "claims": null}, {"metadata": {"year": 2011}, "authors": ["OcasioWilliam"], "summary": "Organizational theory and research has increased attention to the determinants and consequences of attention in organizations. Attention is not, however, a unitary concept but is used differently i...", "title": "Attention to Attention", "claims": null}, {"metadata": {"year": 2005}, "authors": ["M. John"], "summary": "Interruptions and distractions are a serious problem for many tasks, from programming a device to forming a battle plan, and from driving a car to monitoring airspaces. Interruptions and distractions can interfere with short term memory in planning and executing plans, and they can divert attention and reduce processing capacities for the detection and interpretation of significant events in dynamic situations. Attention management across tasks, and even within complex tasks, is a multi-faceted cognitive, perceptual, and social problem for users and display designers alike. The key to improving attention management is understanding the human, task, and environment issues and designing technologies that dovetail with and exploit that understanding. The research presentations in this session delve into several different facets of attention management and interruption. They analyze the issues, develop design principles, and evaluate alternative interface designs.", "title": "Interruptions, Distractions, and Attention Management: A Multi-Faceted Problem for Human Factors", "claims": null}], "query": "organizational attention diversion", "summary_abstract": "The collection of abstracts provides a multifaceted view of organizational attention diversion, highlighting its implications and underlying mechanisms. Yaniv (2011) and Yaniv and Schwartz (2011) emphasize that organizational attention is a limited resource, akin to individual attention, which necessitates selective processing of stimuli. This selective attention is crucial for effective knowledge management and decision-making within organizations. Ferreira (2010) expands on this by discussing the bounded capacity of organizations to process stimuli due to limited cognitive resources, which affects organizational behavior and performance. The research underscores the cross-level nature of organizational attention, suggesting that attentional processes are functionally equivalent across different organizational levels.\n\nWang and Verwijmeren (2017, 2018) provide empirical evidence on the consequences of attention diversion, showing that director distraction leads to reduced board monitoring and increased management inactivity, ultimately resulting in a decline in firm value. This highlights the critical role of focused attention in maintaining effective corporate governance.\n\nJohn (2005) discusses the broader implications of interruptions and distractions, noting their potential to disrupt cognitive processes and reduce the capacity for detecting and interpreting significant events. This underscores the importance of designing systems and environments that support effective attention management.\n\nCollectively, these studies illustrate that organizational attention diversion can significantly impact decision-making, knowledge management, and overall organizational performance. They highlight the necessity for organizations to manage attentional resources effectively to mitigate the adverse effects of distraction and maintain strategic focus.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2011}, "authors": ["Eyal Yaniv", "D. Schwartz"], "summary": "INTRODUCTION Attention is a term commonly used in education, psychiatry, and psychology. Attention can be defined as an internal cognitive process by which one actively selects environmental information (i.e., sensation) or actively processes information from internal sources (i.e., stored memories and thoughts; Sternberg, 1996). In more general terms, attention can be defined as an ability to focus and maintain interest in a given task or idea, including managing distractions. Attention is selective by its nature. According to Pashler (1998, p. 37), \" The process of selecting from among the many potentially available stimuli is the clearest manifestation of selective attention. \" Why do firms respond to certain events or stimuli in their environment while neglecting others? It seems that organizations, just like individuals , have limited attention capacity. Hence, they must select from among the many potentially available stimuli and respond to these selected stimuli only. Organizational attention is defined as the socially structured pattern of attention by decision makers within the organization (Ocasio, 1997). Organizational attention, like human attention , is a limited resource: \" Attentional limits filter or screen incoming information such that a great deal of data pertinent to strategic decision may never get processed \" (2003) show that the extent to which CEOs (chief executive officers) are selective in their attention to sectors of the environment is a significant predictor of performance. Knowledge management (KM) models and process theories, almost without exception, incorporate a stage or phase in which a given knowledge item is brought to bear on a current decision or action. This stage, referred to alternatively as, is of crucial importance in any knowledge-management cycle. The flow of knowledge in and out of an awareness stage is not merely a function of the universe of available organizational memory or the technological tools available to filter and identify such knowledge. It is influenced to a large degree by organizational attention. The second area in which organizational attention is key is knowledge acquisition and creation as discussed by Ocasio (1997), and Yaniv and Elizur (2003). Successful knowledge management requires attention. Davenport and Volpel (2001) argues that attention is the currency of the information age. Knowledge consumers must pay attention to knowledge and become actively involved in the knowledge-transfer processes. This is particularly important when the knowledge to be received is tacit (Nonaka, 1994). Knowledge can be part of the organization's repository, however, if it does not \u2026", "title": "Organizational Attention", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Eyal Yaniv"], "summary": "Organizational attention is an underdeveloped construct that can account for a variety of organizational phenomena. Attention is the means by which individuals select and process a limited amount of input from the enormous amount of internal and environmental inputs bombarding the senses, memories and other cognitive processes. This article develops a coherent theory of organizational attention, drawing on Cornelissen\u0219s domain-interactive metaphor model. Topics that form the building blocks of individual attention research, including selective and divided attention, automatic versus controlled processes, attention and memory, attention and learning, are examined in terms of their applicability to the organizational context.", "title": "ORGANIZATIONAL ATTENTION: A METAPHOR FOR A CORE COGNITIVE PROCESS", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Eyal Yaniv"], "summary": "Organizational attention is an underdeveloped construct that can account for a variety of organizational phenomena. Attention is the means by which individuals select and process a limited amount of input from the enormous amount of internal and environmental inputs bombarding the senses, memories and other cognitive processes. This article develops a coherent theory of organizational attention, drawing on Cornelissen's domain-interactive metaphor model. Topics that form the building blocks of individual attention research, including selective and divided attention, automatic versus controlled processes, attention and memory, attention and learning, are examined in terms of their applicability to the organizational context.", "title": "A METAPHOR FOR A CORE COGNITIVE PROCESS", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Rian Drogendijk", "H. Haq"], "summary": "In 1997, Ocasio attempted to highlight and revive research on attention in organizations by introducing the attention-based view of the firm (ABV), which specifically emphasizes the central role of...", "title": "Has the Attention-Based View Changed Research on Attention in Organizations? A Systematic Review", "claims": null}, {"metadata": {"year": 2011}, "authors": ["OcasioWilliam"], "summary": "Organizational theory and research has increased attention to the determinants and consequences of attention in organizations. Attention is not, however, a unitary concept but is used differently i...", "title": "Attention to Attention", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Longbing Cao", "Chengqi Zhang", "Ruwei Dai"], "summary": "Organization-oriented analysis acts as the key step and foundation in building organization-oriented methodology (OOM) to engineer multi-agent systems especially open complex agent systems (OCAS). A number of existing approaches target OOM, while they are incompatible with each other, and none of them is available as a solid and practical tool for engineering OCAS. This paper summarizes our investigation in building a unified framework for abstracting and analyzing OCAS organizations. Our organizationoriented framework, referred to as ORGANISED, integrating and expanding existing approaches, explicitly captures the main attributes in an OCAS. Following this framework, individual modelbuilding blocks are developed for all ORGANISED members; both visual and formal specifications are utilized to present an intuitive and precise analysis . The above techniques have been deployed in developing an agent service-based trading and mining support infrastructure.", "title": "Organization-Oriented Analysis of Open Complex Agent Systems", "claims": null}, {"metadata": {"year": 2013}, "authors": ["\u00c9. Rouby", "Catherine Thomas"], "summary": "We use longitudinal, qualitative data from a cement plant to develop a model of organizational attention elasticity (OAE). The model shows how coherent attention to emerging threats varies horizontally across different teams and vertically across different levels in the hierarchy. The data reveal the micro-processes of attention through which OAE develops, and how coherent attention is accomplished horizontally and vertically. By showing 'how' OAE and coherent attention intersect, this work identifies the underlying mechanisms of attention that are important to balance the tension between attention elasticity and attention coherence. These findings have implications for theories of organizational attention.", "title": "Individual and collective sensemaking in a complex environment: an attentional perspective", "claims": null}, {"metadata": {"year": 2013}, "authors": ["\u00c9. Rouby", "Catherine Thomas"], "summary": "We use longitudinal, qualitative data from a cement plant to develop a model of Organizational Attention Elasticity (OAE). The model shows how coherent attention to emerging threats varies horizontally across different teams and vertically across different levels in the hierarchy. The data reveal the micro-processes of attention through which OAE develops, and how coherent attention is accomplished horizontally and vertically. By showing \u2018how\u2019 OAE and coherent attention intersect, this work identifies the underlying mechanisms of attention that are important to balance the tension between attention elasticity and attention coherence. These findings have implications for theories of organizational attention.", "title": "Organizational Attention Elasticity: An Exploratory Case of Cement Production", "claims": null}], "query": "organizational attention definition Ocasio", "summary_abstract": "The concept of organizational attention, as defined by Ocasio (1997), is a socially structured pattern of attention by decision-makers within an organization. This definition emphasizes that organizational attention, much like human attention, is a limited resource that requires selective focus on certain stimuli while neglecting others (Yaniv and Schwartz, 2011). Ocasio's attention-based view of the firm (ABV) underscores the central role of attention in organizational decision-making processes (Drogendijk and Haq, 2020). Yaniv (2011) further elaborates on this by developing a coherent theory of organizational attention, drawing parallels with individual attention processes such as selective and divided attention, and applying these concepts to the organizational context. Additionally, Rouby and Thomas (2013) introduce the concept of Organizational Attention Elasticity (OAE), which explores how attention varies across different teams and hierarchical levels, highlighting the micro-processes that contribute to coherent attention within organizations. Collectively, these works illustrate the multifaceted nature of organizational attention and its critical role in shaping organizational behavior and decision-making.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2018}, "authors": ["Alka Ekka", "Neha Namdeo"], "summary": "Amylase is (E.C.3.2.1.1-1,4-alpha D-glucanohydrolase) an extracellular enzyme, which is involved in the starch processing industries where it breaks starch into simple sugar constituents.Amylase has extensive application in starch processing, brewing and sugar production, in textile industries and in detergent manufacturing processes. Interestingly, the first enzyme produced industrially was an amylase.In the present study, amylase producing bacteria were isolated from rice field, sugarcane field and sugarcane dump area and characterized for their morphological and biochemical properties. Then amylase activity of isolated bacterial cultures were determined and it was concluded that 3 (NN1, NN2, NN5)out of 6 bacterial colonies(NN1, NN2, NN3, NN4, NN5, NN6) were potent and their enzyme activity was more than other colonies. The potent colonies were also optimized for enzyme activity under certain conditions like different carbon sources, nitrogen sources, pH, incubation time and chlorides.Agro-industrial wastes were used as substrate for amylase production by Solid-State FermentationSSF) and we have found that wheat bran was the suitable substrate for amylase production.", "title": "Screening , Isolation and Characterization of Amylase Producing Bacteria and optimization for Production of Amylase", "claims": null}, {"metadata": {"year": 2021}, "authors": ["L. Garba", "M. Ibrahim", "E. Sahara", "M. T. Adamu", "S. Isa", "A. Yarma"], "summary": "Amylases are enzymes that are able to hydrolyse starch or glycogen molecules into polymers of glucose units. They have great potential applications in various industrial processes like in pharmaceutical, fermentation and food industries. Research on starch degrading enzymes has resulted into increased applications of amylases in different industrial processes. These enzymes occupy a greater space in the current biotechnological processes such as detergent, starch degradation, pharmaceutical, foodstuff, textile, and paper manufacturing. In fact, amylases constitute nearly 25% of the total sale of global enzymes. Amylases have been screened and identified from various sources, both eukaryotic and prokaryotic organisms such as animals, plants, fungi and bacteria, respectively. To further isolate novel amylases with enhanced desirable properties for such diverse industrial application, more organisms need to be screened. In this study, a total of 27 bacterial isolates were isolated from soil samples in Gombe metropolis. The bacteria were screened for amylase production using plate screening method. Each isolate was streaked onto a 1% starch agar plate and incubated for 24h at 37 \u00c2\u00b0C. The plates were covered with iodine solution and observed for positive amylase isolates based on the formation of clearing zones against the blue black background. The results confirmed eight (8) isolates of amylase-producing bacteria which include Bacillus subtilis, Escherichia coli, Streptococcus spp., Salmonella spp., Pseudomonas spp., Serratia spp., Proteus vulgaris, and Klebsiella spp. In conclusion, bacterial isolates capable of amylase production have been successfully screened and identified. This research may serve as a stepping stone to isolating functional amylase enzymes from these bacteria for promising industrial applications.", "title": "Preliminary Investigation of Amylase Producing-Bacteria from Soil in Gombe Metropolis", "claims": null}, {"metadata": {"year": 2012}, "authors": ["C. Shah"], "summary": "Amylases are among the most important industrial enzymes and also have great significance in Biotechnological studies. In this study cultural, morphological, and metabolic characteristics of the bacterial isolates were studied. Total 18 bacterial cultures were isolated from collected soil samples. Among 18 bacterial isolates, 14 isolates showed the amylolytic activity. These 18 isolate was identified according to Bergey\u2019s manual of systemic Bacteriology .These isolates related to Bacillus sp. The optimum pH for the growth of all the cultures was observed at pH 7. Submerged fermentation was carried out for the production of amylase was observed in the range of 0.045-1.35 U/min/mL. The maximum activity of amylase was 1.35 (U/min/mL) after 48 hours was recorded, have great significance.", "title": "Characterization of Amylase Producing Bacterial Isolates", "claims": null}, {"metadata": {"year": 2012}, "authors": ["L. Bin"], "summary": "Three bacterial strains Jz1\u3001Jz2\u3001Jz3 show high amylase activity,which were achieved from the soil near the rice and starch factories.The bacteria strains Jz1 and Jz3 are Bacillus after preliminary identification,while the bacterial strain Jz2 is Curtobacterium.The amylase activities of the three bacterial strains are 47.29 U /mL,48.48 U /mL and 49.74 U /mL respectively,determined by YoungJ.Y00 modified method.They will be well applied in the future.", "title": "The isolation of amylase-producing bacterial strains", "claims": null}, {"metadata": {"year": 1957}, "authors": ["C. Chen"], "summary": "From different sources, we have isolated 110 cultures belonging to the species Bacillus subtilisand Bacillus cereus. Of these, 2 strains formed considerably more a-amylase (S_(17) and S_(56) thanthe other strains. The optimum conditions for the cultivation of strains S_(17) were 35-37\u2103 and pH 4.4-8.Cultivated in the extract of peanutseed cake liquid medium, the strain formed amylase morethan that in the extracts of soyabean cake, cotton seed cake and wheat brain. The amylase wasmost active in pH 5-5.6 and at 60-65\u2103.", "title": "STUDIES ON THE AMYLASE-PRODUCING BACTERIA", "claims": null}, {"metadata": {"year": 2021}, "authors": ["T. Okunwaye", "P. Uadia", "B.O. Okogbenin", "E. Okogbenin", "D.C. Onyia", "J. U. Obibuzor"], "summary": "Amylases are enzymes that catalyze the hydrolysis of glycosidic bonds present in starch to release simple sugars. They are one of the most important enzymes in numerous commercial processes. In this investigation, fungal and bacterial strains from the following agro-industrial wastes were isolated and screened for amylolytic ability: soil from oil palm plantation, shea seed, date fruit, coconut meat, cassava effluent, cassava peel, cassava tubers, yam and potato tubers, starch medium, parboiled water from noodles and rice. The results revealed the presence of Geotrichum, Aspergillus, Penicillium, Trichoderma, Rhizopus and Fusarium spp. Five major genera of bacterial species namely Corynebacterium, Pseudomonas, Lactobacillus, Micrococcus and Bacillus were isolated and screened for amylase activity. Cassava soil had the highest heterotrophic bacterial count of 5.7 x105cfu/g and coconut meat waste had the lowest heterotrophic bacterial count of 1.3 x105cfu/g. All isolated microorganisms had the amylolytic ability. The fungal isolates had higher amylase activity when compared with the bacterial isolates. This investigation reveals organisms with high amylase activity.", "title": "Amylase-Producing Fungi and Bacteria Associated with Some Food Processing Wastes", "claims": null}, {"metadata": {"year": 2013}, "authors": ["S. Shivaji", "N. Sonune"], "summary": "Amylase is the most important industrial enzymes and having great significance in Biotechnological studies. Amylase production from bacteria is economical because the enzyme production rate is higher in bacteria as compared to other microorganism. The aim of this study was to isolate amylase producing bacteria from domestic wastewater. In this study cultural, morphological and biochemical characteristics of the bacterial isolates were studied. Total 40 bacterial cultures were isolated from collected domestic wastewater samples on nutrient agar medium. All isolates were screened for amylolytic activity by starch agar plate method. Among 40 bacterial isolates, only 8 isolates showed the best amylolytic activity. These isolates related to the genera Bacillus sp. and Paenibacillus sp.", "title": "Garode A. M.", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Ayalew Damte"], "summary": "A Thesis submitted to the School of Graduate Studies of Addis Ababa University \nin Partial Fulfillment of the Requirements for the Degree of Master of Science in \nBiotechnology.", "title": "Production of Alkaline Amylase from A Facultatively Anaerobic Alkaliphilic Bacteria.", "claims": null}], "query": "Amylase-Producing Bacteria", "summary_abstract": "The collection of studies highlights the significant role of amylase-producing bacteria in various industrial applications, emphasizing their potential for efficient enzyme production. Ekka and Namdeo (2018) isolated amylase-producing bacteria from agricultural fields and identified three potent strains with high enzyme activity, optimized under specific conditions. Garba et al. (2021) screened soil samples and identified eight bacterial isolates, including Bacillus subtilis and Escherichia coli, capable of producing amylase, suggesting their potential for industrial applications. Shah (2012) found that 14 out of 18 bacterial isolates from soil samples exhibited amylolytic activity, with Bacillus species showing optimal growth at pH 7. Bin (2012) identified three bacterial strains with high amylase activity, including Bacillus and Curtobacterium, from soil near rice and starch factories. Chen (1957) isolated Bacillus subtilis and Bacillus cereus strains with high amylase production, noting optimal conditions for enzyme activity. Okunwaye et al. (2021) isolated bacterial strains from agro-industrial wastes, identifying Bacillus and other genera with amylolytic ability, though fungal isolates showed higher activity. Shivaji and Sonune (2013) isolated amylase-producing bacteria from domestic wastewater, identifying Bacillus and Paenibacillus species with significant enzyme activity. Collectively, these studies underscore the diversity and industrial relevance of amylase-producing bacteria, particularly from the Bacillus genus, across various environments.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2006}, "authors": ["Junfeng Qiu"], "summary": "This paper extends the recent literature of \u201cliquidity and asset prices\u201d into monetary models by adding money-creating banks. We explain why the money creation function of banks is important to financial stability. We study an economy in which not all assets can be used to make payments, agents may have to sell assets when they need cash. Sale of assets can lead to low asset price because buyers have limited ability to buy assets. Banks can provide liquidity by creating and lending out new deposit. This will reduce the sale of assets and stabilize asset prices. We also compare two types of liquidity provision mechanisms: liquidity-risk sharing through coalitions and liquidity provision through money creation. We show that if people use mutual-fund-like non-bank coalitions to share liquidity risks, then the function of banks to relax the aggregate money constraint is important. Without banks, non-bank coalitions will not be able to insure against aggregate liquidity risks, they will only add more endogenous volatility to asset prices. We also model how the interest rate policy of the central bank is transmitted through the", "title": "Bank money, asset prices and the financial liquidity channel of monetary policy", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Pierpaolo Benigno", "S. Nistic\u00f2"], "summary": "This paper studies monetary policy in models where multiple assets have different liquidity properties: safe and \"pseudo-safe\" assets coexist. A shock worsening the liquidity properties of the pseudo-safe assets raises interest-rate spreads and can cause a deep recession cum deflation. Expanding the central bank's balance sheet fills the shortage of safe assets and counteracts the recession. Lowering the interest rate on reserves insulates market interest rates from the liquidity shock and improves risk sharing between borrowers and savers.", "title": "Safe Assets, Liquidity and Monetary Policy", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Javier Bianchi", "Saki Bigio"], "summary": "We develop a new framework for studying the implementation of monetary policy through the banking sector. Banks are subject to a maturity mismatch problem leading to precautionary holdings of reserves. Through various instruments, monetary policy alters tradeos banks face between lending, holding reserves, holding deposits and paying dividends. This translates into the real economy via eects on real interests and lending. We study how these instruments interact with shocks to the volatility in the payments system, bank losses, the demand for loans and with capital requirements. We use a calibrated version of the model to answer, quantitatively, why have banks held onto a substantial increase in reserves while not increasing lending since 2008.", "title": "Banks, Liquidity Management and Monetary Policy", "claims": null}, {"metadata": {"year": 2006}, "authors": ["P. He", "Lixin Huang", "Randall Wright"], "summary": "One important function of banks is to issue liabilities, like demand deposits, that are relatively safe and also liquid (usable as means of payment). We introduce risk of theft and a safe-keeping role for banks into monetary theory. This provides a general equilibrium framework for analyzing banking in historical and contemporary contexts. The model can generate concurrent circulation of cash and bank liabilities as media of exchange (inside and outside money), and yields novel policy implications. For example, negative nominal interest rates are feasible, and for some parameters optimal; for other parameters, strictly positive rates (inflation above the Friedman Rule) are optimal.", "title": "Money, Banking, and Monetary Policy", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Jin Cao", "G. Illing"], "summary": "In most banking models, money is merely modeled as medium for transaction, but in reality, money is also the most liquid asset for banks. Central banks do not only passively supply money to meet demand for transaction, as often assumed in these models, instead they also actively inject liquidity into market, taking banks\u2019 illiquid assets as collateral. We examine both roles of money in an integrated framework, in which banks are subject to aggregate illiquidity risk. With fixed nominal deposit contracts, the monetary economy with active central bank can replicate constrained efficient allocation. This allocation, however, cannot be implemented in market equilibrium without additional regulation: Due to moral hazard problems, banks invest excessively in illiquid assets, forcing the central bank to provide liquidity at low interest rates. We show that interest rate policy to reduce systemic liquidity risk on its own is dynamically inconsistent. Instead, the constrained efficient solution can be achieved by imposing ex ante liquidity coverage requirement.", "title": "Money in the Equilibrium of Banking1", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Jin Cao", "G. Illing"], "summary": "In most banking models, money is merely modeled as a medium of transactions, but in reality, money is also the most liquid asset for banks. Central banks do not only passively supply money to meet demand for transactions, as often assumed in these models, instead they also actively inject liquidity into market, taking banks\u2019 illiquid assets as collateral. We examine both roles of money in an integrated framework, in which banks are subject to aggregate illiquidity risk. With fixed nominal deposit contracts, the monetary economy with an active central bank can replicate constrained efficient allocation. This allocation, however, cannot be implemented in market equilibrium without additional regulation: Due to moral hazard problems, banks invest excessively in illiquid assets, forcing the central bank to provide liquidity at low interest rates. We show that interest rate policy to reduce systemic liquidity risk on its own is dynamically inconsistent. Instead, the constrained efficient solution can be achieved by imposing an ex ante liquidity coverage requirement.", "title": "Money in the Equilibrium of Banking", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Jin Cao"], "summary": "In most banking models, money is merely modeled as a medium of transactions, but in reality, money is also the most liquid asset for banks. Central banks do not only passively supply money to meet demand for transactions, as often assumed in these models, instead they also actively inject liquidity into market, taking banks\u2019 illiquid assets as collateral. We examine both roles of money in an integrated framework, in which banks are subject to aggregate illiquidity risk. With fixed nominal deposit contracts, the monetary economy with an active central bank can replicate constrained efficient allocation. This allocation, however, cannot be implemented in market equilibrium without additional regulation: Due to moral hazard problems, banks invest excessively in illiquid assets, forcing the central bank to provide liquidity at low interest rates. We show that interest rate policy to reduce systemic liquidity risk on its own is dynamically inconsistent. Instead, the constrained efficient solution can be achieved by imposing an ex ante liquidity coverage requirement.", "title": "Money in the Equilibrium of Banking 1", "claims": null}, {"metadata": {"year": 2007}, "authors": ["Junfeng Qiu"], "summary": "This thesis studies the role of the banking system in several aspects of the macroeconomy, including the likelihood of financial crises, volatility of asset prices and the transmission of monetary policy. In chapter 2, I analyze the accumulation of international reserves by central banks as insurance against financial crises. In the model, private banks borrow from foreign creditors to invest in domestic projects. By lending to banks in response to liquidity shocks, the central bank can reduce the liquidation of bank assets and lower the probability of bank runs. I show that the central bank will hold more reserves when private banks hold lower reserves. I also find that if the central bank can borrow additional loans from external sources, then domestic banks will hold fewer reserves by themselves. If the borrowing cost of external loan is very high, then the central bank may actually want to accumulate more reserves in order to avoid borrowing from external sources at high costs. In chapter 3, I show that the ability of banks to supply liquidity through money creation is important for financial stability. By supplying liquidity, banks can smooth the sale of assets and stabilize asset prices. I find that without elastic money, the attempt of non-bank mutual funds to raise cash by selling assets will only add more volatility into the market. Elastic money provided by banks can help mutual funds better smooth the consumption of their shareholders. In chapter 4, we consider the role of elastic money in an different environment where liquidity shocks affect agents asymmetrically. We show how money growth and interest rate policy can be used to adjust the consumption level of households. We find that the optimal policy is affected by the sensitivity of the supply price to the interest rate. When the supply price is more sensitive to the interest rate, it would be better to adopt a higher inflation rate, and to make the zero-bound of nominal interest rate less likely to be binding.", "title": "Essays on the financial system and the transmission of monetary policy", "claims": null}], "query": "monetary policy, bank assets", "summary_abstract": "The collection of papers provides a comprehensive examination of the interplay between monetary policy and bank assets, highlighting the critical role of banks in financial stability and the transmission of monetary policy. Qiu (2006, 2007) emphasizes the importance of banks' money creation function in providing liquidity, which stabilizes asset prices by reducing the need for asset sales during liquidity shortages. This function is crucial for financial stability, as it prevents excessive volatility in asset prices that could arise from non-bank coalitions attempting to manage liquidity risks.\n\nBenigno and Nistic\u00f2 (2013) explore how monetary policy can mitigate the effects of liquidity shocks on different types of assets. They suggest that expanding the central bank's balance sheet and adjusting interest rates on reserves can help counteract recessions caused by liquidity shortages, thereby stabilizing the economy.\n\nBianchi and Bigio (2014) introduce a framework that examines how monetary policy influences banks' decisions regarding lending, reserves, and deposits. Their model shows that monetary policy affects the real economy by altering the trade-offs banks face, which in turn impacts real interest rates and lending practices.\n\nCao and Illing (2015, 2021) discuss the dual role of money as both a transaction medium and a liquid asset for banks. They argue that central banks actively inject liquidity into the market by using banks' illiquid assets as collateral, which helps manage aggregate illiquidity risk. However, they note that without additional regulation, such as liquidity coverage requirements, banks may over-invest in illiquid assets, necessitating central bank intervention at low interest rates.\n\nOverall, these studies collectively underscore the significance of banks in the monetary policy framework, particularly in their capacity to create money and provide liquidity, which is essential for maintaining financial stability and effective policy transmission.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2010}, "authors": ["L. Puig", "J. J. Guerrero", "Kostas Daniilidis"], "summary": "In this paper we present a new way to compute a topological map using only orientation information. We exploit the natural presence of lines in man-made environments in dominant directions. We extract all the image lines present in the scene acquired by an omnidirectional system composed of 6 aligned cameras. From the parallel lines we robustly compute the three dominant directions using vanishing points. With this information we are able to align the camera with respect to the scene and to identify the turns in the trajectory. Assuming a Manhattan world where the changes of heading in the navigation are related by multiples 90 degrees. We also use geometrical image-pair constraints as a tool to identify the visual traversable nodes that compose our topological map. Experiments with an indoor sequence have been performed to validate this approach.", "title": "Topological map from only visual orientation information using omnidirectional cameras", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Hemanth  Korrapati", "Jonathan  Courbon", "Youcef  Mezouar"], "summary": "Topological maps are vital for fast and accurate localization in large environments. Sparse topological maps can be constructed by partitioning a sequence of images acquired by a robot, according to their appearance. All images in a partition have similar appearance and are represented by a node in a topological map. In this paper, we present a topological mapping framework which makes use of image sequence partitioning (ISP) to produce sparse maps. The framework facilitates coarse loop closure at node level and a finer loop closure at image level. Hierarchical inverted files (HIF) are proposed which are naturally adaptable to our sparse topological mapping framework and enable efficient loop closure. Computational gain attained in loop closure with HIF over sparse topological maps is demonstrated. Experiments are performed on outdoor environments using an omni-directional camera.", "title": "Topological Mapping with Image Sequence Partitioning", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Hemanth Korrapati", "Y. Mezouar", "P. Martinet"], "summary": "Topological maps are vital for fast and accurate localization in large environments. Sparse topological maps can be constructed by partitioning a sequence of images acquired by a robot, according to their appearance. All images in a partition have similar appearance and are represented by a node in a topological map. In this paper, we present a topological mapping framework which makes use of image sequence partitioning (ISP) to produce sparse maps. The framework facilitates coarse loop closure at node level and a finer loop closure at image level. Hierarchical inverted files (HIF) are proposed which are naturally adaptable to our sparse topological mapping framework and enable efficient loop closure. Computational gain attained in loop closure with HIF over sparse topological maps is demonstrated. Experiments are performed on outdoor environments using an omnidirectional camera.", "title": "Efficient Topological Mapping with Image Sequence Partitioning", "claims": null}, {"metadata": {"year": 2000}, "authors": ["Yves  Bertrand", "Guillaume  Damiand", "Christophe  Fiorio"], "summary": "In this paper we define the 3d topological map and give an optimal algorithm which computes it from a segmented image. This data structure encodes totally all the information given by the segmentation. More, it allows to continue segmentation either algorithmically or interactively. We propose an original approach which uses several levels of maps. This allows us to propose a reasonable and implementable solution where other approaches don't allow suitable solutions. Moreover our solution has been implemented and the theoretical results translate very well in practical applications.", "title": "Topological Encoding of 3D Segmented Images", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Yong Li", "Hongzhen Jin", "Hui Wang"], "summary": "A new approach for three-dimensional shape measurement is proposed. The corresponding point pairs between the projector and camera are identified by projecting a special pseudorandom sequence onto the surface of measured objects. The important properties of this sequence are expressed as follows: (1) any subsequence with a length of four symbols is unique; (2) there are no repeated symbols in any subsequence. The pseudorandom sequence is constructed with an alphabet of six symbols. These symbols are encoded with local spatial and temporal information by pixels in vertical strips of the projector plane. During measurement, the patterns for encoding are projected in turn and are captured with a camera. The projected pseudorandom sequence is retrieved by analysing the captured images. Then, the corresponding point pairs are worked out. Finally, the shapes of objects are reconstructed with triangulation. The experimental results reveal that a dense depth image with high resolution can be obtained. However, texture can be acquired simultaneously. The proposed method is robust.", "title": "Three-dimensional shape measurement using binary spatio-temporal encoded illumination", "claims": null}, {"metadata": {"year": 2008}, "authors": ["F. Werner", "J. Sitte", "F. Maire"], "summary": "In this paper we present a system for appearance-based topological mapping and localisation using vision data. The algorithms are designed for robots which are equipped with FPGA cameras. Such cameras do not provide the entire image to the robot but simple image features like colour histograms.", "title": "Visual topological mapping and localisation using colour histograms", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Ishit Mehta", "Manmohan Chandraker", "R. Ramamoorthi"], "summary": "We introduce a theoretical framework for differentiable surface evolution that allows discrete topology changes through the use of topological derivatives for variational optimization of image functionals. While prior methods for inverse rendering of geometry rely on silhouette gradients for topology changes, such signals are sparse. In contrast, our theory derives topological derivatives that relate the introduction of vanishing holes and phases to changes in image intensity. As a result, we enable differentiable shape perturbations in the form of hole or phase nucleation. We validate the proposed theory with optimization of closed curves in 2D and surfaces in 3D to lend insights into limitations of current methods and enable improved applications such as image vectorization, vector-graphics generation from text prompts, single-image reconstruction of shape ambigrams and multi-view 3D reconstruction.", "title": "A Theory of Topological Derivatives for Inverse Rendering of Geometry", "claims": null}, {"metadata": {"year": 2006}, "authors": ["Xuming He", "R. Zemel", "Volodymyr Mnih"], "summary": "We propose an approach to building topological maps of environments based on image sequences. The central idea is to use manifold constraints to find representative feature prototypes, so that images can be related to each other, and thereby to camera poses in the environment. Our topological map is built incrementally, performing well after only a few visits to a location. We compare our method to several other approaches to representing images. During tests on novel images from the same environment, our method attains the highest accuracy in finding images depicting similar camera poses, including generalizing across considerable seasonal variations. \u00a9 2007 Wiley Periodicals, Inc.", "title": "Topological map learning from outdoor image sequences", "claims": null}], "query": "topological mapping with encoded latent vectors from inverse camera projections", "summary_abstract": "The collection of papers provides a diverse exploration of topological mapping, with some insights relevant to the use of encoded latent vectors from inverse camera projections. Puig et al. (2010) discuss a method for computing topological maps using orientation information derived from vanishing points in omnidirectional camera images, which aligns with the concept of using camera projections to inform map creation. Korrapati et al. (2012, 2011) present a framework for sparse topological mapping through image sequence partitioning, which could be adapted to utilize encoded latent vectors for efficient loop closure and localization. Bertrand et al. (2000) introduce a 3D topological map derived from segmented images, emphasizing the encoding of segmentation information, which parallels the idea of encoding latent vectors for map construction. Mehta et al. (2023) propose a framework for differentiable surface evolution using topological derivatives, which could potentially be applied to refine topological maps through encoded latent vectors. He et al. (2006) focus on building topological maps using manifold constraints to relate images and camera poses, suggesting a method that could integrate encoded latent vectors to enhance map accuracy and adaptability. Collectively, these papers highlight various approaches to topological mapping that could be integrated with encoded latent vectors from inverse camera projections to improve map accuracy, efficiency, and adaptability.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2004}, "authors": ["LUShan"], "summary": "Anew method of immunization was discovered in the early 1990s. Several research groups independently demonstrated that direct inoculation of DNA plasmids coding for a specific protein antigen could elicit immune responses against that antigen[1-4].Since in theory the mRNA molecules also have the potential to be translated into the protein antigen, this vaccination approach was officially named by WHO as the nucleic acid vaccination even though the term DNA vaccine has been used more commonly in the literature. This novel approach is considered the fourth generation of vaccines after live attenuated vaccines, killed or inactivated vaccines and recombinant protein based subunit vaccines.", "title": "Nucleic Acid Vaccines", "claims": null}, {"metadata": {"year": 1996}, "authors": ["J B Ulmer", "D L Montgomery", "J J Donnelly", "M A Liu"], "summary": "The use of DNA and mRNA as vectors for immunization is a relatively recent development in the field of vaccines. The first paper demonstrating the efficacy of a DNA vaccine in an animal model of viral disease was published in 1993 (1). The rationale for using nucleic acids as vaccines came from the Initial observations that mtramuscular (im) injection of nonrephcating plasmid DNA expression vectors or mFWA-encoding reporter genes could result in the in vivo expression of proteins in mouse muscle cells (2). This ability to express proteins in vivo offers the opportunity to generate immune responses against foreign antigens encoded by the nucleic acid. In addition, both humoral and cell-mediated immune (CMI) responses, such as cytotoxic T-lymphocytes (CTL), can be induced. In general, CTL responses require endogenous expression of the antigen, such as during immunization with live viruses or replicating vectors, whereas subunit protein, polysaccharide conjugate, or inactivated virus vaccines generate humoral immune responses, but not CTL. Therefore, the technique of DNA injection has potential advantages over certain other vaccine technologies.", "title": "DNA vaccines.", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Liu Guoxia", "Jiang Yu", "Zhang Bin", "Zhang Yi", "Liu Xu", "Fan Zhongxue", "Qian Feng-qin"], "summary": "Nucleic acid vaccine,also called gene vaccine or DNA vaccine,is a newly-developed vaccine,which derived from gene therapy in the early 1990s.From then on it absorbs researchers' attention because of so many advantages,including its ability to induce both cellular and humoral immune responses,no way to spread virus and easy to deposit and transportation.Researchers have designed various strategies to enhance the DNA vaccine potency,including the selection of different DNA encoding-Ag,constructing different plasmid vectors(including selecting different promoter,enhancer and intron),using various adjuvants and different delivery strategies,which provide useful method to produce efficient and practical DNA vaccine.Some main animal infectious diseases,such as influenza,Newcastle disease and foot-and-mouth disease,had been studied deeply,which provide worthy candidate to study other diseases.", "title": "Progress on Nucleic Acid Vaccine in Animal", "claims": null}, {"metadata": {"year": 1994}, "authors": [], "summary": "\n One of the most active areas of vaccine research, nucleic acid vaccination is a new method of vaccination which involves taking a gene from a disease-causing virus or bacterium and injecting it into the person to be vaccinated in such a way that the person's cells produce vaccinating molecules (antigen) which provoke a protective immune response in the host against future infection by the pathogen. In effect, the foreign gene instructs the human body how to produce its own protective antigens against a given disease. This technique has been successfully tested in animal models, but is not yet ready for general use in humans. In principle, a single injection of many different foreign genes could be used to confer immunity against many different diseases. A meeting held last May at the Geneva headquarters of the World Health Organization confirmed current interest in the subject among both vaccine researchers and manufacturers. The following subjects are discussed: how the vaccination procedure is conducted, why bacterial DNA is used, why the DNA is injected into muscle tissue rather than into other tissues, what is revolutionary about nucleic acid vaccination, and potential drawbacks and concerns about the technique.\n", "title": "A Forum brief on nucleic acid vaccines.", "claims": null}, {"metadata": {"year": 1995}, "authors": ["F. Vogel", "N. Sarver"], "summary": "The use of nucleic acid-based vaccines is a novel approach to immunization that elicits immune responses similar to those induced by live, attenuated vaccines. Administration of nucleic acid vaccines results in the endogenous generation of viral proteins with native conformation, glycosylation profiles, and other posttranslational modifications that mimic antigen produced during natural viral infection. Nucleic acid vaccines have been shown to elicit both antibody and cytotoxic T-lymphocyte responses to diverse protein antigens. Advantages of nucleic acid-based vaccines include the simplicity of the vector, the ease of delivery, the duration of expression, and, to date, the lack of evidence of integration. Further studies are needed to assess the feasibility, safety, and efficacy of this new and promising technology.", "title": "Nucleic acid vaccines", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Weng Jian-xin"], "summary": "The nucleic acid vaccine not only causes the immune response of body fluid but also leads the high level cell immun to reply, cytotoxic T lymphocyte react especially.It has heavy advantage in the prevention and cure of the infection with virus,bacterium,parasite,etc. The nucleic acid vaccine is as a new biotechnology of developing in recent years, has already become one of the focuses of the vaccine research field, and get fast development. The experimental results indicates that the nucleic acid vaccine can be regarded as the prevention vaccine of the virus, bacterium or parasite and as treatment vaccine of infectious disease and non-infectious disease. Through introduction of the progress of studying nucleic acid vaccine in preventing and treating infectious diseases, such as virus, bacterium, parasite in recent years,and the constant development with molecule biotechnology, the further research and practice of the nucleic acid vaccine will demonstrate the new hope in improving mankind and animal health.", "title": "Progress on the Application of Nucleic Acid Vaccine", "claims": null}, {"metadata": {"year": 1995}, "authors": ["G J Waine", "D P McManus"], "summary": "The recent successful immunization of experimental animals using nucleic acids has provided a revolutionary new approach in vaccinology. In this article, Gary Waine and Don McManus examine the potential of nucleic acid vaccines for their effectiveness not only against infectious and parasitic organisms exhibiting an intracellular phase during their life cycle, but also against parasitic helminths, whose life cycle stages are either predominantly or completely extracellular.", "title": "Nucleic acids: vaccines of the future.", "claims": null}, {"metadata": {"year": 2022}, "authors": ["W. Devi", "Supriya Kammar", "S. Logesh", "Gareth Dsouza", "Thotegowdanapalya Mohan", "Charukesi Rajulu"], "summary": "DNA vaccines, a type of nucleic acid vaccine, have emerged as one of the recent developments in immunology and recombinant DNA technology, offering great potential in terms of ease of manufacture, maintenance, and safety compared to conventional vaccines. Since their discovery, DNA vaccines have evolved immensely, resulting in the employment of new techniques such as gene guns, in vivo electroporation, and nanoparticle-based carriers to enhance the delivery of vaccines into the cells. Starting from the failures of the first-generation DNA vaccines to the near-success second-generation vaccines, several strategies including codon optimization, antigen design, and heterologous prime-boost have greatly helped in strengthening the vaccine's immunogenicity. The purpose of developing these third-generation vaccines is primarily to solve existing medical complications like cancer, along with therapeutic uses, to address health problems, and to aid the rapid eradication of sudden global outbreaks of infectious diseases including Ebola and COVID-19. In this review, we focus on the recent developments and strategies employed to improve the efficacy of DNA vaccines and discuss their mechanism of action, potential concerns, progress achieved, and a brief update on its clinical applications.", "title": "Nucleic acid vaccines: A rising antidote for the future", "claims": null}], "query": "History of Nucleic Acid Vaccines", "summary_abstract": "The history of nucleic acid vaccines, which includes both DNA and mRNA vaccines, marks a significant evolution in immunization technology. This innovative approach was first explored in the early 1990s, with the World Health Organization officially naming it nucleic acid vaccination (LUShan, 2004). The initial breakthrough came with the demonstration that direct inoculation of DNA plasmids could elicit immune responses, a concept that was first proven effective in animal models in 1993 (Ulmer et al., 1996). This method allows for the in vivo expression of proteins, thereby inducing both humoral and cell-mediated immune responses, including cytotoxic T-lymphocyte responses, which are typically associated with live virus vaccines (Ulmer et al., 1996; Vogel & Sarver, 1995).\n\nNucleic acid vaccines offer several advantages over traditional vaccines, such as the ability to induce comprehensive immune responses without the risk of spreading the virus, ease of storage and transportation, and the potential for rapid development and deployment (Liu et al., 2005; Weng, 2005). These vaccines have been shown to be effective against a range of pathogens, including viruses, bacteria, and parasites, and have been explored for both preventive and therapeutic applications (Weng, 2005; Waine & McManus, 1995).\n\nThe development of DNA vaccines has progressed through several generations, with recent advancements focusing on improving delivery methods and enhancing immunogenicity through techniques like gene guns, in vivo electroporation, and nanoparticle-based carriers (Devi et al., 2022). These advancements aim to address medical challenges such as cancer and infectious disease outbreaks, including Ebola and COVID-19 (Devi et al., 2022).\n\nOverall, nucleic acid vaccines represent a revolutionary step in vaccinology, offering a versatile and promising tool for combating a wide array of diseases (Waine & McManus, 1995; Vogel & Sarver, 1995). Further research and development continue to enhance their efficacy and broaden their clinical applications.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2004}, "authors": ["Brian  Fynes", "Se\u00e1n de B\u00farca", "Donna  Marshall"], "summary": "Abstract Environmental uncertainty is a fact of life in today's supply chains. In this paper we develop a model of environmental uncertainty, supply chain (SC) relationship quality and SC performance. We use data from the electronics sector in Ireland to test our model. Our results provide mixed support for the model, with the moderating role of both demand and supply uncertainty being supported, but technological uncertainty not supported. We reflect on these findings and suggest a research agenda based on our results.", "title": "Environmental uncertainty, supply chain relationship quality and performance", "claims": null}, {"metadata": {"year": 2017}, "authors": ["R.  Sreedevi", "Haritha  Saranga"], "summary": "Abstract In order to remain competitive in the market, firms are forced to expand their product offerings and offer high levels of customization, bringing about high uncertainty in their supply chain. Firms that face high environmental uncertainty are increasingly facing higher risks in terms of supply disruptions, production and delivery delays that ultimately result in poor operational performance. This study aims at understanding the antecedents of supply chain operational risk faced by firms and the conditions under which such risks can be mitigated. Using Indian data from the sixth edition of International Manufacturing Strategy Survey (IMSS) and structural equation modeling, we investigate the relationships between environmental uncertainty and supply chain risk and the moderating effect of supply chain flexibility. We identify appropriate types of flexibility to mitigate the three major aspects of supply chain risk: supply risk, manufacturing process risk and delivery risk. Our empirical investigation reveals that uncertainty in the supply chain leads to high supply chain risk; and in uncertain environments, supply and manufacturing flexibility help in reducing the supply and manufacturing process risks respectively. However, our results also indicate that, in emerging markets such as India where logistic infrastructure is less developed, internal capabilities alone may not be sufficient in reducing supply chain delivery risk. Our findings not only contribute towards filling certain gaps in the supply chain risk management literature, but also provide practicing managers and researchers a better understanding of the types of flexibility that can mitigate supply chain risk in different business environments.", "title": "Uncertainty and supply chain risk: The moderating role of supply chain flexibility in risk mitigation", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Nitya P. Singh"], "summary": "ABSTRACT With increasing global exposure, organisations have started to witness supply chain risks that they traditionally were not exposed to. This article therefore attempts to answer the research question of how environmental uncertainty within a firm\u2019s supply chain, impacts firm financial performance. We further evaluate the role of supply chain risk management practices in mitigating the negative impact of such disruption events. To answer the research question, we conduct a literature review and develop the corresponding hypothesis. We test our hypothesis using both primary and secondary data. The results show that environmental uncertainty has a negative impact on firm financial performance, with entrepreneurial managerial capitalism mediating the impact. However, organisations that adopt macro and network supply chain risk management practices are able to improve managerial decision making frame, and mitigate the negative impact of environmental uncertainty on firm financial performance. The article concludes with our findings, along with managerial and practitioner implications of the research.", "title": "Managing environmental uncertainty for improved firm financial performance: the moderating role of supply chain risk management practices on managerial decision making", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Odkhishig Ganbold", "Y. Matsui"], "summary": "Today\u2019s ever-changing business environment is often described to be highly competitive, dynamic and complex. Customers are demanding more variability, better quality, higher reliability and faster delivery. Organizations are being faced with more uncertainties from its task environment than before. In order to respond to the uncertainties, organizations are internalizing fewer resources and capabilities, while increasing their integration with partners in the supply chain. Drawing on the resource-dependence theory, this study aims to examine the impact of environmental uncertainty on supply chain integration initiatives. Environmental uncertainty is considered in terms of three types, namely, supply uncertainty, demand or customer uncertainty, and technology uncertainty, based on its sources. Supply chain integration is comprised of internal integration, customer integration, and supplier integration. Based on the empirical study with 108 Japanese manufacturing firms, this study makes significant contributions to the knowledge base and provides theoretical and practical implications.", "title": "IMPACT OF ENVIRONMENTAL UNCERTAINTY ON SUPPLY CHAIN INTEGRATION", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Stephan  Vachon", "Sara  Hajmohammad"], "summary": "This manuscript examines the impact of supply chain uncertainty on environmental management spending in manufacturing plants. Building on the attention-based view of the firm (ABV), the basic premise is that with increased uncertainty in the supply chain, managers\u2019 attention to environmental management lessens which in turn leads to (i) fewer resources devoted to green issues within the plant and (ii) a bias to use resources toward less disruptive pollution control approaches rather than pollution prevention approaches. Data from a survey of 251 Canadian manufacturing plants was used to test the link between the level of uncertainty in the supply chain and environmental management decisions. The results indicate that supply chain uncertainty does not have a substantial impact on the level of environmental spending in a plant but has a substantial and significant impact on the allocation of the spending between pollution prevention and pollution control. More particularly, as supply chain uncertainty increases, organizations shift their resources away from pollution prevention to favor pollution control approaches.", "title": "Supply chain uncertainty and environmental management", "claims": null}, {"metadata": {"year": 2021}, "authors": ["R. A. Inman", "K. Green"], "summary": "PurposeToday's businesses are facing a world that is more complex, turbulent and unpredictable than in the past with increasing levels of environmental complexity. Rather than proposing environmental uncertainty as a mediator/moderator of the relationship between agility and performance as others have done, the authors offer an alternative view where supply chain agility is seen as mediating the relationship between environmental uncertainty and supply chain performance.Design/methodology/approachThe authors propose that supply chain agility is a response to the effects of environmental uncertainty and, as such, environmental uncertainty should be seen as a driver of supply chain agility. Few studies test the direct relationship between uncertainty and supply chain performance, and none simultaneously test for agility's mediation and moderation effect between environmental uncertainty and agility.FindingsThe model was statistically assessed using partial-least-squares structural equation modeling (PLS/SEM) by analyzing survey data from manufacturing managers in 136 US firms. The study results did not indicate a significant relationship between environmental uncertainty and supply chain performance. However, the authors did find a significant positive relationship between agile manufacturing and supply chain performance using measures that were primarily operations-centered rather than financial. Additionally, the authors found that agile manufacturing fully mediates the relationship between environmental uncertainty and supply chain performance.Originality/valueThe authors\u2019 model, though simple, provides a base for future research for them and other researchers who can incorporate other impacting variables into the model. The study results show that uncertainty can be a force for good and that utilizing agile manufacturing can be a new source of opportunity.", "title": "Environmental uncertainty and supply chain performance: the effect of agility", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Erdin\u00e7 Ko\u00e7", "Muhammet Burak Deliba\u015f", "Yaprak Anadol"], "summary": "In this study, the direct effect of environmental uncertainty on competitive advantage and its indirect effect through the sequential mediator variables of supply chain integration and supply chain agility were investigated. The sample of the study consists of company managers operating in the manufacturing sector in Turkey. An online survey was sent to company managers through connections established on LinkedIn and an analysis was carried out with the data collected from 414 participants. As a result of the analysis, it has been determined that environmental uncertainty has a direct, significant and positive effect on competitive advantage. In addition, the results of the research show that supply chain integration and supply chain agility have a partial mediating role in the relationship between environmental uncertainty and competitive advantage. According to the results of this study, in conditions of high environmental uncertainty, companies can increase their supply chain agility capabilities by establishing a more integrated structure with their supply chain partners, and thus gain a unique competitive advantage over their competitors. It has been observed that the relationships between the concepts, which are the subject of the study, have been investigated separately in different studies in the literature. This study will contribute to the literature by investigating the relationships between concepts in a holistic way.", "title": "Environmental Uncertainties and Competitive Advantage: A Sequential Mediation Model of Supply Chain Integration and Supply Chain Agility", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Dehui Xu", "Li Zhao", "Gang Li", "Linyan Sun"], "summary": "Supply chain integration nowadays is considered as an important approach to building and sustaining advantages. Many previous empirical researches focus on the effect of supply chain integration on the firms' performance, yet, the antecedence of supply chain integration is still largely unknown. This paper investigates the relationship between environmental uncertainty (including demand uncertainty, supply uncertainty and technology uncertainty) and supply chain integration (including customer integration, supplier integration and internal integration) with 139 samples from Chinese manufacturing industry. The results reveal both supply uncertainty and technology uncertainty significantly influence supply chain integration; the effect of supply uncertainty is negative, while that of technology uncertainty is positive. However, demand uncertainty just has a significant effect on internal integration. The relationship between demand uncertainty and external integration (including customer integration and supplier integration) is mediated by the internal integration.", "title": "The effect of environmental uncertainty on supply chain integration in Chinese manufacturing industry", "claims": null}], "query": "\"Environmental uncertainty would moderate the relationship between demand risk and supply chain disruption impact such that the relationship would become stronger when environmental uncertainty is high.\"", "summary_abstract": "The collection of papers provides insights into the moderating role of environmental uncertainty on the relationship between demand risk and supply chain disruption impact. Fynes et al. (2004) found mixed support for the moderating role of environmental uncertainty, with demand and supply uncertainty being significant factors, though technological uncertainty was not supported. Sreedevi and Saranga (2017) highlighted that high environmental uncertainty increases supply chain risks, including supply, manufacturing, and delivery risks, suggesting that flexibility in supply and manufacturing can mitigate these risks, particularly in uncertain environments. Singh (2020) noted that environmental uncertainty negatively impacts firm financial performance, but effective supply chain risk management practices can mitigate these effects. Ganbold and Matsui (2017) emphasized that environmental uncertainty, particularly demand and supply uncertainty, impacts supply chain integration, which is crucial for managing disruptions. Inman and Green (2021) proposed that supply chain agility, driven by environmental uncertainty, mediates the relationship between uncertainty and supply chain performance, suggesting that agility can be a strategic response to uncertainty. Ko\u00e7 et al. (2022) found that environmental uncertainty positively affects competitive advantage through supply chain integration and agility, indicating that high uncertainty can enhance competitive positioning if managed well. Xu et al. (2010) demonstrated that demand uncertainty significantly affects internal integration, which in turn influences external integration, highlighting the complex interplay between different types of uncertainty and supply chain strategies. Collectively, these studies suggest that environmental uncertainty indeed strengthens the relationship between demand risk and supply chain disruption impact, particularly when firms leverage integration and agility to manage these challenges.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2022}, "authors": ["Karolina A. Serhan", "Tamara L. Kemp"], "summary": "Summary: Phyllodes tumors are rare fibroepithelial breast tumors representing less than 1% of all breast malignancies, with an extremely uncommon presence in the pediatric population.1 Although prognosis is favorable following excision given their indolent course, they often grow rapidly and frequently recur. As such, they can present unique oncologic and reconstructive challenges. Herein we present a case of a malignant phyllodes tumor in an 11-year-old girl treated with total skin-sparing mastectomy and adjustable saline implant, and explore the reconstructive challenges of this unique case.", "title": "Immediate Breast Reconstruction in an 11-year-old Girl with a Large Malignant Phyllodes Tumor", "claims": null}, {"metadata": {"year": 2014}, "authors": ["E. Houseman", "T. Ince"], "summary": "Historically, breast cancer classification has relied on prognostic subtypes. Thus, unlike hematopoietic cancers, breast tumor classification lacks phylogenetic rationale. The feasibility of phylogenetic classification of breast tumors has recently been demonstrated based on estrogen receptor (ER), androgen receptor (AR), vitamin D receptor (VDR) and Keratin 5 expression. Four hormonal states (HR0\u20133) comprising 11 cellular subtypes of breast cells have been proposed. This classification scheme has been shown to have relevance to clinical prognosis. We examine the implications of such phylogenetic classification on DNA methylation of both breast tumors and normal breast tissues by applying recently developed deconvolution algorithms to three DNA methylation data sets archived on Gene Expression Omnibus. We propose that breast tumors arising from a particular cell-of-origin essentially magnify the epigenetic state of their original cell type. We demonstrate that DNA methylation of tumors manifests patterns consistent with cell-specific epigenetic states, that these states correspond roughly to previously posited normal breast cell types, and that estimates of proportions of the underlying cell types are predictive of tumor phenotypes. Taken together, these findings suggest that the epigenetics of breast tumors is ultimately based on the underlying phylogeny of normal breast tissue.", "title": "Normal Cell-Type Epigenetics and Breast Cancer Classification: A Case Study of Cell Mixture\u2013Adjusted Analysis of DNA Methylation Data from Tumors", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Meher Charfi", "S. Ka", "A. Dem"], "summary": "No abstract.", "title": "Breast reconstruction after breast cancer", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Benjamin Yongcheng Tan", "Nur Diyana Md Nasir", "Huan Ying Chang", "Cedric Chuan Young Ng", "Peiyong  Guan", "Sanjanaa  Nagarajan", "Vikneswari  Rajasegaran", "Jing Yi Lee", "Jing Quan Lim", "Aye Aye Thike", "Bin Tean Teh", "Puay Hoon Tan"], "summary": "Breast fibroepithelial lesions (FELs) encompass the common fibroadenoma (FA) and relatively rare phyllodes tumour (PT); the latter entity is usually classified as benign, borderline or malignant. Intratumoural heterogeneity is frequently present in these tumours, making accurate histologic evaluation challenging. Despite their rarity, PTs are an important clinical problem due to their propensity for recurrence and, in the case of malignant PT, metastasis. Surgical excision is the mainstay of management. Recent work has uncovered myriad genetic alterations in breast FELs. In this study, exome sequencing was performed on seven cases of morphologically heterogeneous breast FELs, including FAs, PTs of all grades, and a case of metaplastic spindle cell carcinoma arising in PT, in order to elucidate their intratumoural genetic repertoire. Gene mutations identified encompassed cell signalling, tumour suppressor, DNA repair and cell cycle regulating pathways. Mutations common to multiple tumour regions generally showed higher variant allele frequency. Frequent mutations included MED12 , TP53 , RARA and PIK3CA . Histological observations of increased cellular density and pleomorphism correlated with mutational burden. Phylogenetic analyses revealed disparate pathways of possible tumour progression. In summary, histological heterogeneity correlated with genetic changes in breast FELs.", "title": "Morphologic and genetic heterogeneity in breast fibroepithelial lesions\u2014a comprehensive mapping study", "claims": null}, {"metadata": {"year": 2020}, "authors": ["J. Kutasovic", "A. M. McCart Reed", "A. Sokolova", "S. Lakhani", "P. Simpson"], "summary": "Breast cancer is a remarkably complex and diverse disease. Subtyping based on morphology, genomics, biomarkers and/or clinical parameters seeks to stratify optimal approaches for management, but it is clear that every breast cancer is fundamentally unique. Intra-tumour heterogeneity adds further complexity and impacts a patient\u2019s response to neoadjuvant or adjuvant therapy. Here, we review some established and more recent evidence related to the complex nature of breast cancer evolution. We describe morphologic and genomic diversity as it arises spontaneously during the early stages of tumour evolution, and also in the context of treatment where the changing subclonal architecture of a tumour is driven by the inherent adaptability of tumour cells to evolve and resist the selective pressures of therapy.", "title": "Morphologic and Genomic Heterogeneity in the Evolution and Progression of Breast Cancer", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Kathleen  Sprouffske", "Grainne  Kerr", "Cheng  Li", "Anirudh  Prahallad", "Ramona  Rebmann", "Verena  Waehle", "Ulrike  Naumann", "Hans  Bitter", "Michael R Jensen", "Francesco  Hofmann", "Saskia M Brachmann", "St\u00e9phane  Ferretti", "Audrey  Kauffmann"], "summary": "Graphical abstract", "title": "Genetic heterogeneity and clonal evolution during metastasis in breast cancer patient-derived tumor xenograft models", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Agla Jael Rubner Fridriksdottir", "Ren\u00e9  Villadsen", "Thorarinn  Gudjonsson", "Ole William Petersen"], "summary": "Recent genome-wide expression analysis of breast cancer has brought new life to the classical idea of tumors as caricatures of the process of tissue renewal as envisioned by Pierce and Speers (Cancer Res 1988;48:1996\u20132004) more than a decade ago. The search for a cancer founder cell or different cancer founder cells is only possible if a hierarchy of differentiation has been established for the particular tissue in question. In the human breast, the luminal epithelial and myoepithelial lineages have been characterized extensively in situ by increasingly elaborate panel of markers, and methods to isolate, culture, and clone different subpopulations have improved dramatically. Comparisons have been made with the mouse mammary gland in physiological three-dimensional culture assays of morphogenesis, and the plasticity of breast epithelial cells has been challenged by immortalization and transformation. As a result of these efforts, several candidate progenitor cells have been proposed independently of each other, and some of their features have been compared. This research has all been done to better understand breast tissue homeostasis, cell-type diversification in general and breast cancer evolution in particular. The present review discusses the current approaches to address these issues and the measures taken to unravel and maintain cell type diversification for further investigation.", "title": "Maintenance of Cell Type Diversification in the Human Breast", "claims": null}, {"metadata": {"year": 2018}, "authors": ["M. Ejjiyar", "M. Elgueouatri", "Mehdi Sahibi", "Abdelkoddous Bhihi", "I. Yafi", "M. Mahrouch", "Moulay", "D. Elamrani", "Y. Benchamkha"], "summary": "Article History Received: 19.11.2018 Accepted: 28.11.2018 Published: 30.12.2018 Abstract: Phyllodes tumors of the breast are rare fibroepithelial tumors in their malignant form, the gravity lies in high risk of local recurrence. We report two observations of patients admitted for the management of local recurrence of phyllode breast tumor. The average age of our patients was 39 years old. The size of the tumor ranged from 10 cm to 30 cm in diameter, with a localization exclusively on the right. Surgical management consisted of a enlarged tumor resection, followed by radiotherapeutic complement. Histologically, it was a phyllode tumor initially benign with slowly progressive evolution, to a phyllode sarcoma at one patient, and to a liposarcoma in the other. The evolution towards a malignant transformation can be explained in both cases by incomplete previous excisional gestures. This underlines the importance of early diagnosis and surgical management responding to oncological rules.", "title": "Phyllode Tumors: Breast Tumors with a High Potential for Recurrence- About Two Cases", "claims": null}], "query": "phylogeny reconstruction in breast cancer", "summary_abstract": "The collection of papers provides insights into the phylogeny reconstruction in breast cancer, highlighting the complexity and diversity of the disease. Houseman and Ince (2014) propose a phylogenetic classification of breast tumors based on hormonal receptor expression, suggesting that breast tumors magnify the epigenetic state of their cell-of-origin, which is predictive of tumor phenotypes. This approach underscores the relevance of phylogenetic classification in understanding the epigenetic landscape of breast cancer.\n\nTan et al. (2020) explore the genetic heterogeneity within breast fibroepithelial lesions (FELs), including phyllodes tumors, through exome sequencing. Their phylogenetic analyses reveal diverse pathways of tumor progression, correlating histological heterogeneity with genetic changes. This study emphasizes the role of phylogenetic analysis in elucidating the genetic repertoire and progression pathways of breast tumors.\n\nFridriksdottir et al. (2005) discuss the importance of understanding the hierarchy of differentiation in breast tissue to identify cancer founder cells. They highlight the characterization of luminal epithelial and myoepithelial lineages, which is crucial for understanding breast cancer evolution and the diversification of cell types.\n\nCollectively, these studies demonstrate that phylogeny reconstruction in breast cancer involves understanding the genetic, epigenetic, and cellular lineage complexities that contribute to tumor heterogeneity and progression. The integration of phylogenetic approaches offers a deeper understanding of breast cancer's evolutionary dynamics, aiding in the development of more targeted therapeutic strategies.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2019}, "authors": ["Tarmo  Tuisk", "Gunnar  Prause"], "summary": "Since the 1st of January 2013 the public transport is free for residents of the City of Tallinn. People registered outside Tallinn who want to use the public transport have to pay for their trips. Parallel, Tallinn City Governance raised the parking tariffs in the city centre which caused an increase of using of the public transport by 10% and a decrease of car traffic in the city centre by 6%. Currently, the greening of urban transport enjoys a high rank on the political agenda in a lot of European and even some cities are thinking to follow the example of Tallinn. Literature review reveals that scientific papers are focussing mainly on ecological aspects whereas socio-economic studies are neglected. The paper will present insights in social, economic and political aspects which are related to the experience of free public transport in Tallinn. Furthermore, the paper will highlight the role of digitalisation in public transport and discuss the influence of the Estonian e-governance system for the success of the Tallinn case.", "title": "Socio-Economic Aspects of Free Public Transport", "claims": null}, {"metadata": {"year": 2016}, "authors": ["D. Gabald\u00f3n-Estevan"], "summary": "Urban areas are of increasing relevance when it comes to sustainability. \u2022\u00a0\u00a0\u00a0\u00a0\u00a0 First, about half of the world\u2019s population now lives in cities (increasing to 60% by 2030). \u2022\u00a0\u00a0\u00a0\u00a0\u00a0 Second, cities are nowadays responsible for levels of resource consumption and waste generation that are higher beyond their share on world population. \u2022\u00a0\u00a0\u00a0\u00a0\u00a0 Third, cities are more vulnerable to disruptive events that can lead to restrictions on the provision of resources and to changes on the environment caused by climate change. \u2022\u00a0\u00a0\u00a0\u00a0\u00a0 And fourth, because they concentrate key resources (political, social, cultural\u2026), cities are seen as strategic scenarios where to experiment and develop solutions to cope with the prevailing sustainability challenges driven by the major social and environmental transformations. Urban agglomerations can be seen as complex innovation systems where human activities are shaped in order to transform societies towards sustainable development. For this paper, we focus on the case of an environmental innovation regarding transport policy, the implementation of the fare-free policy on public transport for all inhabitants of Tallinn, Estonia. Tallinn, with 414,000 inhabitants in 2015, is the capital of Estonia and the largest city in the country. Over the last two decades the share of public transport trips decreased dramatically. After a public opinion poll in 2012, in which over 75% of the participants voted for a fare-free public transportation system (FFPTS) in Tallinn, the new policy was implemented on 1st January 2013. From that date on inhabitants of Tallinn could use all public transport services (busses, trams, trolly-busses) operated by city-run operators for free. Later the fare-free system was implemented also on trains within Tallinn. In this paper we analyze the context, in which this policy was implemented, the main characteristics of its implementation and its actual situation. DOI: http://dx.doi.org/10.4995/CIT2016.2016.3532", "title": "Environmental innovation through transport policy. The implementation of the free fare policy on public transport in Tallinn, Estonia", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Daniel Baldwin Hess"], "summary": "Abstract Among many possible interventions in public transport finance and policy designed to enhance the attractiveness of riding public transport, one of the most extreme, which is seldom implemented, is the elimination of passenger fares, effectively making public transport \u201cfree\u201d for riders (with operating costs paid from other funding sources). This article describes a fare-free public transport program in Tallinn, Estonia, launched in 2013, which has exhibited lower-than-expected increases in ridership. Evaluations of Tallinn\u2019s fare-free public transport program are presented and synthesized, with a focus on program goals and how goals are met through program performance. Findings suggest certain flaws limit the program\u2019s potential success since the program design is misaligned with its primary stated goals, and several program goals relating to external effects of fare reform cannot be evaluated. Although it would be valuable for transport managers in other cities to learn about this experience, the Tallinn fare-free public transport program provides scant transferable evidence about how such a program can operate outside of a politicized context, which was crucial to its implementation in Estonia.", "title": "Decrypting fare-free public transport in Tallinn, Estonia", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Derek Galey"], "summary": "The City of Tallinn, capital of Estonia, with a population of 420,000, recently became the world's largest municipality offering free public transportation. Tourists still have to pay to ride the city's bus, trolley, and tram network, but registered residents\u2014including a large population of Russian-speaking non-citizens\u2014only have to tap their municipal transit cards once onboard. Tallinn's leadership has justi\ufffd\u8000ed the policy on environmental and social grounds\u2014namely, reducing carbon dioxide emissions and providing equal rights to freedom of movement. Although only 26% of trips in Tallinn utilize private cars, private transportation accounts for 60% of the city's carbon dioxide emissions. Public transportation, which provides 40% of trips in Tallinn, accounts for only 6-7% of the city's total emissions. On an annual municipal public transport satisfaction survey from 2010, 49% of the respondents were most unsatis- \ufffd\u8000ed with fares, followed by crowding (29%) and frequency (21%). (Cats, Susilo, and Eliasson 2012, 3-4) The city's government responded by calling a March 2012 referendum, in which 75% of voters supported free public transportation. In contrast with past experiences with free public transportation in other cities, preliminary results indicate a \"relatively small increase in passenger demand\" of only 3% citywide in the three-month period after implementation (Cats, Susilo, and Reimal 2014, 5). Notably, however, passenger counts increased 10% in Lasnamae, a populous and dense housing district with a price-sensitive population and many Russian-speaking residents.", "title": "Free Public Transportation for Residents of Tallinn", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Derek Galey"], "summary": "The City of Tallinn, capital of Estonia, with a population of 420,000, recently became the world\u2019s largest municipality offering free public transportation. Tourists still have to pay to ride the city\u2019s bus, trolley, and tram network, but registered residents\u2014including a large population of Russian-speaking non-citizens\u2014only have to tap their municipal transit cards once onboard. This article presents a qualitative account of the world\u2019s largest free public transporta- tion experiment to date. The results challenge and inform the conventional measures and objectives of transportation experts. The analysis is meant to complement the existing literature surveying free public transportation experiments and evaluating transportation pricing schemes.", "title": "License to Ride: Free Public Transportation for Residents of Tallinn", "claims": null}, {"metadata": {"year": 2019}, "authors": ["D. Gabald\u00f3n-Estevan", "K. Orru", "C. Kaufmann", "H. Orru"], "summary": "ABSTRACT In this paper, we focus on the rationale for implementing the fare-free public transportation system (FFPTS) in Tallinn, Estonia, that took place on 1 January 2013. Through a series of interviews with relevant informants, we identify the main enablers and the FFPTS in Tallinn faced and the potential of such a system to contribute to the sustainable city development. Our analysis shows that the interlinking between local and national politics determines not only the type of initiatives implemented and the support they receive but also the degree of their success and their stability. We conclude that to be even more effective, it should be extended to the all potential users, not just to local registered residents as it has been recently applied in state-run bus travels in rural municipalities in Estonia. Finally, more restrictive private car policies should be considered to fuel a sustainable mobility transition and increase cities life quality.", "title": "Broader impacts of the fare-free public transportation system in Tallinn", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Wojciech K\u0119b\u0142owski", "T. Tuvikene", "T. Pikner", "J. Jauhiainen"], "summary": "In this article, we study the largest existing fare-free public transport (FFPT) programme, launched in 2013 in Tallinn, Estonia. Instead of focusing solely on the rationale and impact of fare-free public transport in terms of finances and travel patterns, we propose to analyse FFPT from the perspective of urban political geography, and to inquire into its political and scalar dynamics. We analyse how Tallinn\u2019s fare-free programme was developed, and demonstrate the politics of its conception and implementation. We observe who has access to free travel and we reveal how FFPT is embedded in Estonia\u2019s place-of-residence-based taxation system. Finally, we identify where lies the impact of territorial competition exacerbated by FFPT. Therefore, we argue that transport policies \u2013 of which FFPT is but an example \u2013 should be understood as much more than strategies dealing with transport issues per se. Instead, we propose to approach them as political and spatial projects, whose processual, cross-sectorial and scalar dimensions help to reveal the embeddedness of transport in inherently urban questions of metropolitan governance, electoral strategies, territorial competition and socio-spatial inequalities.", "title": "Towards an urban political geography of transport: Unpacking the political and scalar dynamics of fare-free public transport in Tallinn, Estonia", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Gunnar  Prause", "Tarmo  Tuisk"], "summary": "Abstract Tallinn's fare-free public transportation (FFPT) for the residents of the city has been vital for over 6\u00a0years, and there is no evidence of major setbacks. While expectations about political impact were high, the implementation if FFPT still did not demonstrate any significant change in this respect. The usage of public transport rose from 55% to 63%. Within Estonia's developed e-governance framework, the optimization of transport lines and costs has been taken place effectively. Pollution and emission levels in the city have not increased due to the stricter environmental requirements for vehicles and fuels, although the number of cars is continuously rising. As the satisfaction with FFPT transport services in Tallinn is rather high, also other municipalities of Estonia have started to implement the same concept in their transportation.", "title": "Case study", "claims": null}], "query": "Tallinn emerged as a pioneer by introducing free public transport in 2013.", "summary_abstract": "The introduction of free public transport in Tallinn in 2013 has been a significant urban policy experiment with mixed outcomes. The initiative aimed to increase public transport usage and reduce car traffic, and it has achieved some success in these areas. Tuisk and Prause (2019) report a 10% increase in public transport usage and a 6% decrease in car traffic in the city center following the policy's implementation. Similarly, Prause and Tuisk (2020) note an increase in public transport usage from 55% to 63% over six years, indicating a positive shift towards public transport.\n\nHowever, the increase in ridership was lower than expected, as Hess (2017) points out, suggesting that the program's design may not fully align with its goals. Galey (2014) also highlights a modest citywide increase in passenger demand of only 3% shortly after implementation, although certain districts like Lasnamae saw a more significant rise of 10%.\n\nThe policy has been justified on environmental and social grounds, aiming to reduce carbon emissions and promote equal access to transportation (Galey, 2014). Despite these intentions, the impact on emissions has been limited, as private transportation still accounts for a significant portion of the city's emissions (Galey, 2014).\n\nThe political and socio-economic dimensions of the policy are also noteworthy. Gabald\u00f3n-Estevan et al. (2019) emphasize the role of local and national politics in the policy's implementation and success, suggesting that broader access and more restrictive car policies could enhance its effectiveness. K\u0119b\u0142owski et al. (2019) argue that the policy should be viewed as a political and spatial project, reflecting broader issues of metropolitan governance and socio-spatial inequalities.\n\nOverall, while Tallinn's fare-free public transport initiative has led to increased public transport usage and some reduction in car traffic, its broader goals and impacts remain complex and intertwined with political and socio-economic factors.", "summary_extract": null}, {"papers": [{"metadata": {"year": 1993}, "authors": ["Markus  See\u03b2elberg", "Francesco  Petruccione"], "summary": "Abstract The solution of stochastic partial differential equations generally relies on numerical tools. However, conventional numerical procedures are not appropriate to solve such problems. In this paper an algorithm is proposed which allows the numerical treatment of a large class of stochastic partial differential equations. To this end we reduce stochastic partial differential equations to a system of stochastic ordinary differential equations which can be solved numerically by a well-known stochastic Euler-procedure. We apply our algorithm to two stochastic partial differential equations which are special examples because their stationary two-point correlation functions can be determined analytically. Our algorithm proves to work out very well when numerical results are compared with the analytic correlation function.", "title": "Numerical integration of stochastic partial differential equations", "claims": null}, {"metadata": {"year": 1992}, "authors": ["Tadahisa  Funaki"], "summary": "Abstract We investigate a certain stochastic partial differential equation which is defined on the unit interval with periodic boundary condition and takes values in a manifold. Such equation has particularly two different applications. Namely, it determines the evolution law of an interacting constrained system of continuum distributed over the unit circle, while it defines a diffusive motion of loops on a manifold. We establish the existence and uniqueness results and then show the smoothness property of the solutions. Some examples are given in the final section.", "title": "A stochastic partial differential equation with values in a manifold", "claims": null}, {"metadata": {"year": 1987}, "authors": ["G. Prato", "L. Tubaro"], "summary": "Existence and uniqueness results for a non linear stochastic partial differential equation.- Continuity in non linear filtering some different approacees.- Expectation functionals associated with some stochastic evolution equations.- Dirichlet boundary value problem and optimal control for a stochastic distributed parameter system.- Stochastic product integration and stochastic equations.- Some remarks on a problem in stochastic optimal control.- Passage from two-parameters to infinite dimension.- The heat equation and fourier transforms of generalized brownian functionals.- The separation principle for stochastic differential equations with unbounded coefficients.- Weak convergence of measure valued processes using sobolev-imbedding techniques.- Probability distributions of solutions to some stochastic partial differential equations.- Two-sided stochastic calculus for spdes.- Convergence of implicit discretization schemes for linear differential equations with application to filtering.- Some applications of the Malliavin calculus to stochastic analysis.- Exit problem for infinite dimensional systems.", "title": "Stochastic partial differential equations and applications : proceedings of a conference held in Trento, Italy, Sept. 30-Oct. 5, 1985", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Yijun Li", "Guanggan Chen", "Ting Lei"], "summary": "This work is concerned with a class of stochastic partial differential equations with a fast random dynamical boundary condition. In the limit of fast diffusion, it derives an effective stochastic partial differential equation to describe the evolution of the dominant pattern. Using the multiscale analysis and the averaging principle, it then establishes deviation estimates of the original stochastic system towards the effective approximating system. A concrete example further illustrates the result on a large time scale.", "title": "Approximate dynamics of stochastic partial differential equations under fast dynamical boundary conditions", "claims": null}, {"metadata": {"year": 2013}, "authors": ["N. Hema", "A. Jeyalakshmi"], "summary": "In this paper, we propose a stochastic differential equation model where the underlying stochastic process is a jumpdiffusion process.The stochastic differential equation is represented as a Partial Integro Differential Equation(PIDE) using the Fokker Planck equation. The solution of the PIDE is obtained by the method of finite differences. The consistency, the convergence of the solution and the stability of the finite difference scheme are discussed. The model is applied to forecast the daily price changes in a commodity derivative. The observed values are compared graphically with the values expected from the proposed model.", "title": "A Stochastic Differential Equation Model", "claims": null}, {"metadata": {"year": 1996}, "authors": ["H. Holden", "B. \u00d8ksendal", "J. Ub\u00f8e", "Tusheng Zhang"], "summary": "Framework.- Applications to Stochastic Ordinary Differential Equations.- Stochastic Partial Differential Equations Driven by Brownian White Noise.- Stochastic Partial Differential Equations Driven by L#x00E9 vy Processes.", "title": "Stochastic Partial Differential Equations: A Modeling, White Noise Functional Approach", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Chi Hong Wong", "X. Yang", "Jing Zhang"], "summary": "We study a class of stochastic partial integral-differential equations with an asymmetrical non-local operator 1 2 \u2206+a\u2206 \u03b1 2 +b \u00b7\u2207 and a distribution expressed as divergence of a measurable field. For 0 < \u03b1 < 2, the existence and uniqueness of solution is proved by analytical method, and a probabilistic interpretation, similar to the Feynman-Kac formula, is presented for 0 < \u03b1 < 1. The method of backward doubly stochastic differential equations is also extended in this work.", "title": "Stochastic partial integral-differential equations with divergence terms", "claims": null}, {"metadata": {"year": 2007}, "authors": ["Wei Wang", "Jinqiao Duan"], "summary": "Abstract In order to understand the impact of random influences at physical boundary on the evolution of multiscale systems, a stochastic partial differential equation model under a fast random dynamical boundary condition is investigated. The noises in the model and in the boundary condition are both additive. An effective equation is derived and justified by reducing the random dynamical boundary condition to a simpler one. The effective system is still a stochastic partial differential equation. Furthermore, the quantitative comparison between the solution of the original stochastic system and the effective solution is provided by establishing normal deviations and large deviations principles. Namely, the normal deviations are asymptotically characterized, while the rate and speed of the large deviations are estimated.", "title": "Reductions and Deviations for Stochastic Partial Differential Equations Under Fast Dynamical Boundary Conditions", "claims": null}], "query": "application of algebraic analysis for stochastic partial differential equation", "summary_abstract": "The collection of papers provides a comprehensive exploration of the application of algebraic analysis to stochastic partial differential equations (SPDEs). See\u03b2elberg and Petruccione (1993) propose an algorithm that transforms SPDEs into a system of stochastic ordinary differential equations, which can be numerically solved using the stochastic Euler procedure. This approach is validated by comparing numerical results with analytically determined correlation functions. Funaki (1992) focuses on the existence, uniqueness, and smoothness of solutions for SPDEs defined on a manifold, highlighting their application in modeling diffusive motion and interacting systems. Li et al. (2021) employ multiscale analysis and the averaging principle to derive effective SPDEs, providing deviation estimates that illustrate the evolution of dominant patterns over time. Hema and Jeyalakshmi (2013) convert a stochastic differential equation into a Partial Integro Differential Equation using the Fokker-Planck equation, solving it with finite difference methods to ensure consistency and stability. Wong et al. (2020) prove the existence and uniqueness of solutions for a class of stochastic partial integral-differential equations using analytical methods, offering a probabilistic interpretation akin to the Feynman-Kac formula. Lastly, Wang and Duan (2007) investigate SPDEs under fast random dynamical boundary conditions, deriving effective equations and providing quantitative comparisons through normal and large deviation principles. Collectively, these studies demonstrate the versatility and depth of algebraic analysis in addressing the complexities of SPDEs across various contexts and applications.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2010}, "authors": ["Alexandros  Flamos", "Katherine  Begg"], "summary": "The purpose of technology transfer under the UNFCCC Article 4.5 is to \u201c\u2026promote, facilitate, and finance as appropriate the transfer of, or access to, environmentally sound technologies and know how to other Parties particularly Developing Country parties to enable them to implement the provisions of the Convention.\u201d The key challenge in this respect is that low-carbon sustainable technologies need to be adopted both by developed as well as developing countries. However, this paper focuses on the process of technology transfer to developing countries to allow them to move quickly to environmentally sound and sustainable practices, institutions and technologies. In the above framework, this paper reviews key aspects of technology transfer from a range of perspectives in the literature and discusses insights from this literature for the transfer and innovation process needed to reduce global vulnerability to climate change in the context of current international activities based on the research undertaken by the EU sponsored ENTTRANS project.", "title": "Technology transfer insights for new climate regime", "claims": null}, {"metadata": {"year": 1979}, "authors": ["K. H. Veldhuis"], "summary": "It is said nowadays that commercial companies, and particularly multinational ones, are one of the most important routes via which technology is transferred to developing countries. What is often called into question, however, is how appropriate is a particular technology to the country concerned. What, in fact, is meant by \u2018appropriate\u2019 in this context?", "title": "Transfer and Adaptation of Technology: Unilever as a Case Study", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Charikleia  Karakosta", "Haris  Doukas", "John  Psarras"], "summary": "Climate change mitigation is considered as a high priority internationally and is placed in the top of the agenda for most politicians and decision makers. The key challenge is that low-carbon sustainable technologies need to be adopted both by developed as well as developing countries, in an effort to avoid past unsustainable practices and being locked into old, less sustainable technologies. Technology transfer (TT), as an important feature of both the United Nations Framework Convention on Climate Change (UNFCCC) and its Kyoto Protocol can play a key role. TT can allow countries to move quickly to environmentally sound and sustainable practices, institutions and technologies. Indeed, the transfer or innovation process must be fast enough, to reduce global vulnerability to climate change. The aim of this paper is to analyse the TT challenges and emerging opportunities under the new climate regime, in terms of the process of innovation into an existing energy system, the related barriers and the supporting diffusion mechanisms. Good practices for renewable energy are also presented and discussed by both the developed and the developing countries in this respect.", "title": "Technology transfer through climate change: Setting a sustainable energy pattern", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Nicol\u00e1s M. Perrone"], "summary": "The role of technology transfer in climate change negotiations is vital. If technology is to help us mitigate and adapt to climate change, the international community needs to ensure sufficient innovation and technology transfer. One of the main challenges of the technology transfer regime for environmentally sound technologies is that a private and market-led model may not meet global technology transfer needs. This policy brief suggests that governments should explore market, hybrid and non-market approaches to accelerate the transfer of environmentally sound technologies. Developing countries\u2019 governments should also explore cooperative approaches to improve their bargaining power, reduce costs and ensure adaptation and innovation capacity in the developing world", "title": "Technology Transfer and Climate Change: A developing country perspective", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Chen Zhou"], "summary": "Climate friendly technologies contribute to tackling global climate crisis and the dynamic transfer of these technologies is important to achieve universal climate actions. The UNFCCC, and its recent Paris Agreement, have introduced international assistance to promote climate related-technology transfer. They call for collaborative actions from both technology supplier and demander sides in order to enable environments for a meaningful and effective technology transfer. According to the UNFCCC, the international technology assistance is unlikely to work in a desired way with the absence of indigenous enabling environments. Therefore, it is crucial to identify, assess and overcome potential barriers potentially confronted by host countries in their acquisition of climate technologies, which helps prepare these countries for climate resilience economy and sustainable development. This paper attempts to provide a deep and comprehensive analysis on enabling policy/law environments in host countries and uses Asian countries as examples in most occasions.", "title": "Enabling Law and Policy Environment for Climate Technology Transfer: From Perspectives of Host Countries", "claims": null}, {"metadata": {"year": 2012}, "authors": ["May  Elsayyad", "Florian  Morath"], "summary": "This paper considers investments in cost-reducing technology in the context of contributions to climate protection. Contributions to mitigating climate change are analyzed in a two-period public goods framework where later contributions can be based on better information, but delaying the contribution to the public good is costly because of irreversible damages. We show that, when all countries have access to the new technology, countries have an incentive to invest in technology because this can lead to an earlier contribution of other countries and therefore reduce a country's burden of contributing to the public good. Our results provide a rationale for the support of technology sharing initiatives.", "title": "Technology Transfers for Climate Change", "claims": null}, {"metadata": {"year": 2012}, "authors": ["J. Lovett", "P. Hofman", "K. Morsink", "J. Clancy"], "summary": "A key element of climate change mitigation and adaptation is the transfer of more effective and efficient low-carbon technologies between developed and developing countries. Although several policy mechanisms for technology transfer are in place, most observers agree that these have not been very effective in accelerating the rate of diffusion of energy-efficient and renewable-based technologies. There is a need for market-oriented approaches in order to diffuse efficient technologies more rapidly and to reduce high transaction costs, which are a major factor explaining the low effectiveness of existing mechanisms (Michaelowa and Jotzo, 2005; Jung, 2006; Hofman et al, 2008; Lovett et al, 2009; Byigero et al, 2010; Timilsina et al, 2010). At the 2007 G8 Summit in Heiligendamm, it was recognized that an \u2018expanded approach to collaboratively accelerate the widespread adoption of clean energy and climate friendly technology\u2019 was needed (G8, 2007). In successive outputs from the United Nations Framework Convention on Climate Change (UNFCCC) negotiations, such as the 2007 Bali Action Plan, the 2008 Poznan Strategic Programme on Technology Transfer, the 2009 Copenhagen Accord and the 2010 Cancun Technology Mechanism, the requirement for scaling up technology transfer features prominently. The problem is that project-based funding mechanisms, such as those under the Global Environment Facility (GEF) and Clean Development Mechanism (CDM), can never do more than provide a fraction of the resources needed to transfer sufficient environmentally sound technologies to permit economic advancement of developing countries while minimizing greenhouse gas emissions;", "title": "Technology Transfer and Global Markets", "claims": null}, {"metadata": {"year": 2011}, "authors": ["D. Popp"], "summary": "As the developed world begins efforts to limit its emissions of greenhouse gases, economic growth in developing countries is causing increased emissions from the developing world. Reducing these emissions while still enabling developing countries to grow requires the use of climate-friendly technologies in these countries. In most cases, these technologies are first created in high-income countries. Thus, the challenge for climate policy is to encourage the transfer of these climate-friendly technologies to the developing world. This article reviews the economic literature on environmental technology transfer and discusses the implications of this literature for climate policy, focusing on the Clean Development Mechanism (CDM). A key point is that technology diffusion is gradual. Early adoption of policy by developed countries leads to the development of new technologies that make it easier for developing countries to reduce pollution as well. Since clean technologies are first developed in the world's leading economies, international trade and foreign investments provide access to these technologies. Moreover, evidence suggests that some technologies, such as those enhancing energy efficiency, will diffuse to developing countries even without the aid of policy prescriptions, such as the CDM. This is important for assessing the potential emissions reductions of proposed CDM projects.", "title": "International Technology Transfer, Climate Change, and the Clean Development Mechanism", "claims": null}], "query": "Technology Transfer: COP26 emphasized the importance of technology transfer from developed to developing countries to foster climate adaptation and mitigation. This entails sharing and facilitating access to clean and sustainable technologies that can support low-carbon pathways for development. Unilever, as a global player in the consumer goods industry, has made significant contributions through technology transfer. For instance, the company's Sustainable Living Plan includes initiatives such as sharing renewable energy solutions, water-saving technologies, and sustainable packaging innovations with suppliers and partners across its value chain. Unilever's commitment to technology transfer supports the agenda set at COP26, promoting the equitable distribution of climate-friendly technologies for a more sustainable future.", "summary_abstract": "The collection of papers collectively underscores the critical role of technology transfer in addressing climate change, particularly in the context of COP26's emphasis on facilitating the transfer of clean and sustainable technologies from developed to developing countries. Flamos and Begg (2010) highlight the importance of technology transfer under the UNFCCC framework, emphasizing the need for developing countries to adopt low-carbon technologies to reduce global vulnerability to climate change. Similarly, Karakosta et al. (2010) discuss the necessity of technology transfer to avoid unsustainable practices and promote rapid adoption of environmentally sound technologies in developing nations.\n\nPerrone (2022) points out the challenges of a market-led model for technology transfer, suggesting that a combination of market, hybrid, and non-market approaches could better meet global needs. This aligns with Zhou (2019), who stresses the importance of creating enabling environments in host countries to facilitate effective technology transfer, using Asian countries as examples.\n\nLovett et al. (2012) and Popp (2011) both emphasize the need for more effective mechanisms to accelerate the diffusion of low-carbon technologies. Lovett et al. (2012) argue for market-oriented approaches to reduce transaction costs and enhance technology diffusion, while Popp (2011) notes that international trade and foreign investments are crucial for accessing clean technologies developed in high-income countries.\n\nOverall, the papers collectively suggest that while technology transfer is essential for climate adaptation and mitigation, there are significant challenges and barriers that need to be addressed. These include the need for supportive policy environments, effective diffusion mechanisms, and collaborative international efforts to ensure that developing countries can access and implement the necessary technologies for sustainable development.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2016}, "authors": ["Steven Arnocky"], "summary": "Men, more than women, prefer physically attractive and sexually faithful mates. Women and girls who are high on these desired traits might also be more popular within their same-sex peer networks. Accordingly, women, more than men, are motivated to display and accentuate these matevalue and peer-valued characteristics, which are, in part, signals of reproductively and socially relevant information. Social-cognitive mechanisms such as social comparison, as well as emotions such as envy, appear to underlie some behaviors associated with women\u2019s desire to be considered among desirable women. Across diverse human cultures, men\u2019s and women\u2019s mate preferences have been shown to diverge in reproductively important ways. One of the most consistent sex differences is men\u2019s greater expressed preference for physically attractive partners. For instance, Buss (1989) found that in each of 37 cultures studied, men more than women desired partners who were youthful and physically attractive. Moreover, men are remarkably consistent in the specific features that they find attractive in a mate. These features typically include lustrous hair, clear skin, full breasts, feminine and symmetrical facial features, and a low waist-to-hip ratio (i.e., an hourglass figure) (Buss 1989; see Arnocky et al. 2014 for review). A growing body of evidence suggests that such physical features may have evolved, in part, as relatively honest cues to reproductively important aspects of women\u2019s phenotypic condition, including youth (i.e., being of reproductive age), underlying health, and/or fertility (Arnocky et al. 2014). Ancestral men who happened to be attracted to these features would therefore have out-reproduced those who did not share their preferences (Arnocky et al. 2014). To the extent that mate preferences for condition-linked physical features are heritable, such preferences would proliferate and become prevalent in the population.", "title": "D Desire to Be Included Among Desirable Women", "claims": null}, {"metadata": {"year": 2013}, "authors": ["A. Little", "B. Jones", "D. Feinberg", "D. Perrett"], "summary": "Several evolutionarily relevant sources of individual differences in face preference have been documented for women. Here, we examine three such sources of individual variationinmen\u2019spreferenceforfemalefacialfemininity:termofrelationship,partnership status and self-perceived attractiveness. We show that men prefer more feminine female faces when rating for a short-term relationship and when they have a partner (Study 1). These variables were found to interact in a follow-up study (Study 2). Men who thought themselves attractive also preferred more feminized female faces for short-term relationships than men who thought themselves less attractive (Study 1 and Study 2). In women, similar findings for masculine preferences in male faces have been interpreted as adaptive. In men, such preferences potentially reflect that attractive males are able to compete for high-quality female partners in short-term contexts. When a man has secured a mate, the potential cost of being discovered may increase his choosiness regardingshort-termpartnersrelativetounpartneredmen,whocanbetterincreasetheir short-term mating success by relaxing their standards. Such potentially strategic preferences imply that men also face trade-offs when choosing relatively masculine or feminine faced partners. In line with a trade-off, women with feminine faces were seen as more likely to be unfaithful and more likely to pursue short-term relationships (Study 3), suggesting that risk of cuckoldry is one factor that may limit men\u2019s preferences for femininity in women and could additionally lead to preferences for femininity in short-term mates. Mature features in adult human faces reflect the masculinization or feminization of secondary sexual characteristics that occurs at puberty. These face shape differences in part arise because of the action of hormones such as testosterone. For example, smaller jawbones and fatter cheeks are features of female faces that differentiate them from male faces (e.g., Enlow, 1982). In terms of women\u2019s attraction to masculinity in male faces, the direction of relationship varies across studies and researchers have documented differences in the attractiveness of masculinity according to short-term versus long-term matingcontextsandvarioussourcesofindividualdifferencesinpreferences(Little,Jones,", "title": "Men'sstrategicpreferencesforfemininityinfemale faces", "claims": null}, {"metadata": {"year": 2020}, "authors": ["L. Lamy"], "summary": "When asked to state their ideal romantic-partner preferences, men tend to overestimate women\u2019s physical attractiveness, whereas women tend to overestimate men\u2019s earning prospects [1,2]. Beyond this gender-related difference, however, both men and women prefer attractive, as compared to non-attractive partners. Attractive individuals are more popular, at least among members of the opposite gender, and they are more successful in dating relationships [3]. In line with these findings, it could be expected that feelings of love and affection would be more readily directed towards attractive individuals. But it is striking that this hypothesis was not supported by empirical studies. Among dating partners, neither independent observer, self, nor partner ratings of attractiveness are linked with the level of love an individual receives from his/her partner [4]. For men and women alike, beauty has no advantage in terms of love received, or given. Attractive women are more desired as romantic partners. Attractive men have more cross-gender interactions. Thus, it could be stated that physical attractiveness creates attraction, but it is not enough to create love.", "title": "Physical Attractiveness and Romantic Relationships: A Review", "claims": null}, {"metadata": {"year": 1994}, "authors": ["D. Kenrick", "S. Neuberg", "Kristin L. Zierk", "J. Krones"], "summary": "Previous research indicates that males, compared with females, evaluate their relationships less favorably after exposure to physically attractive members of the other sex. An evolutionary model predicts a converse effect after exposure to opposite-sex individuals high in dominance, which should lead females to evaluate their current relationships less favorably than males. Women and men rated their current relationships after being exposed to opposite-sex targets varying in both dominance and physical attractiveness. Consistent with earlier research, males exposed to physically attractive, as compared with average, targets rated their current relationships less favorably. Males' relationship evaluations were not directly influenced by the targets' dominance, although the effect of physical attractiveness was significant only for men exposed to women low in dominance. However; females' evaluations of their relationships were unaffected by exposure to physically attractive males but were lower after exposure to targets high in dominance. These data support predictions derived from an evolutionary model and suggest that such models can be used to generate testable hypotheses about ongoing social cognition.", "title": "Evolution and Social Cognition: Contrast Effects as a Function of Sex, Dominance, and Physical Attractiveness", "claims": null}, {"metadata": {"year": 2021}, "authors": ["S. Whyte", "R. Brooks", "H. F. Chan", "B. Torgler"], "summary": "Because sexual attraction is a key driver of human mate choice and reproduction, we descriptively assess relative sex differences in the level of attraction individuals expect in the aesthetic, resource, and personality characteristics of potential mates. As a novelty we explore how male and female sexual attractiveness preference changes across age, using a dataset comprising online survey data for over 7,000 respondents across a broad age distribution of individuals between 18 and 65 years. In general, we find that both males and females show similar distribution patterns in their preference responses, with statistically significant sex differences within most of the traits. On average, females rate age, education, intelligence, income, trust, and emotional connection around 9 to 14 points higher than males on our 0\u2013100 scale range. Our relative importance analysis shows greater male priority for attractiveness and physical build, compared to females, relative to all other traits. Using multiple regression analysis, we find a consistent statistical sex difference (males relative to females) that decreases linearly with age for aesthetics, while the opposite is true for resources and personality, with females exhibiting a stronger relative preference, particularly in the younger aged cohort. Exploring non-linearity in sex difference with contour plots for intelligence and attractiveness across age (mediated by age) indicates that sex differences in attractiveness preferences are driven by the male cohort (particularly age 30 to 40) for those who care about the importance of age, while intelligence is driven by females caring relatively more about intelligence for those who see age as very important (age cohort 40 to 55). Overall, many of our results indicate distinct variations within sex at key life stages, which is consistent with theories of selection pressure. Moreover, results also align with theories of parental investment, the gender similarities hypothesis, and mutual mate choice\u2013which speaks to the fact that the broader discipline of evolutionary mate choice research in humans still contains considerable scope for further inquiry towards a unified theory, particularly when exploring sex-difference across age.", "title": "Sex differences in sexual attraction for aesthetics, resources and personality across age", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Guanlin  Wang", "Minxuan  Cao", "Justina  Sauciuvenaite", "Ruth  Bissland", "Megan  Hacker", "Catherine  Hambly", "Lobke M. Vaanholt", "Chaoqun  Niu", "Mark D. Faries", "John R. Speakman"], "summary": "Abstract Parental investment hypotheses regarding mate selection suggest that human males should seek partners featured by youth and high fertility. However, females should be more sensitive to resources that can be invested on themselves and their offspring. Previous studies indicate that economic status is indeed important in male attractiveness. However, no previous study has quantified and compared the impact of equivalent resources on male and female attractiveness. Annual salary is a direct way to evaluate economic status. Here, we combined images of male and female body shape with information on annual salary to elucidate the influence of economic status on the attractiveness ratings by opposite sex raters in American, Chinese and European populations. We found that ratings of attractiveness were around 1000 times more sensitive to salary for females rating males, compared to males rating females. These results indicate that higher economic status can offset lower physical attractiveness in men much more easily than in women. Neither raters' BMI nor age influenced this effect for females rating male attractiveness. This difference explains many features of human mating behavior and may pose a barrier for male engagement in low-consumption lifestyles.", "title": "Different impacts of resources on opposite sex ratings of physical attractiveness by males and females", "claims": null}, {"metadata": {"year": 2022}, "authors": ["M. Kowal", "P. Sorokowski"], "summary": "Background: Public opinion on who performs more beauty-enhancing behaviors (men or women) seems unanimous. Women are often depicted as primarily interested in how they look, opposed to men, who are presumably less focused on their appearance. However, previous studies might have overlooked how masculinity relates to self-modification among men. Methods: We explored this issue in depth by conducting a qualitative Study 1 aimed to establish how men and women enhance their attractiveness (N = 121) and a quantitative Study 2 aimed to test time spent on activities that increase one\u2019s attractiveness in a longitudinal design (with seven repeated measures from 62 participants; N(total) = 367). Results: We observed no sex differences in beauty investments. Although women spent more time on make-up and cosmetics usage, men caught up with women in exercising and bodybuilding. Conclusion: Our study provides evidence that there may not be such wide sex differences in the intensity of enhancing one\u2019s appearance as has been previously thought. We hypothesize that this might partly stem from changes in gender roles regarding masculinity.", "title": "Sex Differences in Physical Attractiveness Investments: Overlooked Side of Masculinity", "claims": null}, {"metadata": {"year": 1995}, "authors": ["Nigel  Barber"], "summary": "Abstract Psychological evidence suggests that sex differences in morphology have been modified by sexual selection so as to attract mates (intersexual selection) or intimidate rivals (intrasexual selection). Women compete with each other for high quality husbands by advertising reproductive value in terms of the distribution of fat reserves and by exaggerating morphological indicators of youthfulness such as a small nose and small feet and pale, hairless skin. Men's physical appearance tends to communicate social dominance, which has the combined effects of intimidating reproductive rivals and attracting mates. In addition to their attractiveness and intimidatory effects, human secondary sexual characters also provide cues to hormonal status and phenotypic quality consistent with the good genes model of sexual selection (which includes parasite resistance). Low waist-hip ratio is sexually attractive in women and indicates a high estrogen/testosterone ratio (which favors reproductive function). Facial attractiveness provides honest cues to health and mate value. The permanently enlarged female breast appears to have evolved under the influence of both the good genes and the runaway selection mechanisms. The male beard is not obviously related to phenotypic quality and may have evolved through a process of runaway intersexual selection.", "title": "The evolutionary psychology of physical attractiveness: Sexual selection and human morphology", "claims": null}], "query": "Female attractiveness evolves compared to male attractiveness because men are more interested in attractiveness?", "summary_abstract": "The collection of papers collectively suggests that female attractiveness has evolved in response to male preferences, which are more focused on physical attractiveness compared to female preferences. Arnocky (2016) highlights that men consistently prefer physically attractive partners, with specific features such as clear skin and a low waist-to-hip ratio being particularly desirable. This preference is rooted in evolutionary pressures, where men who were attracted to these features had greater reproductive success. Little et al. (2013) further support this by showing that men prefer more feminine female faces, especially in short-term relationships, indicating a strategic preference for physical attractiveness.\n\nLamy (2020) notes that while both genders prefer attractive partners, men tend to overestimate women's physical attractiveness, suggesting a heightened emphasis on this trait. Wang et al. (2017) provide evidence that economic status can offset lower physical attractiveness in men more easily than in women, indicating that men place a higher premium on physical traits in mate selection.\n\nBarber (1995) discusses how women compete for high-quality mates by emphasizing traits that signal reproductive value, such as youthfulness and a low waist-hip ratio, which aligns with male preferences. This competition among women for male attention may drive the evolution of female attractiveness.\n\nHowever, Kowal and Sorokowski (2022) challenge the notion of significant sex differences in beauty-enhancing behaviors, suggesting that men are increasingly engaging in activities to enhance their attractiveness, potentially due to changing gender roles.\n\nOverall, the papers collectively indicate that male interest in physical attractiveness has played a significant role in the evolution of female attractiveness, with women adapting to these preferences through both biological and behavioral means.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2021}, "authors": ["P. Robertson"], "summary": ": Misinformation related to climate change has been around for decades, mostly in the form of denying the existence of global warming. Today, misinformation is manifesting in different ways, but it still has the same outcome: undermining science and delaying climate action.", "title": "Introduction and context", "claims": null}, {"metadata": {"year": 2015}, "authors": ["LaRue Allen", "Bridget B. Kelly", "Youth Board on Children", "Families."], "summary": ": Misinformation related to climate change has been around for decades, mostly in the form of denying the existence of global warming. Today, misinformation is manifesting in different ways, but it still has the same outcome: undermining science and delaying climate action. Social can undermine and have", "title": "Introduction and Context", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Tiffany E. Sill", "Jaime R Ayala", "Julianne Rolf", "Spencer Smith", "Shelby Dye"], "summary": "Despite the existence of a substantial amount of climate-related scientific data, misconceptions about climate change are still prevalent within public opinion. Dissemination of misinformation to the public through subjective media sources is a major challenge that climate scientists face. Implementation of climate policy is crucial for mitigation and adaptation measures required to curtail anthropogenic rooted climate change. This paper will discuss student perspectives on the 2022 United Nations climate summit in Egypt (COP27) related to climate literacy and public opinion as the driving forces behind the enactment and execution of important climate-based policy.", "title": "How Climate Literacy and Public Opinion Are the Driving Forces Behind Climate-Based Policy: A Student Perspective on COP27", "claims": null}, {"metadata": {"year": 2019}, "authors": ["J. Cook"], "summary": "While there is overwhelming scientific agreement on climate change, the public has become polarized over fundamental questions such as human-caused global warming. Communication strategies to reduce polarization rarely address the underlying cause: ideologically-driven misinformation. In order to effectively counter misinformation campaigns, scientists, communicators, and educators need to understand the arguments and techniques in climate science denial, as well as adopt evidence-based approaches to neutralizing misinforming content. This chapter reviews analyses of climate misinformation, outlining a range of denialist arguments and fallacies. Identifying and deconstructing these different types of arguments is necessary to design appropriate interventions that effectively neutralize the misinformation. This chapter also reviews research into how to counter misinformation using communication interventions such as inoculation, educational approaches such as misconception-based learning, and the interdisciplinary combination of technology and psychology known as technocognition.", "title": "Understanding and Countering Misinformation About Climate Change", "claims": null}, {"metadata": {"year": 2022}, "authors": ["John  Cook"], "summary": "While there is overwhelming scientific agreement on climate change, the public has become polarized over fundamental questions such as human-caused global warming. Communication strategies to reduce polarization rarely address the underlying cause: ideologically-driven misinformation. In order to effectively counter misinformation campaigns, scientists, communicators, and educators need to understand the arguments and techniques in climate science denial, as well as adopt evidence-based approaches to neutralizing misinforming content. This chapter reviews analyses of climate misinformation, outlining a range of denialist arguments and fallacies. Identifying and deconstructing these different types of arguments is necessary to design appropriate interventions that effectively neutralize the misinformation. This chapter also reviews research into how to counter misinformation using communication interventions such as inoculation, educational approaches such as misconception-based learning, and the interdisciplinary combination of technology and psychology known as technocognition.", "title": "Understanding and Countering Misinformation About Climate Change", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Justin  Farrell", "Kathryn  McConnell", "Robert  Brulle"], "summary": "Nowhere has the impact of scientific misinformation been more profound than on the issue of climate change in the United States. Effective responses to this multifaceted problem have been slow to develop, in large part because many experts have not only underestimated its impact, but have also overlooked the underlying institutional structure, organizational power and financial roots of misinformation. Fortunately, a growing body of sophisticated research has emerged that can help us to better understand these dynamics and provide the basis for developing a coordinated set of strategies across four related areas (public inoculation, legal strategies, political mechanisms and financial transparency) to thwart large-scale misinformation campaigns before they begin, or after they have taken root.This Perspective synthesizes research on the origins and impacts of scientific misinformation campaigns, pointing to public inoculation, legal, political and financial strategies for countering climate change misinformation and limiting its dissemination.", "title": "Evidence-based strategies to combat scientific misinformation", "claims": null}, {"metadata": {"year": 2017}, "authors": ["J. Cook", "S. Lewandowsky", "Ullrich K. H. Ecker"], "summary": "Misinformation can undermine a well-functioning democracy. For example, public misconceptions about climate change can lead to lowered acceptance of the reality of climate change and lowered support for mitigation policies. This study experimentally explored the impact of misinformation about climate change and tested several pre-emptive interventions designed to reduce the influence of misinformation. We found that false-balance media coverage (giving contrarian views equal voice with climate scientists) lowered perceived consensus overall, although the effect was greater among free-market supporters. Likewise, misinformation that confuses people about the level of scientific agreement regarding anthropogenic global warming (AGW) had a polarizing effect, with free-market supporters reducing their acceptance of AGW and those with low free-market support increasing their acceptance of AGW. However, we found that inoculating messages that (1) explain the flawed argumentation technique used in the misinformation or that (2) highlight the scientific consensus on climate change were effective in neutralizing those adverse effects of misinformation. We recommend that climate communication messages should take into account ways in which scientific content can be distorted, and include pre-emptive inoculation messages.", "title": "Neutralizing misinformation through inoculation: Exposing misleading argumentation techniques reduces their influence", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Isyaku Hassan", "R. Musa", "Mohd Nazri Latiff Azmi", "Mohamad Razali Abdullah", "Siti Zanariah Yusoff"], "summary": "Disinformation represents one of the major obstacles to meaningful actions against climate change skepticism. This study, therefore, aimed to investigate climate change disinformation across types, agents, and media platforms as reported in selected online newspapers. A total of 124 news articles gathered from Malaysia's The Star and New Straits Times were subjected to content analysis. The articles were collected from the newspapers\u2019 digital archives between August 2015 and October 2021 using \u201cclimate change\u201d and \u201cdisinformation\u201d as keywords. Subsequently, data were analyzed using the Chi-square test and descriptive statistics. The study found that disinformation about climate change is more likely to be spread by politicians, organizations, and anonymous agencies compared with business tycoons, celebrities, and academics. Also, misleading and fabricated contents represent the most common types of disinformation spread by politicians. The findings suggest that politics and social media represent the most critical factors influencing climate change. The study, therefore, calls for the joint efforts of activists, media practitioners, and governments to mitigate disinformation about climate change.", "title": "Analysis of climate change disinformation across types, agents and media platforms", "claims": null}], "query": "laws and policies governing climate change disinformation or misinformation", "summary_abstract": "The collection of papers highlights the significant challenge posed by climate change misinformation and disinformation, emphasizing the need for effective laws and policies to address this issue. Farrell et al. (2019) discuss the profound impact of scientific misinformation on climate change, particularly in the United States, and suggest a coordinated strategy involving public inoculation, legal strategies, political mechanisms, and financial transparency to counter misinformation campaigns. Cook (2019, 2022) emphasizes the importance of understanding and deconstructing denialist arguments to design interventions that neutralize misinformation, advocating for communication strategies such as inoculation and educational approaches. Hassan et al. (2023) identify politicians and organizations as primary spreaders of climate disinformation, suggesting that political and social media factors are critical in influencing public perception. They call for collaborative efforts from activists, media practitioners, and governments to combat disinformation. Sill et al. (2023) highlight the role of climate policy in addressing misinformation, underscoring the importance of climate literacy and public opinion in driving policy enactment. Collectively, these papers underscore the necessity of comprehensive strategies, including legal and policy measures, to effectively address climate change misinformation and disinformation.", "summary_extract": null}, {"papers": [{"metadata": {"year": 1993}, "authors": ["Anjali  Kumar"], "summary": "Concerns about public enterprise management, especially since the 1980s, have been closely associated with more generalized concerns about the role of the state in processes of production. Whether in specific enterprises or industries in the context of a mixed market economy, or across the entire economy, in a system of centralized planning, state owned and operated production units have been associated with economic inefficiencies leading to lower relative levels of output, a diversion of resources from their most productive uses and internal economic inefficiencies in management and motivation within the productive unit.1 A major cause of the economic inefficiencies of public enterprises is the difficulty of reconciling the multiple political, social and ideological aims of governments and government-appointed managers guiding the operations of these enterprises, with the achievement of economic efficiency. These concerns have led to the adoption of a wide spectrum of mechanisms to modify or alter the controls and obligations between governments and state owned enterprises to better achieve these multiple and sometimes conflicting objectives.", "title": "Public Enterprise Management Through State Holding Companies", "claims": null}, {"metadata": {"year": 2002}, "authors": ["Suthathip  Yaisawarng"], "summary": "A number of studies have documented that public and private sector organisations do not use their limited resources efficiently. A possible implication is that reallocation of resources from the provision of goods and services that have relatively low marginal social benefits to those that have relatively high marginal social benefits would enhance overall social welfare. Another implication is that resources have not been used by the most productive means; ie, it is possible to produce more goods and services without additional resources. Suggested remedies range from industry reforms such as restructuring and deregulation to promote competition, to institutional changes such as adoption of efficient management practices.", "title": "Performance Measurement and Resource Allocation", "claims": null}, {"metadata": {"year": 2002}, "authors": ["Go  Yano", "Maho  Shiraishi"], "summary": "In this paper we attempt to investigate empirically the cause of inefficiency of Chinese state-owned enterprises in 1989\u201395, in the contract relation between Chinese government and state-owned enterprises. It is found that several moral hazards arose, in 1989\u201395 Chinese state-owned enterprises, in textile industry. To put it another way, the enterprises chose less private effort and risk than the first-best levels, because of the incentive structure designed by the government. Especially, concave payoff function to the enterprises designed by the government made them institutional risk averters and choose too little risks.", "title": "State-Owned Enterprises and Their Contract with Government in China: An Econometric Analysis", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Xiong Qing-guo"], "summary": "The state-owned enterprises almost have no room to make their own decision in changing their own fate and have always played a relatively passive role and been in a submissive position. That is the key reason why the state-owned enterprises are confronted with difficulty in the reform. As a result , effective measures should been taken to eliminate the government\u2032s substitution for the state-owned enterprises in making decision, and return the right to conduct self-reform , thus these enterprises can decide their own future.", "title": "The Consequence of Government\u2032s Substitution in Decision-making and its Rectification \u2014\u2014Reflections on the reform of the state-owned enterprises", "claims": null}, {"metadata": {"year": 1974}, "authors": ["Mancur  Olson"], "summary": "Many of those who write about the corporation and the role of government take it for granted that the fundamental problem is the prevalence of monopoly in the corporate sector and that one of the crucial functions of government is to deal with this problem through anti-trust laws, or regulation of uncompetitive industries, or public ownership of firms with monopoly power. Though there is some merit in this commonplace perspective, this paper will attempt to show that there is another way of looking at the corporate and public sectors that is both better balanced and more likely to bring progress in dealing with the major problems modern societies face. More specifically, the first section of this paper will contend that the problem of monopoly, as that term is characteristically understood in economic theory, is of only modest importance, at least in the United States, both in terms of economic efficiency and distributive justice; the second section will argue that the concepts of collective goods and externalities provide a more relevant insight into the major economic problems of the time; the third will conclude that because of the problems involved in dealing with externalities and producing collective goods, and the existing mechanisms in the public sector for dealing with these problems, inefficiencies in this area are probably of extraordinary importance, and in the aggregate presumably dwarf all of those resulting from monopoly; and the fourth will suggest that social efficiency could be substantially increased through certain changes in existing mechanisms in the public sector.", "title": "On the Priority of Public Problems", "claims": null}, {"metadata": {"year": 2017}, "authors": ["A. Tatuev", "Murat A. Kerefov", "N. I. Ovcharova", "Maria L. Vilisova", "Renata A. Shibzuhova"], "summary": "Nowadays, the implementation of state's social policies becomes critically urgent in the context of social development for all countries of the world. For a number of objective reasons, the state is forced to reduce its obligations to a certain extent in the social sphere in such a way as to avoid hampering the markets development. The objective of the current study is to identify the priorities of modern public administration. The structure of budget expenditures demonstrates shifting to national defense, social policy and national economy with lower health-care and education spending. At the same time, the use of tax system as a resource for meeting the State's social obligations is increasingly constrained.", "title": "Budget Constraints and Expanding of Non-Governmental Organizations Participation in the Implementation of State's Social Policies", "claims": null}, {"metadata": {"year": 1985}, "authors": ["T. Goodale"], "summary": "Limited resources and growing needs continue to challenge providers of local government services. Equitable distribution of resources is both more important and more difficult when the local economy is growing slowly if at all. The need to increase revenue and reduce costs leads us to focus on efficiency rather than effectiveness, and market equity rather than equal opportunity. This can lead to a distortion of mandate and objectives. Public purpose and philosophy should join the dialogue of business and finance.", "title": "The Political Economics of Local Leisure Services", "claims": null}, {"metadata": {"year": 2011}, "authors": ["E. Apriatni"], "summary": "Abstract : The role of private enterprise in poverty reduction is still questionable because the company's concern toward social problems is still half-heartedly. Coverage in the media, associated with forest destruction, pollution of river water, as well as other forms of injustice felt by the community is still often heard. This proves the corporate social responsibility in upholding justice in the world of business is still low. Injustices in the economic and social development create a gap, especially in public access to utilize existing resources. On the other hand, companies show their existence in the business world, with their ability to utilize existing resources in the country. Embodiment of business ethics within the context of corporate social responsibility should be accommodated in company policies with the aim to achieve corporate objectives, but do not ignore the social interests of surrounding communities. With good relationship, through the CSR labeled activity, will eventually be able to maintain the existence of the company in the business world.\u00a0 Key words: Social Responsibility: Business Ethics; justice permalink :\u00a0 http://ejournal.undip.ac.id/index.php/forum/article/view/3150", "title": "TANGGUNG JAWAB SOSIAL PERUSAHAAN DALAM MEWUJUDKAN KEADILAN DALAM BISNIS", "claims": null}], "query": "Curry (2005) and Seip (2013) highlight that government-owned businesses often prioritize meeting social obligations over commercial objectives, leading to inefficient resource allocation and financial struggles for these enterprises.", "summary_abstract": "The collection of papers collectively underscores the challenges faced by government-owned businesses in balancing social obligations with commercial objectives, often leading to inefficiencies and financial difficulties. Kumar (1993) highlights that state-owned enterprises struggle with economic inefficiencies due to the conflicting political, social, and ideological aims imposed by governments, which detract from achieving economic efficiency. Yaisawarng (2002) supports this by noting that both public and private sectors often fail to use resources efficiently, suggesting that reallocating resources to areas with higher marginal social benefits could improve social welfare. Yano and Shiraishi (2002) provide empirical evidence from Chinese state-owned enterprises, indicating that government-designed incentive structures lead to risk aversion and suboptimal effort levels, further contributing to inefficiency. Qing-guo (2005) emphasizes the lack of autonomy in decision-making for state-owned enterprises, which hinders their ability to reform and improve efficiency. Collectively, these papers suggest that the prioritization of social obligations over commercial objectives in government-owned businesses often results in inefficient resource allocation and financial struggles, as highlighted by Curry (2005) and Seip (2013).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2013}, "authors": ["Bj\u00f6rn Bartling", "F. Engl", "Roberto A. Weber"], "summary": "This paper studies whether people can avoid punishment by remaining willfully ignorant about possible negative consequences of their actions for others. We employ a laboratory experiment, using modified dictator games in which a dictator can remain willfully ignorant about the payoff consequences of his decision for a receiver. A third party can punish the dictator after observing the dictator\u2019s decision and the resulting payoffs. On the one hand, willfully ignorant dictators are punished less if their actions lead to unfair outcomes than dictators who reveal the consequences before implementing the same outcome. On the other hand, willfully ignorant dictators are punished more than revealing dictators if their actions do not lead to unfair outcomes. We conclude that willful ignorance can circumvent blame when unfair outcomes result, but that the act of remaining willfully ignorant is itself punished, regardless of the outcome.", "title": "Does Willful Ignorance Deflect Punishment? - An Experimental Study", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Robert  St\u00fcber"], "summary": "Altruistic punishment is often thought to be a major enforcement mechanism of social norms. I present experimental results from a modified version of the dictator game with third-party punishment, in which third parties can remain ignorant about the choice of the dictator. I find that a substantial fraction of subjects choose not to reveal the dictator\u2019s choice and not to punish the dictator. I show that this behavior is in line with the social norms that prevail in a situation of initial ignorance. Remaining ignorant and choosing not to punish is not inappropriate. As a result, altruistic punishment is significantly lower when the dictator\u2019s choice is initially hidden. The decrease in altruistic punishment leads to more selfish dictator behavior only if dictators are explicitly informed about the effect of willful ignorance on punishment rates. Hence, in scenarios in which third parties can ignore information and dictators know what this implies, third-party punishment may only ineffectively enforce social norms.", "title": "The benefit of the doubt: willful ignorance and altruistic punishment", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Rainer Michael Rilke"], "summary": "Organizations aim to influence\u2014via their internal guidelines and corporate culture\u2014how unfair treatment of other stakeholders is perceived and condemned by employees. To understand how different frames and forms of publicity influence moralistic punishment, that is, the willingness of employees to take costs in order to foster norm compliance, we employ a modified version of a dictator game. In our dictator game, a bystander observes a dictator\u2019s behavior towards a recipient and can punish the dictator. We vary how the dictator\u2019s action is framed (either as giving money to the recipient or taking money from the recipient) and whether or not the recipient, as a victim of unfair behavior, is informed about the punishment. Our results suggest that bystanders are more likely to punish dictators when their action is framed as giving rather than taking, although both lead to the same consequences. When bystanders cannot inform recipients about their punishment, less punishment can be observed. On average, dictators partially anticipate this effect and behave more generously when recipients are informed about the bystanders\u2019 punishment.", "title": "On the duty to give (and not to take): An experiment on moralistic punishment", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Stefania Ottone", "Ferruccio Ponzano", "L. Zarri"], "summary": "Social norms are ubiquitous in human life. Their role is essential in allowing cooperation to prevail, despite the presence of incentives to free ride. As far as norm enforcement devices are concerned, it would be impossible to have widespread social norms if second parties only enforced them. However, both the quantitative relevance and the motivations underlying altruistic punishment on the part of \u2018unaffected\u2019 third parties are still largely unexplored. This paper contributes to shed light on the issue, by means of an experimental design consisting of three treatments: a Dictator Game Treatment, a Third-Party Punishment Game Treatment (Fehr and Fischbacher, 2004) and a Metanorm Treatment, that is a variant of the Third-party Punishment Game where the Recipient can punish the third party. We find that third parties are willing to punish dictators (Fehr and Fischbacher, 2004; Ottone, 2008) and, in doing so, they are affected by \u2018reference-dependent fairness\u2019, rather than by the \u2018egalitarian distribution norm\u2019. By eliciting players\u2019 normative expectations, it turns out that all of them expect a Dictator to transfer something \u2013 not half of the endowment. Consequently, the Observers\u2019 levels of punishment are sensitive to their subjective sense of fairness. A positive relation between the level of punishment and the degree of negative subjective unfairness emerges. Subjective unfairness also affects Dictators\u2019 behaviour: their actual transfers and their ideal transfer are not significantly different. Finally, we interestingly find that third parties are also sensitive to the receivers\u2019 (credible) threat to punish them: as the Dictator\u2019s transfer becomes lower and lower than the Observer\u2019s ideal transfer, the Observer\u2019s reaction is \u2013 other things being equal \u2013 significantly stronger in the Metanorm Treatment than in the Third-Party Punishment Game Treatment. Hence, despite their being to some extent genuinely nonstrategically motivated, also third parties \u2013 like second parties \u2013 are sensitive to the costs of punishing.", "title": "Moral Sentiments and Material Interests behind Altruistic Third-Party Punishment", "claims": null}, {"metadata": {"year": 2019}, "authors": ["R\u00e9ka  Heim", "J\u00fcrgen  Huber"], "summary": "Abstract In this paper we explore how decisions in a sequential three-person game are influenced by either dynamics of roles or group composition. In the game a dictator decides how much of his endowment to transfer to the recipient. The supervisor can punish the dictator and/or transfer to the recipient after learning about the dictator\u2019s decision. We find that transfers by a dictator are highest and stable when the group of three is fixed, no matter whether roles change or not. There is limited support of a leading-by-example effect, that is, only when both role and group composition of subjects are fixed, supervisors give more the more dictators gave, and dictators transfer more in the next period the more supervisors gave in a period. Punishment partially has a disciplining effect on dictators. Finally, we observe that subjects\u2019 actual actions are consistent with their beliefs and expectations.", "title": "Leading-by-example and third-party punishment: Experimental evidence", "claims": null}, {"metadata": {"year": 2014}, "authors": ["E. Winschel", "Philipp Zahn"], "summary": "In most laboratory experiments concerning prosocial behavior subjects are fully informed how their decision influences the payoff of other players. Outside the laboratory, however, individuals typically have to decide without such detailed knowledge. To assess the effect of information asymmetries on prosocial behavior, we conduct a laboratory experiment with a simple non-strategic interaction. A dictator has only limited knowledge about the benefits his prosocial action generates for a recipient. We observe subjects with heterogenous social preferences. While under symmetric information only individuals with the same type of preferences transfer, under asymmetric information different types transfer at the same time. As a consequence and the main finding of our experiment, uninformed dictators behave more prosocially than informed dictators.", "title": "When Ignorance is Bliss - Information Asymmetries Enhance Prosocial Behavior in Dictator Games", "claims": null}, {"metadata": {"year": 2016}, "authors": ["A. Murata"], "summary": "The aim of this study was to demonstrate that indirect mediation in behaviors leads to insensitivity to unethical behavior through a dictator game and to give some implications for safety management. The indirect involvement in the unethical behavior such as the violation of regulation is believed to lessen the responsibility and the criticism from others for the unethical behavior as compared to the direct involvement in it. The instruction condition for the evaluator of behavior in a dictator game was taken up as an experimental variable. Instruction condition 1 was to pay attention to the behavior of only a dictator. In instruction condition 2, the participant (evaluator) was required to review all players\u2019 behavior and evaluate a dictator. It has been investigated whether allowing indirect actions (mediations) leads to reduced punishment as a function of the instruction condition. While the punishment to the indirectness did not get smaller for instruction condition 2, the punishment to the indirectness tended to get smaller only for instruction condition 1.", "title": "Insensitivity to Unethical Behavior in Dictator Game When Indirectly Intermediated-Implications of Indirect Blindness for Safety Management", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Jo Thori Lind", "Karine  Nyborg", "Anna  Pauls"], "summary": "Our lab experiment tests for strategic ignorance about the environmental consequences of one\u2019s actions. In a binary dictator situation based on the design by Dana, Weber, and Kuang (2007), we test whether the option to remain ignorant about the receiver\u2019s payoffs reduces generosity. Our receiver is a charity that engages in carbon offset. Contrary to previous \ufb01ndings by Dana, Weber, and Kuang (2007) and replications, the option to remain ignorant does not decrease generosity. Only 22% of dictators choose ignorance. We test social interaction by allowing another subject to force the dictator to learn the receiver\u2019s payoff, and by allowing the dictator to sanction that subject. When information can be imposed by another subject, almost all dictators choose information themselves, but this does not increase generosity. The possibility of sanctions does not discourage subjects from providing information to dictators.", "title": "Save the planet or close your eyes? Testing strategic ignorance in a charity context", "claims": null}], "query": "how does deliberate ignorance affect punishment in the dictator game?", "summary_abstract": "The collection of studies provides a nuanced understanding of how deliberate ignorance affects punishment in the dictator game. Bartling et al. (2013) found that willfully ignorant dictators are punished less when their actions result in unfair outcomes compared to those who are informed, suggesting that ignorance can serve as a shield against blame. However, when outcomes are fair, ignorance leads to greater punishment, indicating that the act of remaining ignorant is itself subject to censure.\n\nSt\u00fcber (2019) observed that when third parties can choose to remain ignorant of the dictator's decision, altruistic punishment decreases, which in turn can lead to more selfish behavior by dictators if they are aware of this dynamic. This suggests that the option for third parties to remain ignorant can undermine the enforcement of social norms.\n\nRilke (2017) highlighted that the framing of the dictator's actions influences punishment, with actions framed as giving being punished more than those framed as taking, despite similar outcomes. This indicates that perceptions of fairness and the framing of actions can affect punishment decisions, although this study does not directly address deliberate ignorance.\n\nOttone et al. (2008) and Murata (2016) both emphasize the role of subjective fairness and indirect involvement in influencing punishment. Ottone et al. found that third-party punishment is sensitive to perceived fairness, while Murata demonstrated that indirect involvement in unethical behavior can reduce perceived responsibility and punishment.\n\nHeim and Huber (2019) and Winschel and Zahn (2014) provide additional context by exploring how group dynamics and information asymmetries affect dictator behavior. Heim and Huber found that punishment can have a disciplining effect, while Winschel and Zahn observed that uninformed dictators tend to behave more prosocially than informed ones, suggesting that ignorance can sometimes lead to more generous behavior.\n\nFinally, Lind et al. (2019) found that the option to remain ignorant does not necessarily decrease generosity, challenging previous findings. They also noted that when information can be imposed by another subject, dictators tend to choose to be informed, although this does not increase generosity.\n\nOverall, these studies collectively suggest that deliberate ignorance can reduce punishment in certain contexts, particularly when outcomes are unfair, but the act of remaining ignorant can also be punished. The presence of ignorance options can influence both dictator behavior and the effectiveness of punishment as a social norm enforcement mechanism.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2018}, "authors": ["K. Ram", "Raj Kumar", "Sun Ta", "R. Munjal"], "summary": "Carbon requirements for grain filling in wheat are mainly from current assimilation by photosynthesis and remobilization of reserves from the stems (Yang et al., 2000). Remobilization of assimilates is an active process that involves translocation of stored reserves from stems and sheaths to grains (Gupta et al., 2015). Stem reserves contribute 20 to 40% weight of the grain in non-stressed condition (Vignjevic et al., 2015) and this can be up to 70% under stressed conditions during grain filling (Rebetzke et al., 2008). Drought and high temperature induced earlier mobilization of non-structural reserve carbohydrates from stem and leaf sheaths, which provided a greater proportion of grain International Journal of Current Microbiology and Applied Sciences ISSN: 2319-7706 Volume 7 Number 04 (2018) Journal homepage: http://www.ijcmas.com", "title": "Stem Reserve Mobilization in Relation to Yield under Different Drought and High Temperature Stress Conditions in Wheat (Triticum aestivum L.) Genotypes", "claims": null}, {"metadata": {"year": 2004}, "authors": ["A.  Blum"], "summary": "Stem reserves from pre-anthesis plant assimilation are being increasingly recognised as an important source of carbon for grain filling when current photosynthesis is inhibited by drought, heat or disease stress during this stage. Genotypic and environmental factors affecting reserve accumulation and utilisation for grain filling are reviewed. The genetic improvement of stem reserve storage and utilisation as a potent mechanism for grain filling under stress is discussed, and practical guidelines for selection work are provided.", "title": "Improving wheat grain filling under stress by stem reserve mobilisation", "claims": null}, {"metadata": {"year": 1997}, "authors": ["Abraham  Blum"], "summary": "Stem reserves from pre-anthesis plant assimilation are being increasingly recognised as an important source of carbon for grain filling when current photosynthesis is inhibited by drought, heat or disease stress during this stage. Genotypic and environmental factors affecting reserve accumulation and utilisation for grain filling are reviewed. The genetic improvement of stem reserve storage and utilisation as a potent mechanism for grain filling under stress is discussed, and practical guidelines for selection work are provided.", "title": "Improving wheat grain filling under stress by stem reserve mobilisation", "claims": null}, {"metadata": {"year": 2011}, "authors": ["A. Gupta", "K. Kaur", "N. Kaur"], "summary": "The effect of water deficit on stem reserve mobilization and sink activity in wheat (Triticum aestivum L.) cultivars, viz., C306 (drought tolerant) and PBW343 (drought sensitive) was studied. Drought was maintained in pot raised plants by withholding irrigation at 95 days after sowing (DAS), i.e. just five days before the initiation of anthesis. Drought induced a significant reduction in mean biomass of all the internodes of sensitive cultivar as compared to those of tolerant one. Mobilized dry matter and mobilization efficiency were observed to be higher in the internodes of tolerant cultivar, both under control and stress conditions, which resulted in enhanced translocation of stem reserves to the grains. Water soluble carbohydrates (WSC), which mainly occur as fructans, were observed to be higher in the internodes of tolerant cultivar than those of sensitive one. When drought was applied, fructans were mobilized more effectively from the internodes of tolerant cultivar. A significantly higher sucrose synthase activity in the grains of tolerant cultivar, under drought conditions, increased the sink strength by unloading the assimilates in the sink, thereby increasing further mobilization of assimilates to the grains. Grains of sensitive cultivar attained maturity much earlier as compared to the tolerant one, both under control and stress conditions. The longer duration of grain maturation in tolerant cultivar supported enhanced mobilization of stem reserves, thus restricting heavy decrease in grain yield, under stress conditions, as compared to the sensitive cultivar. It may, therefore, be concluded that certain characteristics viz., enhanced capability of fructan storage, higher mobilization efficiency, stronger sink activity and longer duration of grain maturation might help the drought tolerant cultivar in coping the stress conditions", "title": "Stem Reserve Mobilization and Sink Activity in Wheat under Drought Conditions", "claims": null}, {"metadata": {"year": 2018}, "authors": ["S. Gare", "R. Wagh", "A. Ingle", "N. Soni"], "summary": "Wheat (Triticum aestivum) is the first important and strategic cereal crop for the majority of world\u2019s populations. In 100 grams, wheat provides 327 calories and is an excellent source of multiple essential nutrients, such as protein, dietary fiber, manganese, phosphorus and niacin. Several B vitamins and other dietary minerals are in significant content. High temperature (>30\u00b0C) at the time of grain filling is one of the major constraints in increasing productivity of wheat in tropical countries like India (Zhao 2007). This survey/review may likewise help in interdisciplinary study regards to influence of temperature stress on stem reserve mobilization when wheat plants suffer from arrested photosynthesis during stress", "title": "Effect of temperature on stem reserve mobilization for grain development in wheat", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Pireivatlou As", "Aliyev Rt"], "summary": "Two experiments were conducted on 11 diverse wheat genotypes, under both well watered, and drought stress field experiments. Drought decreased vegetative organs (above ground dry matter) weight at anthesis and maturity, grain yield per spike, grains per spike and 1000 grain weight by 5.7, 24.5, 21.2, 15.7 and 6.4 %, respectively. Translocation of dry matter from vegetative organs (above ground dry matter) to developing kernels and mobilization efficiency were considerably increased under drought stress condition, by 60.1 and 74 %, respectively. The contribution of pre anthesis assimilate was also highly increased under the drought stress condition. Translocation of dry matter from the peduncle, penultimate, and the lower internodes ranged from 51.2 to 76.9, 106.8 to 182.3 and from 100 to 208.6 mg under well watered and drought stress conditions, respectively. Mobilization of dry matter was higher under the drought stress condition than in well watered for peduncle by 50.2 %, penultimate internode 70.7 and for lower internodes 111.6 % respectively. Drought increased the contribution of pre-anthesis assimilates to grain yield by 81.5 % in peduncle, 108.1 % in penultimate internode and by 153.8 % in lowerinternodes respectively. Drought was also increased the translocation efficiency by 84, 91.3 and 111.6 % for peduncle, penultimate internode and lower internodes, respectively. Vegetative organ (above ground dry matter) at anthesis was correlated with grain weight per spike (r = 0.68*), translocation of dry matter (r = 0.34) under drought stress condition. Translocation of dry matter was correlated with lower internodes, penultimate and peduncle maximum weight under drought stress condition by r = 0.55, 0.56 and 0.34, respectively.", "title": "Stem reserve and its contribution to grain yield of wheat (Triticum aestivum L.) genotypes under drought stress conditions", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Samuel C\u00f3rdova S\u00e1nchez", "Iris Amairani Garc\u00eda Alejandro", "C. Ruiz", "S. Garc\u00eda", "Ra\u00fal Casta\u00f1eda Ceja", "S. Vel\u00e1zquez", "M. Villegas", "L. D. C. L. Espinoza", "Cintya Valerio C\u00e1rdenas", "Rosa Graciela Santos Arguelles"], "summary": "For the replanting of the sugarcane crop, plants produced from buds of different reserve sizes and from different positions on the stem are used, without up to now an optimum reserve and position size has been determined to obtain a quality plant. Therefore, five treatments were evaluated for nutritional reserve: T1 (short reserve), T2 (without reserve), T3 (half reserve), T4 (long basal reserve) and T5 (long reserve superior) and for the position a design was used 2x3 factorial (two cultivars: MEX69-290 and COLPOSCTMEX05-223 and three yolk positions: basal, middle and apical), the treatments were distributed in a completely randomized arrangement. The buds were sown in unicel vessels with a liter capacity and sifted sand was used as substrate. It was evaluated; emergency percentage, root length and plant height. The results indicate that the use of buds with different positions in the stem is possible to produce seedlings of good quality, since more than 50% of sprouting was obtained in both cultivars. The results on nutrient reserves showed that T3 had the longest root length (36.2 cm) and T1 had the highest plant height (61.4 cm), showing that there is no direct relationship between plant height and root length. To obtain a quality plant it is enough to use short buds or half reserve and any position. For the commercial production of plants, it is recommended to plant in substrates rich in nutrients.", "title": "EFFECT OF NUTRITIONAL RESERVES AND POSITION OF THE YEMAS IN THE STEM IN THE QUALITY OF SUGAR CANE PLANTS", "claims": null}, {"metadata": {"year": 1982}, "authors": ["R. W. Daniels", "M. Alcock", "D. Scarisbrick"], "summary": "SUMMARY Pre-anthesis stem reserve contribution to grain yield was assessed in two spring barley cultivars of contrasting height. It was greatest in the taller, but final grain yields were similar. Partitioning of total reserve capacity to various plant parts showed that the leaf and sheath below the peduncle were most important, followed by stem internodes which had increasing reserve capacity up to internode 4. It is suggested that stem reserves are valuable for yields commonly achieved in U.K. commercial practice. High grain yields were associated with large positive increases in stem dry weight after anthesis. This would indicate that the source capacity to boost yield is more than proportional to that required to fill the grain alone.", "title": "A reappraisal of stem reserve contribution to grain yield in spring barley (Hordeum vulgare L.)", "claims": null}], "query": "wheat stem sugar reserve mobilization", "summary_abstract": "The collection of papers provides a comprehensive understanding of wheat stem sugar reserve mobilization, particularly under stress conditions. Ram et al. (2018) highlight that stem reserves can contribute significantly to grain weight, especially under stress, where they can account for up to 70% of the grain's weight. This mobilization is crucial when photosynthesis is inhibited by environmental stresses such as drought or high temperatures. Blum (2004, 1997) emphasizes the importance of these stem reserves as a carbon source during grain filling under stress, suggesting that genetic improvements in reserve storage and utilization could enhance grain filling. Gupta et al. (2011) further explore the role of stem reserves under drought conditions, noting that drought-tolerant cultivars exhibit higher mobilization efficiency and stronger sink activity, which supports better grain filling and yield maintenance under stress. Pireivatlou and Aliyev (2008) corroborate these findings, showing increased translocation efficiency and contribution of pre-anthesis assimilates to grain yield under drought conditions. Collectively, these studies underscore the critical role of stem sugar reserves in supporting wheat grain filling, particularly under adverse environmental conditions, and suggest that enhancing these reserves could be a viable strategy for improving wheat resilience and yield.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2004}, "authors": ["M. Brettingham"], "summary": "The Labour government's systematic investment in the health service is delivering concrete results, Nigel Crisp, chief executive of the NHS, said last week in his end of year report.\n\nThe financial expansion, which started in 1998 and has seen the NHS receive several large cash injections, is now resulting in shorter waiting times and falling numbers of premature deaths.\n\nThe reduction in waiting times, which the government has been under particular pressure to deliver, gives the report a focus. The number of inpatients on waiting lists has fallen by 35% since it peaked in 1998, falling from more than a million then to 850 000 today.\n\nThe report shows that about 70 000 outpatients are now waiting \u2026", "title": "NHS waiting list has fallen by 35% since 1998", "claims": null}, {"metadata": {"year": 2022}, "authors": ["A. O'dowd"], "summary": "\u201cOur research highlights hope and opportunities to reduce waiting times in the present day: by addressing shortages of healthcare staff andphysical resources urgently; by working with integrated care systems in the spirit of prevention, collaboration, inclusion, and community based models of care; and in aligning a vision for the health service with a plan that brings staff, patients, and the public along on the journey to get there.\u201d", "title": "Multipronged efforts are needed to reduce NHS waiting times, says review", "claims": null}, {"metadata": {"year": 2010}, "authors": ["A. Harrison", "J. Appleby"], "summary": "Abstract Recent authors have proposed that waiting times for elective treatment should be reduced to the point where the costs of doing so exceed the benefits. This paper considers how this criterion could be put into effect. Taking benefits first it argues that these could be estimated in three different ways \u2013 social cost benefit, clinical and user valuation \u2013 that would not necessarily produce consistent results and hence a choice has to be made between them. It then considers the costs of reducing waits and argues, citing relevant evidence, that these may range widely according to whether or not reductions can be achieved through simple management measures or whether more long-term capacity is required. It concludes therefore that the apparently simple criterion proposed for defining the point where waiting times are optimal is hard to establish. Choice of criterion must be made in the light of the overall values that a given health care system is intended to promote.", "title": "Optimising waiting: a view from the English National Health Service", "claims": null}, {"metadata": {"year": 2009}, "authors": ["A. Harrison", "J. Appleby"], "summary": "In recent years, the English NHS has achieved substantial reductions in waiting times for hospital treatment. This paper considers first whether the data used by the Government provide an accurate description of changes in waiting times and identifies some of the limitations of the measures used. It then attempts to identify how reductions have been achieved. It argues that some features of central government policy have been important - such as the use of targets - others, such as the introduction of new private sector capacity have not. It also shows that changes at local level have been critical to achieving the recorded improvements, but the precise impact of these is hard to identify.", "title": "Reducing Waiting Times for Hospital Treatment: Lessons from the English NHS", "claims": null}, {"metadata": {"year": 1996}, "authors": ["C. Gray"], "summary": "Reforms involving the National Health Service (NHS) have greatly reduced the length of waiting lists in the United Kingdom. The key to the reductions was additional funding from the government, the chief executive of the NHS said during a recent visit to Ottawa. Decreasing the size of the waiting lists created intense stress for NHS personnel, who had to work longer hours, and it also lowered demand for private-sector care.", "title": "NHS reforms reduce length of waiting lists but create widespread unease.", "claims": null}, {"metadata": {"year": 2008}, "authors": ["C. Propper", "M. Sutton", "C. Whitnall", "F. Windmeijer"], "summary": "Abstract Waiting times have been a central concern in the English NHS, where care is provided free at the point of delivery and is rationed by waiting time. Pro-market reforms introduced in the NHS in the 1990s were not accompanied by large drops in waiting times. As a result, the English government in 2000 adopted the use of an aggressive policy of targets coupled with the publication of waiting times data at the hospital level and strong sanctions for poor performing hospital managers. This regime has been dubbed \u0091targets and terror\u0092. We estimate the effect of the English target regime for waiting times for hospital care after 2001 by a comparative analysis with Scotland, a neighbouring country with the same healthcare system that did not adopt the target regime. We estimate difference-in-differences models of the proportion of people on the waiting list who waited over 6, 9 and 12 months. Comparisons between England and Scotland are sensitive to whether published or unpublished data are used but, regardless of the data source, the \u0091targets and terror\u0092 regime in England lowered the proportion of people waiting for elective treatment relative to Scotland.", "title": "Did 'Targets and Terror' Reduce Waiting Times in England for Hospital Care?", "claims": null}, {"metadata": {"year": 1981}, "authors": ["R. Feldman", "D. Ballard"], "summary": "This paper estimates a model of demand for the services of general practitioner physicians (GPs) in the British National Health Service (NHS). A wide range of services is available from the NHS to any person free of charge at the time of use. Money prices have been replaced, except for nominal charges for prescriptions and appliances, b^ nonprice rationing methods such as queuing for hospital admissions and waiting for physicians' office visits. Nonprice rationing has been cited as a major difficulty of the NHS [1, p. 219]. While this problem has not surfaced in the United States to the same degree as in England, it is nevertheless instructive to study the British experience. About 40 percent of U.S. health expenditures in 1980 were paid by public sources [2] and, if national health insurance legislation is enacted here, the public share could rise toward the level observed in the nationalized British system. In addition, some private health insurance plans in the U.S. have features that resemble the NHS, e.g., free care at the time of use. The NHS may thus provide a model for the analysis of these U.S. plans. We examine the effect of office waiting time on the demand for GPs' services. Using a utility-maximizing model of physicians' behavior, we predict that waiting time should be directly related to exogenous demand pressures on the physician's practice. Longer waiting time, in turn, should reduce both the number of visits per person and the number of patients in the physician's practice (which the British call the physician's \"listsize\"). We specify and estimate an empirical version of the model with three equations: waiting time, visits per patient, and listsize. A key finding is that the elasticity of demand for GPs' services with respect to waiting time is between -.48 and -.92, depending on our definition of variables and functional form. This result has important implications for national health insurance for outpatient visits (if such legislation is passed in the U.S.). If insurance were to reduce money prices to zero, demand would become relatively more sensitive to differences in time prices. In turn, as Acton has noted [3], this would permit persons with a lower opportunity cost of time to bid services away from those with a higher opportunity cost of time. National health insurance might, therefore, have important distributional effects, as well as the intended reduction of money price barriers to access. Even without national health insurance, policy-makers in the U.S. might want to study the British National Health Service. Increas-", "title": "The Role of Waiting Time in a Prepaid Health Care System: Evidence from the British National Health Service", "claims": null}, {"metadata": {"year": 1986}, "authors": ["E. Scrivens", "G. Hart"], "summary": "Ministers are anxious to see shorter hospital waiting lists. Instead of buying services from the private sector, health authorities may do better by trading with each other. But, if the full potential of trading is to be realised, the financing and management of the NHS will have to alter.", "title": "How to reduce waiting lists: A case for trading within the NHS", "claims": null}], "query": "Health economic benefits of reducing NHS waiting times", "summary_abstract": "The collection of papers provides a comprehensive view of the health economic benefits associated with reducing NHS waiting times. Brettingham (2004) highlights that significant financial investments in the NHS have led to shorter waiting times and a reduction in premature deaths, indicating a direct health benefit from reduced waiting periods. Harrison and Appleby (2010) discuss the complexities of balancing the costs and benefits of reducing waiting times, suggesting that while the benefits can be measured in various ways, the costs can vary significantly depending on the methods used to achieve reductions. This implies that economic benefits are contingent on the efficiency of the strategies employed.\n\nPropper et al. (2008) provide evidence that the \"targets and terror\" regime in England, which involved setting aggressive targets and publishing waiting times, effectively reduced waiting times compared to Scotland, which did not adopt such measures. This suggests that policy-driven reductions in waiting times can lead to economic benefits by improving service efficiency and patient throughput.\n\nGray (1996) notes that government funding was crucial in reducing waiting lists, which in turn decreased the demand for private-sector care, indicating a potential economic benefit by retaining patients within the public system. Feldman and Ballard (1981) explore the impact of waiting times on the demand for GP services, finding that longer waiting times reduce the number of visits and patients, which could imply economic losses due to decreased service utilization.\n\nOverall, the papers collectively suggest that reducing NHS waiting times can lead to significant health economic benefits, including improved health outcomes, increased efficiency, and reduced reliance on private care. However, the realization of these benefits depends on the strategic implementation of policies and the efficient allocation of resources (Brettingham, 2004; Harrison & Appleby, 2010; Propper et al., 2008; Gray, 1996; Feldman & Ballard, 1981).", "summary_extract": null}, {"papers": [{"metadata": {"year": 1999}, "authors": ["Raymond Papp"], "summary": "Last Fall, my university began an initiative to offer courses online in what many call a \"distance learning\" environment. Being an early adopter of the Internet and WWW in my traditional on-campus courses, I jumped at the chance to offer a course in an on-line environment. This paper will briefly explore the differences between on-campus and on-line education, discuss my approach for implementing the course (including a comparison/contrast of student perceptions and performance in both on-campus and on-line sections of the same course), and conclude with some implications for educators endeavoring to teach in a distance learning environment.", "title": "\"On-campus\" vs. \"On-line\": Student Perceptions & Performance", "claims": null}, {"metadata": {"year": 2001}, "authors": ["Xiaoyan Xie", "Fuzong Lin", "Zhang Tao"], "summary": "More and more universities and colleges are providing online courses not only for on-campus students but also for off-campus students. Tutors have to consider the differences between on- and off-campus students in order to improve effective instruction. Comparisons are made in this paper between on- and off-campus performances in online learning from four areas: learning time, path of browsing courseware, intercommunication and adaptability towards online learning. The last two areas are emphasized. Multiple approaches were adopted to collect data, which include questionnaires, posted documents, online logs, interviews and observations. This study shows that the rush time of online learning, paths of browsing courseware and favourite intercommunication means of on- and off-campus students are similar. But there are also some differences between these two groups such as competence of self-learning, enthusiasm of interpersonal exchange, dependence on tutors, feeling of learning stress, etc.", "title": "Comparison between on- and off-campus behaviour and adaptability in online learning: A case from China", "claims": null}, {"metadata": {"year": 2005}, "authors": ["G. Ury", "M. McDonald", "G. McDonald", "Brian Dorn"], "summary": "In previous papers, the authors reported results of smaller-scale studies. The purpose of this study is to combine and expand those individual smaller scaled studies, to determine if significant differences exist between student performance in online and traditional classroom environments. The study includes more than 1300 observations spread across seven courses that are part of the computer science and information systems curriculum at Northwest Missouri State University. Student performance was compared by grade point average, ACT composite scores, number of credit hours completed, instructor, and delivery method. The only significant difference found was between student performance and delivery method in three high volume courses that serve multiple majors and minors. Online students in these three courses obtained a significantly lower average grade than onground students. In four other courses that service upper-level computer science majors no significant differences in performance were found. The varied results of the study could be a simple statement of fact. Different courses in different programs might have different performance results. It could be concluded that online students are simply satisfied with a little lower grade in particular courses or that traditional students perform better because of the availability of added resources planned and implemented for online curriculum. It could be concluded that faculty continue to deal with problems in effectively transferring traditional classroom learning to the online environment. It could also be speculated that the difference is any combination of the above conclusions.", "title": "Student Performance Online vs Onground: A Statistical Analysis of IS Courses", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Di  Xu", "Shanna S. Jaggars"], "summary": "Using a dataset containing nearly 500,000 courses taken by over 40,000 community and technical college students in Washington State, this study examines the performance gap between online and face-to-face courses and how the size of that gap differs across student subgroups and academic subject areas. While all types of students in the study suffered decrements in performance in online courses, those with the strongest declines were males, younger students, Black students, and students with lower grade point averages. Online performance gaps were also wider in some academic subject areas than others. After controlling for individual and peer effects, the social sciences and the applied professions (e.g., business, law, and nursing) showed the strongest online performance gaps.", "title": "Performance Gaps Between Online and Face-to-Face Courses: Differences Across Types of Students and Academic Subject Areas", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Di Xu", "S. Jaggars"], "summary": "Using a dataset containing nearly 500,000 courses taken by over 40,000 community and technical college students in Washington State, this study examines the performance gap between online and face-to-face courses and how the size of that gap differs across student subgroups and academic subject areas. While all types of students in the study suffered decrements in performance in online courses, those with the strongest declines were males, younger students, Black students, and students with lower grade point averages. Online performance gaps were also wider in some academic subject areas than others. After controlling for individual and peer effects, the social sciences and the applied professions (e.g., business, law, and nursing) showed the strongest online performance gaps.", "title": "Performance Gaps between Online and Face-to-Face Courses: Differences across Types of Students and Academic Subject Areas", "claims": null}, {"metadata": {"year": 2016}, "authors": ["C. B. Gregory", "J. Lampley"], "summary": "As part of a nationwide effort to increase the postsecondary educational attainment levels of citizens, community colleges have expanded offerings of courses and programs to more effectively meet the needs of students. Online courses offer convenience and flexibility that traditional face-to-face classes do not. These features appeal to students with family and work responsibilities that typically make attending classes on campus difficult. However, many of the students who tend to take courses in this instructional format have characteristics that place them at high-risk for academic failure. Because of the traditional mission of community colleges, they generally serve more students who fit this high-risk profile. Despite the promise and potential of online delivery systems, studies have associated distance with higher student withdrawal rates. In addition, research has indicated that online students tend to earn lower grades than students in comparable face-to-face classes. The existence of contrasting findings in the literature exposes the need for additional empirical research relative to the overall success of students in online courses, as well as on factors associated with success in distance education. This is especially true for community college students. The purpose of this study was to determine if significant differences existed in student success at the community college level in online courses as compared to face-to-face courses. In addition, the researchers investigated the relationship between selected demographic, academic, enrollment, and external environmental factors and student success in online courses. The study involved secondary data analysis of quantitative data relevant to students enrolled in course sections taught by instructors who taught both online and face-to-face sections of the same course within the same semester from fall 2012 through spring 2015. The target population included 4,604 students enrolled at a public 2-year community college located in Tennessee. Results indicated there was a significant difference in success between students taking a course online and students taking a course face-to-face. Also, there was a significant difference in success based on instructional method when the following factors were considered: age group, gender, student academic classification, and Pell Grant eligibility status. There was no significant difference in success based on instructional method when first-generation college student status was considered.", "title": "Community College Student Success in Online Versus Equivalent Face-to-Face Courses", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Di  Xu", "Shanna Smith Jaggars"], "summary": "Using a large administrative dataset from a statewide system including 34 community and technical colleges, the authors employed an instrumental variable technique to estimate the impact of online versus face-to-face course delivery on student course performance. The travel distance between each student's home and college campus served as an instrument for the likelihood of enrolling in an online section of a given course. In addition, college-by-course fixed effects controlled for within- and between-course selection bias. Analyses yield robust negative estimates for online learning in terms of both course persistence and course grade, contradicting the notion that there is no significant difference between online and face-to-face student outcomes\u2014at least within the community college setting. Accordingly, both two-year and four-year colleges may wish to focus on evaluating and improving the quality of online coursework before engaging in further expansions of online learning.", "title": "The impact of online learning on students\u2019 course outcomes: Evidence from a large community and technical college system", "claims": null}, {"metadata": {"year": 1999}, "authors": ["R. Sims", "Allan H. Schuman"], "summary": "The past five years have borne witness to a revolution in education with an acceleration in the use of online technologies to assist or, in many cases, supplant traditional modes of instruction (Bjorner 1993; Velsmid 1997). Peterson's Guide reports that nearly 400 accredited colleges and universities in North America currently employ online instruction of some sort (Velsmid). In addition, Herther (1997) noted that over 150 accredited institutions offer entire bachelor's degree programs to students who rarely, if ever, visit campus. The asynchronous nature of many online programs together with their accessibility from home, office, or hotel room are obvious advantages to students (see Bjorner). Additionally, as the cost of traditional education increases, market pressures are forcing more and more institutions to consider online offerings (see Gubernick and Ebeling 1997) that do not incur the costs of dormitories, athletic programs, etc. The Florida State University system expects online programs to save about 40% of the cost of in-class programs (\"Caught\" 1998). It should be noted, however, that Duke University charges a premium for its online MBA ($82,500 vs. $50,000 for its on-campus equivalent). As more and more online courses and programs proliferate, the questions of quality and comparability of such instruction with traditional methods naturally arise. Gubernick & Ebeling report a study conducted by the University of Phoenix (a private, for-profit institution) that demonstrated standardized achievement test scores of its online graduates were 5% to 10% higher than graduates of competing on-campus programs at three Arizona public universities. While one may legitimately question the degree of comparability of the subject populations, these results are similar to those summarized by Vasarhelyi and Graham (1997) in which investigators at the University of Michigan concluded that computer-based instruction yielded higher average scores than traditional instruction. To date, the most methodologically sound investigation to evaluate the effectiveness of online instruction was conducted by Gerald Schutte at Cal State, Northridge (as cited by McCollum 1997). \"Schutte randomly divided his statistics class into two groups. One attended class as usual, listening to lectures, handing in homework assignments, and taking examinations. The other took an online version of the course, completing assignments on a World Wide Web site, posting questions and comments to an electronic discussion list, and meeting with their professor in an Internet chat room. After an orientation session, students in the virtual class went to Dr. Schutte's classroom only for their midterm and final exams. On both tests, Dr. Schutte found, the wired students outscored their traditional counterparts by an average of 20 percent.\" The present study extends Schutte's paradigm by looking at pre and posttest scores of students enrolled in online and inclass versions of the same class taught by the same instructors over a variety of disciplines. Methodology Students enrolled in five different undergraduate online courses during the Fall semester 1997 participated in a test-retest study designed to measure their learning of the course material. These students were compared with students enrolled in traditional inclass courses taught by the same instructors. The course titles were Organization Behavior, Personal Finance, Managerial Accounting, Sociological Foundations of Education, and Environmental Studies. Student participation was voluntary; names were only used to ensure a matching of the pre / posttest results. Subjects In total, 40 undergraduate students were enrolled in the online courses and 59 undergraduate students were enrolled in the inclass courses during the testing period. Pretests Instructors designed pretests to measure the level of knowledge students had of the course content prior to the start of the course. \u2026", "title": "Learning in an Online Format versus an In-Class Format: An Experimental Study", "claims": null}], "query": "Online university versus on campus and academic performance differences", "summary_abstract": "The collection of studies provides a nuanced view of the differences in academic performance between online and on-campus university courses. Several studies indicate that students in online courses often perform worse than their on-campus counterparts. For instance, Ury et al. (2005) found that online students in certain high-volume courses achieved significantly lower grades compared to on-campus students, although no significant differences were observed in upper-level courses. Similarly, Xu and Jaggars (2013, 2014) reported that students generally performed worse in online courses, with notable performance gaps in specific demographics such as males, younger students, Black students, and those with lower GPAs. These gaps were also more pronounced in certain subject areas like social sciences and applied professions.\n\nGregory and Lampley (2016) also observed that online students at community colleges tend to earn lower grades and have higher withdrawal rates compared to those in face-to-face classes. This trend was consistent across various demographic factors, although no significant difference was found concerning first-generation college student status.\n\nConversely, some studies suggest that online education can be as effective as traditional methods under certain conditions. Sims and Schuman (1999) highlighted a study where online students outperformed their on-campus peers in standardized tests, suggesting that online instruction can yield higher average scores. However, the comparability of these findings is questioned due to potential differences in subject populations.\n\nOverall, while online education offers flexibility and accessibility, it often comes with challenges that can impact student performance negatively compared to traditional on-campus education. The findings underscore the need for further research and improvements in online course delivery to bridge these performance gaps.", "summary_extract": null}, {"papers": [{"metadata": {"year": 1999}, "authors": ["Hui Chen", "Yan-jun Fang", "Li Yuan", "T. An"], "summary": "An on-line determination of chlorine dioxide (ClO2) in potable water using chlorophenol red (CPR) by gas diffusion flow-injection analysis (FIA) was investigated in the presence of various chlorinated species that can occur under normal water-treatment conditions. A gas diffusion membrane is used to separate the donor (sample) stream from the acceptor (detecting) stream (the donor stream transports the sample stream to the membrane separate device, and the acceptor stream collects all of the penetrated analytes and transports quantitatively to the detector) and makes it possible for this method to eliminate interference from metal ions, as well as other oxychlorinated compounds such as chlorite and chlorate. The system is more selective for chlorine dioxide than chlorine. The linear range of ClO2 concentration is 0\u20130.5 mg\u00b7mL\u22121 with a detection limit of 0.02 \u03bcg\u00b7mL\u22121 (S/N = 3) and a sampling frequency of 50 h\u22121. \u00a9 1999 John Wiley & Sons, Inc. Lab Robotics and Automation 11: 157\u2013161, 1999", "title": "An on\u2010line determination of chlorine dioxide using chlorophenol red by gas diffusion flow\u2010injection analysis", "claims": null}, {"metadata": {"year": 2007}, "authors": ["F. Tran", "D. Rouleau", "D. Couillard"], "summary": "A new model for mixtures (two and more solutes) of aqueous electrolyte solutions was found to be as accurate as other models, or more accurate, in prediction of new experimental results of the ternary systems HClO4\u2013NaClO3\u2013H2O and HClO4\u2013NaCl\u2013H2O. The water activity values are then used to study the mechanism of the chloride\u2013chlorate reaction, generating chlorine dioxide: \n \n \n \n2H+ + ClO\u22123 + Cl\u2212 ClO2 + 1/2Cl2 + H2O \n \n \n \nSpectrophotometric measurements of the production rate of ClO2 have confirmed that the intermediary species in the proposed equilibrium \n \n \n \nH+(mH2O) H+(m-n)H2O + nH2O \n \n \n \nIs actually H(H20)+m-n. The final kinetic expression for the reaction rate of chlorine dioxide generation, used in pulp bleaching, is then derived to explain the high order with respect to acid concentration.", "title": "Study of ClO2 generation reaction via experimental data of aqueous mixed electrolyte solutions: HClO4\u2013NaCLO3\u2013H2O and HCLO4\u2013NaCl\u2013H2O", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Liu Xiao-geng"], "summary": "A new spectrophotometric method for the determination of chlorine dioxide(ClO2) in tap water was developed based on methyl red(MR) oxidation and discoloration.Wavelength scanning showed that MR had maximal absorption at 518 nm.Under the experiment conditions of c(MR)/c(ClO2) = 6.4,pH 4.0-4.5,25 \u2103 and reaction time of 40 min,a linear equation was obtained as \u0394A518 =-1.7845c(ClO2) +1.5228(r = 0.992) in the range of 0.0004-0.80 mg/L,the apparent molar absorptivitye = 1.2 \u00d7 105 L/(mol\u00b7cm),and the detection of limit 0.0036 mg/L.MR discoloration by ClO2 oxidation was a first-order reaction.The dynamic equation at 25 \u2103 was ln[c\u221e/(c\u221e-c)] =-0.000689t + 2.263669 with the activation energy Ea = 87.3 kJ/mol and the half-time t1/2 = 16.8 min.The average recoveries for ClO2 in tap water and ClO2 disinfectant were 98.2%-103.2%(n = 5) with a relative standard deviation of 2.43%-3.83%.This method and the recommended standard N,N-diethyl-p-phenylendiamin(DPD) method showed no significant difference(P = 0.05) according to F-test and t-test.Furthermore,satisfying determination results were obtained from this method.", "title": "Spectrophotometric Determination of Chlorine Dioxide in Tap Water Using Methyl Red", "claims": null}, {"metadata": {"year": 2005}, "authors": ["L. Zhong-bin"], "summary": "The optimum reaction conditions of ClO_2-Fe \u2161 -luminal chemiluminescence CL system were studied by flow injection technique in detail . A new method of CL for the determination of chlorine dioxide is proposed . The detection limit of the method is 4.0 \u03bcg/L ClO_2 and the linear range is 4.0\uff5e680 \u03bcg/L. The method is highly selective , simple , sensitive and has been applied to the determination of ClO_2 in tap water with satisfactory results .", "title": "ClO_2-Fe(II)-Luminal Flow Injection Chemiluminescence System and Its Analytical Application", "claims": null}, {"metadata": {"year": 2001}, "authors": ["Z. Ying", "S. Yao"], "summary": "A method was described for determination of chlorine dioxide in water samples by using porosity polytetrafluoroethylene membrane to separate ClO 2 from ClO -,ClO - 2,ClO - 3 et al and UV absorption spectroscopy. The linear range is 0.24 mg/L\uff5e11.81 mg/L for ClO 2 with detection limit of 5.2 mg/L. The recovery was 96.8\uff5e102 8% with the standard addition method.", "title": "Continuous Determination of Chlorine Dioxide in Water Samples by Membrane Seperation and UV Spectroscopy", "claims": null}, {"metadata": {"year": 2001}, "authors": ["Z. Ying"], "summary": "A method is described for the determination of chlorine dioxide by using porosity polytetrafluoroethylene membrane In this paper, the method is studied according to analytical application in a drinking water plant The conclusion is come to that there is no obvious difference between the method and another method which is commended by Vulan company(USA)if the concentration of ClO 2 in water is more than or equal to 0 08mg/L", "title": "The application of continuous determination of chlorine dioxide by UV-Vis and using membrane to separate gas from water", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Sun Da-hu"], "summary": "Objective To establish differential spectrophotometry method to determine chlorine dioxide(ClO2) in drinking water.Methods The differential spectrophotometry method was conducted to scan in wave length of 574 nm.Results The experiment demonstrated that linear correlation coefficient was high when the ranges were 0.10-0.50 mg/L,0.26-1.31 mg/L,and 0.50-2.70 mg/L,the linearity was good.Conclusion The method has good accuracy and precision,it is suitable for determination of low concentration chlorine dioxide.", "title": "Determination of Chlorine Dioxide in Water by Chlorophenol Red Differential Spectrophotometry", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Zhao Chang-jun"], "summary": "A reverse flow injection spectrophometric method for the determination of chlorine dioxide was developed with leucomethylene blue as the coloration reagent. By this method low concentration chlorine dioxide in water was determined in the presence of Cl_2 and anion species such as ClO~-, ClO~-_2 and ClO~-_3. The linear range of ClO_2 measurement is 0~1.91 mg/L with a detection limit of 0.028 mg/L. The presence of chlorine and hypochlorite ion can be masked by using oxalic acid. Chlorate does not interfere with the measurement. The permissible maximum concentration of ClO~-_2 is 3.0 mg/L.", "title": "New method for the flow injection spectrophotometric deternination of low concentration chlorine dioxide in water using methylene blue", "claims": null}], "query": "ClO/ClO2 data in water analysis", "summary_abstract": "The collection of papers provides a comprehensive overview of various methods for analyzing chlorine dioxide (ClO2) in water, highlighting their effectiveness, sensitivity, and selectivity. Chen et al. (1999) developed an on-line gas diffusion flow-injection analysis method that effectively separates ClO2 from other chlorinated species, offering a detection limit of 0.02 \u03bcg/mL and a high sampling frequency. Tran et al. (2007) explored the kinetics of ClO2 generation in aqueous solutions, providing insights into the reaction mechanisms involved. Xiao-geng (2011) introduced a spectrophotometric method using methyl red oxidation, achieving a detection limit of 0.0036 mg/L and demonstrating high accuracy and precision. Zhong-bin (2005) proposed a chemiluminescence method with a detection limit of 4.0 \u03bcg/L, which is both simple and sensitive. Ying and Yao (2001) utilized a porosity polytetrafluoroethylene membrane for UV absorption spectroscopy, achieving a detection limit of 5.2 mg/L. Da-hu (2010) established a differential spectrophotometry method with good accuracy for low concentrations of ClO2. Lastly, Chang-jun (2005) developed a reverse flow injection spectrophotometric method using leucomethylene blue, with a detection limit of 0.028 mg/L, effectively masking interference from chlorine and hypochlorite ions. Collectively, these studies demonstrate a range of analytical techniques for ClO2 detection in water, each with unique advantages in terms of sensitivity, selectivity, and applicability.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2022}, "authors": ["S. Veretenenko"], "summary": "The stratospheric polar vortex is a large-scale cyclonic circulation that forms in a cold air mass in the polar region and extends from the middle troposphere to the stratosphere. The polar vortex is implicated in a variety of atmospheric processes, such as the formation of ozone holes, the North Atlantic and the Arctic Oscillations, variations in extratropical cyclone tracks, etc. The results presented in this work show that the vortex plays an important part in the mechanism of solar activity influence on lower atmosphere circulation, with variations in the vortex intensity being responsible for temporal variability in the correlation links observed between atmospheric characteristics and solar activity phenomena. In turn, the location of the vortex is favorable for the influence of ionization changes associated with charged particle fluxes (cosmic rays, auroral and radiation belt electrons) that affect the chemical composition and temperature regime of the polar atmosphere as well as its electric properties and cloudiness state. In this work, recent results concerning solar activity effects on the state of the stratospheric polar vortex as well as its role in solar\u2013atmospheric links are discussed.", "title": "Stratospheric Polar Vortex as an Important Link between the Lower Atmosphere Circulation and Solar Activity", "claims": null}, {"metadata": {"year": 1959}, "authors": ["C. E. Palmer"], "summary": "In winter the polar stratospheric air within the earth's shadow forms the core of an intense \u2018cold low\u2019 which extends from about 10 km to at least 50 km and possibly to the base of the ionosphere. Compared with the tropospheric general circulation, this vortex seems to be remarkably stable, particularly in the Southern Hemisphere. \n \nRecent research work in Canada and the United States on the characteristics of the vortex is reviewed. The chief conclusions are that the vortex is more stable in the Southern than in the Northern Hemisphere, that \u2018explosive warmings\u2019 in the lower stratosphere of the Northern Hemisphere follow the breakdown of the vortex at high levels close to the pole, and that the breakdown extends from above downward over a period of several days. It is suggested that the high-level breakdown is correlated with solar activity.", "title": "The stratospheric polar vortex in winter", "claims": null}, {"metadata": {"year": 2019}, "authors": ["S. Lee", "A. Butler"], "summary": "The stratospheric polar vortex is a westerly circulation that forms over the winter pole around 10-50 km above the surface, which is known to influence mid-latitude weather patterns. During 2018-19, the Arctic polar vortex demonstrated an unusually large amount of variability,\r\nincluding a strong and persistent sudden stratospheric warming (SSW) event, a strong vortex event, and a dynamic final stratospheric warming (FSW). In this article we discuss the evolution of the vortex, placing it in the context of wider observed climatology, and comment on its apparent impacts on tropospheric weather patterns \u2013 notably, the lack of a surface climate response to the SSW of similar magnitude to the February-March 2018 \u201cBeast from the East\u201d cold-wave.", "title": "The 2018\u20132019 Arctic stratospheric polar vortex", "claims": null}, {"metadata": {"year": 2000}, "authors": ["G. Manney", "J. L. Sabutis"], "summary": "The 1999\u20132000 Arctic stratospheric vortex was unusually cold, especially in the early winter lower stratosphere, with a larger area near polar stratospheric cloud formation temperatures in Dec and Jan, and much lower temperatures averaged over Nov\u2013Jan, than any previously observed Arctic winter. In Nov and early Dec, there was a double jet in the upper stratosphere, with the anticyclone cutoff in a region of cyclonic material. By late Dec, there was a discontinuous vortex, large in the upper stratosphere, small in the lower stratosphere; evolving to a strong, continuous, relatively upright vortex by mid\u2010Jan. This vortex evolution in 1999\u20132000 is typical of that in other cold early winters. Despite unusually low temperatures, the lower stratospheric vortex developed more slowly than in previous unusually cold early winters, and was weaker than average until late Dec.", "title": "Development of the polar vortex in the 1999\u20132000 Arctic winter stratosphere", "claims": null}, {"metadata": {"year": 2013}, "authors": ["D. Waugh", "L. Polvani"], "summary": "The Stratosphere: Geophysical Mon Copyright 2010 b 10.1029/2009GM The intense cyclonic vortices that form over the winter pole are one of the most prominent features of the stratospheric circulation. The structure and dynamics of these \u201cpolar vortices\u201d play a dominant role in the winter and spring stratospheric circulation and are key to determining distribution of trace gases, in particular ozone, and the couplings between the stratosphere and troposphere. In this chapter, we review the observed structure, dynamical theories, and modeling of these polar vortices. We consider both the zonal mean and three-dimensional potential vorticity perspective and examine the occurrence of extreme events and long-term trends.", "title": "Stratospheric Polar Vortices", "claims": null}, {"metadata": {"year": 2020}, "authors": ["S. Lee"], "summary": "The polar vortex is the dominant feature of the wintertime stratosphere. Sometimes, it is unusually strong, while at others it is very weak or destroyed in an event known as a sudden stratospheric warming (SSW). Both can play an important role in driving the type and predictability of the weather we experience at the surface.", "title": "The stratospheric polar vortex and sudden stratospheric warmings", "claims": null}, {"metadata": {"year": 2018}, "authors": ["V. Harvey", "C. Randall", "L. Goncharenko", "E. Becker", "J. France"], "summary": "The polar vortices play a central role in vertically coupling the atmosphere from the ground to geospace by shaping the background wind field through which atmospheric waves propagate. This work extends the vertical range of previous polar vortex climatologies into the upper mesosphere. The mesospheric polar vortices are defined using the CO gradient method with Microwave Limb Sounder satellite data; the stratospheric polar vortices are defined using a stream function\u2010based algorithm with data from meteorological reanalyses. Strengths and weaknesses of the two vortex definitions are given, as well as recommendations for when, where, and why to use each definition. Midwinter mean vortex geometry in the mesosphere is funnel shaped in the Arctic, with a wide top and narrow bottom. The Antarctic mesospheric vortex tapers with height in early winter and broadens with height in late winter. The seasonal evolution of mesospheric vortex frequency of occurrence, size, and zonal symmetry in both hemispheres is presented. Unexpected behavior above 60 km includes late season vortex broadening in both hemispheres, especially following winters without sudden stratospheric warmings. Following extreme stratospheric disturbances the polar night jet in the mesosphere strengthens and shifts poleward, resulting in a mesospheric vortex that contracts. Overall, the mesospheric polar vortices are more similar between the two hemispheres than their stratospheric counterparts. The vortex climatology presented here serves as an observational benchmark to which the mesospheric polar vortices in high\u2010top climate models can be evaluated.", "title": "On the Upward Extension of the Polar Vortices Into the Mesosphere", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Alvaro de la C\u00e1mara", "Carlos R. Mechoso", "Kayo  Ide", "Richard L. Walterscheid", "Gerald  Schubert"], "summary": "The present paper examines the vortex breakdown and large-scale stirring during the final warming of the Southern Hemisphere stratosphere during the spring of 2005. A unique set of in situ observations collected by 27 superpressure balloons (SPBs) is used. The balloons, which were launched from McMurdo, Antarctica, by the Strat\u00e9ole/VORCORE project, drifted for several weeks on two different isopycnic levels in the lower stratosphere. We describe balloon trajectories and compare them with simulations obtained on the basis of the velocity field from the GEOS-5 and NCEP/NCAR reanalyses performed with and without VORCORE data. To gain insight on the mechanisms responsible for the horizontal transport of air inside and outside the well-isolated vortex we examine the balloon trajectories in the framework of the Lagrangian properties of the stratospheric flow. Coherent structures of the flow are visualized by computing finite-time Lyapunov exponents (FTLE). A combination of isentropic analysis and FTLE distributions reveals that air is stripped away from the vortex\u2019s interior as stable manifolds eventually cross the vortex\u2019s edge. It is shown that two SPBs escaped from the vortex within high potential vorticity tongues that developed in association with wave breaking at locations along the vortex\u2019s edge where forward and backward FTLE maxima approximately intersect. The trajectories of three SPBs flying as a group at the same isopycnic level are examined and their behavior is interpreted in reference to the FTLE field. These results support the concept of stable and unstable manifolds governing transport of air masses across the periphery of the stratospheric polar vortex.", "title": "Polar night vortex breakdown and large-scale stirring in the southern stratosphere", "claims": null}], "query": "polar vortex stretching stratosphere", "summary_abstract": "The collection of papers provides a comprehensive view of the dynamics and implications of the polar vortex stretching in the stratosphere. The polar vortex is a significant feature of the stratospheric circulation, influencing atmospheric processes and weather patterns (Waugh & Polvani, 2013; Lee, 2020). The stability and variability of the vortex differ between hemispheres, with the Southern Hemisphere generally exhibiting more stability (Palmer, 1959). However, events such as sudden stratospheric warmings (SSWs) can disrupt this stability, leading to significant changes in the vortex structure and subsequent impacts on weather patterns (Lee & Butler, 2019; Lee, 2020).\n\nThe stretching and breakdown of the vortex are influenced by solar activity and atmospheric waves, which can lead to changes in the chemical composition and temperature of the polar atmosphere (Veretenenko, 2022; Harvey et al., 2018). The 1999\u20132000 Arctic vortex, for instance, demonstrated a complex evolution with a double jet structure and significant temperature variations, highlighting the dynamic nature of the vortex (Manney & Sabutis, 2000).\n\nThe interaction between the vortex and atmospheric waves is crucial for understanding the transport of air masses and the distribution of trace gases like ozone (de la C\u00e1mara et al., 2010; Waugh & Polvani, 2013). The mesospheric polar vortices, which extend the vertical range of the stratospheric vortices, also play a role in coupling the atmosphere from the ground to geospace, further illustrating the interconnectedness of atmospheric layers (Harvey et al., 2018).\n\nOverall, the papers collectively emphasize the importance of the polar vortex in atmospheric dynamics, its susceptibility to external influences such as solar activity, and its role in shaping weather patterns and atmospheric composition.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2018}, "authors": ["W. Young", "R. Gearry", "Paul Cotter", "K. Fraser", "W. McNabb", "N. Roy"], "summary": "Irritable Bowel Syndrome (IBS) is a functional gastrointestinal (GI) disorder featuring chronic or recurrent abdominal discomfort, usually with changes in GI habit. However, the mechanisms responsible for IBS are poorly understood. Although alterations in the GI microbiome has been implicated in IBS, there is a lack of consensus on what the exact role of the microbiome is, and how it changes, in IBS. To gain a better understanding of the link between the microbiome and IBS, we undertook shotgun metagenomic sequencing of faecal samples from a case\u2010control study. The overall aim of the study was to identify microbial and metabolomic factors that provide mechanistic insights into functional GI disorders and increase the predictability of phenotypes for use in nutrition intervention studies. Faecal samples from 112 individuals with functional GI symptoms (cases) or those that were asymptomatic (controls) were shotgun sequenced using the Illumina NextSeq platform. Taxonomic classifications were determined using Metaxa2 and the SILVA 128 database. Gene functions were assigned with the MG\u2010RAST pipeline. Bacterial genera that discriminated case\u2010controls from IBS groups included Faecalibacterium, Blautia, Roseburia, Bilophila, and Streptococcus, which were relatively more abundant in certain IBS subtypes. For example, Faecalibacterium was relatively more abundant in individuals presenting with constipation associated IBS (IBS\u2010C; P<0.001), while Blautia was more prominent in those with the diarrhea form of IBS (IBS\u2010D; P=0.015). Gene functions that best separated groups included those related to carbohydrate metabolism (higher in IBS compared to case\u2010controls), protein metabolism, and virulence factors. Our results suggest that carbohydrate fermentation by the microbiome may be an important factor in IBS. This work shows that efforts to understand the role of the microbiome in IBS need to include consideration of microbial function rather than just microbial composition.", "title": "Exploring the link between Irritable Bowel Syndrome and the microbiome", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Sampan  Attri", "Ravinder  Nagpal", "Gunjan  Goel"], "summary": "Abstract The present study characterized the colonization and development of gut microbial communities in healthy Indian infants from North-Western Himalayan region in the province Himachal Pradesh. The diversity and transitions of core genera was assessed targeting the 16S rRNA V3-V4 hypervariable region on an Illumina platform. Analysis of more than 17,000 filtered high quality reads indicated that the diversity was lowest in the month 2 followed by gradual increase towards month 4 (1.24 folds increase in Shannon index). The microbial population in month 1 was dominated by Firmicutes and Proteobacteria followed by dominance of Actinobacteria and Firmicutes in the month 4. The analysis of aggregate microbiota at class level indicated relatively higher abundance of Clostridia, Bacteroides and Actinobacteria in month 1, 3 and 4, respectively. The global comparison of dominance of different phyla with the similar subjects indicated that the Indian microbiome is more similar with studies conducted with Swedish infants, although the differences in DNA extraction protocols, geographical location and sequencing platforms as confounding factors cannot be neglected. The findings in this small cohort study could facilitate future studies exploring various aspects of the human gut microbiome in Indian subcontinent.", "title": "High throughput sequence profiling of gut microbiome in Northern Indian infants during the first four months and its global comparison", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Yuxia Liu", "Wenhui Li", "Hongxia Yang", "Xiaoying Zhang", "Wenxiu Wang", "Sitong Jia", "Beibei Xiang", "Yi Wang", "Lin Miao", "Han Zhang", "Lin Wang", "Yujing Wang", "Jixiang Song", "Yingjie Sun", "Lijuan Chai", "Xiaoxuan Tian"], "summary": "Irritable bowel syndrome (IBS) is a chronic gastrointestinal disorder characterized by abdominal pain or discomfort. Previous studies have illustrated that the gut microbiota might play a critical role in IBS, but the conclusions of these studies, based on various methods, were almost impossible to compare, and reproducible microorganism signatures were still in question. To cope with this problem, previously published 16S rRNA gene sequencing data from 439 fecal samples, including 253 IBS samples and 186 control samples, were collected and processed with a uniform bioinformatic pipeline. Although we found no significant differences in community structures between IBS and healthy controls at the amplicon sequence variants (ASV) level, machine learning (ML) approaches enabled us to discriminate IBS from healthy controls at genus level. Linear discriminant analysis effect size (LEfSe) analysis was subsequently used to seek out 97 biomarkers across all studies. Then, we quantified the standardized mean difference (SMDs) for all significant genera identified by LEfSe and ML approaches. Pooled results showed that the SMDs of nine genera had statistical significance, in which the abundance of Lachnoclostridium, Dorea, Erysipelatoclostridium, Prevotella 9, and Clostridium sensu stricto 1 in IBS were higher, while the dominant abundance genera of healthy controls were Ruminococcaceae UCG-005, Holdemanella, Coprococcus 2, and Eubacterium coprostanoligenes group. In summary, based on six published studies, this study identified nine new microbiome biomarkers of IBS, which might be a basis for understanding the key gut microbes associated with IBS, and could be used as potential targets for microbiome-based diagnostics and therapeutics.", "title": "Leveraging 16S rRNA Microbiome Sequencing Data to Identify Bacterial Signatures for Irritable Bowel Syndrome", "claims": null}, {"metadata": {"year": 2019}, "authors": ["R. Pittayanon", "Jennifer T. Lau", "Yuhong Yuan", "G. Leontiadis", "F. Tse", "M. Surette", "P. Moayyedi"], "summary": "BACKGROUND & AIMS\nIrritable bowel syndrome (IBS) is common but difficult to treat. Altering the gut microbiota has been proposed as a strategy for treatment of IBS, but the association between the gut microbiome and IBS symptoms has not been well established. We performed a systematic review to explore evidence for this association.\n\n\nMETHODS\nWe searched databases, including MEDLINE, EMBASE, Cochrane CDSR, and CENTRAL, through April 2, 2018 for case-control studies comparing the fecal or colon microbiomes of adult or pediatric patients with IBS with microbiomes of healthy individuals (controls). The primary outcome was differences in specific gut microbes between patients with IBS and controls.\n\n\nRESULTS\nThe search identified 2631 citations; 24 studies from 22 articles were included. Most studies evaluated adults presenting with various IBS subtypes. Family Enterobacteriaceae (phylum Proteobacteria), family Lactobacillaceae, and genus Bacteroides were increased in patients with IBS compared with controls, whereas uncultured Clostridiales I, genus Faecalibacterium (including Faecalibacterium prausnitzii), and genus Bifidobacterium were decreased in patients with IBS. The diversity of the microbiota was either decreased or not different in IBS patients compared with controls. More than 40% of included studies did not state whether cases and controls were comparable (did not describe sex and/or age characteristics).\n\n\nCONCLUSIONS\nIn a systematic review, we identified specific bacteria associated with microbiomes of patients with IBS vs controls. Studies are needed to determine whether these microbes are a product or cause of IBS.", "title": "Gut Microbiota in Patients With Irritable Bowel Syndrome-A Systematic Review.", "claims": null}, {"metadata": {"year": 2021}, "authors": ["L. Aldars-Garc\u00eda", "M. Chaparro", "J. Gisbert"], "summary": "Inflammatory bowel disease (IBD) is a chronic relapsing\u2013remitting systemic disease of the gastrointestinal tract. It is well established that the gut microbiome has a profound impact on IBD pathogenesis. Our aim was to systematically review the literature on the IBD gut microbiome and its usefulness to provide microbiome-based biomarkers. A systematic search of the online bibliographic database PubMed from inception to August 2020 with screening in accordance with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines was conducted. One-hundred and forty-four papers were eligible for inclusion. There was a wide heterogeneity in microbiome analysis methods or experimental design. The IBD intestinal microbiome was generally characterized by reduced species richness and diversity, and lower temporal stability, while changes in the gut microbiome seemed to play a pivotal role in determining the onset of IBD. Multiple studies have identified certain microbial taxa that are enriched or depleted in IBD, including bacteria, fungi, viruses, and archaea. The two main features in this sense are the decrease in beneficial bacteria and the increase in pathogenic bacteria. Significant differences were also present between remission and relapse IBD status. Shifts in gut microbial community composition and abundance have proven to be valuable as diagnostic biomarkers. The gut microbiome plays a major role in IBD, yet studies need to go from casualty to causality. Longitudinal designs including newly diagnosed treatment-na\u00efve patients are needed to provide insights into the role of microbes in the onset of intestinal inflammation. A better understanding of the human gut microbiome could provide innovative targets for diagnosis, prognosis, treatment and even cure of this relevant disease.", "title": "Systematic Review: The Gut Microbiome and Its Potential Clinical Application in Inflammatory Bowel Disease", "claims": null}, {"metadata": {"year": 2020}, "authors": ["M. Agnello", "L. Carroll", "N. Imam", "R. Pino", "C. Palmer", "Ignacio Varas", "C. Greene", "Maureen Hitschfeld", "Sarah Gupta", "D. Almonacid", "M. Hoaglin"], "summary": "Objective Irritable bowel syndrome (IBS) is a common gastrointestinal disorder that is difficult to diagnose and treat due to its inherent heterogeneity and unclear aetiology. Although there is evidence suggesting the importance of the microbiome in IBS, this association remains poorly defined. In the current study, we aimed to characterise a large cross-sectional cohort of patients with self-reported IBS in terms of microbiome composition, demographics, and risk factors. Design Individuals who had previously submitted a stool sample for 16S microbiome sequencing were sent a comprehensive survey regarding IBS diagnosis, demographics, health history, comorbidities, family history, and symptoms. Log ratio-transformed abundances of microbial taxa were compared between individuals reporting a diagnosis of IBS without any comorbidities and individuals reporting no health conditions. Univariable testing was followed by a multivariable logistic regression model controlling for relevant confounders. Results Out of 6386 respondents, 1692 reported a diagnosis of IBS without comorbidities and 1124 reported no health conditions. We identified 3 phyla, 15 genera, and 19 species as significantly associated with IBS after adjustment for confounding factors. Demographic risk factors include a family history of gut disorders and reported use of antibiotics in the last year. Conclusion The results of this study confirm important IBS risk factors in a large cohort and support a connection for microbiome compositional changes in IBS pathogenesis. The results also suggest clinical relevance in monitoring and investigating the microbiome in patients with IBS. Further, the exploratory models described here provide a foundation for future studies.", "title": "Gut microbiome composition and risk factors in a large cross-sectional IBS cohort", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Nuttachat Wisittipanit", "H. Rangwala", "P. Gillevet"], "summary": "The interaction and inter-play of microbes with human host cells is responsible for several disease conditions and of criticality to human health. In this study we analyze the microbial communities within the human gut and their roles in Inflammatory Bowel Disease (IBD). These microbial communities can be profiled using either Length Heterogeneity PCR (LH-PCR) or small subunit (SSU) rRNA sequences. Classification methods based on support vector machines (SVM) and k-nearest neighbor (KNN) were developed to differentiate between healthy controls and IBD patients at various intestinal locations using those profiles. The results show that there exist significant operational taxonomic units (OTUs) or microbial species that are differentially abundant between IBD and healthy control state at specific intestinal locations. Moreover, the classification performances of the sequence data outperform those of LH-PCR profiles and the lowest taxonomic level (Genus-Species) is more likely to have superior classification performances than the higher taxonomic levels.", "title": "Analysis of Microbiome Data across Inflammatory Bowel Disease Patients", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Valentina Taverniti", "S. Guglielmetti"], "summary": "Irritable bowel syndrome (IBS) is an intestinal functional disorder with the highest prevalence in the industrialized world. The intestinal microbiota (IM) plays a role in the pathogenesis of IBS and is not merely a consequence of this disorder. Previous research efforts have not revealed unequivocal microbiological signatures of IBS, and the experimental results are contradictory. The experimental methodologies adopted to investigate the complex intestinal ecosystem drastically impact the quality and significance of the results. Therefore, to consider the methodological aspects of the research on IM in IBS, we reviewed 29 relevant original research articles identified through a PubMed search using three combinations of keywords: \"irritable bowel syndrome + microflora\", \"irritable bowel syndrome + microbiota\" and \"irritable bowel syndrome + microbiome\". For each study, we reviewed the quality and significance of the scientific evidence obtained with respect to the experimental method adopted. The data obtained from each study were compared with all considered publications to identify potential inconsistencies and explain contradictory results. The analytical revision of the studies referenced in the present review has contributed to the identification of microbial groups whose relative abundance significantly alters IBS, suggesting that these microbial groups could be IM signatures for this syndrome. The identification of microbial biomarkers in the IM can be advantageous for the development of new diagnostic tools and novel therapeutic strategies for the treatment of different subtypes of IBS.", "title": "Methodological issues in the study of intestinal microbiota in irritable bowel syndrome.", "claims": null}], "query": "I need all research papers which used Shotgun sequencing to study gut microbiome of IBS patients in India", "summary_abstract": "The research question seeks studies that utilized shotgun sequencing to investigate the gut microbiome of IBS patients in India. Among the provided abstracts, only the study by Young et al. (2018) explicitly mentions the use of shotgun metagenomic sequencing to analyze the gut microbiome in IBS patients. This study, however, does not specify that it was conducted in India, nor does it focus exclusively on Indian IBS patients. The research identified specific bacterial genera, such as Faecalibacterium and Blautia, that were more abundant in certain IBS subtypes, and highlighted the importance of microbial function, particularly carbohydrate metabolism, in IBS. Other abstracts either focus on different methodologies, such as 16S rRNA sequencing (Attri et al., 2018; Liu et al., 2021; Agnello et al., 2020), or do not specify the use of shotgun sequencing or the Indian context. Therefore, based on the provided abstracts, there is no study that directly answers the research question regarding the use of shotgun sequencing to study the gut microbiome of IBS patients specifically in India.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2023}, "authors": ["Yue-Ming Zhang", "Ru-Meng Wei", "Ming Ni", "Qi-Tao Wu", "Yun Li", "Y. Ge", "X. Kong", "Xue-Yan Li", "Guiping Chen"], "summary": "Growing evidence clearly demonstrates that maternal rodents exposure to sleep deprivation (SD) during late pregnancy impairs learning and memory in their offspring. Epigenetic mechanisms, particularly histone acetylation, are known to be involved in synaptic plasticity, learning, and memory. We hypothesize that the cognitive decline induced by SD during late pregnancy is associated with histone acetylation dysfunction, and this effect could be reversed by an enriched environment (EE).", "title": "An enriched environment improves maternal sleep deprivation\u2010induced cognitive deficits and synaptic plasticity via hippocampal histone acetylation", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Yan  Peng", "Wei  Wang", "Tao  Tan", "Wenting  He", "Zhifang  Dong", "Yu Tian Wang", "Huili  Han"], "summary": "BackgroundSleep deprivation during pregnancy is a serious public health problem as it can affect the health of pregnant women and newborns. However, it is not well studied whether sleep deprivation at different stages of pregnancy has similar effects on emotional and cognitive functions of the offspring, and if so, the potential cellular mechanisms also remain poorly understood.MethodsIn the present study, the pregnant rats were subjected to sleep deprivation for 6\u00a0h per day by gentle handling during the first (gestational days 1\u20137), second (gestational days 8\u201314) and third trimester (gestational days 15\u201321) of pregnancy, respectively. The emotional and cognitive functions as well as hippocampal long-term potentiation (LTP) were tested in the offspring rats (postnatal days 42-56).ResultsThe offspring displayed impaired hippocampal-dependent spatial learning and memory, and increased depressive- and anxiety-like behaviors. Quantification of BrdU-positive cells revealed that adult hippocampal neurogenesis was significantly reduced compared to control. Electrophysiological recording showed that maternal sleep deprivation impaired hippocampal CA1 LTP and reduced basal synaptic transmission, as reflected by a decrease in the frequency and amplitude of miniature excitatory postsynaptic current in the hippocampal CA1 pyramidal neurons.ConclusionsTaken together, these results suggest that maternal sleep deprivation at different stages of pregnancy disrupts the emotional and cognitive functions of the offspring that might be attributable to the suppression of hippocampal LTP and basal synaptic transmission.", "title": "Maternal sleep deprivation at different stages of pregnancy impairs the emotional and cognitive functions, and suppresses hippocampal long-term potentiation in the offspring rats", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Grace Violeta Espinoza Pardo", "J\u00e9ferson Ferraz Goularte", "Ana L\u00facia Hoefel", "Alexandre Luz de Castro", "Luiz Carlos Kucharski", "Alex Sander da Rosa Araujo", "Aldo Bolten Lucion"], "summary": "The present study aimed to analyze the effects of sleep restriction (SR) during pregnancy in rats. The following three groups were studied: home cage (HC pregnant females remained in their home cage), Sham (females were placed in tanks similar to the SR group but with sawdust) and SR (females were submitted to the multiple platform method for 20 h per day from gestational days (GD) 14 to 20). Plasma corticosterone after 6 days of SR was not different among the groups. However, the relative adrenal weight was higher in the SR group compared with the HC group, which suggests possible stress impact. SR during pregnancy reduces the body weight of the female but no changes in liver glycogen, cholesterol and triglycerides, and muscle glycogen were detected. On GD 20, the fetuses of the females submitted to SR exhibited increased brain derived neurotrophic factor (BDNF) in the hippocampus, which indicates that sleep restriction of mothers during the final week of gestation may affect neuronal growth factors in a fetal brain structure, in which active neurogenesis occurs during the deprivation period. However, no changes in the total reactive oxygen species (ROS) in the cortex, hippocampus, or cerebellum of the fetuses were detected. SR females showed no major change in the maternal behavior, and the pups' preference for the mother's odor on postpartum day (PPD) 7 was not altered. On GD 20, the SR females exhibited increased plasma prolactin (PRL) and oxytocin (OT) compared with the HC and Sham groups. The negative outcomes of sleep restriction during delivery could be related, in part, to this hormonal imbalance. Sleep restriction during pregnancy induces different changes compared with the changes described in males and affects both the mother and offspring.", "title": "Effects of sleep restriction during pregnancy on the mother and fetuses in rats", "claims": null}, {"metadata": {"year": 2023}, "authors": ["K. Gulia"], "summary": "Abstract We spend one-third of our lives in sleep, yet the core function of it still remains an enigma due to underlying complex neural processing in this altered state of consciousness. Sleep requirement varies with phase of development. Neonates spent about 85% of their time in sleep, which is polyphasic in nature. Gradually, this pattern takes the shape of a monophasic sleep in adolescents and adults, with changing micro- and macroarchitecture in every phase. Deprivation of sleep in adults impairs learning and memory, and reduces theta coherence among hippocampus and amygdale during sleep. However, sleep loss during pregnancy can affect the ontogenetic development of networks for sleep\u2013wakefulness and the cognitive development of offspring. Even in normal pregnancy, poor sleep quality, reduced rapid eye movement (REM) sleep, and sleep fragmentation are common observation during the last trimester of pregnancy. Delta power, a marker for the homeostatic drive for sleep, in the NREM sleep during the last trimester of pregnancy and postpartum is increased. However, further sleep loss during late pregnancy is a growing concern. Neonates that are born to the total sleep-restricted dams present significant alterations in their emotional development (symptoms of hyperactivity, increased risk-taking behavior during periadolescence) and immature sleep\u2013wakefulness patterns. The REM sleep restriction during late pregnancy elicits depressionlike traits in neonates, which persist until middle age. For a healthy development of brain and body, thorough understanding of the dynamic nature of sleep in relation to age and state (pregnancy) is instrumental in preventing the above-mentioned conditions of prenatal origin. Although sleep is essential for an active brain (for work during day), it remains an underestimated phenomenon. This review highlights the importance of sleep during pregnancy for a healthy brain network programming in offspring.", "title": "Effect of Sleep Restriction during Pregnancy on Fetal Brain Programming and Neurocognitive Development of Offspring: A Review", "claims": null}, {"metadata": {"year": 2016}, "authors": ["E. Hoekzema", "E. Barba-M\u00fcller", "C. Pozzobon", "M. Picado", "F. Lucco", "D. Garc\u00eda\u2010Garc\u00eda", "J. Soliva", "A. Tobe\u00f1a", "M. Desco", "E. Crone", "A. Ballesteros", "Susanna Carmona", "\u00d3. Vilarroya"], "summary": "\uf0b7 Abstract\u2022 \uf0b7 Introduction\u2022 \uf0b7 Results\u2022 \uf0b7 Discussion\u2022 \uf0b7 Methods\u2022 \uf0b7 References\u2022 \uf0b7 Acknowledgments\u2022 \uf0b7 Author information\u2022 \uf0b7 Supplementary information Pregnancy involves radical hormone surges and biological adaptations. However, the effects of pregnancy on the human brain are virtually unknown. Here we show, using a prospective ('pre''post' pregnancy) study involving first-time mothers and fathers and nulliparous control groups, that pregnancy renders substantial changes in brain structure, primarily reductions in gray matter (GM) volume in regions subserving social cognition. The changes were selective for the mothers and highly consistent, correctly classifying all women as having undergone pregnancy or not inbetween sessions. Interestingly, the volume reductions showed a substantial overlap with brain regions responding to the women's babies postpartum. Furthermore, the GM volume changes of pregnancy predicted measures of postpartum maternal attachment, suggestive of an adaptive process serving the transition into motherhood. Another follow-up session showed that the GM reductions endured for at least 2 years post-pregnancy. Our data provide the first evidence that pregnancy confers long-lasting changes in a woman's brain. Woman\u2019s Brain change to prepare for Baby 4 Subject terms: \uf0b7 Brain \uf0b7 Cognitive neuroscience \uf0b7 Neuroscience \uf0b7 Social neuroscience", "title": "Woman\u2019s Brain change to prepare for Baby", "claims": null}, {"metadata": {"year": 2015}, "authors": ["A. Wilkerson"], "summary": "New mothers often complain of impaired cognitive functioning, and it is well documented that women experience a significant increase in sleep disturbance after the birth of a child. Sleep disturbance has been linked to impaired cognitive performance in several populations, including commercial truck drivers, airline pilots, and medical residents, though this relationship has rarely been studied in postpartum women. In the present study 13 pregnant women and a group of 22 non-pregnant controls completed one week of actigraphy followed by a battery of neuropsychological tests and questionnaires in the last month of pregnancy (Time 1) and again at four weeks postpartum (Time 2). Pregnant women experienced significantly more objective and subjective sleep disturbance than the control group at both time points. They also demonstrated more impairment in objective, but not subjective cognitive functioning. Preliminary analyses indicated increased objective sleep fragmentation from Time 1 to Time 2 predicted decreased objective cognitive performance from Time 1 to Time 2, though small sample size limited the power of these findings. Implications for perinatal women and need for future research were discussed.", "title": "Cognitive Performance as a Function of Sleep Disturbance in the Postpartum Period", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Gabriel Natan Pires", "Monica Levy Andersen", "M\u00e1rcia  Giovenardi", "Sergio  Tufik"], "summary": "The modern living standard has imposed upon society a situation of chronic sleep deprivation. This chronic loss of sleep affects women more than it does men. As a result, the postponement of pregnancy has become a common choice due to the priority given to social and domestic activities. For women, pregnancy represents a condition of intense physical and physiological changes that subject the pregnant woman to a number of potentially stressful situations, ultimately interfering with their quality of sleep. Chronic sleep deprivation, along with the changes imposed on women through pregnancy, can lead to several harmful consequences for the pregnant woman and the child, and can potentially undermine the mother-infant relationship. This article discusses circumstances under which sleep deprivation and poor sleep quality during pregnancy could result in damage to the mother-infant relationship, specifically through maternal fatigue, postpartum depression and changes in pregnancy-related hormonal secretions and activity.", "title": "Sleep impairment during pregnancy: possible implications on mother-infant relationship.", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Jen Jen Chang", "Grace W Pien", "Stephen P Duntley", "George A Macones"], "summary": "Sleep duration in the population has been declining. Women occupy an increasingly prominent place in the work force without reducing most of their responsibilities at home. Consequently, sleep needs are often pushed to the bottom of women's daily priority list. Prior research has indicated that sleep deprivation is associated with higher levels of pro-inflammatory serum cytokines. This is important because higher plasma concentrations of pro-inflammatory serum cytokine levels are associated with postpartum depression and adverse birth outcomes such as preterm delivery. However, little research has directly examined how sleep deprivation may affect maternal and fetal outcomes. This review summarizes the existing data on the effect of sleep deprivation during pregnancy on maternal and fetal outcomes. We review supporting evidence for the hypotheses that sleep deprivation during pregnancy increases the risk of preterm delivery and postpartum depression, and that systemic inflammation is the causal mechanism in the association. Prior research on sleep in pregnancy has been limited by varying data collection methods, subjective self-reported sleep measures, small and non-representative samples, cross-sectional designs; descriptive or non-hypothesis driven studies. Future research with longitudinal study designs is needed to allow examination of the effect of sleep deprivation on adverse maternal and fetal outcomes.", "title": "Sleep deprivation during pregnancy and maternal and fetal outcomes: is there a relationship?", "claims": null}], "query": "how pregnancy reshape the mother brain to cope with sleep deprivation", "summary_abstract": "The collection of papers provides a multifaceted view of how pregnancy reshapes the mother's brain to cope with sleep deprivation. Hoekzema et al. (2016) reveal that pregnancy induces significant changes in brain structure, particularly reductions in gray matter volume in regions associated with social cognition. These changes are thought to be adaptive, aiding the transition into motherhood and potentially helping mothers manage the cognitive demands of sleep deprivation. \n\nGulia (2023) highlights that sleep loss during pregnancy can affect the development of sleep-wake networks and cognitive functions in offspring, suggesting that the mother's brain may undergo changes to mitigate these effects. The study by Wilkerson (2015) notes that pregnant women experience more sleep disturbances and cognitive impairments compared to non-pregnant controls, indicating that the brain may adapt to manage these challenges.\n\nPardo et al. (2016) discuss hormonal changes, such as increased prolactin and oxytocin, which may play a role in coping with sleep deprivation during pregnancy. These hormonal shifts could be part of the brain's adaptation to ensure maternal behavior and bonding despite sleep loss. \n\nOverall, these studies suggest that pregnancy involves both structural and hormonal changes in the brain that may help mothers cope with the cognitive and emotional challenges posed by sleep deprivation, ultimately supporting maternal and offspring well-being.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2014}, "authors": ["Daniel C. Moos", "D. Pitton"], "summary": "Cognitive load theory (CLT) can explain the challenges faced by student teachers. This study, guided by the CLT, included 26 pre-service teachers. Participants completed a cognitive load self-report questionnaire and were interviewed at two points during their student teaching. Results revealed that student teachers decreased mental effort related to monitoring their students\u2019 level of attention, meeting needs of diverse learners, and managing internal and external distractions. Qualitative analysis revealed: (1) student teachers became aware of limited cognitive resources; (2) lesson planning imposes cognitive load during student teaching; and (3) cognitive overload limits the ability to make modifications during teaching.", "title": "Student teacher challenges: using the cognitive load theory as an explanatory lens", "claims": null}, {"metadata": {"year": 2010}, "authors": ["\u00d6. Springer"], "summary": "Cognitive load is a theoretical notion with an increasingly central role in the educational research literature. The basic idea of cognitive load theory is that cognitive capacity in working memory is limited, so that if a learning task requires too much capacity, learning will be hampered. The recommended remedy is to design instructional systems that optimize the use of working memory capacity and avoid cognitive overload. Cognitive load theory has advanced educational research considerably and has been used to explain a large set of experimental findings. This article sets out to explore the open questions and the boundaries of cognitive load theory by identifying a number of prob lematic conceptual, methodological and application-related issues. It concludes by pre senting a research agenda for future studies of cognitive load.", "title": "Cognitive load theory, educational research, and instructional design: some food for thought", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Ton de Jong"], "summary": "Cognitive load is a theoretical notion with an increasingly central role in the educational research literature. The basic idea of cognitive load theory is that cognitive capacity in working memory is limited, so that if a learning task requires too much capacity, learning will be hampered. The recommended remedy is to design instructional systems that optimize the use of working memory capacity and avoid cognitive overload. Cognitive load theory has advanced educational research considerably and has been used to explain a large set of experimental findings. This article sets out to explore the open questions and the boundaries of cognitive load theory by identifying a number of problematic conceptual, methodological and application-related issues. It concludes by presenting a research agenda for future studies of cognitive load.", "title": "Cognitive load theory, educational research, and instructional design: some food for thought", "claims": null}, {"metadata": {"year": 2018}, "authors": [], "summary": "Cognitive load theory is supported by a significant number of randomised controlled trials (RCTs). This large body of evidence indicates that instruction is most effective when it is designed according to the limitations of working memory. Cognitive load theory indicates that when teaching students new content and skills, teachers are more effective when they provide explicit guidance accompanied by practice and feedback, not when they require students to discover for themselves many aspects of what they must learn.", "title": "Cognitive load theory: Research that teachers really need to understand", "claims": null}, {"metadata": {"year": 2018}, "authors": [], "summary": "Cognitive load theory is supported by a significant number of randomised controlled trials (RCTs). This large body of evidence indicates that instruction is most effective when it is designed according to the limitations of working memory. Cognitive load theory indicates that when teaching students new content and skills, teachers are more effective when they provide explicit guidance accompanied by practice and feedback, not when they require students to discover for themselves many aspects of what they must learn.", "title": "Cognitive load theory: Research that teachers really need to understand", "claims": null}, {"metadata": {"year": 2021}, "authors": ["M. Kennedy"], "summary": "There are numerous reasons why students with disabilities struggle in school. A key reason is professionals in the field may not pay enough attention to students\u2019 overwhelmed cognitive capacity. Cognitive load theory explains that all humans have limited capacity at any given time to use their auditory, visual, and tactile inputs (independently or collectively) to acquire new information and store it in long-term memory. When available cognition is overwhelmed \u2013 which can be caused by any number of reasons \u2013 learning cannot occur. In this article, we introduce the key aspects of cognitive load theory and give specific examples of how special educators can use this information to shape their instruction to support students\u2019 unique needs.", "title": "Cognitive Load Theory: An Applied Reintroduction for Special and General Educators", "claims": null}, {"metadata": {"year": 2016}, "authors": ["L. Longo"], "summary": "Cognitive Load Theory is an approach that considers the limitations of the information processing system of the human mind. It is a cognitivist theory that has been conceived in the context of instructional design. One of the main open problems in the literature is the lack of reliable models and technologies to assess cognitive load of learners, thus limiting the application of the theory in practice. This project was aimed at tackling this open problem through the use of a previously developed mobile, responsive web-based prototypical technology, to assess the cognitive load of students in a typical third-level classroom. It was also aimed at exploring the impact of such a technology to instructional design and the potential benefits it can bring to lecturers to improve teaching practices and optimally align their instructional materials to learners.", "title": "Informing Instructional Design by Cognitive Load Assessment in the Classroom.", "claims": null}, {"metadata": {"year": 2021}, "authors": ["T. K. Rao"], "summary": "The present education system has a lot of load in itself. As the education market increasing, the stakeholders of it are also looking to make the curriculum tougher and harder. The competitiveness is clearly visible in the content to be learnt rather than psychological satisfaction of the learning. It is often observed that the curriculum developers are keen to focus more on knowledge and skills in comparison to individual capabilities and pace of learning. When it comes to education system, the parents\u2019 expectations and society needs are far away from needs of the learner actually. Now the student has a big role to play to meet all those expectations. The students have to learn mathematics, they should speak English fluently, and they should get all the prizes in sports and so on. In the process, the learner may undergo a tremendous pressure for each activity he or she takes up. Somewhere, the cognitive fluctuations or cognitive conflict sets in mind and leading work very hard on. The learner has to process all the related incoming information in each and every area of subject he or she chooses, ultimately results in load on memory. As the amount of information to be processed heaps up, heavy cognitive load the outcome will be. Many past researches have been reviewed in this direction to come to solid conclusion.", "title": "Educational Underpinnings of Cognitive Load Theory", "claims": null}], "query": "cognitive load theory in prekindergarten education", "summary_abstract": "The collection of abstracts provides insights into the application and implications of cognitive load theory (CLT) in education, though not specifically focused on prekindergarten education. Moos and Pitton (2014) highlight the challenges faced by student teachers, noting that cognitive load affects their ability to manage classroom dynamics and adapt lesson plans. This suggests that even at early educational levels, such as prekindergarten, educators must be mindful of cognitive load to effectively manage diverse learning needs and distractions.\n\nSpringer (2010) and de Jong (2010) emphasize the importance of designing instructional systems that optimize working memory capacity to prevent cognitive overload, a principle that can be applied to prekindergarten education by ensuring that learning tasks are appropriately scaled to young children's cognitive capacities.\n\nThe 2018 abstracts underscore the effectiveness of instruction designed with cognitive load limitations in mind, advocating for explicit guidance and feedback over discovery-based learning. This approach is particularly relevant for prekindergarten education, where structured guidance can help young learners process new information without overwhelming their limited cognitive resources.\n\nKennedy (2021) discusses the challenges faced by students with disabilities, highlighting the need for educators to consider cognitive load when designing instruction. This is pertinent to prekindergarten settings, where diverse learning needs must be accommodated without exceeding cognitive capacities.\n\nLongo (2016) and Rao (2021) address the broader educational context, noting the challenges of assessing cognitive load and the pressures of modern curricula. These insights suggest that in prekindergarten education, careful consideration of cognitive load can help tailor instruction to individual learning paces and capabilities, potentially reducing stress and enhancing learning outcomes.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2000}, "authors": ["Ian Stumpf"], "summary": "Medium\u2010sized regional building contractors in the UK are exhibiting poorer performance in the 1980s and the 1990s and are less likely to survive than their larger or smaller counterparts. The market structure of contracting appears to be changing, putting pressure on these intermediate firms. Evidence drawn from the Department of the Environment (DoE) statistical series shows industry composition is changing, in particular the gradual decline over time of the middle market. An analysis of company accounts for a sample of approximately 200 contractors shows that medium\u2010sized firms are also displaying inferior business ratios. Possible explanations are offered, including barriers to entry, such as capitalization, economies of scale (pecuniary and market), along with changes in construction demand.", "title": "Competitive pressures on middle\u2010market contractors in the UK", "claims": null}, {"metadata": {"year": 2008}, "authors": ["T. Brown"], "summary": "This paper explores the dynamics of government-to-government contracting at the local level in order to examine how governments' shared organizational characteristics, notably a governance structure based on political accountability, potentially make them more attractive vendors for services that risk contract failure. Relying on panel data from the 1992 and 1997 International City/County Manager Association's (ICMA) Alternative Service Delivery surveys along with data from the U. S. Census and other sources, this paper identifies service areas in which governments most frequently turn to government vendors. In particular, a comparison of public works and transportation services\u2014a service area with low risks of opportunism leading to contract failure\u2014and health and human services\u2014a service area with high risks of opportunism leading to contract failure\u2014shows that contracting governments are more likely to utilize governments over private firms and nonprofits for high-risk services. This is not the case for low-risk services, suggesting that governments view other governments as trusted contract vendors.", "title": "The Dynamics of Government-to-Government Contracts", "claims": null}, {"metadata": {"year": 2012}, "authors": ["J. Johnston", "Amanda M. Girth"], "summary": "Theory tells us that competition is the chief driver of improved efficiency and effectiveness in government contracting, yet contract provider markets are often noncompetitive. This study offers a detailed, contextualized examination of public administrative responses to thin contract markets. Following an inductive approach with data from semistructured interviews with contract administrators, the authors offer a preliminary typology of the conditions that give rise to thin markets, and the \u201cmarket management\u201d strategies used to create, enhance, and sustain competition in the markets from where governments purchase goods and services. The authors then review the efficacy and implications of these strategies for public services to citizens.", "title": "Government Contracts and \u201cManaging the Market\u201d", "claims": null}, {"metadata": {"year": 1991}, "authors": ["S. Macmanus"], "summary": "responses varied by type ofproduct or service being provided. Contracting out is one of the most popular forms of cost containment utilized by governments at all levels (David, 1988; Morley, 1989; Chi, 1988; Butler, 1985; MacManus, 1990a). Ironically, at the very time when governments are most actively and aggressively seeking more opportunities to rely on the private sector, news media accounts indicate that the business community has become more cautious about selling to government (Goldstein, 1989). It is estimated that less than 2 percent of all U.S. businesses sell to the federal government (Holtz, 1980: xii) in spite of the fact that the U.S. government spent $195 billion on goods, services, and research and development in 1988 alone. Although the proportion of businesses selling to state and local governments is probably somewhat higher, it is evident that businesses are skeptical about selling to government in spite of the size of the public-sector market. Yet for governments to get the most benefit from contracting out, genuine competition for government contracts must exist. If a significant proportion of the business community does not perceive that government procurement is competitive, or that the competition is fair and equitable and produces costs savings, then government procurement experts must devise strategies to broaden the vendor pools.", "title": "Why Businesses are Reluctant to Sell to Governments", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Pamela Healy", "Sang Won Sok", "Alejandro Ramirez"], "summary": "Abstract : In the environment of shrinking budgets, there is a trend toward competitive contracting. Research indicates that the government can achieve significant cost savings from competition among industry. This paper will determine how much cost savings could be achieved. It will also analyze numerous contract-related Government Accountability Office reports and provide a summary of the Better Buying Power initiatives. This paper will also identify circumstances that prohibit full and open competition and patterns where competition is most successful. Finally, this paper will provide recommendations to assist federal executives in maximizing competitive contracting and provide the knowledge needed help achieve mandates for improved efficiency and reduced costs.", "title": "The value of competitive contracting", "claims": null}, {"metadata": {"year": 1990}, "authors": ["R. H. DeHoog"], "summary": "Contracting for public services from public or private suppliers is now a common prescription to improve government efficiency. The competitive bidding model is usually viewed as the ideal contracting process. However, this article explains that two other approaches\u2014the negotiation model and the cooperation model-may be more appropriate under certain conditions. The primary factors that are likely to determine which of the three approaches is most suitable are (a) the characteristics of the external environment (especially the number of service suppliers), (b) the level of organizational resources (e.g., personnel, funds, time, and expertise), and (c) the degree of uncertainty about funding, future events, service technologies, and causal relationships between service outputs and desired outcomes. The main point is that there is no one best way to contract for services; rather, government units should adapt their contracting procedures to both internal external conditions to implement service contracting in an effective manner.", "title": "Competition, Negotiation, or Cooperation", "claims": null}, {"metadata": {"year": 1993}, "authors": ["S. F. Bovet"], "summary": "Adapting to a tight economy, recession and a client switch from retainer to project assignments, midsize firms around the country have developed new management structures. Taking on senior partners, stressing firm strengths, a team approach and revised employee reporting and motivation procedures are among restructuring moves firms with 10 to 30 practitioners have made in recent months. To cope with recessionary pressures of the past few years, top executives at \"midsize\" firms--those with 10 to 30 practitioners--have revamped management structures in a variety of creative ways. Innovations include making partial \"of counsel\" or full partnership arrangements with experienced independent counselors, who often share office staff and overhead expenses. Executives at leading midsize firms are refocusing their businesses to handle more projects. Some clients are rejecting retainer arrangements, while others are demanding much more service on a retainer basis. PRJ talked recently with chief operating officers of four midsize firms. Clients today are demanding better and more targeted service, all participants agreed. Clients want senior executives of a firm working on their accounts, they added. There is also much higher client demand for strategic counseling and risk assessment or issues management advice, these sources said. Firms are also zeroing in on specialties as a means of attracting new business, roundtable participants pointed out. Besides strategic planning and issues or risk management, coalition building, public affairs and marketing communications were listed as sought-after areas of expertise. In one case, a firm is developing a unique database service that can be marketed as a separate profit center in addition to its more traditional public relations services. The firms represented in this article illustrate a variety of structures in terms of national and international affiliations. A. Brown-Olmstead Associates (ABOA), Atlanta, is part of the international Shandwick group of companies. Agnew, Carter, McCarthy, Inc., Boston, is a stockholder in the Pinnacle Worldwide corporation of independent firms. DMB&B Public Relations in the Detroit area (Bloomfield Hills, MI) functions as an office of Manning, Selvage & Lee, which is owned by DMB&B, an advertising agency. Nuffer, Smith, Tucker, Inc., San Diego, is a Worldcom partner and Hill and Knowlton associate. In addition to reshaping managements, midsize firms have extensively revamped their employee motivation and reward systems, according to PRJ's executive panel. While the recession has kept salary increases low for most staff, CEOs look to share rewards via bonus payments, profit-sharing or group rewards, such as special luncheons or spot incentive payments. Changing reward systems Midsize firms today stress a team approach to account work, those at the roundtable agreed. They also stress rewards such as staff training and the ability of younger executives to work with senior practitioners on interesting projects. Job satisfaction is a key factor in retaining good workers, roundtable participants said. Employee empowerment can be fostered by flattening the bureaucracy, one executive said. Another firm has established a system of peer reviews to keep employee performance and satisfaction high. Roundtable talk about reward systems also led to a debate on how to find, keep and motivate qualified employees. Characteristics of a firm's own culture were considered important in recruiting and retaining good performers. Created hourglass structure Less than a year ago, the traditional pyramid structure at A. Brown-Olmstead Associates (ABOA) was scrapped in favor of \"an hourglass shape,\" according to CEO and President Amanda Brown-Olmstead. Brown-Olmstead founded the firm 20 years ago as a traditional pyramid, with a CEO, several senior people reporting to her, and account service and support staff in layers below that. \u2026", "title": "PRJ Roundtable Report: Midsize Firms Take New Shapes to Suit Service, Profit Goals", "claims": null}, {"metadata": {"year": 1995}, "authors": ["Karen D. Sorber", "R. Straight"], "summary": "INTRODUCTION Acquiring supplies and services is one function common to most government organizations.(1) Generally, customers submit their requirements to a designated contracting office, which then follows a set of complex rules and regulations while taking action to execute a contract eventually to satisfy the requirements. This process is often frustrating to customers who have no choice but to accept whatever quality of service is provided by their designated (i.e., monopoly) contracting office. Even the people who are doing the contracting have themselves expressed frustration with the system. According to Kelman (1990:10), 29 percent of those providing he service said that the process takes too long. In spite of these problems, contracting offices in the public sector are routinely being asked to take on more and more responsibilities. Today there is much emphasis on contracting out services that government organizations have traditionally performed. According to Osborne and Gaebler (1992:87), procurement of services is challenging primarily because of the difficulty in writing and monitoring contracts for services. Despite the difficulty, Osborne and Gaebler found that customers have tended to be highly satisfied with contracted services (Ibid., 89). The authors believe that establishing competitive contracting offices (CCOs) that succeed or fail on their own performance will reduce much of the frustration associated with the federal government's contracting process. Adopting the CCO concept can lead to reductions in costs and improvements in performance--key goals for the current administration. OVERVIEW OF CCOs The authors presented the concept of CCOs at the 1989 Acquisition Research Symposium (Straight and Sorber, 1989:287). Under that concept, project officers (customers) would have a free hand in selecting any CCO to prepare and award their contracts. Customers would pay the CCO for its services and the chief of each office would be responsible for its financial \"bottom line.\" The project officer's freedom to select the best CCO, and the resultant financial pressure on CCOs to provide timely and effective support, should naturally lead to a more effective and efficient contracting process. In the 1989 paper, the authors discussed the advantages to the customer when dealing in a competitive environment rather than with a monopoly, given the latter's tendency toward inadequate service, inflexibility, and indifference to customer requirements. An important feature of the CCO concept is the provision of financial awards to managers and employees when the organization exceeds its financial operational goals (i.e., revenues exceed expenses). In conjunction with the pressure to compete for contracting business, those awards should motivate both managers and employees to develop a keener customer focus. IMPLEMENTATION ISSUES In a subsequent paper, presented at the 1991 symposium, the authors examined several implementation issues associated with CCO establishment, many of which focused on customer satisfaction (Straight and Dean, 1991:171). Drawing upon the results of a survey of people in the acquisition community, the authors concluded that the proposed CCO system should result in greater customer satisfaction. However, many respondents were concerned that contracting personnel would face a conflict of interest in that they might be tempted to satisfy the customer at the expense of regulatory and legal compliance. PROFESSIONALS' ETHICS Several thoughts on the morality of contracting personnel are offered. First, and most important, contracting personnel are held to a high standard of ethical behavior. They routinely have to disclose financial interests, certify compliance with various procurement integrity laws and regulations, and receive ethics training. When scandals do occur, senior government officials, inspectors general, and others scrutinizing the procurement process generally find that, with the exception of the bad apples involved in the scandal, the procurement workforce is highly ethical. \u2026", "title": "COMPETING CONTRACTING OFFICES: WORKING BETTER, COSTING LESS", "claims": null}], "query": "What are the competitive dynamics of a mid sized company in government contracting", "summary_abstract": "The competitive dynamics of mid-sized companies in government contracting are shaped by several factors, as highlighted by the collection of papers. Stumpf (2000) notes that medium-sized firms face significant challenges due to changing market structures, including barriers to entry and economies of scale, which can lead to poorer performance compared to larger or smaller firms. This suggests that mid-sized companies may struggle to compete effectively in the government contracting space due to these structural disadvantages.\n\nJohnston and Girth (2012) emphasize the importance of competition in driving efficiency and effectiveness in government contracting. However, they point out that contract provider markets are often noncompetitive, which can disadvantage mid-sized firms that rely on competitive dynamics to thrive. They suggest that strategies to enhance competition could benefit these firms.\n\nHealy, Sok, and Ramirez (2014) discuss the trend towards competitive contracting in the context of shrinking budgets, highlighting that significant cost savings can be achieved through competition. This environment could provide opportunities for mid-sized companies to leverage their agility and specialized expertise to compete effectively, provided they can navigate the barriers to full and open competition.\n\nDeHoog (1990) and Sorber and Straight (1995) both discuss the importance of adapting contracting approaches to the external environment and internal resources. For mid-sized companies, this means tailoring their strategies to the specific conditions of the government contracting market, such as the number of service suppliers and the level of organizational resources available.\n\nOverall, the competitive dynamics for mid-sized companies in government contracting are complex and influenced by market structure, competition levels, and the ability to adapt to changing conditions. These companies must navigate barriers to entry and leverage their unique strengths to remain competitive in a challenging environment.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2006}, "authors": ["R. Lam"], "summary": "Generalized anxiety disorder (GAD) is a common, chronic and disabling anxiety disorder with considerable comorbidity with depression as well as with other anxiety disorders. Although tricyclic antidepressants and benzodiazepines have been found to be efficacious in patients with GAD, tolerability problems and other risks limit their use in clinical practice. In placebo-controlled, acute (<8\u2009weeks) trials, several medications, including the selective serotonin reuptake inhibitors ([SSRIs] escitalopram, paroxetine, and sertraline) and others (venlafaxine, buspirone, pregabalin), have demonstrated efficacy in patients with GAD. Indeed, current guidelines for the treatment of GAD recommend SSRIs as first-line pharmacological therapy because of their efficacy and tolerability profiles. Although GAD is a chronic condition that is usually present for years, with symptoms typically fluctuating in intensity over time, there have been few randomized, controlled trials of pharmacotherapy beyond the acute phase of treatment. However, data from recent relapse-prevention studies and longer-term maintenance studies with paroxetine, venlafaxine and escitalopram strongly support the value of continued treatment for at least a further 6 months. This article focuses on pharmacological treatment, and reviews recently available data from acute, long-term and relapse-prevention trials in patients with GAD. In addition, issues relating to the natural course of GAD are highlighted as important considerations to guide selection of pharmacotherapy.", "title": "Generalized anxiety disorder: how to treat, and for how long?", "claims": null}, {"metadata": {"year": 2004}, "authors": ["Fr\u00e9d\u00e9ric  Rouillon"], "summary": "Abstract Generalized anxiety disorder (GAD) is a common (lifetime prevalence: 5.1%), recurrent condition, which often heralds other psychiatric disorders, notably depression. As by definition it is a disorder progressing over months, treatment should be designed on a long term basis. And yet, few studies have been conducted beyond the classical 6\u20138 weeks characterizing the acute treatment phase. This is especially true of anxiolytics, but also of antidepressants, with the exception of paroxetine and venlafaxine, which are the only drugs approved in this indication in Western countries. The efficacy of psychotherapy, notably relaxation and cognitive-behavioral therapy, is established in the treatment of GAD, but its preferred indications and possible combination with antidepressants are still to be specified. Long term, not to say very long term studies of GAD, as well as depression, will still be required in the future to improve its management and specify therapeutic modalities (combination treatment, optimal duration, continuous or intermittent therapy, choice of psychotherapeutic techniques or agents, \u2026). Early and adequately prolonged treatment should not only result in more numerous remission periods, but also in decreased frequency of co-morbidities whether depressive, addictive, or of another nature, and should also reduce the social impact of GAD.", "title": "Long term therapy of generalized anxiety disorder", "claims": null}, {"metadata": {"year": 2015}, "authors": ["M. Stein", "J. Sareen"], "summary": "Key Clinical PointsGeneralized Anxiety Disorder Generalized anxiety disorder is characterized by persistent anxiety and uncontrollable worry that occurs consistently for at least 6 months. This disorder is commonly associated with depression, alcohol and substance abuse, physical health problems, or all these factors. In primary care, patients with this disorder often present with physical symptoms such as headaches, muscle tension, gastrointestinal symptoms, back pain, and insomnia. Brief validated screening tools such as the Generalized Anxiety Disorder 7 (GAD-7) scale should be used to assess the severity of symptoms and response to treatment. First-line treatments for generalized anxiety disorder are cognitive behavioral therapy, pharmacotherapy with a selective serotonin-reuptake inhibitor (SSRI) or a serotonin\u2013norepinephrine reuptake inhibitor (SNRI), or cognitive behavioral therapy in conjunction with either an SSRI or an SNRI. Pregabalin and buspirone are suitable second-line or adjunctive medicat...", "title": "CLINICAL PRACTICE. Generalized Anxiety Disorder.", "claims": null}, {"metadata": {"year": 2000}, "authors": ["V. Mah\u00e9", "A. Balogh"], "summary": "&NA; Generalized anxiety disorder (GAD) is one of the most common anxiety disorders and has a poor prognosis, although it is often thought to be a minor complaint. This disorder has a chronic course of 5\u201015 years and longer. Long\u2010term treatment with the commonly used benzodiazepines is controversial because of concerns over tolerance and dependence. We performed a thorough search of the literature for clinical trials of a duration of over 2 months conducted in patients with generalized anxiety disorder in order to identify any successful long\u2010term treatment of this disorder. Only eight long\u2010term reports of studies conducted in well\u2010defined homogeneous groups of patients diagnosed with generalized anxiety disorder were found with the methodology of these studies presenting a number of limiting factors. The results are inconclusive and no reference drug could be identified. In addition, an adequate evaluation of the long\u2010term treatment of GAD has not yet been performed.", "title": "Long\u2010term pharmacological treatment of generalized anxiety disorder", "claims": null}, {"metadata": {"year": 1987}, "authors": ["G. Butler", "A. Cullington", "G. Hibbert", "I. Klimes", "M. Gelder"], "summary": "A preliminary controlled investigation of the effectiveness of Anxiety Management as a treatment for generalised anxiety disorder (GAD) is described. Patients with a primary diagnosis of GAD, in which the current episode had lasted for at least 6 months but not more than 2 years, were included. Anxiety Management, a self-help treatment including procedures for managing somatic and cognitive symptoms, and for dealing with avoidance and low self-confidence, was given either immediately or after a 12-week waiting period. The average length of treatment was 8.7 sessions. Highly significant changes in anxiety, depression, and problem ratings were shown after treatment. These changes were replicated when the waiting list group had also received treatment, and gains were maintained by both groups for 6 months. Similar degrees of improvement and maintenance of change were shown in subgroups with and without minor depressive disorder or recurrent panic attacks.", "title": "Anxiety Management for Persistent Generalised Anxiety", "claims": null}, {"metadata": {"year": 2000}, "authors": ["M F Gliatto"], "summary": "Patients with generalized anxiety disorder experience worry or anxiety and a number of physical and psychologic symptoms. The disorder is frequently difficult to diagnose because of the variety of presentations and the common occurrence of comorbid medical or psychiatric conditions. The lifetime prevalence is approximately 4 to 6 percent in the general population and is more common in women than in men. It is often chronic, and patients with this disorder are more likely to be seen by family physicians than by psychiatrists. Treatment consists of pharmacotherapy and various forms of psychotherapy. The benzodiazepines are used for short-term treatment, but because of the frequently chronic nature of generalized anxiety disorder, they may need to be continued for months to years. Buspirone and antidepressants are also used for the pharmacologic management of patients with generalized anxiety disorder. Patients must receive an appropriate pharmacologic trial with dosage titrated to optimal levels as judged by the control of symptoms and the tolerance of side effects. Psychiatric consultation should be considered for patients who do not respond to an appropriate trial of pharmacotherapy.", "title": "Generalized anxiety disorder.", "claims": null}, {"metadata": {"year": 2000}, "authors": ["M. F. Gliatto"], "summary": "Patients with generalized anxiety disorder experience worry or anxiety and a number of physical and psychologic symptoms. The disorder is frequently difficult to diagnose because of the variety of presentations and the common occurrence of comorbid medical or psychiatric conditions. The lifetime prevalence is approximately 4 to 6 percent in the general population and is more common in women than in men. It is often chronic, and patients with this disorder are more likely to be seen by family physicians than by psychiatrists. Treatment consists of pharmacotherapy and various forms of psychotherapy. The benzodiazepines are used for short-term treatment, but because of the frequently chronic nature of generalized anxiety disorder, they may need to be continued for months to years. Buspirone and antidepressants are also used for the pharmacologic management of patients with generalized anxiety disorder. Patients must receive an appropriate pharmacologic trial with dosage titrated to optimal levels as judged by the control of symptoms and the tolerance of side effects. Psychiatric consultation should be considered for patients who do not respond to an appropriate trial of pharmacotherapy.", "title": "Generalized anxiety disorder.", "claims": null}, {"metadata": {"year": 2003}, "authors": ["C. Allgulander", "B. Bandelow", "E. Hollander", "S. Montgomery", "D. Nutt", "A. Okasha", "M. Pollack", "Dan J Stein", "R. Swinson"], "summary": "ABSTRACT What are the current recommendations for the long-term treatment of generalized anxiety disorder (GAD)? GAD is a common disorder with a lifetime prevalence of 4% to 7% in the general population. GAD is characterized by excessive, uncontrollable worry or anxiety about a number of events or activities that the individual experiences on more days than not over a 6-month period. Onset of GAD symptoms usually occurs during an individual's early twenties; however, high rates of GAD have also been seen in children and adolescents. The clinical course of GAD is often chronic, with 40% of patients reporting illness lasting >5 years. GAD is associated with pronounced functional impairment, resulting in decreased vocational function and reduced quality of life. Patients with GAD tend to be high users of outpatient medical care, which contributes significantly to healtcare costs. Currently, benzodiazepines and buspirone are prescribed frequently to treat GAD. Although both show efficacy in acute treatment trials, few long-term studies have been perform Benzodiazepines are not recommended for long-term treatment of GAD, due to associated development of tolerance, psychomotor impairment, cognitive and memory changes, physical dependence, and a withdrawal reaction on discontinuation. The antidepressant venlafaxine extended-release (XR) has received approval for the treatment of GAD in the United States and many other countries. Venlafaxine XR has demonstrated efficacy over placebo in two randomized treatment trials of 6 months' duration as well as in other acute trials. Paroxetine is the first of the selective serotonin reuptake inhibitors (SSRIs) to receive US approval for the treatment of GAD. Paroxetine demonstrated superiority to placebo in short-term trials, and investigations into the use of other SSRIs are ongoing. This suggests that other SSRIs, and serotonin and noradrenaline reuptake inhibitors, are likely to be effective in the treatment of GAD. Of the psychological therapies, cognitive-behavioral therapy (CBT) shows the greatest benefit in treating GAD patients. Treatment gains after a 12-week course of CBT may be maintained for up to 1 year. Currently, no guidelines exist for the long-term treatment of GAD.", "title": "WCA Recommendations for the Long-Term Treatment of Generalized Anxiety Disorder", "claims": null}], "query": "how long a patient with generalized anxiety disorder should take medication for", "summary_abstract": "The collection of abstracts provides insights into the duration of medication treatment for patients with generalized anxiety disorder (GAD). GAD is recognized as a chronic condition, often persisting for years, which necessitates long-term management strategies (Lam, 2006; Rouillon, 2004; Mah\u00e9 & Balogh, 2000). While acute treatment phases typically last 6-8 weeks, there is a consensus that continued pharmacotherapy is beneficial beyond this period. Lam (2006) and Rouillon (2004) suggest that medications such as SSRIs and SNRIs should be continued for at least an additional 6 months to prevent relapse and manage symptoms effectively. \n\nHowever, the literature indicates a lack of extensive long-term studies, with most research focusing on short-term efficacy (Mah\u00e9 & Balogh, 2000; Allgulander et al., 2003). Despite this, some evidence supports the use of medications like venlafaxine and paroxetine for longer durations, with venlafaxine showing efficacy in trials lasting up to 6 months (Allgulander et al., 2003). Gliatto (2000) also notes that due to the chronic nature of GAD, medications such as benzodiazepines may be required for months to years, although their long-term use is controversial due to potential dependence issues.\n\nOverall, while specific guidelines for the optimal duration of medication treatment in GAD are not well-established, the prevailing view is that long-term treatment, extending beyond the acute phase, is necessary to achieve sustained symptom control and prevent relapse (Lam, 2006; Rouillon, 2004; Allgulander et al., 2003).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2011}, "authors": ["W. Leap"], "summary": "Gay men\u2019s stories about homophobic violence will, at times, disguise reference to the persons who initiate harassment and injury. Instead, these stories emphasize the details of location or allow the victim of narrated violence to confirm the status of narrative hero. Each approach to story-telling has noticeable effects on the connections between sexual sameness and violence as displayed within the narrative. This paper examines these approaches to story-telling and their effects on homophobic formation (Leap, introduction, this volume), and traces the broader implications of these narrative styles within contemporary politics of gay visibility, assimilation and whiteness.", "title": "Homophobia as Moral Geography", "claims": null}, {"metadata": {"year": 1996}, "authors": ["Debbie  Epstein"], "summary": "Since September 1991 I have been engaged in a research project in which we set out to explore the experiences of lesbian and gay students, teachers, and parents in relation to the English system of education.2 In the course of the project, we have interviewed or held group discussions with some 30 lesbians and gay men, as well as carrying out ethnographic work in four schools and in a lesbian and gay youth group.3 While the majority of our respondents have been white, we have also spoken with lesbians and gays of African and of South Asian descent.4 My own contribution to the field work for this project has been that of interviewing many of our lesbian and gay respondents and participant observation in one of the schools. Doing the research, I have been struck forcibly by the various forms of harassment experienced by our female respondents as well as by those men and boys who identified as and/or were perceived as gay or effeminate by their peers and/or teachers. During the same period, and partly because of the research findings, I have found myself reflecting on my own experiences of harassment and those of my students in both the recent and more distant past. This chapter is, in large part, the result of these reflections and of discussions I have had with students, colleagues, and friends about the issues involved.5", "title": "Keeping them in their Place: Hetero/sexist Harassment, Gender and the Enforcement of Heterosexuality", "claims": null}, {"metadata": {"year": 2018}, "authors": ["B. M. Dank", "R. Refinetti"], "summary": "\"Sexuality and Culture\" serves as a compelling forum for the analysis of ethical, cultural, psychological, social, and political issues related to sexual relationships and sexual behavior. These issues include, but are not limited to: sexual consent and sexual responsibility; sexual harassment and freedom of speech and association; sexual privacy; censorship and pornography; impact of film/literature on sexual relationships; and university and governmental regulation of intimate relationships. The central theme of this volume is the politics of sexuality. Theoretical essays, research reports, and book reviews examine the topics of sexual harassment law as a sexual control mechanism, censorship of sexual materials, and criminalization of commercialized sexuality. A special section focuses on the Clinton-Lewinsky affair with contributions by David Steinberg, John Furedy, and Joseph Fulda. Other articles include: \"Trends Towards Increased Sexual Repression in the Final Two Decades of the Twentieth Century\" by Elizabeth Allgeier; \"Naked but Unseen: Sex and labor conflict in San Francisco's Adult Entertainment Theaters\" by Kerwin Kay; \"A test of the Biopolitics Hypothesis\" by Kenneth Westhues; \"Scientific and Fictive Sociology: The Viability of Research\" by Edwina Taborsky and Reena Sommer; and \"Sex Entertainment for Women on the Web\" by Marjorie Kibby. Also included are reviews of books, including \"Faculty-Student Sexual Involvement: Issues and Interventions, \" by Virginia Stamler and Gerald Stone; \"Heterophobia: Sexual Harassment and the Future of Feminism,\" by Daphne Patai; \"Sex among Allies: Military Prostitution in US-Korea Relations, \"by Katharine H. Moon; and \"American Homo\" by Jeffrey Escoffier. \"The Politics of Sexuality\" will be of interest to general readers as well as to scholars (sociologists, psychologists, legal analysts), policymakers, and members of the sex work and sex entertainment communities.", "title": "The Politics of Sexuality", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Marielle Roy"], "summary": "The harassment of sexual minority students in school has recently become a topic of interest to researchers interested in issues of school safety. The Preventing School Harassment (PSH) survey, which specifically addresses harassment based on sexual orientation, is an important part of this line of inquiry. The present analysis is based on a free-response question included in the 2008 version of the PSH survey, which addressed students\u2019 experiences with the inclusion of sexual minority issues in the classroom. Students\u2019 responses were analyzed to identify the major themes in both the content and context of what students learned in school about these issues. Whereas one-quarter of respondents indicated they learned nothing about sexual minority issues in school, the majority of students who learned about these issues reported learning about ideals of acceptance and equality. Respondents reported learning about LGBTQ issues most often outside of the classroom, and most commonly from personal experiences or from members of student clubs. These results provide insight into how students learn about these issues in school, and may inform future research into methods of increasing students\u2019 feelings of safety at school. Exploring the role of LGBTQ issues 4 Introduction \u201cThere\u2019s nothing wrong with being LGBTQ, it's just the way you are. They're people and equal too... Although that also feels obvious.\u201d \u201cPeople are just uninformed about the GLBT community. I strongly believe that ignorance is the basis of unacceptance and that if people knew what we were and that we are normal, they would not be afraid of us, because that is what they are.\u201d \u2014Student respondents to the 2008 Preventing School Harassment survey As the quotes above demonstrate, today\u2019s high school students have strong opinions about discrimination against people who identify as lesbian, gay, bisexual, transgender, or queer/questioning (LGBTQ). However, the experiences of sexual minority students in schools have only very recently become a topic of research, through the use of both widespread surveys and smaller studies. The findings of the research on this issue are disheartening: harassment based on anti-LGBTQ bias is both intense and pervasive, and sexual minority students often feel unsafe in school (Kosciw, Diaz, & Greytak, 2008; O\u2019Shaughnessy, Russell, Heck, Calhoun, & Laub, 2004). One of the most significant findings of these studies is that anti-LGBTQ harassment is qualitatively different from other types of harassment. In particular, antiLGBTQ slurs are damaging both to sexual minority and heterosexual students. For example, male respondents to a survey of middleand high-schoolers have reported that homophobic slurs are significantly more threatening to a student\u2019s reputation than other types of slurs (Thurlow, 2001). Additionally, respondents in a study of 107 New Zealand high schools revealed pervasive anti-LGBTQ sentiment: only 5% of students and 8% of staff believed that LGBTQ students would feel safe at their school. Content analysis of the participants\u2019 responses revealed two main themes: descriptions of harassment Exploring the role of LGBTQ issues 5 (predominantly verbal) and perceived invisibility (for example, \u201cI don\u2019t know any lesbian/gay/bisexual students\u201d) (Nairn & Smith, 2003). The significance of homophobic slurs in sexual harassment is further highlighted in a study performed by the American Association of University Women (AAUW). Equal numbers (approximately 75%) of respondents to a national survey of American high school students reported that they would be \u201cvery upset\u201d if someone \u201cpulled off or down their clothing\u201d or called them gay or lesbian (American Association of University Women [AAUW] Educational Foundation, 2001). The fact that being called gay or lesbian is as upsetting as being the victim of physical sexual harassment is astounding. Additionally, this particular example of harassment (being called gay or lesbian) was the only example rated by both boys and girls to be of equal significance: 74% of boys and 73% of girls reported that they would be \u201cvery upset\u201d by such an incident (AAUW, 2001). Anti-LGBTQ harassment is both powerful and pervasive. The latest version of the National School Climate Survey (NSCS), distributed by the Gay, Lesbian and Straight Education Network (GLSEN), was released in 2007 and reached approximately 6200 sexual minority students across the United States. Of the survey respondents, 86.2% had been verbally harassed and nearly half (44%) physically harassed because of their sexual orientation. Nearly a third of respondents had missed school at least once because they had felt unsafe (Kosciw et al., 2008). In comparison, under 10% of respondents to the California Healthy Kids Survey (CHKS), a survey representative of all California high school students, reported ever missing school because they felt unsafe (California Safe and Healthy Kids Program Office, 2008). Exploring the role of LGBTQ issues 6 Repeated harassment based on sexual orientation affects students\u2019 mental health, performance in school, and future educational and social development. Eight percent of all respondents (both heterosexual and sexual minority students) to the 2001-2002 CHKS had been the victims of harassment based upon their \u201cactual or perceived sexual orientation.\u201d The students harassed on the basis of anti-LGBTQ bias were over twice as likely as non-harassed students to report depressive symptoms (55% compared to 23%), and three times as likely to have seriously considered suicide (45% compared to 14%). In addition, victims of this type of harassment were more likely than non-harassed students to report having engaged in substance abuse, including tobacco and marijuana use, binge drinking, and use of other illegal drugs (O\u2019Shaughnessy et al., 2004). Victims of anti-LGBTQ harassment report lower grades than non-harassed students: 24% of harassed students earned Cs or lower, compared to 17% of students who were not harassed (O\u2019Shaughnessy et al., 2004). Twelve percent of sexual minority students had no plans for future education beyond high school, nearly twice the percentage of the national student body (Kosciw et al., 2008). One might go so far as to say that students who experience these effects as a direct result of harassment at school are being denied an education. These are serious and complicated problems, but multiple studies have identified several key actions that schools can make that produce strong, pervasive improvements. These changes include adopting an anti-harassment policy that explicitly includes sexual minority students; active involvement of teachers in stopping acts of harassment; the presence of a Gay-Straight alliance on campus; and inclusion of LGBTQ-related issues in regular classroom curricula (O\u2019Shaughnessy et al., 2004). Exploring the role of LGBTQ issues 7 The first two actions are important in that they provide a direct and obvious signal to the student body that staff and administration are committed to preventing harassment on their campus. A school\u2019s responsibility to protect all students is included in district, state and national policies. Including sexual minorities in an anti-harassment policy does not increase a school\u2019s liability, but simply emphasizes its pre-existing responsibility to prevent harassment based on sexual orientation. Including sexual minority students in the anti-harassment policy is a first step towards ending harassment in a school: \u201cA comprehensive policy that is publicized and implemented will clarify for all staff that such behavior is unacceptable and must be responded to\u201d (GLSEN, 2002). Once such a policy is implemented, teachers must take an active role in changing their school climate by addressing slurs and other forms of harassment whenever they occur. Inclusive antiharassment policies provide incentive but not instruction, and so teachers should be trained in ways to address teasing, name-calling, and more serious forms of harassment. When teachers are prepared to handle situations of harassment, these situations can be used as \u201cteachable moments\u201d for the students involved. The formation of Gay-Straight Alliances (GSAs) as student-run clubs has also been related with improvements in school environment. The CHKS showed that 75% of students with a GSA on their campus reported feeling safe at school, compared to 61% of students without a GSA at their school. The presence of a GSA was also correlated with decreased anti-LGBTQ harassment and an increase in students\u2019 perceived support from both school staff and other adults (O\u2019Shaughnessy et al., 2004). Respondents to the NSCS reported similar effects: students at schools with GSAs heard fewer homophobic Exploring the role of LGBTQ issues 8 remarks, felt more a part of their school community, and reported acts of harassment to school administrators more often (Kosciw et al., 2008). These positive effects are most likely due to a combination of factors, rather than the simple fact of the club\u2019s formation. Gay-Straight Alliances may be more likely to form in schools that have already established more positive environments; indeed, the formation of GSAs is often a difficult process in schools with pervasive anti-LGBTQ bias. Opposition to GSAs can often be traced to a misunderstanding of their purpose. However, the Gay-Straight Alliance Network provides a description of the true goals of high school GSAs: \u201c[to] create safe environments in schools for students to support each other and learn about homophobia and other oppressions, educate the school community about homophobia, gender identity, and sexual orientation issues, and fight discrimination, harassment, and violence in schools\u201d (GSA Network). The positive effects of GSAs, as described above, are the strongest possible testament to these goals. Finally, schools that present LGBTQ issues as part of their curriculum evidence the greatest increase in the perc", "title": "Exploring the Role of LGBTQ Issues in the Classrooms of California Schools", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Stephen Tomsen"], "summary": "This chapter examines research evidence to argue that violence directed at gay men, lesbians and transsexual/transgendered people are not wholly distinct from other forms of male-perpetrated violence. It insists that harassment and violence directed against sexual groups are highly gendered and everyday phenomena and narrow views of homophobic prejudice should be refined in order to appreciate this. Furthermore, these acts are widespread and collective social phenomena built on masculine understandings of a sexual mainstream and subordinate others.", "title": "Homophobic violence and masculinities in Australia", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Stephen  Tomsen", "Kevin  Markwell"], "summary": "Prejudice and violence directed against gay men, lesbians and other sexual groups have been viewed as ubiquitous and relatively fixed phenomena in contemporary societies. This perspective must be reconciled with the increased depiction of marginal sexualities and commercial \u2018queering\u2019 of mainstream media and popular culture. This paper presents and discusses data from two sources. Firstly, interviews conducted with self-identifying heterosexuals at the annual Sydney Gay and Lesbian Mardi Gras (SGLMG) parade suggest attendance and participation can occur through a widely enjoyed public display and the temporary suspension of sexual prejudice in such specific carnivalesque occasions. Secondly, gay and lesbian responses to an internet-based questionnaire concerning perceptions and experiences of safety and hostility at this and similar other public events, suggest an undercurrent of threat and incivility, especially in the post-event context. These data sources are not directly compared but analysed in a complementary way to throw new light on how different groups view and experience this event. Our findings reflect how sexual prejudice is a shifting and contradictory collective social practice.", "title": "Violence, Cultural Display and the Suspension of Sexual Prejudice", "claims": null}, {"metadata": {"year": 2002}, "authors": ["S. Fineran"], "summary": "This article provides a historical and legal framework for defining peer sexual harassment from three different perspectives: sex discrimination, mental health, and sexual violence. Major court decisions that define sexual harassment in both education and the workplace are highlighted, and arguments regarding sexual harassment between peers of the same sex are profiled. This research also identifies sexism and heterosexism as a major social violence problem in U.S. education and argues that peer sexual harassment is sexual violence with considerable mental health implications for both boys and girls. Recommendations for social work practice regarding peer sexual harassment in schools are discussed.", "title": "Sexual harassment between same-sex peers: intersection of mental health, homophobia, and sexual violence in schools.", "claims": null}, {"metadata": {"year": 1990}, "authors": ["G. Herek"], "summary": "Hate crimes against lesbians and gay men occur within a broader cultural context that is permeated by heterosexism. Heterosexism is defined here as an ideological system that denies, denigrates, and stigmatizes any nonheterosexual form of behavior, identity, relationship, or community. It operates principally by rendering homosexuality invisible and, when this fails, by trivializing, repressing, or stigmatizing it. This article focuses upon the nexus between cultural heterosexism and individual prejudice against lesbians and gay men. Key components of the ideologies of sex and gender from which heterosexism derives are identified: (a) the personal-public dichotomy, (b) the stigmatization of particular forms of sexuality, and (c) the linkage of heterosexuality to gender-role conformity. Supported by these ideological underpinnings, cultural heterosexism fosters individual anti-gay attitudes by providing a ready-made system of values and stereotypical beliefs that justify such prejudice as \u201cnatural.\u201d By imbuing homosexuality with a variety of symbolic meanings, cultural heterosexism enables expressions of individual prejudice to serve various psychological functions. Further, by discouraging lesbians and gay men from coming out to others, heterosexism perpetuates itself. Recent social trends that may affect the ideology of heterosexism are identified, and their potential for reducing anti-gay prejudice is discussed.", "title": "The Context of Anti-Gay Violence", "claims": null}], "query": "Give me references about gay culture and sexual harassment", "summary_abstract": "The collection of papers provides a multifaceted exploration of gay culture and sexual harassment, highlighting the pervasive nature of harassment and violence against sexual minorities. Epstein (1996) and Roy (2010) both emphasize the widespread harassment experienced by gay and lesbian individuals, particularly in educational settings. Epstein (1996) notes the various forms of harassment faced by lesbian and gay students and teachers, while Roy (2010) highlights the intense and pervasive nature of anti-LGBTQ harassment in schools, which significantly impacts students' mental health and academic performance.\n\nTomsen (2013) and Tomsen and Markwell (2009) discuss the broader social context of violence and harassment against gay men, lesbians, and transgender individuals, arguing that these acts are deeply gendered and embedded in societal norms. They suggest that such violence is not distinct from other forms of male-perpetrated violence and is a collective social phenomenon.\n\nHerek (1990) provides a theoretical framework for understanding the cultural underpinnings of anti-gay prejudice, identifying heterosexism as a key ideological system that perpetuates discrimination and violence against non-heterosexual individuals. This cultural context, as Herek argues, supports individual prejudices and discourages openness about sexual orientation, thereby sustaining the cycle of harassment.\n\nFineran (2002) offers a legal and historical perspective on peer sexual harassment, identifying sexism and heterosexism as significant social violence issues within U.S. education. This research underscores the mental health implications of such harassment for both boys and girls.\n\nCollectively, these papers underscore the pervasive and systemic nature of harassment and violence against gay individuals, rooted in cultural norms and ideologies that perpetuate discrimination and prejudice. They highlight the need for comprehensive anti-harassment policies and educational initiatives to foster safer environments for sexual minorities (Roy, 2010; Fineran, 2002).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2014}, "authors": ["Weidong Huang", "T. Bednarz"], "summary": "How we visualize graph data is important for us to make sense of it. A number of aesthetic criteria have been used in practice to guide the visualization process and judge the quality of graph drawings. These aesthetics are limited since they often conflict with each other. It is generally agreed that in order to make visualizations effective, well-grounded perception and cognitive theories and design principles are needed. Some attempts have been made to develop visualization theories. In this paper, we present a preliminary study which we conducted with a cognitive approach to add to this growing body of research. More specifically, we propose a graph visualization model, which is further conceptualized into a two-stage assessment cycle. Examples of potentially useful methodologies and theories are introduced and their implications for producing user-friendly visualizations are discussed.", "title": "Towards a Cognitive Approach to User-Centered Visualization Design and Evaluation", "claims": null}, {"metadata": {"year": 2007}, "authors": ["Michael  Workman"], "summary": "In knowledge-work, there are increasing amounts of complex information rendered by information technology, which has led to the common term, information overload. Information visualization is one area where empirically tested semantic theory has not yet caught up with that of the underlying information storage and retrieval theory, contributing to information overload. In spite of a vast body of cognitive theory, much of the human factors research on information visualization has overlooked it. Specifically, information displays have facilitated the data gathering (ontological) aspects of human problem-solving and decision-making, but have exacerbated the meaning-making (epistemological) aspects of those activities by presenting information in linear rather than in graphical (holistic) forms. Drawing from extant empirical research, we present a thesis suggesting that cognitive load may be reduced when holistic information is imbued with transformational grammar to help alleviate the information overload problem, along with a methodological approach for investigation.", "title": "Cognitive Load Research and Semantic Apprehension of Graphical Linguistics", "claims": null}, {"metadata": {"year": 2006}, "authors": ["Weidong Huang", "Seok-Hee Hong", "P. Eades"], "summary": "Performance and preference measures are commonly used in the assessment of visualization techniques. This is important and useful in understanding differences in effectiveness between different treatments. However, these measures do not answer how and why the differences are caused. And sometimes, performance measures alone may not be sensitive enough to detect differences. In this paper, we introduce a cognitive approach for visualization effectiveness and efficiency assessment. A model of user performance, mental effort and cognitive load (memory demand) is proposed and further mental effort and visualization efficiency measures are incorporated into our analysis. It is argued that 1) combining cognitive measures with traditional methods provides us new insights and practical guidance in visualization assessment. 2) analyzing human cognitive process not only helps to understand how viewers interact with visualizations, but also helps to predict user performance in initial stage. 3) keeping cognitive load induced by a visualization low allows more memory resources to be available for high level complex cognitive activities. A case study conducted supports our arguments.", "title": "Predicting graph reading performance: a cognitive approach", "claims": null}, {"metadata": {"year": 2011}, "authors": ["M. Hoffmann", "F. Paglieri"], "summary": "External representations play a crucial role in learning. At the same time, cognitive load theory suggests that the possibility of learning depends on limited resources of the working memory and on cognitive load imposed by instructional design and representation tools. Both these observations motivate a critical look at Computer-Supported Argument Visualization (CSAV) tools that are supposed to facilitate learning. This paper uses cognitive load theory to compare the cognitive efficacy of Rationale TM 2 and AGORA.", "title": "Cognitive effects of argument visualization tools", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Muhammed Yousoof Mohd Sapiyan"], "summary": "Computer programming is a complex skill to acquire for novice learners who are in their initial phase of learning programming. There are many factors that results in difficulties in learning programming. This paper addresses to resolve one core difficulty which is cognitive load [2] [3].Cognitive load theory [13] is a famous theory of learning. It states that the schema of the long term memory is not well built in the case of novices and also there is a limitation of working memory\u2019s capacity. This makes it hard for novices to understand the concepts and equip with the skills necessary to become programmers. Some efforts used to overcome the cognitive load are the visualization tools for learning programming [14] .There is no accountability on how effective these visualization systems helped in reducing the load. The mechanism to measure cognitive load is not used in the visualization systems. There are two methods of cognitive load measurement namely physiological and non physiological measures. Physiological measures include EKG,GSR[12],EEG[11],Temperature[11] etc. and non physiological measures includes rating scale[6] and recent research studies have used EEG as a index for cognitive load measurement [7].We felt that using the physiological measures could be accurate as they are the reflections of the body impulses. There is no user\u2019s control over the measurement. We also found out that among the physiological measures EEG could be more effective as the latest efforts of measuring the cognitive uses EEG. This paper addresses the cognitive load measurement while using visualization tools by the novice programmers using EEG as an index of cognitive load.", "title": "COGNITIVE LOAD MEASUREMENT FOR VISUALIZATIONS IN LEARNING COMPUTER PROGRAMMING USING EEG (ELECTROENCELOGRAPHY)", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Weidong  Huang", "Peter  Eades", "Seok-Hee  Hong"], "summary": "Graph visualizations are typically evaluated by comparing their differences in effectiveness, measured by task performance such as response time and accuracy. Such performance-based measures have proved to be useful in their own right. There are some situations, however, where the performance measures alone may not be sensitive enough to detect differences. This limitation can be seen from the fact that the graph viewer may achieve the same level of performance by devoting different amounts of cognitive effort. In addition, it is not often that individual performance measures are consistently in favor of a particular visualization. This makes design and evaluation difficult in choosing one visualization over another. In an attempt to overcome the above-mentioned limitations, we measure the effectiveness of graph visualizations from a cognitive load perspective. Human memory as an information processing system and recent results from cognitive load research are reviewed first. The construct of cognitive load in the context of graph visualization is proposed and discussed. A model of user task performance, mental effort and cognitive load is proposed thereafter to further reveal the interacting relations between these three concepts. A cognitive load measure called mental effort is introduced and this measure is further combined with traditional performance measures into a single multi-dimensional measure called visualization efficiency. The proposed model and measurements are tested in a user study for validity. Implications of the cognitive load considerations in graph visualization are discussed.", "title": "Measuring Effectiveness of Graph Visualizations: A Cognitive Load Perspective", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Ricarda Moses", "S. Humayoun", "Ragaad Altarawneh", "A. Ebert"], "summary": "Understanding cognitive processes during the interaction with visualizations, speci\ufb01cally limitations of working memory, opens a new perspective on evaluations and encourages a more user-centric design approach. In a user study we evaluated two graph data visualization approaches (i.e., the force-directed layout and the Chord layout) using a cognitive load questionnaire to assess the three load types: intrinsic, extraneous, and germane load. Tasks were designed to encourage insight and sensemaking in the participants during the evaluation. This type of evaluation helps to assess the users\u2019 mental processes during sensemaking and graph reading in the underlying layouts. Further, such study \ufb01ndings would help visualization designers in choosing the appropriate layout type for their graph data.", "title": "Evaluating Cognitive Load: Force-directed Layout vs. Chord Layout", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Juan C. Castro-Alonso", "Paul  Ayres", "John  Sweller"], "summary": "There are basically two formats used in instructional visualizations, namely, static pictures and dynamic visualizations (e.g., animations and videos). Both can be engaging and fun for university students in the fields of health and natural sciences. However, engagement by itself is not always conducive to learning. Consequently, teachers, lecturers, and instructional designers need to utilize the cognitive processing advantages of visualizations as well as engagement to achieve full instructional effectiveness. A cognitive processing focus has outlined many ways in which instructional visualization can be optimized. Specifically, cognitive load theory and the cognitive theory of multimedia learning are two research paradigms that provide several methods for directing the design of visualizations by considering how learners process visuospatial information. In this chapter, we describe five methods based on these cognitive theories: (a) the split attention effect and spatial contiguity principle, (b) the modality effect, (c) the redundancy effect and coherence principle, (d) the signaling principle, and (e) the transient information effect. For each of these effects, examples of applications for education in health and natural sciences are provided, where the influence of visuospatial processing is also considered. We end this chapter by discussing instructional implications for science education and providing future directions for research.", "title": "Instructional Visualizations, Cognitive Load Theory, and Visuospatial Processing", "claims": null}], "query": "Cognitive Load Theory toward visualization", "summary_abstract": "The collection of papers provides a comprehensive exploration of how Cognitive Load Theory (CLT) can be applied to visualization, highlighting its potential to enhance the effectiveness of visual representations by managing cognitive demands on users. Huang and Bednarz (2014) emphasize the need for well-grounded cognitive theories in visualization design, proposing a model that incorporates cognitive approaches to improve user-friendliness. Workman (2007) suggests that cognitive load can be reduced by presenting information in holistic forms, which can alleviate information overload, a common issue in information visualization.\n\nHuang, Hong, and Eades (2006) propose a model that integrates cognitive load considerations with traditional performance measures, arguing that low cognitive load allows more memory resources for complex cognitive activities. This is supported by their later work (Huang, Eades, and Hong, 2009), which introduces a cognitive load measure called mental effort, combined with traditional performance metrics to assess visualization efficiency.\n\nMoses et al. (2017) further explore cognitive load in graph visualizations, using a cognitive load questionnaire to evaluate different layout types, which aids in understanding user mental processes during sensemaking. This approach helps designers choose appropriate visualization layouts based on cognitive load considerations.\n\nCastro-Alonso, Ayres, and Sweller (2019) discuss instructional visualizations, emphasizing the importance of cognitive processing advantages alongside engagement. They outline methods based on CLT and multimedia learning theory to optimize visualizations, such as the split attention effect and signaling principle, which can guide the design of educational visualizations.\n\nOverall, these studies collectively underscore the importance of incorporating cognitive load considerations into visualization design to enhance user comprehension and efficiency, providing practical insights and methodologies for achieving this goal.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2018}, "authors": ["Mingfong Jan", "Wan-Lin Yang"], "summary": "There is a need to reframe teachers' roles from content area experts to that of learning experience designers because of 21st century teaching challenges. As learning experience designers, teachers help students develop 21st century competencies via guided cognitive and social participation in designed learning activities such as games and gamified activities. In this qualitative case study, we explore teachers' design thinking in designing a lesson plan that involves the use of a card game designed for complex system understanding. Six teachers' thoughts about learning activity design are unpacked via the following activities: playing a card game, crafting a game-based lesson plan, and reporting their design thinking via semi-structural interviews. We discuss similarities in the teachers' views on game-based learning and the structure of their designed lessons, as well as implications of the study. This baseline study helps us map out how teachers think about learning experience design. Such understanding is critical for developing teachers as designers.", "title": "Understanding Teachers' Design Thinking in Designing Game-Based Activities", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Cesar C. Navarrete"], "summary": "In a case study on middle-school student educational game creation course in south central US state, the students' creative thinking process is investigated in order to understand perceptions of the digital design and programming involved in the game creation learning approach. Interviewing 12 students at with three different levels of game design experience, students in grade 6, 7 and 8, with 1, 2, and 3 years of game design experience respectively, findings suggest that students enjoyed the learning approach as satisfying and engaging, yet technologically challenging. The students experienced positive opportunities for engaging the creative thinking process in synthesizing social issue information for constructing their understanding through the creation of interactive, educational digital games. Findings suggest that the creative thinking process in student-centered game creation learning approach may provide learners a rich and enjoyable learning experience with the authentic technology use as well as provide for deep, insightful learning.", "title": "Creative thinking in digital game design and development: A case study", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Qing Li", "Shahista Nathoo", "Elise Vandermeiden", "Collette Lemieux"], "summary": "\"This research intends to explore practicing teachers\u00e2\u20ac\u2122 experiences with game design and development. Specifical-ly, the study is guided by the following research questions: 1. What design considerations were taken into account during the game design process of practicing teach-ers? 2. What pedagogical components (if any) emerge when practicing teachers design and build digital games? Employing a naturalistic and qualitative approach, data are collected from 32 graduate students who partici-pated in the study. The analysis of the games created by these practicing teachers indicates some key themes: (A) curriculum goals were predominantly the games\u00e2\u20ac\u2122 objec-tives; (B) sounds and animations provided visual appeal and feedback; (C) the game could be or needed to be used with supporting materials within the class; (D) the need to create a sense of accomplishment for the gamer; and (F) the emphasis of the gamer being able to project or custom-ize his/her identity within the game. Further discussion is also provided.\"", "title": "Practicing Teachers as Digital Game Creators: A Study of the Design Considerations", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Ana C. R. Martins", "Lia Raquel Moreira Oliveira"], "summary": "The design of educational games is a powerful pedagogical strategy that can only enter schools if teachers are given the necessary training and support. In this paper we present a training action course for teachers interested in learning how to use simple frameworks and tools to be able to either design educational games themselves or facilitate educational game design by their students. We propose a syllabus, a design framework, and a software solution for digital game creation. This approach has been used in Portugal with middle school teachers and students and has been shown to be effective, resulting in the production of functional educational games (digital and non-digital), with preliminary results showing positive outcomes in learning and engagement.", "title": "Students as creators of educational games: learning to use simple frameworks and tools to empower students as educational game designers", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Fr\u00e9d\u00e9rique  Frossard", "Anna  Trifonova", "Mario  Barajas"], "summary": "Abstract Creativity has become a key educational objective. How can game-based learning enhance creative pedagogies? This chapter proposes an approach in which teachers become game designers. It provides a model which analyzes creativity according to three dimensions: process, product, and teaching. We describe practical experiences in which teachers designed and applied their own learning games. Results highlight that game design promotes teaching practices that foster students\u2019 creativity.", "title": "Teachers Designing Learning Games", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Zhen Zhigao"], "summary": "With the development of electronic game industry,more and more people have focused on how to apply the new technology into the field of education,on the purpose of developing educational games with unique educational goals,content or method.Games-to-Teach Project is a partnership between MIT and Microsoft to develop conceptual prototypes for the next generation of interactive educational entertainment.The members of this project involve instructors,researchers and students from different subjects or fields.As main practical results,fifteen conceptual prototypes focus on college-level science,engineering,social science,and humanities education,which will make a significant milestone.The paper introduces the research objectives firstly,and then places importance on the fifteen conceptual prototypes which is the main practical results.During the description,an example is provided to introdue the design progress.Besides,the main conclusions of this proiect is summaried.According to the introduction above,the authors propose five suggestions for us to design and develop educational digital games.The educational digital games have broad prospects for instruction.The integration of games and course content can be achieved through a variety of ways.The design of educational digital games is necessary to be supported with teaching and learning theory.The design of educational digital games is better to be instructional targeted.", "title": "Introduction and Enlightenment on the Games-to-Teach Project", "claims": null}, {"metadata": {"year": 2013}, "authors": ["R. Sandford"], "summary": "Summary form only given. Researchers have offered, in recent years, compelling reasons for considering the potential of digital games to support learning (Klopfer et al., 2009; Pelletier, 2009; Mitchell & Savill-Smith, 2004; Gee, 2008), and in response policymakers and educators around the world have demonstrated a commitment to exploring their practical use in school (Ulicsak, 2010; Koh et al. 2009). There remain, however, many questions about how games can best support learning, particularly in formal education. For teachers, the use of games in a formal curriculum setting can present practical and operational issues, as well as surfacing more fundamental tensions: between generational expectations of games and technology, between home and school identities, and between pedagogies associated with accounts of games as learning tools and those more commonly embraced within the context of formal schooling (Sandford et al., 2011; Felicia, 2009; Ito\u0305 et al., 2009; Sandford et al. 2006). In managing these tensions, teachers are increasingly asked to construct themselves as `designers' (Towndrow, 2005; Foo, et al., 2006; Carlgren, 1999), mobilising their professional knowledge in the creation of new strategies and practices that enable them to negotiate these practical and pedagogical challenges. This paper examines the implications of formal game-based learning for teachers developing their own digital learning games, exploring two guiding overarching research questions. What strategies are employed by teachers to manage intergenerational, technological, operational and pedagogic tensions in the classroom? And to what extent is the notion of being a `designer' visible in their professional practice? The paper suggests that there are multiple ways of `being a designer' for teachers, and that the notion of `designer' may be a more problematic representation of teacher agency and identity than currently visible in the literature.", "title": "Teachers as game designers: Using a game in formal learning in a Singapore primary school", "claims": null}, {"metadata": {"year": 2018}, "authors": ["M. Cezarotto", "A. Battaiola", "Virg\u00ednia Kistmann"], "summary": "Digital educational games have been widely used in educational practices. These games are considered as valuable artefacts that can be used to teach a broad range of subjects to different kinds of learners, thus performing a significant role in the internalisation of contents. A large number of companies, both national and international, develop this kind of media. However, the design of educational games is a complex task, encompassing multiple knowledge and several different stages of development. Companies must excel in order to succeed in this highly competitive market. Studies that focus on the connection between design management and game development are still unsatisfactory. With this in mind, this paper aims to contribute to this debate and to the digital educational game industry, by proposing the use of knowledge from design management as a way of improving strategic planning. From a design management perspective, this paper aims to identify ways to promote innovation during the development process of educational games. In order to do so, this study considers the process of development of games and the communication between the academic field and the industry. This research\u2019s approach is exploratory and qualitative, and this study makes use of a literature review and a case study. This paper offers a set of contributions focusing on innovation for developing digital educational games based on strategic design. Therefore, it provides useful information regarding the relevance of the use of design management by educational game companies. Moreover, this study provides a set of actions to improve strategic planning in these companies through a design management perspective.", "title": "The design of educational games and innovation : a case study based on design management", "claims": null}], "query": "what is design thinking in teachers developing digital educational games?", "summary_abstract": "Design thinking in the context of teachers developing digital educational games is a multifaceted process that involves reimagining teachers' roles as designers of learning experiences. Jan and Yang (2018) emphasize the shift from teachers being content experts to becoming learning experience designers, where they facilitate the development of 21st-century competencies through game-based learning activities. This involves teachers engaging in activities such as playing games, crafting lesson plans, and reflecting on their design thinking processes.\n\nLi et al. (2012) highlight that when teachers design digital games, they consider curriculum goals, visual appeal, and the integration of games with classroom materials. They also focus on creating a sense of accomplishment for students and allowing for personalization within the game, which are crucial pedagogical components.\n\nMartins and Oliveira (2018) discuss the importance of providing teachers with training and support to effectively design educational games. They propose a framework and tools that enable teachers to either design games themselves or guide students in game creation, which has shown positive outcomes in learning and engagement.\n\nFrossard et al. (2015) suggest that when teachers take on the role of game designers, it enhances creative pedagogies and fosters students' creativity. This approach aligns with the broader educational objective of promoting creativity through game-based learning.\n\nSandford (2013) explores the challenges teachers face in integrating games into formal education, such as managing generational and pedagogical tensions. The study suggests that teachers must adopt a designer mindset, using their professional knowledge to navigate these challenges and create effective learning strategies.\n\nOverall, the collective insights from these studies underscore the transformative potential of design thinking in empowering teachers to create engaging and educational digital games. This process not only enhances learning experiences but also supports teachers in developing innovative pedagogical practices.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2021}, "authors": ["N. Anh"], "summary": "Japan is the world's 3rd largest economy with a nominal GDP of USD 5380 billion. Japan's economy has a rapid industrial development process after its defeat in World War II. Vietnam's economy in 2020 will achieve GDP growth of 2.9%, nominal GDP of 300 billion dollars, per capita income reached $2786, if calculated according to purchasing power parity (PPP), the average income of Vietnamese people is about $ 8,500. In 2020, Vietnam has a trade surplus of 19.1 billion USD (an important contribution to this result is the total two-way turnover of 39.6 billion USD, a trade deficit of 1.1 billion USD from Japan. No small contribution to the economic success of Vietnam as a strategic partner of Japan - the third largest economy in the world. Japan is the first country in the G7 group to recognize Vietnam as a market economy and partner, the largest ODA sponsor for Vietnam, the number 1 investor in Vietnam and the 3rd largest trading partner of Vietnam. This paper also refers the lessons learned for Vietnam.", "title": "Experience in Supporting Industry Development in Japan and Lessons for Viet Nam", "claims": null}, {"metadata": {"year": 2013}, "authors": [], "summary": "Vietnam is an emerging country that over the last twenty years has experienced a high GDP growth rate, although in recent years it has not been saved from the effects of the global economic crisis. Since the introduction of the Doi Moi policy (the Renovation) in 1986, Vietnam aims at transforming the structure of its economy. From a purely centralized and state-controlled economy heavily dependent on agriculture, Vietnam aims to evolve into a socialist-oriented market economy structured towards a more balanced equilibrium between manufacturing and services. In 2010 the composition of GDP by sector showed an economic structure in which agriculture accounted for 20.6% of GDP, while industry and services accounted respectively for 41.1% and 38.3%.", "title": "COMMISSIONS SERVICES' ANNEX ON VIETNAM TO THE POSITION PAPER ON THE TRADE SUSTAINABILITY IMPACT ASSESSMENT OF THE FREE TRADE AGREEMENT BETWEEN THE EU AND ASEAN", "claims": null}, {"metadata": {"year": 2004}, "authors": [], "summary": "Vietnam is a developing country and a member of the Association of Southeast Asian Nations (ASEAN). The country has a surface area of 331,000 square kilometres and is located in the tropics. It has a total population of nearly 80 million, of which 75 percent are farmers living in rural areas. Vietnam\u2019s GDP in 2000 was approximately US$32 billion. Beginning in 1986, it has implemented reform policies to shift from a centralised economy to a market economy with socialist orientation. The Vietnamese economy is structured in terms of GDP in 2000 at current prices, as follows (Vietnam Statistic Yearbook 2000): By economic sector: \u2022 Service: 39.1 percent \u2022 Agriculture, forestry, fishery: 14.3 percent \u2022 Industry, construction: 36.6 percent By ownership: \u2022 State: 39 percent \u2022 Household: 32 percent \u2022 Collective: 8.5 percent \u2022 Private: 3.3 percent \u2022 Mixed: 3.9 percent \u2022 Foreign-invested sector: 13.3 percent", "title": "296-304 Vietnam Final May", "claims": null}, {"metadata": {"year": 2006}, "authors": ["V. Suri", "V. Dinh"], "summary": "The Gross Domestic Product (GDP) growth is anticipated to surpass eight percent for a second year running. Domestic investment has been robust and manufactured exports have experienced solid growth. Foreign Direct Investment (FDI), boosted by imminent World Trade Organization (WTO) accession, has soared. Buoyant revenue growth is expected to help contain the budget deficit below its targeted level. However, the impact of increases in the minimum wage on government expenditures needs to be watched. The trade and external current account balances are projected to be in surplus, and foreign reserves have shown a sizable increase in 2006. The stock market, though still small by regional standards, has grown rapidly to reach nearly eight percent of GDP. Inflation, while lower, is still around seven percent and price pressures remain. Credit growth from state owned banks moderated, partly due to the need to adhere to stricter prudential standards. Reforms in the banking sector assume even greater urgency after WTO accession. Vietnam's accession was ratified by its national assembly in November 2006 and commitments start to take effect in 2007. The increasing liberalization of trade in goods and services offers potential for sustaining rapid growth and poverty reduction. The government will need to continue building support for implementing policy changes entailed by accession, while responding to any genuine hardships that may arise. This report takes a look at these factors contributing to the Vietnam economy as well as exports, imports, GDP growth, inflation, stock market regulations, bonds, and the government budget balance. The report gives a good snap shot of the Vietnamese economy as of year 2006.", "title": "Taking stock : an update on Vietnam's economic developments by the World Bank in Vietnam", "claims": null}, {"metadata": {"year": 1993}, "authors": ["G. Jenkins", "S. Terkper"], "summary": "Vietnam is an Asian country with a population of about 70million and annual per capita income of US $190-200. Despite its current status as one of the poorest countries in the world, it has tremendous potential for growth because of its abundant natural resources and an excellent human resource base. Unlike other centrally planned economies of Asia and Eastern Europe, Vietnam had a relatively small public sector administration, aside from the state enterprise structure, and legacy of capitalism in the south of the country. Nevertheless, the impact of public sector on revenue generation had remained very important.", "title": "VIETNAM\u2019S TAX REFORMS: POLICIES IN TRANSITION ECONOMIES", "claims": null}, {"metadata": {"year": 2004}, "authors": ["D. Viet", "V. Suri"], "summary": "The Gross Domestic Product (GDP) growth in 2004 continues to remain strong and will likely exceed last year's level of 7.2 percent. Non-oil exports have remained robust despite facing external constraints. The current account and budget deficits are both expected to narrow. The main macroeconomic development in the last ten months has been the sharp rise in prices generating considerable debate among policy makers on response strategies. Inflation has, however, begun to decelerate. The high international price of oil, on the one hand has been an important factor in the recent upsurge in inflation, has on the other hand boosted export receipts and government revenue. This report takes a look at these factors contributing to the Vietnam economy as well as exports, imports, GDP growth, inflation, stock market regulations, bonds, and the government budget balance. The report gives a good snap shot of the Vietnamese economy as of year 2004.", "title": "Taking stock : an update on recent economic developments and reforms by the World Bank in Vietnam", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Shozo  Sakata"], "summary": "Since the beginning of the new century, Vietnam has enjoyed rapid economic growth. Its economic performance, which had declined in the late 1990s because of the effects of the Asian economic crisis, began to recover in 2000, with Vietnam continuing to enjoy a high growth rate until 2008. The average GDP growth rate between 2001 and 2005 was 7.5%. In three consecutive years \u2014 from 2005 to 2008 \u2014 the growth rate exceeded 8%. Between 1997 and 2007, the value of GDP increased 3.6 times in nominal terms and twofold in real terms (GSO various years).", "title": "Introduction: The Changing Status of Economic Entities in Vietnam", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Nguyen", "Van", "Luan"], "summary": "Viet Nam's impressive progress toward the achievement of socio-economic development goals up to 2010 and 2020 continues as confirmed by a new set of data surveys. Although the development rate has been rather high over the last 10 years, Viet Nam' GDP scale is below 40 billions USD, still less than the turnover of many multinational companies. Viet Nam's GDP per person is at some 482 USD and GDP per day at over 1.3 USD. That rate is higher than the poverty border as earlier measured but only at 65% the new standard of poverty border. At some 2,200 USD, purchasing power is below the 13th in the world and at 60% the average purchasing power in the region. Therefore, while the economic size is still small, sustainability and high developing speed is the right \u6771\u4eac\u60c5\u5831\u5927\u5b66\u7814\u7a76\u8ad6\u96c6 Vol.8 No.2, pp.11-16(2005) 11", "title": "Economic Development and Challenges for Asian Traditional Economies (Vietnam's Case)", "claims": null}], "query": "The Gross Domestic Product (GDP) in Vietnam was worth 408.80 billion US dollars in 2022, according to official data from the World Bank. The GDP value of Vietnam represents 0.18 percent of the world economy.", "summary_abstract": "The collection of abstracts provides a comprehensive overview of Vietnam's economic growth and development over the years, highlighting its journey towards achieving a significant GDP value of 408.80 billion US dollars in 2022. Vietnam's economic transformation began with the Doi Moi policy in 1986, which aimed to shift from a centralized, agriculture-dependent economy to a socialist-oriented market economy with a balanced focus on manufacturing and services (2013). This shift has been marked by consistent GDP growth, with notable achievements such as a 2.9% growth rate in 2020 and a nominal GDP of 300 billion dollars (N. Anh, 2021).\n\nThe country's economic growth has been supported by robust domestic investment, strong non-oil exports, and significant foreign direct investment, particularly following its accession to the World Trade Organization in 2007 (V. Suri & V. Dinh, 2006). Vietnam's trade surplus and increasing foreign reserves have also contributed to its economic stability (V. Suri & V. Dinh, 2006). Despite challenges such as inflation and external economic constraints, Vietnam has maintained a high growth rate, with GDP growth exceeding 8% in several years between 2001 and 2008 (Shozo Sakata, 2013).\n\nVietnam's strategic partnerships, particularly with Japan, have played a crucial role in its economic success. Japan has been a major investor and trading partner, contributing to Vietnam's trade surplus and economic development (N. Anh, 2021). Overall, Vietnam's economic journey reflects a successful transition towards a more diversified and resilient economy, achieving a significant GDP value that represents 0.18 percent of the world economy.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2018}, "authors": ["Batya Friedman", "David G. Hendry", "A. Borning"], "summary": "Value sensitive design is a theoretically grounded approach to the designof technology that accounts for human values in a principled andsystematic manner throughout the design process. In this article weprovide a survey of 14 value sensitive design methods: 1 direct andindirect stakeholder analysis; 2 value source analysis; 3 co-evolutionof technology and social structure; 4 value scenario; 5 value sketch;6 value-oriented semi- structured interview; 7 scalable informationdimensions; 8 value-oriented coding manual; 9 value-oriented mockup,prototype, or field deployment; 10 ethnographically informed inquiryregarding values and technology; 11 model of informed consentonline; 12 value dams and flows; 13 value sensitive action-reflectionmodel; and 14 Envisioning Cards TM. Each of these methods is honedto the investigation of values in technology, serving such purposesas stakeholder identification and legitimation, value representation andelicitation, and values analysis. While presented individually, the methodsare intended to be integrated in a robust value sensitive designprocess. The survey article begins with a brief summary of value sensitivedesign methodology and theoretical constructs. We next providean overview of the 14 methods. Then, we turn to a broader discussion ofvalue sensitive design practice, focussing on some methodological strategiesand heuristics to support skillful value sensitive design practice.Following the broad discussion of practice, we illustrate one method inaction-value scenarios-providing details on its range of purposes andcontexts. We conclude with reflections on core characteristics of valuesensitive design methodology, and heuristics for innovation.", "title": "A Survey of Value Sensitive Design Methods", "claims": null}, {"metadata": {"year": 2002}, "authors": ["Batya Friedman", "P. Kahn", "A. Borning"], "summary": "Value Sensitive Design is a theoretically grounded approach to the design of technology that accounts for human values in a principled and comprehensive manner throughout the design process. It employs an integrative and iterative tripartite methodology, consisting of conceptual, empirical, and technical investigations. We explicate Value Sensitive Design by drawing on three case studies. The first study concerns information and control of web browser cookies, implicating the value of informed consent. The second study concerns using high-definition plasma displays in an office environment to provide a virtual window to the outside world, implicating the values of physical and psychological well-being and privacy in public spaces. The third study concerns an integrated land use, transportation, and environmental simulation system to support public deliberation and debate on major land use and transportation decisions, implicating the values of fairness (and specifically freedom from bias), accountability, and support for the democratic process, as well as a highly diverse range of values that might be held by different stakeholders, such as environmental sustainability, opportunities for business expansion, or walkable neighborhoods. We conclude with direct and practical suggestions for how to engage in Value Sensitive Design.", "title": "Value Sensitive Design: Theory and Methods", "claims": null}, {"metadata": {"year": 2017}, "authors": ["FriedmanBatya", "G  HendryDavid", "BorningAlan"], "summary": "Value sensitive design is a theoretically grounded approach to the designof technology that accounts for human values in a principled andsystematic manner throughout the design process. In this art...", "title": "A Survey of Value Sensitive Design Methods", "claims": null}, {"metadata": {"year": 2002}, "authors": ["Batya Friedman", "P. Kahn", "A. Borning"], "summary": "Value Sensitive Design is a theoretically grounded approach to the design of technology that accounts for human values in a principled and comprehensive manner throughout the design process. It employs an integrative and iterative tripartite methodology, consisting of conceptual, empirical, and technical investigations. We explicate Value Sensitive Design by drawing on three research and design projects. One project involves cookies and informed consent in web browsers; the second involves projection technology in an office environment; the third involves user interactions and interface for an integrated land use, transportation, and environmental simulation.", "title": "Report 02-1201 1 Value Sensitive Design : Theory and Methods", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Batya  Friedman", "Peter H. Kahn", "Alan  Borning", "Alina  Huldtgren"], "summary": "Value Sensitive Design is a theoretically grounded approach to the design of technology that accounts for human values in a principled and comprehensive manner throughout the design process. It employs an integrative and iterative tripartite methodology, consisting of conceptual, empirical, and technical investigations. We explicate Value Sensitive Design by drawing on three case studies. The first study concerns information and control of web browser cookies, implicating the value of informed consent. The second study concerns using high-definition plasma displays in an office environment to provide a \u201cwindow\u201d to the outside world, implicating the values of physical and psychological well-being and privacy in public spaces. The third study concerns an integrated land use, transportation, and environmental simulation system to support public deliberation and debate on major land use and transportation decisions, implicating the values of fairness, accountability, and support for the democratic process, as well as a highly diverse range of values that might be held by different stakeholders, such as environmental sustainability, opportunities for business expansion, or walkable neighborhoods. We conclude with direct and practical suggestions for how to engage in Value Sensitive Design.", "title": "Value Sensitive Design and Information Systems", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Jianan Guo", "Fan-Shing Chen"], "summary": "The applied technology has made great development in today\u2019s society, which were included information technology, nanotechnology, genetic technology, and so on. However, no matter what kind of technology there are still problems that design value demand of technology contradict with human beings, the safety design of technology should not only posses the technical characteristics of the ideal, but give full consideration to the value of the patients demands. Value sensitive design is a kind of theory of technology design method, it pays attention to the technology user\u2019s values of society and ethics psychological demands, it can provide guidance and advice for method of security design of applied technology as represented by high technical, which has has an important significance for the design and application of cutting-edge technology of our nation.", "title": "The Aanlysis on Value Sensitive Design of Applied Technology", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Batya Friedman", "David G. Hendry", "A. Huldtgren", "C. Jonker", "J. Hoven", "A. V. Wynsberghe"], "summary": "In the 2010\u2019s it is widely recognized by computer and information scientists, social scientists, designers, and philosophers of technology that the design of information systems is not value neutral [5-8,11]. Rather, such systems are value laden in part because societal values are major factors in shaping systems, and at the same time the design of the technology reinforces, restructures or uproots societal value structures. Of the many theories and methods to design for this phenomenon one continues to gain traction for its systematic and overarching consideration of values in the design process: Value Sensitive Design (VSD) [5-7]. The aim of this multidisciplinary workshop is to bring together scholars and practitioners interested in ways values can be made to bear upon design and to help continue to build a community by sharing experiences, insights, and criticism.", "title": "Charting the Next Decade for Value Sensitive Design", "claims": null}, {"metadata": {"year": 2021}, "authors": ["David G. Hendry", "Batya  Friedman", "Stephanie  Ballard"], "summary": "In this article, we first offer a model of design knowledge types (know-about, know-that, know-how) and their interrelationships in value sensitive design. Then we demonstrate that value sensitive design is a formative framework, which provides a shaping influence on practice, enables creative appropriation, and supports theory and method development.", "title": "Value sensitive design as a formative framework", "claims": null}], "query": "what are Value sensitive design methods?", "summary_abstract": "Value Sensitive Design (VSD) is a comprehensive approach to technology design that systematically incorporates human values throughout the design process. This methodology is grounded in a tripartite framework consisting of conceptual, empirical, and technical investigations (Friedman et al., 2002; Friedman et al., 2013). The approach is not value-neutral; rather, it acknowledges that societal values significantly influence and are influenced by technological systems (Friedman et al., 2015).\n\nFriedman, Hendry, and Borning (2018) provide a detailed survey of 14 specific methods within VSD, such as stakeholder analysis, value scenarios, and ethnographically informed inquiry, which are designed to identify, represent, and analyze values in technology. These methods are intended to be integrated into a cohesive design process that addresses stakeholder identification, value elicitation, and analysis.\n\nThe practical application of VSD is illustrated through various case studies, such as managing web browser cookies to ensure informed consent, using projection technology to enhance well-being and privacy, and developing simulation systems to support democratic processes and diverse stakeholder values (Friedman et al., 2002; Friedman et al., 2013). These examples highlight VSD's ability to address a wide range of values, including fairness, accountability, and environmental sustainability.\n\nOverall, VSD provides a robust framework for integrating ethical and societal considerations into technology design, offering guidance for addressing the complex interplay between technology and human values (Guo & Chen, 2014; Hendry et al., 2021).", "summary_extract": null}]