[{"papers": [{"metadata": {"year": 1976}, "authors": ["G. Panayi"], "summary": "Auto-immune disease may result from the interaction of the genetic load of the individual, modification of self-tissue antigens by environmental agents such as virus or drugs and abnormalities of the immunological system itself such as the loss of controlling or suppressor T cells with age. In the majority of people the outcome is tolerance, maintenance of normal tissue architecture and function. In the unfortunate few the outcome is auto-immune disease, that is, failure to recognize \"self\".", "title": "Auto-immune disease.", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Angela  Richard-Eaglin", "Benjamin A Smallheer"], "summary": "Autoimmune disorders are a category of diseases in which the immune system attacks healthy cells as a result of a dysfunction of the acquired immune system. Clinical presentation and diagnosis are disease specific and often correspond with the degree of inflammation, as well as the systems involved. Treatment varies based on the specific disease, its stage of presentation, and patient symptoms. The primary goal of treatment is to decrease inflammation, minimize symptoms, and lessen the potential for relapse. Graves disease, Hashimoto thyroiditis, rheumatoid arthritis, Crohn disease, ulcerative colitis, systemic lupus erythematosus, and multiple sclerosis are discussed in this article.", "title": "Immunosuppressive/Autoimmune Disorders.", "claims": null}, {"metadata": {"year": 2016}, "authors": ["A. M\u00e9kinian", "O. Fain"], "summary": "\n        \n      ", "title": "Autoimmune diseases. Epidemiological, diagnostic and treatment principles", "claims": null}, {"metadata": {"year": 2001}, "authors": ["Sue R. Beers"], "summary": "An autoimmune disorder is the body\u2019s reaction to an unknown stimulus causing the overproduction of antibodies that are directed against body tissue (Wallace, 1995). Although most autoimmune diseases are associated with significant and chronic morbidity and disability, epidemiological studies are generally limited to a few of the more prevalent disorders such as rheumatoid arthritis or diabetes mellitus. Recent estimates indicate that 1 in 31 individuals in the United States currently are diagnosed with an autoimmune disease, with women at higher risk than men (Jacobson, Gange, Rose, & Graham, 1997).", "title": "Systemic Autoimmune Disease", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Rukhshanda Nosheen", "Dr. Muhammad Bilal", "Dr Madiha Afzal"], "summary": ": Immune system disorders cause abnormally low activity or over activity of the immune system. In cases of immune system over activity, the body attacks and damages its own tissues (autoimmune diseases). Immune deficiency diseases decrease the body's ability to fight invaders, causing vulnerability to infections. Some of the autoimmune disease are as follows rheumatoid arthritis, systemic lupus erythematosus (lupus), inflammatory bowel disease (IBD),multiple sclerosis (MS),Type 1 diabetes mellitus, Guillain-Barre syndrome, chronic inflammatory demyelinating polyneuropathy, psoriasis. Human", "title": "STUDY OF CURRENT AND EMERGING HUMAN AUTOIMMUNE DISEASES", "claims": null}, {"metadata": {"year": 2008}, "authors": ["J. B. Soep", "J. R. Hollister"], "summary": "Autoimmune diseases occur when the immune system attacks the body\u2019s own organs and tissues. Rheumatologic diseases encompass conditions in which this autoimmune response causes inflammation of the musculoskeletal system, skin, and blood vessels. Rheumatologic conditions often present with arthritis, fever, and rash. Due to the frequent joint and muscle symptoms, patients may experience gross motor and fine motor delays.", "title": "Illnesses: Autoimmune Rheumatological Diseases", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Shradha  Agarwal", "Charlotte  Cunningham-Rundles"], "summary": "Common variable immunodeficiency (CVID) is the most common clinically significant primary immune defect. Although the hallmark of CVID is hypogammaglobulinemia, the intrinsic dysregulation of the immune system leads to defective T-cell activation and proliferation, as well as dendritic cell and cytokine defects. Although 70% to 80% of patients have had recurrent sinopulmonary infections, autoimmunity and inflammatory complications are also common. The most common autoimmune conditions are immune thrombocytopenic purpura and hemolytic anemia, but other autoimmune complications arise, including rheumatoid arthritis, pernicious anemia, primary biliary cirrhosis, thyroiditis, sicca syndrome, systemic lupus, and inflammatory bowel disease. Treatment of autoimmunity includes highdose immunoglobulins, corticosteroids, selected immu no suppressants, and other immune modulators. This review focuses on autoimmune conditions associated with CVID, potential mechanisms of immune dysregulation, and therapeutic strategies.", "title": "Autoimmunity in common variable immunodeficiency", "claims": null}, {"metadata": {"year": 2012}, "authors": ["E. Choi"], "summary": "The immune system is the body\u2019s main line of defense against invasion by infectious organisms, such as bacteria, virus, and fungi. In normal immune systems, an immune response does not occur against the self-antigen, and is called self-tolerance. Autoimmune diseases occur when body tissues are attacked by the body\u2019s own immune system due to loss of tolerance to self-antigens (Dejaco et al., 2006). Under these conditions, body tissues are destroyed by antigen-specific cytotoxic T cells or auto-antibodies, and the accompanying inflammation can cause functional disability and morbidity. Autoimmune diseases are a heterogenous group of diseases with a wide spectrum of symptoms that affect approximately 6% of the population (Siatskas et al., 2006). They can be broadly classified as organ-specific or systemic depending on the location of the target antigen and clinical features (Sakaguchi, 2000). Common examples of systemic autoimmune diseases include systemic lupus erythematosus (SLE), rheumatoid arthritis, systemic sclerosis, ankylosing spondylitis, and polymyositis; examples of organ-specific autoimmune diseases include type 1 diabetes, Addison\u2019s disease, Hashimoto thyroiditis, Graves\u2019 disease, Sjogren's syndrome, vitiligo, pernicious anemia, glomerulonephritis, myasthenia gravis, Goodpasture\u2019s syndrome, autoimmune hemolytic anemia, idiopathic thrombocytopenia purpura, and pulmonary fibrosis. The clinical features of autoimmune diseases are very different, but immunemediated mechanisms are associated with the generation of an adaptive immune response toward the target antigen (Kuby, 1994; Siatskas et al., 2006).", "title": "New Therapeutic Challenges in Autoimmune Diseases", "claims": null}], "query": "Autoimmune disease", "summary_abstract": "The research on autoimmune diseases reveals a complex interplay of genetic, environmental, and immunological factors that contribute to the development of these conditions. Panayi (1976) highlights that autoimmune diseases may arise from a combination of an individual's genetic predisposition, environmental modifications to self-tissue antigens, and immunological abnormalities, particularly the loss of regulatory T cells. This suggests that while many individuals maintain tolerance to their own tissues, a subset fails to do so, leading to autoimmune disorders.\n\nRichard-Eaglin and Smallheer (2018) further elaborate on the nature of autoimmune disorders, describing them as conditions where the immune system mistakenly attacks healthy cells. They note that the clinical presentation varies significantly depending on the specific disease and the degree of inflammation involved. Common autoimmune diseases mentioned include Graves' disease, Hashimoto thyroiditis, rheumatoid arthritis, and systemic lupus erythematosus, among others.\n\nBeers (2001) emphasizes the prevalence of autoimmune diseases, indicating that approximately 1 in 31 individuals in the U.S. are affected, with a higher incidence in women. This statistic underscores the significant public health impact of these disorders.\n\nNosheen et al. (2021) and Soep and Hollister (2008) both describe the mechanisms by which autoimmune diseases manifest, focusing on the immune system's overactivity leading to tissue damage. They identify a range of autoimmune conditions, including rheumatoid arthritis and multiple sclerosis, which are characterized by the immune system's failure to distinguish between self and non-self.\n\nChoi (2012) categorizes autoimmune diseases into organ-specific and systemic types, providing examples of each and noting that these diseases affect approximately 6% of the population. This classification helps in understanding the diverse clinical features and underlying immune-mediated mechanisms that characterize autoimmune disorders.\n\nIn summary, autoimmune diseases are a heterogeneous group of disorders resulting from a malfunctioning immune system that attacks the body's own tissues. The interplay of genetic predisposition, environmental factors, and immune dysregulation plays a crucial role in their development and manifestation (Panayi, 1976; Richard-Eaglin & Smallheer, 2018; Beers, 2001; Nosheen et al., 2021; Soep & Hollister, 2008; Choi, 2012).", "summary_extract": null}, {"papers": [{"metadata": {"year": 1990}, "authors": ["J. Hall"], "summary": "Genomic imprinting is the modification of gene expression depending on whether the genetic material is inherited from the mother or the father. The article reviews the concept and its implications in various human disorders such as the chromosomal deletion syndromes, human cancers, endocrine disorders, and various other diseases which show parent-of-origin effects.", "title": "Genomic imprinting.", "claims": null}, {"metadata": {"year": 1990}, "authors": ["J. Casades\u00fas", "R. Maldonado"], "summary": "Genomic imprinting is an epigenetic mark introduced on a DNA molecule without alteration of the base sequence. Upon replication, the primary mark is propagated to the daughter DNA molecules. Epigenetic DNA modification often serves as a regulatory signal and may play a crucial role in many developmental processes. Although this mode of gene regulation was first discovered in multicellular eukaryotes, cases of imprinting have been recently found in lower eukaryotes, bacteria and phage. Thus it may be reasonable to list DNA modification among the major mechanisms that regulate gene expression.", "title": "Genomic imprinting in microorganisms.", "claims": null}, {"metadata": {"year": 2007}, "authors": ["\u0410. \u0410. \u041f\u0435\u043d\u0434\u0438\u043d\u0430", "\u041e. \u0410. \u0415\u0444\u0438\u043c\u043e\u0432\u0430", "\u0422. \u0412. \u041a\u0443\u0437\u043d\u0435\u0446\u043e\u0432\u0430", "\u0412\u043b\u0430\u0434\u0438\u0441\u043b\u0430\u0432 \u0421\u0435\u0440\u0433\u0435\u0435\u0432\u0438\u0447 \u0411\u0430\u0440\u0430\u043d\u043e\u0432"], "summary": "Genomic imprinting is an epigenetic mechanism of homologous gene expression regulation, depending on their parent-of-origin. The present review is focused on pathologies, heritable diseases and syndromes, caused by disruption of imprinting resulted from hemizygosity of imprinted genes, uniparental disomies and epigenetic disorders.", "title": "\u0411\u043e\u043b\u0435\u0437\u043d\u0438 \u0433\u0435\u043d\u043e\u043c\u043d\u043e\u0433\u043e \u0438\u043c\u043f\u0440\u0438\u043d\u0442\u0438\u043d\u0433\u0430", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Lara K Abramowitz", "Marisa S Bartolomei"], "summary": "Genomic imprinting is an epigenetic process resulting in the monoallelic parent-of-origin-specific expression of a subset of genes in the mammalian genome. The parental alleles are differentially marked by DNA methylation during gametogenesis when the genomes are in separate compartments. How methylation machinery recognizes and differentially modifies these imprinted regions in germ cells remains a key question in the field. While studies have focused on determining a sequence signature that alone could distinguish imprinted regions from the rest of the genome, recent reports do not support such a hypothesis. Rather, it is becoming clear that features such as transcription, histone modifications and higher order chromatin are employed either individually or in combination to set up parental imprints.", "title": "Genomic imprinting: recognition and marking of imprinted loci.", "claims": null}, {"metadata": {"year": 1997}, "authors": ["N. Niikawa"], "summary": "Genomic imprinting is a new concept proposed to explain unusual observations in early mammalian development, the occurrence of certain genetic diseases, genetic anticipation or incomplete penetrance, and tumorigenesis. The basic mechanism of the imprinting has remained obscure, although DNA-methylation, chromatin structure, and/or DNA replication may have a role. Genomic imprinting is a biological phenomenon determined by an evolutionally acquired, underlying system that may control harmonious development and growth in mammals. It is also relevant to the occurrence of some genetic disorders in man.", "title": "Genomic imprinting relevant to genetic diseases.", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Parul Gupta", "D. Chakraborty", "R. Taggar", "Dhirendra Kumar", "R. Sharma", "V. P. Singh"], "summary": "Genomic imprinting refers to an epigenetic mark that distinguishes parental alleles and results in a monoallelic, parental-specific expression pattern in mammals. The alleles of imprinted genes are marked epigenetically as discrete elements termed imprinting control regions with their parental origin in gametes through the use of DNA methylation, at the very least. Imprinted genes are normally involved in foetal growth and behavioural development. Consequently, aberrant imprinting disturbs development and is the cause of numerous well-known imprinting disorders, including Beckwith-Wiedemann syndrome, Prader-Willi syndrome, Cancer and Angelman syndrome.", "title": "Genomic imprinting in mammals - a review.", "claims": null}, {"metadata": {"year": 2011}, "authors": ["J. Killian"], "summary": "Genomic imprinting is the biological process whereby a gene or genomic domain exists in a state of epigenetic differentiation that depends upon its parent of origin. Importantly, the establishment and propagation of these parent-specific genomic conformations does not alter the primary DNA sequence comprised of A, C, G, and T nucleotides. Genomic imprints may be covalent (DNA methylation) or non-covalent (DNA-protein and DNA-RNA interactions, genomic localization in nuclear space), and the process of imprinting encompasses the specialized nuclear enzymatic machinery that maintains parental epigenetic markings throughout the cell cycle. Because of genomic imprinting, the parent of origin of homologous genetic alleles in diploid individuals can be determined in the absence of DNA sequence polymorphisms and without recourse to parental DNA samples. As illustrated in Figure 1, alleles of imprinted genes look and behave differently, as determined by parent of origin.", "title": "Genomic Imprinting: Parental differentiation of the genome", "claims": null}, {"metadata": {"year": 2021}, "authors": ["E. Sazhenova", "I. Lebedev"], "summary": "Genomic imprinting is an epigenetic phenomenon that differentiates maternal and paternal copies of genes in the genome and causes monoallelic expression depending on parental origin. Imprinting is an evolutionary puzzle, as it bears the costs of diploidization without its advantages, namely, protection from recessive mutations. The aim of this review is to answer the question of why genomic imprinting arose and became fixed in the evolution of angiosperms, insects, marsupials, and placental mammals.", "title": "[Evolutionary Aspects of Genomic Imprinting].", "claims": null}], "query": "genomic imprinting", "summary_abstract": "Genomic imprinting is a fascinating epigenetic phenomenon characterized by the differential expression of genes based on their parental origin. This process results in monoallelic expression, meaning that only one allele of a gene is expressed while the other is silenced, depending on whether it is inherited from the mother or the father (Hall, 1990; Gupta et al., 2014). The mechanism of genomic imprinting involves specific epigenetic marks, primarily DNA methylation, which are established during gametogenesis and are crucial for regulating gene expression (Abramowitz & Bartolomei, 2012; Killian, 2011).\n\nThe implications of genomic imprinting are significant, particularly in relation to various human disorders. Disruptions in imprinting can lead to a range of pathologies, including well-known syndromes such as Prader-Willi syndrome, Angelman syndrome, and Beckwith-Wiedemann syndrome, as well as contributing to certain cancers and endocrine disorders (P\u0435\u043d\u0434\u0438\u0438\u043d\u0430 et al., 2007; Gupta et al., 2014). The understanding of how these epigenetic modifications occur and their consequences on development and disease is a key area of research, as aberrant imprinting can disturb normal growth and behavior (Niikawa, 1997; Sazhenova & Lebedev, 2021).\n\nMoreover, the evolutionary significance of genomic imprinting remains a topic of interest, as it presents a unique case of how certain genetic mechanisms can persist despite their apparent costs (Sazhenova & Lebedev, 2021). Overall, genomic imprinting is a complex and critical aspect of genetics that influences both normal development and the etiology of various diseases.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2004}, "authors": ["A. Christe", "P. Vock"], "summary": "The most common causes of pulmonary edema are cardiac failure, renal failure and injury edema (diffuse alveolar damage). The injury edema typically shows airspace consolidation due to exsudation of fluid in the periphery of the lung with air bronchograms, no interstitial fluid accumulation can be found and only rarely pleural effusions are present. Cardiac and renal edemas often show a mixed interstitial and alveolar transudation without air bronchograms. Pleural effusions are often present. Both usually have an increased heart-size and an increased vascular pedicle width. To distinguish them better one has to look at the distribution of the pulmonary edema: The cardiac edema typically shows a gravitational and the renal edema a central distribution.", "title": "[Radiologic criteria to differentiate pulmonary edema].", "claims": null}, {"metadata": {"year": 1980}, "authors": ["Hans  Ewerbeck"], "summary": "In patients with nonspecific bronchopneumonias due to viral diseases such as influenza and measles or to mycoplasmal and rickettsial diseases, both hila are enlarged and frequently have indistinct borders. The perihilar markings are increased and consist of streaky densities extending outward, with peribronchial and interstitial infiltrates. Unilateral hilar reactions are seen with unilateral pulmonary lesions (pneumonia, abscess).", "title": "Significant Radiologic Findings in Pulmonary Diagnosis", "claims": null}, {"metadata": {"year": 1962}, "authors": ["J. P. Lavender", "J. Doppman"], "summary": "The anatomy of the normal hilar shadow as shown on tomograms of 48 normal patients has been reviewed and the importance of the superior pulmonary vein emphasised. The changes occurring in the hilar appearance have been described in 33 patients with mitral valve disease and 45 patients with left ventricular failure. The pulmonary artery was shown to be enlarged in those cases of mitral stenosis having pulmonary arterial hypertension and a similar enlargement in over 50 per cent of the cases of left ventricular failure suggests the existence of pulmonary arterial hypertension in these patients. In the majority of both mitral and left ventricular failure cases, the lateral border of the hilum was straightened or convex, a change shown to be due to enlarged upper lobe veins. This sign, together with a clearly defined, oblique lower border of the superior pulmonary vein, are of value in analysing the hilum in states of pulmonary venous hypertension.", "title": "The hilum in pulmonary venous hypertension.", "claims": null}, {"metadata": {"year": 1987}, "authors": ["J. F. Nunn"], "summary": "Publisher Summary \nThe pulmonary edema is defined as an increase in pulmonary extravascular water, which occurs when transudation or exudation exceeds the capacity of the lymphatic drainage. The pulmonary capillary endothelial cells abut against one another at fairly loose junctions that are of the order of 5 nm wide. These junctions permit the passage of quite large molecules and the pulmonary lymph contains albumin at about half the concentration in plasma. The epithelial cells meet at tight junctions with a gap of only about 1 nm. The tightness of these junctions is crucial for the prevention of the escape of large molecules, such as albumin, from the interstitial fluid into the alveoli. The lung has a well developed lymphatic system draining the interstitial tissue through a network of channels around the bronchi and pulmonary vessels towards the hilum. The lymphatic vessels cannot be identified at alveolar level but can be seen in association with bronchioles.", "title": "Chapter 23 \u2013 Pulmonary oedema", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Maria  Barile"], "summary": "Pulmonary edema is a common clinical entity caused by the extravascular movement of fluid into the pulmonary interstitium and alveoli. The four physiologic categories of edema include hydrostatic pressure edema, permeability edema with and without diffuse alveolar damage (DAD), and mixed edema where there is both an increase in hydrostatic pressure and membrane permeability. As radiographic manifestations and etiologies are varied, an appreciation for both the common and uncommon manifestations and causes of pulmonary edema is essential for accurate diagnosis.", "title": "Pulmonary Edema: A Pictorial Review of Imaging Manifestations and Current Understanding of Mechanisms of Disease", "claims": null}, {"metadata": {"year": 1989}, "authors": ["J. Gurney", "L. Goodman"], "summary": "Focal patterns of pulmonary edema are confusing and often mistaken for the more common causes of focal lung disease, pneumonia, infarction, or aspiration. The authors report four cases of right upper lobe edema secondary to mitral regurgitation. The pathogenesis believed to be responsible for this condition is the vector of blood flow from the left ventricle to left atrium, which may be targeted at the right superior pulmonary vein, locally accentuating the forces for edema formation in the right upper lobe. Pulmonary edema accompanying mitral regurgitation should be suspected whenever right upper lobe consolidation develops in a patient with known or suspected mitral valve disease. The presence of interstitial edema in the remainder of the lungs can help in the differentiation of this condition from pneumonia and other disorders.", "title": "Pulmonary edema localized in the right upper lobe accompanying mitral regurgitation.", "claims": null}, {"metadata": {"year": 1971}, "authors": [], "summary": "obstruction of the superior vena cava as its main clinical characteristic. When the fibrosis develops at the hilar level, typically in the age group 15 to 35, a mainly unilateral mass may cause only mild respiratory disability. If both hila are affected, increasing incapacity may lead to death. The diagnosis of hilar fibrosis is often difficult, especially when the mass is not obvious radiologically. The clinical findings vary according to whether the arteries or veins are predominantly affected.3 When the arteries bear the brunt, recurrent pulmonary and pleural infections are the rule; haemoptyses may result from widespread bronchopulmonary anastomoses. Radiographs show a shift of the mediastinum to the affected side, a small pulmonary artery, and a thickened pleura. Angiograms and studies of lung function confirm the diminished arterial blood flow. Pulmonary hypertension develops when both pulmonary arteries are involved. When the veins are mainly infiltrated, severe haemoptyses may arise from capillary congestion. Pulmonary hypertension may lead to right heart failure. Radiographs usually show a small lung with venous congestion and pleural thickening. Cardiac catheterization may show an exceptionally high pulmonary artery pressure. Additional diagnostic evidence of idiopathic hilar fibrosis may come from bronchography or bronchoscopy, when a long smooth bronchial stricture may be evident owing to compression of the associated large airways by fibrous tissue. The main differential diagnosis is from primary lung cancer, and it may be resolved only at thoracotomy. Primary pulmonary venous sclerosis is not associated with a hilar mass, nor are the larger bronchi affected. Medical treatment has nothing effective to offer. Surgical correction of obstruction is rarely feasible and attempts to dilate bronchial strictures may lead to dangerous haemorrhage. Perhaps these unfortunate patients, few as they are, will be afforded relief when more is known of the cause of this obscure malady.", "title": "Bites and blains.", "claims": null}, {"metadata": {"year": 1971}, "authors": ["R. Singer", "C. Ogilvy", "G. Rordorf", "J. Biller", "J. Wilterdink"], "summary": "obstruction of the superior vena cava as its main clinical characteristic. When the fibrosis develops at the hilar level, typically in the age group 15 to 35, a mainly unilateral mass may cause only mild respiratory disability. If both hila are affected, increasing incapacity may lead to death. The diagnosis of hilar fibrosis is often difficult, especially when the mass is not obvious radiologically. The clinical findings vary according to whether the arteries or veins are predominantly affected.3 When the arteries bear the brunt, recurrent pulmonary and pleural infections are the rule; haemoptyses may result from widespread bronchopulmonary anastomoses. Radiographs show a shift of the mediastinum to the affected side, a small pulmonary artery, and a thickened pleura. Angiograms and studies of lung function confirm the diminished arterial blood flow. Pulmonary hypertension develops when both pulmonary arteries are involved. When the veins are mainly infiltrated, severe haemoptyses may arise from capillary congestion. Pulmonary hypertension may lead to right heart failure. Radiographs usually show a small lung with venous congestion and pleural thickening. Cardiac catheterization may show an exceptionally high pulmonary artery pressure. Additional diagnostic evidence of idiopathic hilar fibrosis may come from bronchography or bronchoscopy, when a long smooth bronchial stricture may be evident owing to compression of the associated large airways by fibrous tissue. The main differential diagnosis is from primary lung cancer, and it may be resolved only at thoracotomy. Primary pulmonary venous sclerosis is not associated with a hilar mass, nor are the larger bronchi affected. Medical treatment has nothing effective to offer. Surgical correction of obstruction is rarely feasible and attempts to dilate bronchial strictures may lead to dangerous haemorrhage. Perhaps these unfortunate patients, few as they are, will be afforded relief when more is known of the cause of this obscure malady.", "title": "Treatment of aneurysmal subarachnoid haemorrhage.", "claims": null}], "query": "why does pulmonary edema tend to occur in hilar area rather than peripheral?", "summary_abstract": "Pulmonary edema tends to occur more frequently in the hilar area rather than the peripheral regions of the lungs due to several physiological and anatomical factors. The distribution of pulmonary edema is influenced by the underlying causes, such as cardiac and renal failures, which typically lead to a central distribution of edema. For instance, cardiac edema is characterized by a gravitational distribution, often affecting the hilar region more prominently (Christe & Vock, 2004). This centralization is attributed to the increased hydrostatic pressure in the pulmonary circulation, which is more pronounced in the hilar area where the pulmonary vessels converge.\n\nAdditionally, the anatomy of the pulmonary vasculature plays a significant role. The hilar region contains a dense network of blood vessels, including the pulmonary arteries and veins, which can become engorged in conditions like mitral valve disease and left ventricular failure, leading to pulmonary venous hypertension (Lavender & Doppman, 1962). This engorgement can result in increased fluid transudation into the interstitial spaces and alveoli, particularly in the central areas of the lungs.\n\nMoreover, the lymphatic drainage system, which is crucial for managing extravascular fluid, is also more developed around the hilum. The lymphatic vessels drain interstitial fluid from the lung tissue towards the hilum, making it a focal point for fluid accumulation when drainage capacity is overwhelmed (Nunn, 1987). \n\nIn summary, the tendency for pulmonary edema to manifest in the hilar area rather than the periphery can be attributed to the central distribution of edema in cardiac and renal failures, the anatomical configuration of the pulmonary vasculature, and the efficiency of the lymphatic drainage system in that region (Christe & Vock, 2004; Lavender & Doppman, 1962; Nunn, 1987).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2010}, "authors": ["Chunyan  He", "Abrar A.  Qureshi", "Jiali  Han"], "summary": "Ultraviolet (UV)-induced oxidative stress has been implicated in skin carcinogenesis [1]. Several antioxidant enzymes, such as glutathione peroxidase (GPX) and catalase (CAT), counteract oxidative damage and constitute a primary defense against oxidative stress. GPX is a soluble selenoprotein that reduces H2O2 and organic hydroperoxides to H2O, and GPX1 is the most abundant and ubiquitous intracellular isoform [1]. GPX1 activity is not strongly affected by UV and is considered to be the most important antioxidant enzyme defense mechanism in the skin [2]. CAT is a heme enzyme that neutralizes reactive oxygen species by converting H2O2 to H2O and O2. CAT activity in the skin is significantly reduced after exposure to UV [2], which suggests its effect may be prone to effect modification by environmental factors. \n \nInherited variants in the encoding genes that affect the activity or expression of these antioxidant enzymes are hypothesized to modulate oxidative stress and thus influence skin cancer risk. A polymorphism in the GPX1 gene (Pro198Leu, rs1050450) and a polymorphism in the promoter region of the CAT gene (C-262T, rs1001179) have been shown to be associated with lower enzyme activities of their encoded enzymes [3, 4]. To test our main hypothesis that these two genetic polymorphisms are associated with skin cancer risk, we conducted a nested case-control study of Caucasians (218 melanoma, 285 squamous cell carcinoma (SCC), and 300 basal cell carcinoma (BCC) cases, and 870 age-matched controls) within the Nurses\u2019 Health Study. We further investigated potential gene-environment interactions between these polymorphisms and lifestyle factors such as dietary antioxidant intake and sun exposure related risk factors. A detailed description of the characteristics of cases and controls was published previously [5]. Information on dietary intake was collected prospectively by food-frequency questionnaires, and total-energy-adjusted cumulative average of dietary intake was used to reduce within-person variation and represent long-term dietary intake [6]. \n \nWe genotyped the two single nuclear polymorphisms (SNPs) (rs1050450 and rs1001179) by the 5\u2032 nuclease assay (TaqMan\u00ae) in 384-well format, using the ABI PRISM 7900HT Sequence Detection System (Applied Biosystems, Foster City, CA). The distributions of genotypes for the two SNPs were in Hardy-Weinberg equilibrium among controls (p=0.94, 0.83, respectively). We compared the cases of each type of skin cancer to the common control series. We used unconditional multivariate logistic regression to model the association between genetic polymorphisms and skin cancer risk and to estimate multivariate Odds Ratios (ORs) and 95% Confidence Intervals (CIs). To test statistical significance of gene-environmental interactions, we used dominant model for genotypes and dichotomized environmental exposures as low versus high based on median values among controls. We tested the statistical significance of a single multiplicative interaction term. \n \nIn the main effect analysis (Table 1), we observed that the GPX1 198 Leu/Leu genotype was significantly associated with a two-fold increased risk of melanoma (OR, 2.14; 95% CI, 1.22\u20133.72), after adjustment for age and other covariates. No association was found between this polymorphism and SCC or BCC risk, which was consistent with one previous study [7]. This polymorphism has been shown to be associated with lung cancer [8] and breast cancer [3] previously. We did not observe significant association between the CAT C-262T polymorphism and the risk of any type of skin cancer. \n \n \n \nTable 1 \n \nAssociation between GPX Pro198Leu and CAT C-262T genetic polymorphisms and skin cancer risk a \n \n \n \nAs exploratory analyses, we further tested gene-environment interactions between the genetic variants and lifestyle factors that modulate oxidative stress. We found the association between the CAT C-262T polymorphism and melanoma risk was significantly modified by history of severe sunburns (p for interaction, 0.008, Table 2), a variable combining exposure intensity and biological response to sun exposure. The positive association between history of severe sunburns and melanoma risk was restricted to T carriers (OR, 1.73; 95% CI, 1.02\u20132.92), compared to women with CC genotype (OR, 1.03; 95% CI, 0.63\u20131.69). We also observed a significant gene-diet interaction between the CAT C-262T polymorphism and total carotenoid intake on melanoma risk (p for interaction, 0.01). The inverse association of total carotenoid intake with melanoma risk was limited among women with CC genotype (OR, 0.63; 95% CI, 0.41\u20130.97), whereas no association was observed among T carriers (OR, 1.23; 95% CI, 0.77\u20131.97). Inconsistent results were reported on the relationship between dietary carotenoid intake and melanoma risk in several previous case-control studies. An inverse association between the intake and the risk of melanoma was observed in some studies [9], but not in other studies [10]. Our results suggest that the inconsistency in the literature may reflect a potential gene-diet interaction. As we tested different genetic polymorphisms, multiple environmental exposures and dietary factors, and three types of skin cancer, multiple testing in our study may lead to false positive results. Replications in independent studies are needed to validate these results. No significant interactions were observed between the GPX1 polymorphism and these lifestyle factors on melanoma risk. We did not observe any significant interaction between these genetic variants and environmental exposures on the risk of SCC or BCC. \n \n \n \nTable 2 \n \nInteraction between the CAT C-262T genetic polymorphism and history of severe sunburns and total carotenoid intake on melanoma risk \n \n \n \nIn summary, we first observed the GPX1 198 Leu/Leu genotype was significantly associated with a two-fold increased risk of melanoma, and the association between the CAT C-262T polymorphism and melanoma risk was significantly modified by history of severe sunburns and total carotenoid intake. Further research is needed to confirm these possible associations and illustrate the underlying molecular mechanisms.", "title": "Polymorphisms in genes involved in oxidative stress and their interactions with lifestyle factors on skin cancer risk.", "claims": null}, {"metadata": {"year": 2011}, "authors": ["D. Robbins", "Yunfeng Zhao"], "summary": "Recent studies have shown that antioxidant enzyme expression and activity are drastically reduced in most human skin diseases, leading to propagation of oxidative stress and continuous disease progression. However, antioxidants, an endogenous defense system against reactive oxygen species (ROS), can be induced by exogenous sources, resulting in protective effects against associated oxidative injury. Many studies have shown that the induction of antioxidants is an effective strategy to combat various disease states. In one approach, a SOD mimetic was applied topically to mouse skin in the two-stage skin carcinogenesis model. This method effectively reduced oxidative injury and proliferation without interfering with apoptosis. In another approach, Protandim, a combination of 5 well-studied medicinal plants, was given via dietary administration and significantly decreased tumor incidence and multiplicity by 33% and 57%, respectively. These studies suggest that alterations in antioxidant response may be a novel approach to chemoprevention. This paper focuses on how regulation of antioxidant expression and activity can be modulated in skin disease and the potential clinical implications of antioxidant-based therapies.", "title": "The Role of Manganese Superoxide Dismutase in Skin Cancer", "claims": null}, {"metadata": {"year": 2017}, "authors": ["W. Chowdhury", "Shahida Arbee", "S. Debnath", "Shah Mehedi Bin Zahur", "Sharmim Akter", "A. Karim", "Md. Mohabbulla Mohib", "A. Tisha", "Md Abu Taher Sagor", "Sarif Mohiuddin"], "summary": "Evidence reported that cancers are spreading every nook and corner of the world at an alarming rate. Skin diseases like chronic skin inflammations, psoriasis and skin cancers have also been burning topic in today. Protections of a biological system are often hampered while skin gets damaged. Factors like UV, radiation, viruses, chronic diseases, genetic predispositions, food habits and environmental exposures might lead to skin cancers. In addition to these, urbanization and globalization may also contaminate the environment that may eventually modify several biological and genetic functions. USA, Europe and Australia are in the most dangerous zone to be exposed. We basically performed detailed search of PubMed, Google Scholar and Science Direct for literature search and collecting related information. On the other hand, experiments suggested that antioxidant components such as phenolic acid derivatives, flavonoids and flavonol found to be preventive against cancer cell proliferations. Moreover, antioxidants have been also evaluated as a protective agent against chronic inflammatory diseases as well. These molecules may participate as an additional therapy which could exert synergistic effects while applying with other chemotherapeutic agents. Our literature findings and hypothetical figure may establish a good correlation between skin cancer and antioxidant therapy. Therefore this study will be focused on skin cancer biology and some possible management strategies using antioxidant phyto-nutrients.", "title": "Potent Role of Antioxidant Molecules in Prevention and Management of SkinCancer", "claims": null}, {"metadata": {"year": 2014}, "authors": ["A. Godic", "B. Polj\u0161ak", "M. Adami\u010d", "R. Dahmane"], "summary": "Skin cells are constantly exposed to reactive oxygen species (ROS) and oxidative stress from exogenous and endogenous sources. UV radiation is the most important environmental factor in the development of skin cancer and skin aging. The primary products caused by UV exposure are generally direct DNA oxidation or generation of free radicals which form and decompose extremely quickly but can produce effects that can last for hours, days, or even years. UV-induced generation of ROS in the skin develops oxidative stress when their formation exceeds the antioxidant defense ability. The reduction of oxidative stress can be achieved on two levels: by lowering exposure to UVR and/or by increasing levels of antioxidant defense in order to scavenge ROS. The only endogenous protection of our skin is melanin and enzymatic antioxidants. Melanin, the pigment deposited by melanocytes, is the first line of defense against DNA damage at the surface of the skin, but it cannot totally prevent skin damage. A second category of defense is repair processes, which remove the damaged biomolecules before they can accumulate and before their presence results in altered cell metabolism. Additional UV protection includes avoidance of sun exposure, usage of sunscreens, protective clothes, and antioxidant supplements.", "title": "The Role of Antioxidants in Skin Cancer Prevention and Treatment", "claims": null}, {"metadata": {"year": 2003}, "authors": ["S. Briganti", "M. Picardo"], "summary": "Due to its interface function between the body and the environment, the skin is chronically exposed to both endogenous and environmental pro\u2010oxidant agents, leading to the harmful generation of reactive oxygen species (ROS). There is compelling evidence that oxidative stress is involved in the damage of cellular constituents, such as DNA, cell membrane lipids or proteins. To protect the skin against the over\u2010load of oxidant species, it contains a well\u2010organised system of both chemical and enzymatic antioxidant which are able to work in a synergistic manner. Skin antioxidant network protects cells against oxidative injury and prevent the production of oxidation products, such as 4\u2010hydroxy\u20102\u2010nonenal or malonaldehyde, which are able to induce protein damage, apoptosis or release of pro\u2010inflammatory mediators, such as cytokines. When oxidative stress overwhelms the skin antioxidant capacity the subsequent modification of cellular redox apparatus leads to an alteration of cell homeostasis and a generation of degenerative processes. Topical application or oral administration of antioxidants has been recently suggested as preventive therapy for skin photoaging and UV\u2010induced cancer. The recognition that ROS can act as second messengers in the induction of several biological responses, such as the activation of NF\u2010kB or AP\u20101, the generation of cytokines, the modulation of signalling pathways, etc., has led many researchers to focus on the possible effects of antioxidants in many pathological processes. The recent demonstration that the peroxisome proliferators\u2010activated receptors, whose natural ligands are polyunsaturated fatty acids and theirs oxidation products, have a central role in the induction of some skin diseases, such as psoriasis or acne, has indicated new links between free radicals and skin inflammation. Based on these findings, the review summarises the possible correlations between antioxidant imbalance, lipid oxidative breakage and skin diseases, from both a pathological and therapeutic points of view.", "title": "Antioxidant activity, lipid peroxidation and skin diseases. What's new", "claims": null}, {"metadata": {"year": 2023}, "authors": ["K. Sable", "B. Shields"], "summary": "Dietary supplements, including vitamins and their derivatives, have been utilized within the field of dermatology to treat a variety of skin conditions. Antioxidants inhibit oxidation and decrease cellular damage caused by free radicals, potentially preventing DNA damage due to UV radiation. Laboratory studies have demonstrated promising results supporting the possible role of antioxidants for prevention of skin cancer related to UV exposure. We review the effects of frequently encountered antioxidants and vitamins suggested for the chemoprevention of melanoma and nonmelanoma skin cancer (NMSC) in humans.", "title": "The Role of Dietary Antioxidants in Melanoma and Nonmelanoma Skin Cancer.", "claims": null}, {"metadata": {"year": 2017}, "authors": ["M. Furue", "H. Uchi", "C. Mitoma", "A. Hashimoto-Hachiya", "T. Chiba", "Takamichi Ito", "T. Nakahara", "G. Tsuji"], "summary": "Skin is the outermost part of the body and is, thus, inevitably exposed to UV rays and environmental pollutants. Oxidative stress by these hazardous factors accelerates skin aging and induces skin inflammation and carcinogenesis. Aryl hydrocarbon receptors (AHRs) are chemical sensors that are abundantly expressed in epidermal keratinocytes and mediate the production of reactive oxygen species. To neutralize or minimize oxidative stress, the keratinocytes also express nuclear factor-erythroid 2-related factor-2 (NRF2), which is a master switch for antioxidant signaling. Notably, there is fine-tuned crosstalk between AHR and NRF2, which mutually increase or decrease their activation states. Many NRF2-mediated antioxidant phytochemicals are capable of up- and downmodulating AHR signaling. The precise mechanisms by which these phytochemicals differentially affect the AHR and NRF2 system remain largely unknown and warrant future investigation.", "title": "Antioxidants for Healthy Skin: The Emerging Role of Aryl Hydrocarbon Receptors and Nuclear Factor-Erythroid 2-Related Factor-2", "claims": null}, {"metadata": {"year": 2004}, "authors": ["C. Sander", "Hong Chang", "F. Hamm", "P. Elsner", "J. Thiele"], "summary": "Melanoma and nonmelanoma skin cancers are among the most prevalent cancers in the human population. Solar ultraviolet radiation is considered a major etiological factor but the relationship between dose, timing, and nature of exposure to tumor development is still unclear. Free radicals are generated by normal physiologic processes, including aerobic metabolism and inflammatory response, but may inflict cellular damage when generation is increased and antioxidant defense mechanisms are overwhelmed. Important findings supporting the free radical hypothesis in skin carcinogenesis are: (1) Reactive oxygen species (ROS) are generated in UVA\u2010 and UVB\u2010irradiated skin in excessive doses, (2) the natural cutaneous antioxidant defense is impaired upon UV\u2010exposure, (3) free radicals are involved in all steps of carcinogenesis, (4) supplementation with antioxidants can inhibit skin carcinogenesis, and (5) conditions that increase ROS generation enhance photocarcinogenesis. These findings provide a promising rationale for the development of powerful new antioxidant strategies in the prevention and therapy of skin cancer.", "title": "Role of oxidative stress and the antioxidant network in cutaneous carcinogenesis", "claims": null}], "query": "Antioxidant enzymes and Skin Cancer risk", "summary_abstract": "The relationship between antioxidant enzymes and skin cancer risk has been explored in various studies, highlighting the critical role of oxidative stress in skin carcinogenesis and the potential for antioxidant interventions.\n\nHe et al. (2010) conducted a nested case-control study that identified a significant association between the GPX1 Pro198Leu genotype and an increased risk of melanoma, suggesting that genetic variations affecting antioxidant enzyme activity may influence susceptibility to skin cancer. They also found that the CAT C-262T polymorphism's association with melanoma risk was modified by factors such as history of severe sunburns and dietary carotenoid intake, indicating that both genetic and environmental factors play a role in skin cancer risk.\n\nRobbins and Zhao (2011) emphasized the importance of antioxidant enzymes in combating oxidative stress, noting that their expression is often reduced in skin diseases, which can lead to disease progression. They suggested that enhancing antioxidant defenses, either through topical applications or dietary supplements, could serve as a chemopreventive strategy against skin cancer.\n\nChowdhury et al. (2017) reviewed the protective effects of various antioxidant compounds, including phenolic acids and flavonoids, against skin cancer. They proposed that these antioxidants could be integrated into treatment regimens to enhance the efficacy of existing therapies.\n\nGodic et al. (2014) discussed the dual approach of reducing UV exposure and increasing antioxidant defenses to mitigate oxidative stress in the skin. They highlighted the role of melanin and enzymatic antioxidants as the skin's primary defense mechanisms against UV-induced damage.\n\nBriganti and Picardo (2003) provided evidence that oxidative stress damages cellular components, leading to skin diseases, including cancer. They advocated for the use of topical and oral antioxidants as preventive measures against UV-induced skin damage.\n\nSable and Shields (2023) reviewed the potential of dietary antioxidants in preventing skin cancer, particularly in relation to UV exposure. They noted that antioxidants could inhibit oxidative damage and reduce the risk of DNA damage, which is a precursor to skin cancer.\n\nFurue et al. (2017) explored the interplay between aryl hydrocarbon receptors and NRF2, a key regulator of antioxidant responses in skin cells. They suggested that phytochemicals that modulate these pathways could offer protective effects against oxidative stress and skin carcinogenesis.\n\nIn summary, the collective findings from these studies underscore the significance of antioxidant enzymes in mitigating skin cancer risk. Genetic polymorphisms affecting antioxidant enzyme activity, combined with environmental factors such as UV exposure and dietary intake, contribute to individual susceptibility to skin cancer. Furthermore, enhancing antioxidant defenses through dietary and topical interventions presents a promising avenue for skin cancer prevention and management.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2022}, "authors": ["Madeline E. McCarthy", "William B. Dodd", "Xiaoming Lu", "Nishi Patel", "Charlotte V. Haskell", "H. Sanabria", "M. Blenner", "M. Birtwistle"], "summary": "Systematic, genome-scale genetic screens have been instrumental for elucidating genotype-phenotype relationships, but approaches for probing genetic interactions have been limited to at most \u223c100 pre-selected gene combinations in mammalian cells. Here, we introduce a theory for high-throughput genetic interaction screens. The theory extends our recently developed Multiplexing using Spectral Imaging and Combinatorics (MuSIC) approach to propose \u223c105 spectrally unique, genetically-encoded MuSIC barcodes from 18 currently available fluorescent proteins. Simulation studies based on constraints imposed by spectral flow cytometry equipment suggest that genetic interaction screens at the human genome-scale may be possible if MuSIC barcodes can be paired to guide RNAs. While experimental testing of this theory awaits, it offers transformative potential for genetic perturbation technology and knowledge of genetic function. More broadly, the availability of a genome-scale spectral barcode library for non-destructive identification of single-cells could find more widespread applications such as traditional genetic screening and high-dimensional lineage tracing.", "title": "A Theory for High-Throughput Genetic Interaction Screening", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Dorothea  Emig", "Hagen  Blankenburg", "Fidel  Ram\u00edrez", "Mario  Albrecht"], "summary": "Complex biological systems comprise a large number of interacting molecules. The identification and detailed characterization of the functions of the involved genes and proteins are crucial for modeling and understanding such systems. To interrogate the various cellular processes, high-throughput techniques such as the Affymetrix Exon Array or RNA interference (RNAi) screens are powerful experimental approaches for functional genomics. However, they typically yield long gene lists that require computational methods to further analyze and functionally annotate the experimental results and to gain more insight into important molecular interactions. Here, we focus on bioinformatics software tools for the functional interpretation of exon expression data to discover alternative splicing events and their impact on gene and protein architecture, molecular networks, and pathways. We additionally demonstrate how to explore large lists of candidate genes as they also result from RNAi screens. In particular, our exemplary application studies show how to analyze the function of human genes that play a major role in human stem cells or viral infections.", "title": "Functional characterization of human genes from exon expression and RNA interference results.", "claims": null}, {"metadata": {"year": 2004}, "authors": ["Adam  Friedman", "Norbert  Perrimon"], "summary": "The availability of complete genome sequences from many organisms has yielded the ability to perform high-throughput, genome-wide screens of gene function. Within the past year, rapid advances have been made towards this goal in many major model systems, including yeast, worms, flies, and mammals. Yeast genome-wide screens have taken advantage of libraries of deletion strains, but RNA-interference has been used in other organisms to knockdown gene function. Examples of recent large-scale functional genetic screens include drug-target identification in yeast, regulators of fat accumulation in worms, growth and viability in flies, and proteasome-mediated degradation in mammalian cells. Within the next five years, such screens are likely to lead to annotation of function of most genes across multiple organisms. Integration of such data with other genomic approaches will extend our understanding of cellular networks.", "title": "Genome-wide high-throughput screens in functional genomics.", "claims": null}, {"metadata": {"year": 2004}, "authors": ["Anne E. Carpenter", "David M. Sabatini"], "summary": "By using genome information to create tools for perturbing gene function, it is now possible to undertake systematic genome-wide functional screens that examine the contribution of every gene to a biological process. The directed nature of these experiments contrasts with traditional methods, in which random mutations are induced and the resulting mutants are screened for various phenotypes. The first genome-wide functional screens in Caenorhabditis elegans and Drosophila melanogaster have recently been published, and screens in human cells will soon follow. These high-throughput techniques promise the rapid annotation of genomes with high-quality information about the biological function of each gene.", "title": "Systematic genome-wide screens of gene function", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Maximilian  Billmann", "Michael  Boutros"], "summary": "Genetic screens have identified many novel components of various biological processes, such as components required for cell cycle and cell division. While forward genetic screens typically generate unstructured \u2018hit\u2019 lists, genetic interaction mapping approaches can identify functional relations in a systematic fashion. Here, we discuss a recent study by our group demonstrating a two-step approach to first screen for regulators of the mitotic cell cycle, and subsequently guide hypothesis generation by using genetic interaction analysis. The screen used a high-content microscopy assay and automated image analysis to capture defects during mitotic progression and cytokinesis. Genetic interaction networks derived from process-specific features generate a snapshot of functional gene relations in those processes, which follow a temporal order during the cell cycle. This complements a recently published approach, which inferred directional genetic interactions reconstructing hierarchical relationships between genes across different phases during mitotic progression. In conclusion, this strategy leverages unbiased, genome-wide, yet highly sensitive and process-focused functional screening in cells.", "title": "Systematic epistatic mapping of cellular processes", "claims": null}, {"metadata": {"year": 2013}, "authors": ["J. Millstein"], "summary": "Next-generation sequencing and other high-throughput technologies have made it feasible to characterize millions of sequence variations on large numbers of study participants. But when it comes to identifying a small number of these genetic features (or feature sets) that are associated with a disease trait, the investigator is faced with a formidable multiple-testing challenge. It can be thought of as a signal-to-noise problem, where the large number of unrelated genetic features tends to drown out the faint signal of the small number of biologically relevant features.", "title": "Screening-testing approaches for gene-gene and gene-environment interactions using independent statistics", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Alina Guna", "K. R. Page", "Joseph R. Replogle", "Theodore K. Esantsi", "Maxine L. Wang", "J. Weissman", "R. M. Voorhees"], "summary": "The ability to map genetic interactions has been essential for determining gene function and defining biological pathways. Therefore, a system to readily perform genome-wide genetic modifier screens in human cells is a powerful platform for dissecting complex processes in mammalian cells, where redundancy and adaptation commonly mask the phenotype of a single genetic perturbation. Here, we report a CRISPR interference (CRISPRi) based platform, compatible with Fluorescence Activated Cell Sorting (FACS)-based reporter screens, that can be used to query epistatic relationships at scale. This is enabled by a flexible dual-sgRNA library design that allows for the simultaneous delivery and selection of a fixed sgRNA and a second randomized guide, comprised of a genome-wide library, with a single transduction. As a proof of principle, we apply our approach to study the pathways that mediate tail-anchored (TA) protein insertion at the endoplasmic reticulum (ER). We show that this dual-guide library approach can be successfully coupled with FACS-based reporter screening, to identify genetic epistasis and thereby place TA biogenesis factors in their respective parallel pathways. We demonstrate that this dual-guide approach is both more sensitive and specific than traditional growth screening approaches, and is ideally suited for dissecting the complex interplay between factors in human cells.", "title": "A dual sgRNA library design to probe genetic modifiers using genome-wide CRISPRi screens", "claims": null}, {"metadata": {"year": 2005}, "authors": ["F. Al-Shahrour", "R. D\u00edaz-Uriarte", "J. Dopazo"], "summary": "MOTIVATION\nThe analysis of genome-scale data from different high throughput techniques can be used to obtain lists of genes ordered according to their different behaviours under distinct experimental conditions corresponding to different phenotypes (e.g. differential gene expression between diseased samples and controls, different response to a drug, etc.). The order in which the genes appear in the list is a consequence of the biological roles that the genes play within the cell, which account, at molecular scale, for the macroscopic differences observed between the phenotypes studied. Typically, two steps are followed for understanding the biological processes that differentiate phenotypes at molecular level: first, genes with significant differential expression are selected on the basis of their experimental values and subsequently, the functional properties of these genes are analysed. Instead, we present a simple procedure which combines experimental measurements with available biological information in a way that genes are simultaneously tested in groups related by common functional properties. The method proposed constitutes a very sensitive tool for selecting genes with significant differential behaviour in the experimental conditions tested.\n\n\nRESULTS\nWe propose the use of a method to scan ordered lists of genes. The method allows the understanding of the biological processes operating at molecular level behind the macroscopic experiment from which the list was generated. This procedure can be useful in situations where it is not possible to obtain statistically significant differences based on the experimental measurements (e.g. low prevalence diseases, etc.). Two examples demonstrate its application in two microarray experiments and the type of information that can be extracted.", "title": "Discovering molecular functions significantly related to phenotypes by combining gene expression data and biological information", "claims": null}], "query": "high throughput genetic screen are a powerful tool in identifying individual gene function and interaction", "summary_abstract": "High-throughput genetic screens have emerged as a transformative tool for elucidating gene function and interactions across various biological systems. These screens enable researchers to systematically investigate the roles of numerous genes simultaneously, providing insights into complex genetic networks and phenotypic outcomes.\n\nMcCarthy et al. (2022) highlight the potential of their novel MuSIC approach, which proposes a method for conducting genome-scale genetic interaction screens in mammalian cells. This method could significantly enhance our understanding of genetic interactions by allowing for the pairing of MuSIC barcodes with guide RNAs, thus facilitating the identification of gene functions on a large scale.\n\nEmig et al. (2012) emphasize the importance of high-throughput techniques, such as RNA interference (RNAi) screens, in functional genomics. They note that these approaches generate extensive lists of candidate genes, which require sophisticated computational methods for analysis and functional annotation, ultimately aiding in the understanding of molecular interactions.\n\nFriedman and Perrimon (2004) discuss the advancements in high-throughput, genome-wide screens across various model organisms, including yeast and mammals. They assert that these screens are crucial for annotating gene functions and understanding cellular networks, as they allow for the identification of gene functions in a systematic manner.\n\nCarpenter and Sabatini (2004) further elaborate on the systematic nature of genome-wide functional screens, contrasting them with traditional random mutation methods. They assert that these high-throughput techniques are essential for rapidly annotating genomes and understanding the biological contributions of individual genes.\n\nBillmann and Boutros (2016) present a two-step approach to genetic interaction mapping, which combines high-content microscopy with automated image analysis. This method allows for the identification of functional relationships among genes, particularly in processes like the mitotic cell cycle, thereby enhancing our understanding of gene interactions.\n\nGuna et al. (2023) introduce a CRISPR interference-based platform that enables genome-wide genetic modifier screens in human cells. This platform is designed to dissect complex biological processes by mapping genetic interactions, demonstrating its effectiveness in identifying epistatic relationships and enhancing the specificity of genetic screening.\n\nIn summary, the collective findings from these studies underscore the power of high-throughput genetic screens in identifying gene functions and interactions. These methodologies not only facilitate the exploration of genetic networks but also enhance our understanding of the underlying biological processes that govern cellular behavior. The integration of advanced techniques, such as CRISPR and multiplexing approaches, promises to further expand the capabilities of genetic screening in both model organisms and human cells.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2019}, "authors": ["Boer  Cui", "Genevi\u00e8ve  Boisjoly", "Ahmed M El-Geneidy", "David M Levinson"], "summary": "Abstract Inequality in transport provision is an area of growing concern among transport professionals, as it results in low-income individuals travelling at lower speeds while covering smaller distances. Accessibility, the ease of reaching destinations, may hold the key in correcting these inequalities through providing a means to evaluate land use and transport interventions. This article examines the relationship between accessibility and commute duration for low-income individuals compared to the higher-income, in three major Canadian metropolitan regions, Toronto, Montreal, and Vancouver using separate multilevel mixed effects statistical models for car and public transport commuters. Accessibility measures are generated for jobs and workers both at the origin (home) and the destination (place of work) to account for the impact of competing labor and firms. Our models show that the impacts of accessibility on commute duration are present and in many cases stronger for low-income individuals than for higher income groups. The results suggest that low-income individuals have more to gain (in terms of reduced commute time) from increased accessibility to low-income jobs at the origin and to workers at the destination. Similarly, they also have more to lose from increased accessibility to low-income workers at the origin and to low-income jobs at the destination, which are proxies for increased competition. Policies targeting improvements in accessibility to jobs, especially low-income ones, by car and public transport while managing the presence of competition can serve to bridge the inequality gap that exists in commuting behavior.", "title": "Accessibility and the journey to work through the lens of equity", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Boer Cui", "G. Boisjoly", "A. El-geneidy", "D. Levinson"], "summary": "Inequality in transport provision is an area of growing concern among transport professionals, as it results in low-income individuals travelling at lower speeds while covering smaller distances. Accessibility, the ease of reaching destinations, may hold the key in correcting these inequalities through providing a means to evaluate land use and transport interventions. This article examines the relationship between accessibility and commuting duration for low-income individuals, compared to the general population, in three major Canadian metropolitan regions, Toronto, Montreal, and Vancouver using multilevel mixed effects statistical models for car and public transport commuters separately. Accessibility measures are generated for jobs and workers both at the origin (home) and the destination (place of work) to account for the impact of competing labor and firms. Our models show that the impacts of accessibility on commuting duration are present and stronger for low-income individuals than for the general population, and the differences in impact are more visible for public transport commuters. The results suggest that low-income individuals have more to gain (in terms of reduced commute time) from increased accessibility to low-income jobs at the origin and to workers at the destination. Similarly, they also have more to lose from increased accessibility to low-income workers at the origin and to low- income jobs at the destination, which are proxies for increased competition. Policies targeting improvements in accessibility to jobs, especially low-income ones, by car and public transport while managing the presence of competition can serve to bridge the inequality gap that exists in commuting behavior.", "title": "Accessibility, equity, and the journey to work", "claims": null}, {"metadata": {"year": 2011}, "authors": ["A. Soltani", "Yousef Esmaeili Ivaki"], "summary": "Neo-classical economic doctrine dominating governmental policies shows its impact on recent transport policies, causing these policies; tend to base on demand and efficiency criteria instead of equity concerns. Public transit operating for remote areas is less cost-effective eventually leading to have a low level of service quality. In metropolitan areas of developing countries, a large part of socially disadvantaged and vulnerable groups live in outer suburban locations not in the inner-city. Transit equity evaluation is required by in order to consider the requirements of more vulnerable populations, as well as to foster equal benefits. The evaluation approach is based on highlighting the spatial distribution and clustering patterns of bus network and service as well as some disadvantaged social groups including unemployed, migrated, less educated, elderly, young, and disabled. The hypothesis is that vulnerable groups and economically disadvantaged communities receive a less than equal share of public bus services. The findings show that poor accessibility is associated both with low-income neighborhoods and with neighborhoods with disproportionately high populations of migrated, less-educated, unemployed and low-income groups. Modifications need to make in transport planning and policy system to achieve a better distribution of public transport services in hope of increasing level of service for minority groups and economically disadvantaged communities.", "title": "Inequity in the Provision of Public Bus Service for Socially Disadvantaged Groups", "claims": null}, {"metadata": {"year": 2004}, "authors": ["D. Mignot"], "summary": "The current context of increasing social and spatial disparities raises the issue of universal accessibility to the city and its services, in particular for low incomes persons. The issue of inequality with regard to travel, which we have considered first of all at an aggregate level using the usual travel indicators (number of trips, distance covered, travel time budget), essentially comes down to inequality in access to the car. An analysis of recent changes in urban public transport pricing policy and a survey conducted within welfare and social integration agencies in the conurbations of Lyon, Nancy and Nantes have been used to obtain a more accurate qualitative and quantitative picture of transport difficulties.", "title": "Transport et justice sociale", "claims": null}, {"metadata": {"year": 2011}, "authors": ["D. Lewis"], "summary": "Poverty, inequality and social exclusion are closely tied to personal mobility and the accessibility of goods and services. Evidence of the economic role of transport in promoting better living standards and greater wellbeing can be seen in the effects of both overall public investment in transport infrastructure, and in the impacts of specific transport policies, projects and multi-project plans.", "title": "Economic perspectives on transport and equality", "claims": null}, {"metadata": {"year": 2005}, "authors": ["David Caubel"], "summary": "The social dimension of urban transport policies becomes a paramount issue of the public action. This may result from the observed growth of social inequalities, and the persistence or even the worsening of segregation processes. In particular, this issue leads to questions about the access to activities and services that individuals need everyday. It also relates to the characterization of an equal access for all to the city.In this context, the aim of the paper is to validate, by the measure in the case of the Lyons conurbation, the existence of urban accessibility disparities between the richest and the deprived territories residents, by accounting for the private car and the public transport. We show that there are equal accesses to amenities by car, but the travel modes unequal access implies inequities in accessing urban activities. Then, our paper analyses the social impacts of the public transport policy contents aiming at reducing the urban accessibility disparities or the gaps to the \"standard society\". The assessment made, based on the interpretation of the Lyons Urban Travel Plan, accounts for concrete answers of the access to amenities improvement for everyone, and in particular for the underprivileged populations. Nevertheless, these answers prove to be limited. A public transport policy cannot claim to reduce the unequal access between territories and to fight against the exclusion of a minority of the population, without accounting for the local and overall contexts of urban morphology, without being in line with regional planning policies \u2013 in particular the activities location control policies.", "title": "Public transport development and equal access for all to urban amenities", "claims": null}, {"metadata": {"year": 1980}, "authors": ["E. Boer"], "summary": "In practice there are not equal chances for everybody to use the transport system. The national government aims to lessen the inequality in this respect as regards certain groups of the population. Such groups are for instance the poor, children, old people, the disabled, the people without a driving licence, and housewives. The question of when there can be said to be a large inequality in the use of transport is difficult to answer. This report describes what is known about the phenomenon of unequal accessibility to the transport system and discusses how far this phenomenon can be studied. Social scientific studies in different countries of the choice behaviour of travellers are reviewed. (TRRL)", "title": "INEQUALITY IN THE USE OF TRANSPORT, ITS RELEVANCE AND THE POSSIBILITY OF STUDYING IT. AN INTERNATIONAL LITERATURE REVIEW", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Anik Pramanik", "Pan Xu", "Yifang Xu"], "summary": "There are many news articles reporting the obstacles con- fronting poverty-stricken households in access to public transits. These barriers create a great deal of inconveniences for these impoverished families and more importantly, they con-tribute a lot of social inequalities. A typical approach ad- dressing the issue is to build more transport infrastructure to o\ufb00er more opportunities to access the public transits es- pecially for those deprived communities. Examples include adding more bus lines connecting needy residents to railways systems and extending existing bus lines to areas with low socioeconomic status. Recently, a new strategy is proposed, which is to harness the ubiquitous ride-hailing services to con-nect disadvantaged households with the nearest public trans- portations. Compared with the former infrastructure-based solution, the ride-hailing-based strategy enjoys a few exclusive bene\ufb01ts such as higher e\ufb00ectiveness and more \ufb02exibility. In this paper, we propose an optimization model to study how to integrate the two approaches together for equity-promotion purposes. Speci\ufb01cally, we aim to design a strategy of allo- cating a given limited budget to di\ufb00erent candidate programs such that the overall social equity is maximized, which is de- \ufb01ned as the minimum covering ratio among all pre-speci\ufb01ed protected groups of households (based on race, income, etc.). We have designed a linear-programming (LP) based rounding algorithm, which proves to achieve an optimal approximation ratio of 1 \u2212 1 / \ud835\udc52 . Additionally, we test our algorithm against a few baselines on real data assembled by outsourcing multiple public datasets collected in the city of Chicago. Experimental results con\ufb01rm our theoretical predictions and demonstrate the e\ufb00ectiveness of our LP-based strategy in promoting social equity, especially when the budget is insu\ufb03cient.", "title": "Equity Promotion in Public Transportation", "claims": null}], "query": "competition public transport accessibility inequality poverty", "summary_abstract": "The research question regarding competition, public transport accessibility, inequality, and poverty is addressed through various studies that highlight the disparities in transport provision and its implications for low-income populations.\n\nBoer Cui et al. (2019) and (2018) emphasize that low-income individuals experience longer commute durations compared to higher-income groups, particularly in public transport scenarios. Their findings suggest that enhancing accessibility to low-income jobs can significantly reduce commute times for these individuals, indicating that they stand to benefit more from improved transport options. However, increased competition for low-income jobs can also lead to negative outcomes, as these individuals may face greater challenges in accessing employment opportunities.\n\nSoltani and Esmaeili Ivaki (2011) further illustrate the inequities in public transport services, particularly in developing countries, where vulnerable populations often reside in outer suburban areas with limited access to quality transit. Their research indicates that low-income neighborhoods are disproportionately affected by poor public transport accessibility, necessitating a reevaluation of transport policies to ensure equitable service distribution.\n\nMignot (2004) and Lewis (2011) discuss the broader implications of transport accessibility on social inclusion and mobility. Mignot highlights the need for universal access to urban services, particularly for low-income individuals, while Lewis connects transport investment to improved living standards, underscoring the economic role of accessible transport in alleviating poverty.\n\nCaubel (2005) adds to this discourse by examining urban accessibility disparities in Lyon, revealing that while car access may be equitable, public transport access remains unequal, particularly affecting underprivileged populations. This suggests that public transport policies must be aligned with regional planning to effectively address these disparities.\n\nFinally, Pramanik et al. (2022) propose innovative solutions to enhance public transport access for impoverished households, such as integrating ride-hailing services with existing public transport systems. Their optimization model aims to maximize social equity in transport access, particularly for disadvantaged groups.\n\nCollectively, these studies underscore the critical relationship between public transport accessibility and social equity, highlighting the need for targeted policies that address the unique challenges faced by low-income populations in accessing transport services.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2016}, "authors": ["Florian Dumpert", "Katja von Eschwege", "M. Beck"], "summary": "This article shows the motivation for, approach to, and results of applying support vector machines in official statistics concerning the business register for test purposes. A support vector machine is a universally applicable machine learning and classification method. Based on a mathematical optimisation approach, objects can be classified by specific variables and be allocated to corresponding classes. The non-parametric statistical method succeeded in classifying enterprises with respect to the so-called third sector and is therefore suitable to improve and complement the method used up to now.", "title": "Einsatz von Support Vector Machines bei der Sektorzuordnung von Unternehmen", "claims": null}, {"metadata": {"year": 2001}, "authors": ["D. Sebald", "J. Bucklew"], "summary": "Two enhancements are proposed to the application and theory of support vector machines. The first is a method of multicategory classification based on the binary classification version of the support vector machine (SVM). The method, which is called the M-ary SVM, represents each category in binary format, and to each bit of that representation is assigned a conventional SVM. This approach requires only [log/sub 2/(K)] SVMs, where K is the number of classes. We give an example of classification on an octaphase-shift-keying (8-PSK) pattern space to illustrate the main concepts. The second enhancement is that of adding equality constraints to the conventional binary classification SVM. This allows pinning the classification boundary to points that are known a priori to lie on the boundary. Applications of this method often arise in problems having some type of symmetry, We present one such example where the M-ary SVM is used to classify symbols of a CDMA two-user, multiuser detection pattern space.", "title": "Support vector machines and the multiple hypothesis test problem", "claims": null}, {"metadata": {"year": 2011}, "authors": ["R. Gilbert", "T. Trafalis", "I. Adrianto"], "summary": "In this article, we introduce the subject of support vector machines (SVMs), describing their applications to binary and multiclass classification as well as different SVM formulations that are used in such supervised learning problems. This article is not exhaustive and many approaches have not been considered. More detailed treatments are discussed in the books by Abe, Cristianini and Shawe-Taylor, Scholkopf, and Smola and Vapnik. \n \n \nKeywords: \n \nsupport vector machines; \nstatistical learning; \nmulticlass classification; \noptimization", "title": "Support Vector Machines for Classification", "claims": null}, {"metadata": {"year": 1999}, "authors": ["M. Niranjan"], "summary": "Summary form only given. There has been much interest in the use of support vector machines (SVM) as an approach to high performance pattern classification. In the linearly separable case, SVMs attempt to position a class boundary so that the margin from the nearest example is maximised. This criterion can be implemented by solving a quadratic programming problem, and the solution turns out to be one in which the class boundary may be expressed as a linear combination of a subset of the training data (the support vectors). The elegance of the QP formulation, and the relationship between control of complexity in this formulation and Vapnik-Chervonenkis dimensions are seen as prime attractions of the SVM method. A related idea in high performance pattern classification is that of boosting multiple classifiers. The author shows that the standard SVM formulation is not robust to noise and explains the performance of boosting algorithms by reference to receiver operating characteristics curves.", "title": "Support vector machines: a tutorial overview and critical appraisal", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Yoonkyung  Lee"], "summary": "The support vector machine is a supervised learning technique for classification increasingly used in many applications of data mining, engineering, and bioinformatics. This chapter aims to provide an introduction to the method, covering from the basic concept of the optimal separating hyperplane to its nonlinear generalization through kernels. A general framework of kernel methods that encompass the support vector machine as a special case is outlined. In addition, statistical properties that illuminate both advantage and limitation of the method due to its specific mechanism for classification are briefly discussed. For illustration of the method and related practical issues, an application to real data with high-dimensional features is presented.", "title": "Support vector machines for classification: a statistical portrait.", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Shahab  Araghinejad"], "summary": "Classifying data is a common task in data-driven modeling. Using support vector machines, we can separate classes of data by a hyperplane. A support vector machine (SVM) is a concept for a set of related supervised learning methods that analyze data and recognize patterns, used for classification and regression analysis. The formulation of SVM uses the structural risk minimization principle, which has been shown to be superior to the traditional empirical risk minimization principle used by conventional neural networks. This chapter presents principles of classification and regression analysis by support vector machines, briefly. Also related MATLAB programs are presented.", "title": "Support Vector Machines", "claims": null}, {"metadata": {"year": 2002}, "authors": ["J. Currall"], "summary": "In the last decade Support Vector Machines (SVMs) have emerged as an important learning technique for solving classification and regression problems in various fields, most notably in computational biology, finance and text categorization. This is due in part to built-in mechanisms to ensure good generalization which leads to accurate prediction, the use of kernel functions to model non-linear distributions, the ability to train relatively quickly on large data sets using novel mathematical optimization techniques and most significantly the possibility of theoretical analysis using computational learning theory. In this thesis, we discuss the theoretical basis and computational approaches to Support Vector Machines.", "title": "Support Vector Machines for Classification and Regression", "claims": null}, {"metadata": {"year": 2012}, "authors": ["H. Bhavsar", "Mahesh Panchal"], "summary": "185 All Rights Reserved \u00a9 2012 IJARCET Abstract-With increasing amounts of data being generated by businesses and researchers there is a need for fast, accurate and robust algorithms for data analysis. Improvements in databases technology, computing performance and artificial intelligence have contributed to the development of intelligent data analysis. Support vector machines are a specific type of machine learning algorithm that are among the most widelyused for many statistical learning problems, such as spam filtering, text classification, handwriting analysis, face and object recognition, and countless others. Support vector machines have also come into widespread use in practically every area of bioinformatics within the last ten years, and their area of influence continues to expand today. The support vector machine has been developed as robust tool for classification and regression in noisy, complex domains. The two key features of support vector machines are generalization theory, which leads to a principled way to choose an hypothesis; and, kernel functions, which introduce nonlinearity in the hypothesis space without explicitly requiring a non-linear algorithm.", "title": "A Review on Support Vector Machine for Data Classification", "claims": null}], "query": "which classifications are suitable for support vector machines?", "summary_abstract": "The research question regarding suitable classifications for support vector machines (SVMs) is addressed across several papers, highlighting the versatility and effectiveness of SVMs in various classification tasks.\n\nDumpert et al. (2016) demonstrate the application of SVMs in classifying enterprises within the business register, particularly focusing on the third sector. This illustrates SVMs' capability to handle specific classification challenges in official statistics. Similarly, Sebald and Bucklew (2001) introduce a multicategory classification method, termed M-ary SVM, which allows for efficient classification across multiple categories by representing each category in binary format. This method showcases SVMs' adaptability to multiclass problems, which is a significant enhancement over traditional binary classification.\n\nGilbert et al. (2011) further elaborate on the application of SVMs in both binary and multiclass classification scenarios, emphasizing their broad applicability in supervised learning. Niranjan (1999) discusses the foundational principles of SVMs, particularly their effectiveness in high-performance pattern classification, which is crucial for tasks requiring robust decision boundaries.\n\nLee (2010) provides insights into the kernel methods associated with SVMs, which enable the classification of non-linear data distributions, thus expanding the types of classification problems SVMs can effectively address. Araghinejad (2014) reinforces this by discussing the structural risk minimization principle that underpins SVMs, highlighting their superiority in classification tasks compared to traditional methods.\n\nCurrall (2002) notes the widespread use of SVMs in diverse fields such as computational biology and finance, indicating their versatility across various domains. Bhavsar and Panchal (2012) also emphasize the robustness of SVMs in noisy and complex environments, further supporting their suitability for a wide range of classification tasks, including spam filtering and text classification.\n\nIn summary, support vector machines are suitable for a variety of classification tasks, including binary and multiclass problems, and are particularly effective in handling non-linear data through kernel methods. Their application spans numerous fields, demonstrating their robustness and adaptability in diverse classification scenarios (Dumpert et al., 2016; Sebald & Bucklew, 2001; Gilbert et al., 2011; Niranjan, 1999; Lee, 2010; Araghinejad, 2014; Currall, 2002; Bhavsar & Panchal, 2012).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2021}, "authors": ["Antonio  Cano", "Paul  Ar\u00e9valo", "Dar\u00edo  Benavides", "Francisco  Jurado"], "summary": "Abstract The problem of climate change because greenhouse gas emissions is aggravating, especially in public transport, which encourages the development of new technologies and clean energy control methods for the propulsion of vehicles such as tramways. A new energy control for a real tramway has been proposed in this paper, combining renewable sources, supercapacitors and lithium ion batteries, both components will absorb the energy from the regenerative braking of the tramway. The system has been modeled in Matlab considering certain restrictions in each component in order to supply the load on the round trip. Finally, a techno-economic and environmental analysis has been done identifying new patterns with respect to existing tramway systems. The annual energy required by the tramway is 867.62 MWh/year. The power variations are mainly supplied by the supercapacitor and the lithium ion battery functions as a backup. In this regard, the proposed system saves $ 2205,724 by supplying energy to the tramway and selling the excess energy to the grid for 20 years. Finally, the renewable system will have avoided 8,445.4 tCO2/MWh.", "title": "Sustainable tramway, techno-economic analysis and environmental effects in an urban public transport. A comparative study", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Xin  Wang", "Inge  Norstad", "Kjetil  Fagerholt", "Marielle  Christiansen"], "summary": "In this chapter we examine, from a tramp ship operator\u2019s point of view, how potential CO2 emission reduction measures impact the operational decisions and their economic and environmental consequences. Two market-based measures (MBMs) are discussed, the bunker levy scheme and the emission trading scheme, and we show that both can be incorporated in a similar way into a typical tramp ship routing and scheduling model. We also demonstrate with a computational study the environmental benefits of these CO2 reduction schemes.", "title": "Green Tramp Shipping Routing and Scheduling: Effects of Market-Based Measures on CO2 Reduction", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Minje Choia", "Gayoung Kanga", "Juhyeon Kwaka", "Yoonjung Jangb", "S. Leec"], "summary": "The Net-Zero standard was proposed by Science Based Target (SBTi) to bring the net greenhouse gas emissions to zero as global warming intensifies, and this policy is spreading worldwide. Public transportation plays an important role in eco-friendly transportation and establishing a railway-oriented public transportation system is important. Among modes of railway traffic, trams are easy to access compared with subways (a representative modes of railway transportation) and are economical because of their low construction and operation costs. If a priority signal is given to the tram operation, the scheduled speed increases; The efficiency can be further improved. The purpose of this study was to analyse how the conversion of modes to public transportation caused by tram construction can affect the atmosphere and to study how much the increase in physical activity caused by the increase in public transportation affects the reduction of disease. Dongtan New Town in Korea, where trams are scheduled to be introduced, was set as the study area, and the effect of the conversion of modes of transportation resulting from tram construction was analysed through the modal split process of the four-stage transportation demand prediction model. The analysis shows that trams will generate a 54,700 trips/d conversion to public transportation within the affected area. The benefit from air pollution reduction is 25.13", "title": "Calculating the Environmental Benefits of Trams", "claims": null}, {"metadata": {"year": 1998}, "authors": ["C. Pronello"], "summary": "The paper presents the results of a simulation model to calculate the tramsport pollutant emissions in different utirban sites", "title": "A predictive model of atmospheric pollution with environmental capacity constraint: an application in the city of Turin", "claims": null}, {"metadata": {"year": 1999}, "authors": ["R. Soberman", "E. Miller"], "summary": "Certain concerns about sustainable transportation derive from the premise that significant atmospheric concentrations of carbon dioxide (CO2) and other greenhouse gases which contribute to global warming can be attributed to automotive emissions. Among measures considered to curtail automobile dependency and increase transit modal split, \"full cost pricing\" \u2192 a means of extracting both environmental costs and hidden subsidies which road users allegedly impose on society \u2192 has emerged as an increasingly popular suggestion. This paper examines the potential impact of full cost pricing on the \"sustainability\" of urban transportation, using Toronto as a case study. The analysis deals with the direct effects on mode choice and vehicle utilization resulting from increased user costs as might be achieved through fuel taxes or road pricing, as well as the indirect, longer term effects on location decisions. To investigate the land-use impacts, population distributions were altered to reflect more compact developm...", "title": "Impacts of full cost pricing on the sustainability of urban transportation: towards Canada's Kyoto commitments", "claims": null}, {"metadata": {"year": 2020}, "authors": ["J. Niemann", "Julian Bruckmann", "Florian Krautzer"], "summary": "The population is growing constantly in urban areas. This results in an increasing demand for mobility solutions while it is also worldwide aimed to reduce greenhouse gas emissions. This paper summarizes the results of a comparative study concerning the greenhouse gas emissions (based on carbon footprint) caused by alternative urban passenger transportation systems. The emissions for the vehicles and their infrastructure are analyszed over the entire life cycle from manufacturing up to their end of life. An existing cable car sytem in La Paz, Bolivia was analysed and evaluated in comparison to other modes of transportation such as small busses, large busses and a tram. According to the system definitions and the considered balance framework the study shows that beside the use of the systems the materials and the manufacturing as well as the infrastructure have a significant impact on the total emissions over the life cycle. To put focus on the \u201etrue and real\u201c impacts to the society it is preferable to consider the measurement results in total absolute emissions.", "title": "Green City Deals: A study on the global warming potential of alternative urban transportation systems", "claims": null}, {"metadata": {"year": 2009}, "authors": ["T. Litman"], "summary": "This paper describes climate change impacts and costs, presents methods for quantifying and monetizing (measuring in monetary units) these impacts, summarizes published unit cost estimates, and explains the values used in the report, Transportation Cost and Benefit Analysis. Climate change emission valuation depends on many factors including the range of impacts considered, the methods used to quantify impacts, and emission reduction targets. Recent studies predict that damage costs are potentially very high if atmospheric greenhouse gas levels exceed critical thresholds, while emission control costs are $20-50 per tonne of carbon dioxide equivalent (CO2e). Some transportation emission reduction strategies have relatively low costs when co-benefits such as consumer savings, congestion reductions and safety are considered. Climate Change Emission Valuation for Transportation Economic Analysis Victoria Transport Policy Institute", "title": "Climate change emission valuation for transportation economic analysis", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Baek-Jae  Kim", "Kwang-Eui  Yoo"], "summary": "The aviation sector has developed dramatically in recent decades. However, environmental and social concerns are gradually posing limitations on the growth of the air transport industry. Air travel contributes to climate change, and causes significant environmental damage. It is therefore subject to environmental impact charges, which will intensify over time. These environmental charges will influence air travel. This research aims to investigate the impact of environment costs by examining their influence on air passenger demand within the context of two major airline network models: the hub-and-spoke network and the point-to-point network. The additional costs, caused by environmental rehabilitation, will affect both the networks and profit structures of airlines, and these structures will need to be amended in time. This study may contribute to the South Korean aviation industry, especially to the airlines that will be affected by the CO2 constraints imposed by the European Union Emissions Trading System from 2012 onwards.", "title": "The Implication of Environmental Costs on Air Passenger Demand for Airline Networks", "claims": null}], "query": "tramsport fare prices environmental impacts climate change", "summary_abstract": "The research question regarding the impact of transport fare prices on environmental effects and climate change is addressed through various studies focusing on public transportation, particularly tram systems. \n\nCano et al. (2021) highlight the potential for tramways to reduce greenhouse gas emissions through the integration of renewable energy sources and energy storage systems. Their analysis indicates that a new energy control system for trams could avoid approximately 8,445.4 tons of CO2 emissions per MWh, demonstrating a significant environmental benefit from improved tramway technology.\n\nChoia et al. (2022) further emphasize the role of trams in promoting eco-friendly transportation. Their study on the Dongtan New Town in Korea reveals that the introduction of trams could lead to a substantial increase in public transport usage, with an estimated conversion of 54,700 trips per day. This shift not only enhances accessibility but also contributes to air pollution reduction, showcasing the broader environmental advantages of investing in tram infrastructure.\n\nNiemann et al. (2020) provide a comparative analysis of greenhouse gas emissions across various urban transportation systems, including trams. Their findings suggest that the life cycle emissions of tram systems, when compared to other modes of transport, underscore the importance of considering both operational and infrastructural impacts on total emissions.\n\nAdditionally, Soberman and Miller (1999) discuss the concept of \"full cost pricing\" in urban transportation, which includes environmental costs in fare structures. This approach could potentially influence mode choice and reduce automobile dependency, thereby contributing to lower greenhouse gas emissions.\n\nIn summary, the collective findings from these studies indicate that tram systems, when effectively integrated into urban transport networks and supported by appropriate pricing strategies, can significantly mitigate environmental impacts and contribute to climate change mitigation efforts. The emphasis on renewable energy and increased public transport usage further reinforces the potential of trams as a sustainable transportation solution.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2006}, "authors": ["L. Troelsen", "S. Jacobsen"], "summary": "Rheumatoid arthritis is associated with increased cardiovascular morbidity and mortality due to atherosclerosis. This cannot be explained by an increased presence of traditional risk factors but seems to depend on inflammatory mechanisms. The association of inflammatory pathways with atherosclerosis is complex, and more research is required to optimise preventative measures against cardiovascular complications in inflammatory rheumatic diseases.", "title": "[Chronic inflammation increases the risk of cardiovascular disease in patients with rheumatoid arthritis].", "claims": null}, {"metadata": {"year": 2014}, "authors": ["E. Choy", "K. Ganeshalingam", "A. Semb", "Z. Szekanecz", "M. Nurmohamed"], "summary": "Risk of cardiovascular (CV) disease is increased among RA patients. High inflammatory burden associated with RA appears to be a key driver of the increased cardiovascular risk. Inflammation is linked with accelerated atherosclerosis and associated with a paradoxical inversion of the relationship between CV risk and lipid levels in patients with untreated RA, recently coined the lipid paradox. Furthermore, the inflammatory burden is also associated with qualitative as well as quantitative changes in lipoproteins, with the anti-inflammatory and atheroprotective roles associated with high-density lipoprotein cholesterol significantly altered. RA therapies can increase lipid levels, which may reflect the normalization of lipids due to their inflammatory-dampening effects. However, these confounding influences of inflammation and RA therapies on lipid profiles pose challenges for assessing CV risk in RA patients and interpretation of traditional CV risk scores. In this review we examine the relationship between the increased inflammatory burden in RA and CV risk, exploring how inflammation influences lipid profiles, the impact of RA therapies and strategies for identifying and monitoring CV risk in RA patients aimed at improving CV outcomes.", "title": "Cardiovascular risk in rheumatoid arthritis: recent advances in the understanding of the pivotal role of inflammation, risk predictors and the impact of treatment", "claims": null}, {"metadata": {"year": 2004}, "authors": ["Lexin  Wang", "Guoqin  Feng"], "summary": "Patients with rheumatoid arthritis (RA) have an increased prevalence of coronary heart disease and a high cardiovascular mortality rate. The causes of increased coronary heart disease in RA patients are poorly understood. Conventional cardiovascular risk factors, such as inactivity, overweight or dyslipidemia may play a role, but they do not seem to be wholly responsible for the increased cardiovascular risk. RA is associated with a high incidence of inflammation and vascular endothelial injuries. Endothelial dysfunction is one of the key steps in the pathogenesis of atherosclerosis in non-RA patients. Therefore, we hypothesized that inflammation-induced vascular endothelial injuries may be responsible for the increased risk of coronary heart disease and high rates of cardiovascular mortality in patients with RA.", "title": "Rheumatoid arthritis increases the risk of coronary heart disease via vascular endothelial injuries.", "claims": null}, {"metadata": {"year": 2010}, "authors": ["J. Rovensk\u00fd", "M. Vlcek", "R. Imrich"], "summary": "Risk of cardiovascular diseases is significantly higher in patients with rheumatoid arthritis (RA) than in normal population, leading to higher mortality of these patients. An accelerated atherosclerosis has been considered a basis for the increased cardiovascular risk in RA. Besides classical atherosclerosis risk factors, systemic inflammation plays a substantial role. Indirect mechanisms such as insulin resistance and dyslipidemia may play a role, however, inflammation probably causes direct damage to blood vessels. Thus, systemic inflammation has a primary role and other factors accelerate this process. An adequate anti-inflammatory therapy can have a positive effect also on cardiovascular diseases in RA.", "title": "[Cardiovascular diseases in rheumatoid arthritis].", "claims": null}, {"metadata": {"year": 2013}, "authors": ["I. V. D. van den Oever", "A. V. van Sijl", "M. Nurmohamed"], "summary": "The risk of cardiovascular morbidity and mortality is increased in rheumatoid arthritis. The classical cardiovascular risk factors, including smoking, hypertension, dyslipidaemia, insulin resistance and diabetes mellitus, obesity and physical inactivity do not appear to explain the excess cardiovascular risk in rheumatoid arthritis, although they do contribute, albeit in a different way or to a lesser extent, to rheumatoid arthritis in comparison with the general population. A very important link between rheumatoid arthritis and cardiovascular disease is inflammation as it plays a key role in all stages of atherosclerosis: from endothelial dysfunction to plaque rupture and thrombosis. It also has an influence on and accentuates some traditional cardiovascular risk factors, such as dyslipidaemia, obesity and insulin resistance. To date, the exact pathophysiologic mechanism by which this relation between cardiovascular disease and rheumatoid arthritis can be explained is not completely clear. Cardiovascular risk management in rheumatoid arthritis is mandatory. Unfortunately, the way this should be done remains a point of discussion. In this review issues regarding cardiovascular risk in rheumatoid arthritis and its management will be addressed, according to evidence presented in the latest studies and our own experience-based opinion.", "title": "Management of cardiovascular risk in patients with rheumatoid arthritis: evidence and expert opinion", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Sarah  Skeoch", "Ian N. Bruce"], "summary": "Rheumatoid arthritis (RA) has long been associated with increased cardiovascular risk, but despite substantial improvements in disease management, mortality remains high. Atherosclerosis is more prevalent in RA than in the general population, and atherosclerotic lesions progress at a faster rate and might be more prone to rupture, causing clinical events. Cells and cytokines implicated in RA pathogenesis are also involved in the development and progression of atherosclerosis, which is generally recognized as an inflammatory condition. The two diseases also share genetic and environmental risk factors, which suggests that patients who develop RA might also be predisposed to developing cardiovascular disease. In RA, inflammation and atherosclerosis are closely linked. Inflammation mediates its effects on atherosclerosis both through modulation of traditional risk factors and by directly affecting the vessel wall. Treatments such as TNF inhibitors might have a beneficial effect on cardiovascular risk. However, whether this benefit is attributable to effective control of inflammation or whether targeting specific cytokines, implicated in atherosclerosis, provides additional risk reduction is unclear. Further knowledge of the predictors of cardiovascular risk, the effects of early control of inflammation and of drug-specific effects are likely to improve the recognition and management of cardiovascular risk in patients with RA.", "title": "Atherosclerosis in rheumatoid arthritis: is it all about inflammation?", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Michael T Nurmohamed"], "summary": "The increased mortality in rheumatoid arthritis (RA) is mainly due to (atherosclerotic) cardiovascular disease. The cardiovascular morbidity is also increased in comparison with the general population. This increased cardiovascular burden could be caused by 1) an enhanced prevalence of cardiovascular risk factors 2) under treatment of cardiovascular risk factors or 3) RA itself, particularly due to its chronic inflammatory component. Cardiovascular risk factors only partially explain the increased cardiovascular risk and it is becoming increasingly acknowledged that the underlying inflammation in RA plays an essential role. This is probably related to the fact that atherosclerosis also has an inflammatory etiology that is accelerated by RA. Similarly, it can be expected that effective suppression of this inflammatory process by disease modifying antirheumatic drugs and/or biologicals lowers the cardiovascular risk. Altogether, there is accumulating evidence that the increased cardiovascular risk in RA is comparable to that of type 2 diabetes and actually RA should be seen as a new, independent, cardiovascular risk factor for which cardiovascular risk management is essential.", "title": "Cardiovascular risk in rheumatoid arthritis.", "claims": null}, {"metadata": {"year": 2010}, "authors": ["G. Kitas", "S. Gabriel"], "summary": "Rheumatoid arthritis is associated with an increased risk for cardiovascular events, such as myocardial infarction and stroke. Epidemiological evidence suggests that classic cardiovascular risk factors, such as hypertension, dyslipidaemia, insulin resistance and body composition alterations are important but not sufficient to explain all of the excess risk. High-grade systemic inflammation and its interplay with classic risk factors may also contribute. Some associations between classic risk factors and cardiovascular risk in people with rheumatoid arthritis appear counterintuitive but may be explained on the basis of biological alterations. More research is necessary to uncover the exact mechanisms responsible for this phenomenon, develop accurate systems used to identify patients at high risk, design and assess prevention strategies specific to this population of patients.", "title": "Cardiovascular disease in rheumatoid arthritis: state of the art and future perspectives", "claims": null}], "query": "How does inflammation increase the risk of cardiovascular disease in rheumatoid arthritis?", "summary_abstract": "The relationship between inflammation and the increased risk of cardiovascular disease (CVD) in patients with rheumatoid arthritis (RA) is well-documented across several studies. Inflammation is identified as a primary driver of cardiovascular morbidity and mortality in RA, significantly contributing to the development of atherosclerosis, which is the underlying cause of many cardiovascular events.\n\nTroelsen and Jacobsen (2006) highlight that the increased cardiovascular risk in RA cannot be solely attributed to traditional risk factors, suggesting that inflammatory mechanisms play a crucial role. This notion is supported by Choy et al. (2014), who emphasize that the high inflammatory burden associated with RA accelerates atherosclerosis and alters lipid profiles, leading to what is known as the \"lipid paradox.\" They note that inflammation not only affects the quantity of lipoproteins but also their quality, complicating the assessment of cardiovascular risk in RA patients.\n\nWang and Feng (2004) further elaborate on the role of inflammation by linking it to vascular endothelial injuries, which are critical in the pathogenesis of atherosclerosis. They propose that these inflammation-induced injuries may be a significant factor in the heightened risk of coronary heart disease among RA patients. Rovensk\u00fd et al. (2010) also support this view, indicating that systemic inflammation directly damages blood vessels, thereby exacerbating cardiovascular risk.\n\nMoreover, van den Oever et al. (2013) discuss how inflammation influences traditional cardiovascular risk factors, such as dyslipidemia and insulin resistance, thereby accentuating the overall cardiovascular risk profile in RA patients. Skeoch and Bruce (2015) add that the inflammatory processes involved in RA are closely linked to those in atherosclerosis, suggesting a shared pathophysiological pathway that may predispose RA patients to cardiovascular events.\n\nNurmohamed (2009) reinforces the idea that the chronic inflammatory nature of RA is a significant contributor to the increased cardiovascular burden, comparable to that seen in type 2 diabetes. Kitas and Gabriel (2010) conclude that while traditional risk factors are important, they do not fully account for the excess cardiovascular risk in RA, underscoring the need for further research to elucidate the mechanisms at play and to develop targeted prevention strategies.\n\nIn summary, the collective findings from these studies indicate that inflammation is a central factor in the increased risk of cardiovascular disease in rheumatoid arthritis, influencing both the development of atherosclerosis and the modulation of traditional cardiovascular risk factors. Effective management of inflammation in RA may therefore be crucial in reducing cardiovascular risk in these patients.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2001}, "authors": ["Deborah G. . Ancona", "P. Goodman", "B. Lawrence", "M. Tushman"], "summary": "The article discusses the issue of time as it pertains to organizational research. The author believes that looking at research in terms of time is a powerful tool in assessing organizational phenomena. According to the author, temporal research allows researchers to gain more perspective when looking at organizational issues such as decision making, group performance and organizational transformation. The author notes that the field of temporal research is translated into concepts including pacing, timing and sequencing.", "title": "Time: A New Research Lens", "claims": null}, {"metadata": {"year": 1973}, "authors": ["R. Ebert", "D. Piehl"], "summary": "This article provides an operational measure of the time frame within which a decision-maker is operating and empirical data indicating the importance of the time frame to the accomplishment of an organization9s goals. It presents an initial analysis of the organizational climate that may encourage a long or a short time horizon on the part of decision-makers.", "title": "Time Horizon: A Concept for Management", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Andrew Brodsky", "Sanford E. DeVoe"], "summary": "From the time we get up to the time we go to bed, there is no part of the day that does not have a temporal facet. One of the core aspects of temporal research in organizational contexts has been t...", "title": "Management Research on Time: Exploring Temporal Aspects of Work and Organizations", "claims": null}, {"metadata": {"year": 2019}, "authors": ["A. Basu", "M. Maciejewski"], "summary": "When designing a comparative outcomes or a cost-effectiveness analysis, the time horizon defining the duration of time for outcomes assessment must be carefully considered. The time horizon must be long enough to capture the intended and unintended benefits and harms of the intervention(s).1,2 In some instances, the time horizon should extend beyond the duration of a clinical trial when a specific end point is measured, whereas in other instances modeling outcomes over a longer period is unnecessary. Using a longer time horizon than is necessary may add unnecessary cost and complexity to the cost-effectiveness analysis model. In the May 2017 issue of JAMA Ophthalmology, Wittenborn et al3 examined costs and effectiveness of home-based macular degeneration monitoring systems using a lifetime horizon in a cost-effectiveness analysis and a 10-year horizon in a budget impact analysis. The rationale for selection of time horizons and their implications for interpreting the research is reviewed in this JAMA Guide to Statistics and Methods article.", "title": "Choosing a Time Horizon in Cost and Cost-effectiveness Analyses.", "claims": null}, {"metadata": {"year": 1988}, "authors": ["Allen C. Bluedorn", "R. Denhardt"], "summary": "The concept of time is introduced as a major topic for organizational and management research. Including a discussion of differing times and temporalities, macro level research and theory are described that relate time to such substantive areas as organizational culture, strategic planning, and organizational contingency theory. At the micro level, theory and research on time and individual differences, decision making, motivation, and group behavior are reviewed critically. Organizational and management topics of particular salience forfuture temporal research and management practice are identified.", "title": "Time and Organizations", "claims": null}, {"metadata": {"year": 1999}, "authors": ["Heejin Lee", "J. Liebenau"], "summary": "While there is much `time-related research', there is little `research on time'. This is striking since time is a key point in understanding organizations, their actions, culture, efficacy, etc. Most studies of time in management and organizational theory take time for granted. While there are numerous studies that address temporal issues, they are widely dispersed and unsystematic. This paper provides a classification of temporal studies of organizations and management. The scheme is built around two criteria: concepts of time and the role of time in research design. In the former, there are two contrasting concepts of time: clock time and social time. In the latter, time plays the roles of independent or dependent variables. By intersecting the two criteria, four notions of temporality (`deciding', `working', `varying' and `changing' times) are introduced to account for a variety of studies of time. The resulting classification not only reveals the current situation of studies about time, but it also indicates a direction which further research effort should take. We conclude by showing that temporally sensitive approaches will benefit research on organizations.", "title": "Time in Organizational Studies: Towards a New Research Direction", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Pierre-Luc Brunelle"], "summary": "This review depicts the theoretical perspectives on time and their implications for organizational research. It takes the shape of a critical review of time conceptualizations across the social and behavioral sciences. To describe time as duration and to decipher the role of the past, present, and future in organizational life, one must put time as the focal point of its investigation. First we reconsolidate the most widely used conceptual perspectives of time that have been considered in organization studies. Secondly we address time in relation to the research design and approaches in organizational research. As a synthesis, this paper put forth the implications that derive for theory, research and practice. This review logically provides a summary of how time has been, and can be, considered when dealing with organizational issues. Lastly, taking the form of a discussion, we bring forth how time and temporality across organizations may be interpreted, studied and used to deepen our knowledge of organizational research topics.", "title": "Time in Organization Studies: An Overview of the Literature on the State of Theory, Research and Practice", "claims": null}, {"metadata": {"year": 1982}, "authors": ["Clark E. Chastain"], "summary": "AN appropriate time span reflects a balance between near\u2010term and long\u2010term factors. Strong pressures are placed on organizations in our competitive society for quick results and, usually, near\u2010term results are overemphasized.", "title": "An appropriate time horizon: Now vs. then", "claims": null}], "query": "In project management research, time horizon is to the duration of a study's data collection period or the length of time over which data is analyzed (Wong et al., 2021). Different time horizons can be employed when studying employee engagement and organizational performance.", "summary_abstract": "The research on time horizons in project management, particularly concerning employee engagement and organizational performance, highlights the significance of temporal considerations in organizational studies. Ancona et al. (2001) emphasize that understanding time as a variable can enhance perspectives on organizational phenomena, including decision-making and group performance. They advocate for temporal research as a means to assess these issues more effectively.\n\nEbert and Piehl (1973) provide empirical evidence that the time frame within which decision-makers operate is crucial for achieving organizational goals. Their findings suggest that the organizational climate can influence whether decision-makers adopt a long or short time horizon, which is essential for understanding employee engagement strategies.\n\nBasu and Maciejewski (2019) discuss the importance of selecting appropriate time horizons in comparative outcomes analysis, noting that the duration must be sufficient to capture both intended and unintended effects of interventions. This consideration is vital when evaluating the impact of employee engagement initiatives on organizational performance.\n\nBluedorn and Denhardt (1988) introduce the concept of time as a central theme in organizational research, linking it to various substantive areas such as strategic planning and organizational culture. They argue that different temporalities can affect individual behaviors and decision-making processes, which are critical for understanding employee engagement.\n\nLee and Liebenau (1999) critique the existing body of time-related research, pointing out that while many studies address temporal issues, they often lack systematic approaches. They propose a classification of temporal studies that could guide future research, emphasizing the need for temporally sensitive methodologies in organizational studies.\n\nBrunelle (2017) further consolidates the theoretical perspectives on time, arguing that a focused investigation of time can yield deeper insights into organizational dynamics. He highlights the necessity of considering time in research design to enhance the understanding of organizational issues.\n\nChastain (1982) warns against the overemphasis on short-term results in competitive environments, advocating for a balanced approach that considers both near-term and long-term factors. This balance is crucial for fostering sustainable employee engagement and improving organizational performance.\n\nCollectively, these studies underscore the importance of carefully considering time horizons in project management research, as they significantly influence the analysis of employee engagement and organizational performance outcomes.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2010}, "authors": ["B. Koletzko", "S. Baker", "G. Cleghorn", "U. F. Neto", "S. Gopalan", "O. Hernell", "Q. S. Hock", "P. Jirapinyo", "B. Lonnerdal", "P. Pencharz", "Hildegard Pzyrembel", "J. Ram\u00edrez-mayans", "R. Shamir", "D. Turck", "Y. Yamashiro", "Ding Zongyi"], "summary": "The Codex Alimentarius Commission of the Food and Agriculture Organization of the United Nations (FAO) and the World Health Organization (WHO) develops food standards, guidelines and related texts for protecting consumer health and ensuring fair trade practices globally. The major part of the world's population lives in more than 160 countries that are members of the Codex Alimentarius. The Codex Standard on Infant Formula was adopted in 1981 based on scientific knowledge available in the 1970s and is currently being revised. As part of this process, the Codex Committee on Nutrition and Foods for Special Dietary Uses asked the ESPGHAN Committee on Nutrition to initiate a consultation process with the international scientific community to provide a proposal on nutrient levels in infant formulae, based on scientific analysis and taking into account existing scientific reports on the subject. ESPGHAN accepted the request and, in collaboration with its sister societies in the Federation of International Societies on Pediatric Gastroenterology, Hepatology and Nutrition, invited highly qualified experts in the area of infant nutrition to form an International Expert Group (IEG) to review the issues raised. The group arrived at recommendations on the compositional requirements for a global infant formula standard which are reported here.", "title": "Global standard for the composition of infant formula: recommendations of an ESPGHAN coordinated international expert group.", "claims": null}, {"metadata": {"year": 2010}, "authors": ["S. Zlotkin", "J. Siekmann", "A. Lartey", "Zhenyu Yang"], "summary": "The Codex Alimentarius is a collection of internationally recognized standards, codes of practice, guidelines, and other recommendations relating to foods, food production, and food safety. Among other functions, it is responsible for setting international standards for safety and hygiene. Codex food standards and guidelines directed at foods produced primarily for young infants and children have important implications for maintaining nutritional status and health, especially given the positioning of these products as components of established World Health Organization (WHO)/UNICEF-recommended feeding strategies. Recently, new products targeted at this age group (e.g., lipid-based nutrient supplements and micronutrient powders) have been produced and used, but these are not totally covered under existing Codex guidelines or standards. The objective of this paper is to review the role of the Codex process and specifically to suggest revisions to existing Codex guidelines on formulated complementary foods (Guidelines for Formulated Supplementary Foods for Older Infants and Young Children, CAC/GL 08\u20131991) to encompass this new category of fortified complementary foods and home fortificants. In reviewing the existing guidelines, potential areas for revision included the sections on the recommended nutrients in these foods and their intended use. Updating the Codex guidelines provides the opportunity to encourage production and use of new products for children and help ensure that such foods, when used as directed, do not interfere with breastfeeding. The revised guidelines would help governments develop national regulations covering all forms of formulated complementary foods. They would also lessen impediments to international trade by providing clear guidance for foods used in feeding programs and for young children, particularly in developing countries.", "title": "The Role of the Codex Alimentarius Process in Support of New Products to Enhance the Nutritional Health of Infants and Young Children", "claims": null}, {"metadata": {"year": 2011}, "authors": ["R. Dossa", "E. Ahouandjinou", "F. Houngbe"], "summary": "Infant feeding practices do not always fit with quantity and quality requirements,\u00a0leading to low expression of growth potential. In Benin, 43.1% of children less than6\u00a0months old are exclusively breastfed with 68% of children aged 6-8 months receiving\u00a0complementary food. The study aimed to produce infant flour from raw food\u00a0ingredients available in Bopa district and to test its acceptability by 6-12 months old children. In a first step of the study, formulation and determination of nutritional\u00a0characteristics of the infant flour occurred. A second step concerned acceptability\u00a0tests of gruel made from formulated infant flour. The study sample was composed of\u00a0sixty five mothers and their children. Children\ufffds acceptability test took place in the\u00a0morning for three consecutive days. The gruel was consumed ad libitum. Mothers\ufffd acceptability test consisted of appreciation of organoleptic characteristics of the gruel\u00a0and the infant flour processing.The infant flour was made of maize (65 %), bean (20\u00a0%) and peanut (15 %) and was manually processed. Chemical analysesshowed that it\u00a0contains 4.3% of moisture, 69.3% of carbohydrates, 15.1% of proteins, 10.7% of\u00a0lipids, less than 5% of crude fibres and 1.9% of ash. Its energy density (433.9 kcal/100g) was significantly greater than Codex Alimentarius standards (p<0.05).The\u00a0infant flour contained microbial germs up to 4.8log CFU/g which was closed to\u00a0maximum standard values. Total coliforms (1.7log CFU/g) were significantly lower\u00a0than standard values. The flour was yeast, mould and pathogen (Escherichia coli)\u00a0free. Hundred grams gruel was made from 40g of infant flour, 6g of malted maize and\u00a0250ml of stock of boiled greens leaves (Solanummacrocarpum). Dry matter content of\u00a0gruel was 19.2% and its energy density was 81.5 kcal/100g. Basedon the ratio of\u00a0intake and amount served, 83.3% of children accepted the gruel. However based on\u00a0the ratio of the amount of porridge consumed during the testto the amount usually\u00a0consumed by the children, 65.2% of the childrenaccepted the gruel. Mothers\ufffd appreciation of the gruel ranged from unpleasant to very pleasant with 40% as\u00a0pleasant. Sixty percent of mothers judged the infant flour processing as easy and\u00a0feasible. All mothers expressed their desire to feed their children with the gruel.\u00a0Improving nutritional status of their children motivated their decision. It is concluded\u00a0that integrating this infant flour in nutrition and counselling package targeted to\u00a0mothers may be of a great benefit to the children.", "title": "Evaluation of the suitability and acceptability of a newly designed infant flour for infant feeding in the district of Bopa in south of Benin", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Loba Sonia Euphrasie", "Gbakayoro Jean Brice", "Kouame Kouassi Appolinaire", "Grodji Gbogouri Albarin", "Brou Kouakou"], "summary": "The quality of infant flours used during the infant weaning period is of great importance. The purpose of this study was to formulate infantile flours to cover the nutritional needs of children during weaning to fight against child malnutrition. Two types of flours were formulated, one made from rice enriched with soy, palm oil and sugar and the other made from maize enriched with beans, palm oil and sugar. Amounts and ingredients were combined as recommended. The physico-chemical, microbiological and organoleptic characteristics of these flours were determined according to standard methods. The results obtained showed a satisfaction rate of over 100% for energy and protein compared to nutritional standards. The lipid levels obtained (12.40 \u00b1 0.99 and 9.65 \u00b1 0.01) respectively for the flours FRSHS and FMHHS were in the range of the recommended values and the fiber levels of the flours were less than 5 g, as recommended. The iron contents (11.19 \u00b1 0.12 and 14.27 \u00b1 0.07 mg / 100 g dry matter) and zinc (4.91 \u00b1 0.05 and 6.29 \u00b1 0.35 mg / 100 g of dry matter) flours were largely above the recommendation. The microbiological loads detected in these flours were below the microbiological criteria applicable to infant flours. All the boils prepared from the compound flours were appreciated. The acceptability of the FRSHS boil compared to that of the FMHHS boil by mothers for their children is justified by the color, aroma and taste of the FRSHS boils most appreciated by the mothers. These formulations could then be recommended for infants, thus contributing to the fight against child malnutrition. La qualite des farines infantiles utilisees pendant la periode de sevrage du nourrisson est d\u2019une grande importance. L\u2019objectif de cette etude est de mettre au point deux formulations de farine infantile dont l\u2019une fabriquee a partir de riz enrichi en soja, en huile de palme et en sucre (FRSHS) et l\u2019autre (FMHHS) a partir de mais enrichi en haricot, en huile de palme et en sucre. Pour ce fait, les doses des ingredients pour la formulation des deux farines (FRSHS et FMHHS) ont ete combinees selon lesrecommandations nutritionnelles.Lescaracteristiques physico-chimiques, microbiologiques et organoleptiques de ces farines ont ete determinees selon les methodes standards. Les resultats revelent un taux de satisfaction de plus de 100% pour l\u2019energie et les proteines compare aux normes nutritionnelles. Les teneurs respectives en lipides (12,40 \u00b1 0,99 % et 9,65 \u00b1 0,01%) pour les farines (FRSHS et FMHHS) sont comprises dans l\u2019intervalle des valeurs recommandees et les taux de fibres des farines sont inferieurs a 5 g, tel que recommande. Les teneurs en fer (11,19 \u00b1 0,12 et 14,27\u00b1 0,07 mg /100 g de matieres seche), de zinc (4,91\u00b1 0,05et 6,29 \u00b1 0,35 mg /100 g de matieres seche) et de calcium (358,194 \u00b1 0,09 et 343,87\u00b1 0,44) des farines sont au-dela de la recommandation. Aussi, les farinespresentent-elles de bonnes teneurs en la plupart des mineraux. Les charges microbiologiques detectees dans ces farines sont inferieures aux criteres microbiologiques applicables aux farines infantiles. Les bouillies preparees a partir des farines composees ont ete appreciees par les meres panelistes. Toutefois, la bouillie preparee a partir de la farine FRSHS a ete plus acceptee. L\u2019acceptabilite de la bouillie de FRSHS par rapport a celle de FMHHS par les meres pour leurs enfants se justifie par la couleur, l\u2019arome et le gout de la bouillie de farine FRSHS. Ces farines composees pourraient alors etre recommandees pour les nourrissons, contribuant ainsi a lutter contre la malnutrition infantile.", "title": "Formulations de Farines Compos\u00e9es Dont l\u2019une \u00e0 Base de Riz (Oryza Sativa) et L\u2019autre \u00e0 Base de Ma\u00efs (Zea Mays) Pour Enfants en \u00e2ge de Sevrage", "claims": null}, {"metadata": {"year": 2019}, "authors": ["B. Maxwell", "N. K. Valery", "Combo Agnan Marie-Michel", "Yao N\u2019zu\u00e9 Binjamin", "G. Dago"], "summary": "Aim: This study aimed to evaluate the nutritional quality of the infant flours offered to mothers received in the dietary service of the CHR of Daloa. \nIntroduction: Ivory Coast\u2019s membership in Scaling up Nutrition (SUN) is a momentum in a collective effort to improve the nutrition and nutritional status of the population.\u00a0 \nMethod\u00a0: For this purpose, analyses of biochemical compositions, in particular the levels of protein, fat and minerals in the proposed infant flours, were carried out.\u00a0 \nResults: The formulations of the flours proposed have high nutritional values. The protein content of compound flours increases proportionally with the amount of soy incorporated. Indeed, for FC2 and FC3 formulations, these contents are 17.12 \u00b1 0.19 g / 100 g (FC3) and 17.50 \u00b1 0.56 g / 100 g (FC2) with a rate of incorporation of 25% soy. In addition, the FC1 flour formulation enriched with peanuts is low in protein with a value of 8.69 \u00b1 0.11 g / 100 g. These flours also had mineral contents in accordance with WHO standards of calcium (> 125 mg / kg), iron (> 4 mg / kg) and zinc (> 0.8 mg / kg). In addition these formulations are highly digestible. \nConclusion: However, to use the proposed meal formulations as food for malnutrition, it would necessarily be necessary to supplement them with available local fruits and vegetables, rich in vitamins and minerals.", "title": "Nutritional Quality of Food Supplements for Children from 6 to 59 Months Proposed to the Dietary Service of Regional Hospital of Daloa (Ivory Coast)", "claims": null}, {"metadata": {"year": 2003}, "authors": ["Joint Fao"], "summary": "This publication contains guidance on the development and application of international food hygiene standards, which covers practices from primary production through to final consumption, highlighting key hygiene controls at each stage. It also contains guidance on the use and application of the Hazard Analysis and Critical Control Point (HACCP) system to promote food safety, as well as principles for the establishment and application of microbiological criteria for foods and the conduct of microbiological assessment.", "title": "Codex alimentarius: food hygiene basic texts.", "claims": null}, {"metadata": {"year": 2008}, "authors": ["\u738b\u65ed\u658c"], "summary": "The invention relates to nutrition powder and a preparation method thereof, in particular to infant nutrition corn flour and a preparation method thereof. The main points of the nutrition corn flour are as follows: each 100g of the nutrition corn flour contains 300mg to 600mg of calcium, 6.0mg to 10mg of iron, 2.5mg to 7.0mg of zinc, 300mg to 500mg of sodium, 20mu g to 50mu g of iodine, 10mg to 15mg of DHA, 1000IU to 4000IU of vitamin A, 200IU TO 600IU of vitamin D3, 200mu g to 260mu g of vitamin B1, and 200mu g to 260mu g of vitamin B2. The main points of the preparation method are as follows: sweet corn is taken as the main raw material; dry material prepared with the sweet corn as the main raw material is ground; an appropriate amount of calcium, iron, zinc, sodium, iodine, DHA, vitamin A, vitamin D3, vitamin B1 and vitamin B2 are added into the ground material to receive strengthened nutrition mixing. The infant nutrition corn flour is not only auxiliary food which can improve the nutrition structure of infants, but also the auxiliary food which can promote the infants to grow healthily.", "title": "Infantile nutrition maize flour and preparation thereof", "claims": null}, {"metadata": {"year": 2021}, "authors": ["A. Vasyukova", "K. Krivoshonok", "A. Alekseev", "V. I. Karpov"], "summary": "The article provides basic information about the functional properties of flour culinary products developed on the basis of rice and corn flour with additives of powders of vegetable origin: jerusalem artichoke, sea buckthorn, apples, carrots, tomato, paprika, dill, daminaria, and stevia extract was used as a sweetener. Balanced nutritionally adapted nutrition, corresponding to the physiological needs of schoolchildren, gender and individual characteristics of children aged 7\u201311 years, is necessary for the full development of physical and intellectual abilities. In this connection, the development of flour culinary products nutritionally adapted to the peculiarities of baby food, the optimization of the ingredient composition of flour culinary products, taking into account the technological properties of raw materials and biomedical recommendations for the diet of this category of nutritionists will contribute not only to achieving the best results in their development, physical and mental improvement, but will also prevent a number of alimentary-dependent diseases and conditions of this category of the population. This determines the main task of research and is relevant and promising. The purpose of the development of new specialized products is to obtain enriched gluten-free compositions for the production of high-quality pancake tape by an industrial method. The article presents data on the optimization of the ingredient composition of flour culinary products, considering the technological properties of raw materials and biomedical recommendations for the nutrition of schoolchildren of grades 1\u20134. The list of ingredients with immunomodulatory and antioxidant properties and the possibility of their use in the preparation of specialized flour culinary products for school meals is scientifically substantiated. Optimization was carried out using modern programs providing mathematical modeling of product formulations in the realized range of parameter changes. The qualitative and quantitative prescription composition is modeled in accordance with sanitary norms and requirements of the SanPiN 2.3/2.4.3590\u201320 requirements for children's food products, considering their physical exertion. The relationship between the mass fraction of protein components (rice and corn flour and egg white) and physico-chemical indicators of the quality of flour culinary products for the nutrition of schoolchildren has been established. It is experimentally proved that the use of the developed recipe compositions will contribute to an increase in the content of protein, fats, carbohydrates, vitamins, minerals, increase the biological value and the total content of water-soluble antioxidants in flour culinary products for the nutrition of students aged 7\u201311 years.", "title": "Influence of additions on the structure of the free fares", "claims": null}], "query": "What is the codex alimentarius recommendation for second-age infant flour?", "summary_abstract": "The Codex Alimentarius provides essential guidelines and standards for infant nutrition, particularly concerning second-age infant flour. According to Koletzko et al. (2010), the Codex Alimentarius Commission, in collaboration with international experts, is revising its standards for infant formula and complementary foods to ensure they meet current scientific knowledge and nutritional needs. This revision aims to encompass new products, such as lipid-based nutrient supplements and micronutrient powders, which are increasingly used but not fully covered by existing guidelines.\n\nZlotkin et al. (2010) emphasize the importance of updating the Codex guidelines for formulated supplementary foods for older infants and young children. They advocate for revisions that would include fortified complementary foods and home fortificants, ensuring that these products do not interfere with breastfeeding while promoting better nutritional outcomes for infants.\n\nResearch by Dossa et al. (2011) and Loba et al. (2019) highlights the nutritional composition of locally formulated infant flours. Dossa et al. developed an infant flour from maize, beans, and peanuts, which showed a higher energy density than Codex standards, while Loba et al. created rice and maize-based flours that exceeded energy and protein recommendations. Both studies indicate that these formulations can significantly contribute to addressing malnutrition in infants.\n\nMaxwell et al. (2019) further support the notion that infant flours can meet or exceed Codex standards for protein, fat, and minerals, suggesting that local formulations can be tailored to improve the nutritional status of children. The findings collectively underscore the necessity for Codex Alimentarius to adapt its recommendations to include a broader range of fortified and locally produced infant flours, ensuring they align with contemporary nutritional science and the needs of infants globally.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2012}, "authors": ["H. Son", "H. J. Kim", "C. K. Kim"], "summary": "To investigate the effect of resistance and endurance training on muscle proteome expression, samples of vastus lateralis from 10 physically active young men were analysed by 2-dimensional electrophoresis (2-DE) and matrix-assisted laser desorption ionization time-of-flight mass spectrometry (MALDI-TOF MS). Differential patterns of protein expression were determined after 4 weeks of endurance or resistance exercise training. Following endurance exercise training, carbonic anhydrase III immunoglobulin heavy chain, myosin heavy chain 1, titin, chromosome 12, and fructose-1,6-bisphosphatase 2 were up-regulated while pyruvate kinase 3 isoform, ubiquitin carboxyl-terminal hydrolase, and phosphoglucomutase were down-regulated. After the 4 weeks of resistance exercise training, five proteins, apolipoprotein A-IV precursor, microtubule-actin cross linking factor 1, myosin light chain, growth hormone inducible transmembrane protein, and an unknown protein were up-regulated and pyruvate kinase 3 isoform, human albumin, and enolase 3 were down-regulated. We conclude that endurance and resistance exercise training differently alter the expression of individual muscle proteins, and that the response of muscle protein expression may be associated with specific myofibre adaptations to exercise training. Proteomic studies represent one of the developing techniques of metabolism which may substantially contribute to new insights into muscle and exercise physiology.", "title": "THE EFFECT OF RESISTANCE AND ENDURANCE EXERCISE TRAINING ON MUSCLE PROTEOME EXPRESSION IN HUMAN SKELETAL MUSCLE", "claims": null}, {"metadata": {"year": 2002}, "authors": ["P. L. Kim"], "summary": "Muscle proteim are in a continuous state of recycling. This process involves a balance between synthesis and breakdown. These opposing processes dictate muscle protein gains and losses. Musck hypertrophy occurs when synthesis exceeds breakdown. In order for the accretion of new muscle proteins, a chronic state of net positive muscle protein balance (synthesis> breakdown) is required. Resistance exerdse is a potent stimulus of protein turnover and the combined effects of exercise and feeding have shown to be necessary for net protein anabolism. Resistance training has been reported to increase muscle strength and induce changes in skeletal muscle morphology. These positive strength adaptations include muscle fibre hypertrophy and a shift in fibre type from IJX to IIA. Previous investigations of resistance training-induced changes in muscle protein synthesis and fibre morphology have utilized cross-sectional or longitudinal, bilateral training designs. Thus, the purpose of this study was to investigate the effects of a progressive eight week unilateral leg resistance training program on skeletal muscle morphology, and resting and exercise-stimulated mixed muscle protein fractional synthesis rate (FSR). Eight young men performed two training sessions each week, and each session consisted of four sets of knee extension (KE) and four sets of leg press (LP) at 80% 1 repetition maximum (1 RM). Needle biopsies from the vastus lateralis muscle of the trained (T) leg were taken before and after training and analyzed for fibre composition, cross\u00ad sectional area (CSA), and myosin heavy chain (MHC) content. Muscle protein FSR was determined using a primed constant stable isotope infusion of [C6]-phenylalanine in both", "title": "Resistance Training-Induced Changes in Human Muscle Protein Synthesis and Fibre Morphology", "claims": null}, {"metadata": {"year": 2017}, "authors": ["B. Petriz", "C. Gomes", "J. Almeida", "G. P. de Oliveira", "Filipe M. Ribeiro", "R. Pereira", "O. L. Franco"], "summary": "Skeletal muscle plasticity and its adaptation to exercise is a topic that is widely discussed and investigated due to its primary role in the field of exercise performance and health promotion. Repetitive muscle contraction through exercise stimuli leads to improved cardiovascular output and the regulation of endothelial dysfunction and metabolic disorders such as insulin resistance and obesity. Considerable improvements in proteomic tools and data analysis have broth some new perspectives in the study of the molecular mechanisms underlying skeletal muscle adaptation in response to physical activity. In this sense, this review updates the main relevant studies concerning muscle proteome adaptation to acute and chronic exercise, from aerobic to resistance training, as well as the proteomic profile of natural inbred high running capacity animal models. Also, some promising prospects in the muscle secretome field are presented, in order to better understand the role of physical activity in the release of extracellular microvesicles and myokines activity. Thus, the present review aims to update the fast\u2010growing exercise\u2010proteomic scenario, leading to some new perspectives about the molecular events under skeletal muscle plasticity in response to physical activity. J. Cell. Physiol. 232: 257\u2013269, 2017. \u00a9 2016 Wiley Periodicals, Inc.", "title": "The Effects of Acute and Chronic Exercise on Skeletal Muscle Proteome", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Seung-Lyul Oh", "S. Oh"], "summary": "The aim was to examine resistance exercise-related genes after 8 weeks of resistance training. Thirty-two male Sprague-Dawley rats were divided into four groups: 4 weeks sedentary (4 wks CON, n=8), 8 weeks sedentary (8 wks CON, n=8), 4 weeks exercise training (4 wks REG, n=8), and 8 weeks exercise training (8 wks REG, n=8). The rats were trained to climb a 1-m vertical incline (85-degree), with weights secured to their tails. They climbed 10 times, 3 days per week, for 8 consecutive weeks. Skeletal muscle was taken from the flexor halucis longus after the exercise training. After separating the total RNA, large-scale gene expression was investigated by beadarray (Illumina RatRef-12 Expression BeadChip) analysis, and qPCR was used to inspect the beadarray data and to analyze the RNA quantitatively. The detection p-value for the genes was p\uff1c0.01, the M-value {M=log\u2082(condition)-log\u2082(reference)} was \uff1e1.0, and the DiffScore was \uff1e20. In total, the expression of 30 genes significantly increased 4 weeks after the exercise training, and the expression of six genes decreased. At 8 weeks, the expression of five genes significantly increased and that of 12 decreased. Several genes are potentially involved in resistance exercise and muscle hypertrophy, including 1) regulation of cell growth (IGFBP1, PLA2G2A, OKL38); 2) myogenesis (CSRP3); 3) tissue regeneration and muscle development (MUSTN1, MYBPH); 4) hypertrophy (CYR61, ATF3, NR4A3); and 5) glucose metabolism (G6PC, PCK1). These results may help to explain previously reported physiological changes of the skeletal muscle and suggest new avenues for further investigation.", "title": "Effect of Resistance Training on Skeletal Muscle Gene Expression in Rats", "claims": null}, {"metadata": {"year": 2016}, "authors": ["A. Padr\u00e3o", "R. Ferreira", "F. Amado", "R. Vitorino", "J. Duarte"], "summary": "Exercise training has been recommended as a nonpharmacological strategy for the prevention and attenuation of skeletal muscle atrophy in distinct pathophysiological conditions. Despite the well\u2010established phenotypic alterations, the molecular mechanisms underlying exercise\u2010induced skeletal muscle remodeling are poorly characterized. Proteomics based on mass spectrometry have been successfully applied for the characterization of skeletal muscle proteome, representing a pivotal approach for the wide characterization of the molecular networks that lead to skeletal muscle remodeling. Nevertheless, few studies were performed to characterize the exercise\u2010induced proteome remodeling of skeletal muscle, with only six research papers focused on the cross\u2010talk between exercise and pathophysiological conditions. In order to add new insights on the impact of distinct exercise programs on skeletal muscle proteome, molecular network analysis was performed with bioinformatics tools. This analysis highlighted an exercise\u2010related proteome signature characterized by the up\u2010regulation of the capacity for ATP generation, oxygen delivery, antioxidant capacity and regulation of mitochondrial protein synthesis. Chronic endurance training up\u2010regulates the tricarboxylic acid cycle and oxidative phosphorylation system, whereas the release of calcium ion into cytosol and amino acid metabolism are the biological processes up\u2010regulated by a single bout of exercise. Other issues as exercise intensity, load, mode and regimen as well as muscle type also influence the exercise\u2010induced proteome signature. The comprehensive analysis of the molecular networks modulated by exercise training in health and disease, taking in consideration all these variables, might not only support the therapeutic effect of exercise but also highlight novel targets for the development of enhanced pharmacological strategies.", "title": "Uncovering the exercise\u2010related proteome signature in skeletal muscle", "claims": null}, {"metadata": {"year": 2017}, "authors": ["T. Groennebaek", "K. Vissing"], "summary": "Skeletal muscle metabolic and contractile properties are reliant on muscle mitochondrial and myofibrillar protein turnover. The turnover of these specific protein pools is compromised during disease, aging, and inactivity. Oppositely, exercise can accentuate muscle protein turnover, thereby counteracting decay in muscle function. According to a traditional consensus, endurance exercise is required to drive mitochondrial adaptations, while resistance exercise is required to drive myofibrillar adaptations. However, concurrent practice of traditional endurance exercise and resistance exercise regimens to achieve both types of muscle adaptations is time-consuming, motivationally demanding, and contended to entail practice at intensity levels, that may not comply with clinical settings. It is therefore of principle interest to identify effective, yet feasible, exercise strategies that may positively affect both mitochondrial and myofibrillar protein turnover. Recently, reports indicate that traditional high-load resistance exercise can stimulate muscle mitochondrial biogenesis and mitochondrial respiratory function. Moreover, fatiguing low-load resistance exercise has been shown capable of promoting muscle hypertrophy and expectedly entails greater metabolic stress to potentially enhance mitochondrial adaptations. Consequently, fatiguing low-load resistance exercise regimens may possess the ability to stimulate muscle mitochondrial adaptations without compromising muscle myofibrillar accretion. However, the exact ability of resistance exercise to drive mitochondrial adaptations is debatable, not least due to some methodological challenges. The current review therefore aims to address the evidence on the effects of resistance exercise on skeletal muscle mitochondrial biogenesis, content and function. In prolongation, a perspective is taken on the specific potential of low-load resistance exercise on promoting mitochondrial adaptations.", "title": "Impact of Resistance Training on Skeletal Muscle Mitochondrial Biogenesis, Content, and Function", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Juha P. Ahtiainen"], "summary": "High muscle contraction forces that lead to gains in muscle function, size, and strength characterize resistance exercise training. The purpose of this chapter is to outline the adaptations in myofiber size and metabolism that occur by stimuli of hormones and local growth factors, mechanical and metabolic stress of muscle tissue, and myofibrillar disruptions induced by a resistance exercise bout. The chapter will highlight the network of intracellular pathways (including mTOR signaling) that ultimately lead to increases in gene expression and protein synthesis. Accumulation of acute exercise responses by systematic training over time modulate the muscle proteome that can be observed as changes in skeletal muscle phenotype.", "title": "Physiological and Molecular Adaptations to Strength Training", "claims": null}, {"metadata": {"year": 2006}, "authors": ["Yifan Yang", "B. Jemio\u0142o", "S. Trappe"], "summary": "The purpose of this study was to characterize changes in mRNA expression of select proteolytic markers in human slow-twitch [myosin heavy chain (MHC) I] and fast-twitch (MHC IIa) single skeletal mu...", "title": "Proteolytic mRNA Expression in Response to Acute Resistance Exercise in Human Single Skeletal Muscle Fibers", "claims": null}], "query": "what are the effects of resistance exercise on the skeletal muscle proteome?", "summary_abstract": "The collective findings from the reviewed papers highlight the significant effects of resistance exercise on the skeletal muscle proteome, demonstrating both qualitative and quantitative changes in protein expression and gene regulation.\n\nSon et al. (2012) conducted a study that revealed distinct patterns of protein expression in response to resistance training, identifying specific proteins that were up-regulated, such as myosin light chain and apolipoprotein A-IV precursor, while others like pyruvate kinase 3 isoform were down-regulated. This suggests that resistance exercise induces specific adaptations in muscle proteins that are crucial for muscle function and hypertrophy.\n\nKim (2002) emphasized the importance of resistance exercise in promoting a net positive muscle protein balance, which is essential for muscle hypertrophy. The study indicated that resistance training not only enhances muscle strength but also leads to morphological changes in skeletal muscle, including fiber type shifts and increased cross-sectional area.\n\nPetriz et al. (2017) provided a broader perspective on skeletal muscle plasticity, noting that resistance exercise leads to adaptations in the muscle proteome that are critical for performance and health. They highlighted advancements in proteomic tools that allow for a deeper understanding of the molecular mechanisms involved in these adaptations.\n\nOh and Oh (2013) focused on gene expression changes following resistance training, identifying several genes associated with muscle hypertrophy and metabolism that were significantly up-regulated after training. This underscores the molecular basis of how resistance exercise influences muscle growth and function.\n\nAhtiainen (2018) discussed the role of mechanical and metabolic stress induced by resistance exercise in modulating the muscle proteome. The chapter outlined how these stresses activate intracellular pathways, such as mTOR signaling, leading to increased protein synthesis and changes in muscle phenotype.\n\nGroennebaek and Vissing (2017) explored the potential of resistance exercise to stimulate mitochondrial adaptations, traditionally associated with endurance training. They suggested that high-load resistance exercise could enhance mitochondrial biogenesis, indicating that resistance training may have broader implications for muscle metabolism than previously understood.\n\nIn summary, resistance exercise significantly alters the skeletal muscle proteome through specific protein expression changes, gene regulation, and adaptations in metabolic pathways. These findings collectively illustrate the complex interplay between resistance training and muscle biology, highlighting its importance for muscle health and performance.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2015}, "authors": ["Luyddy Pires", "J. Melchior", "E. Ferruzzi"], "summary": "Introduction: Discovered in the 90s by Rizzolati and collaborators in the pre-motor cortex of Rhesus monkeys, mirror neurons are a type of neuron that responds both during a practical and directed action as in the observation of this same action being performed by another individual. Neurology says that this system should not be considered as a separate and self-sufficient system, but as an intrinsic mechanism of the various cortical areas, such as inferior frontal gyrus, temporal and parietal lobes and cingulate cortex. Therefore, it allows immediate and unconscious understanding of the observed activity, allowing even its reproduction. In this way, it is postulated that this neuronal type is directly linked to the autism spectrum disorders. Objective: Analyze the autism by the premise of dysfunction of the mirror neuron system. Materials and methods: Systematic review of current literature from the PubMed database. Discussion: The ability of mirroring actions provided by the mirror neuron system (MNS) is closely linked to the mechanisms of learning. This imitation can also be directly related to the development of empathy, an affective response that allows the emotional processing, giving the ability to sentimental approach to other people. Starting from these premises, these properties are directly correlated to the autism spectrum disorders (ASD), which are difficulties in communication and social interaction, restriction in understanding and mechanical and mental reproduction of new expressions or feelings, as well as limitation in the perception of intent with who they relate. In order to prove this correlation between the MNS and the ASD, comparative experiments were performed in patients with diagnostic confirmation of autism and healthy patients. Exemplifying with the experiments of Schunke et al. and Swoden et al., in the first instance, they have measured the reaction time to imitation of simple, meaningless movements (lifting a finger, a dot or both). As a second experiment, subjects were asked to answer as quickly as possible, to two sound stimuli of different tones (500 or 1000 Hz), and lift the index finger or little finger according to each tone (pattern previously stipulated). The third test blended visual and auditory stimuli so that images were reproduced as the first experiment, but the answer should be given according to the sound stimulus. In all tests, the results proved no significant variation between groups of healthy patients and autistic patients. It was noticed that there was \"interference effects\" in the tests when visual and additives stimuli have been presented together, but present in both groups. Conclusion: Since the emergence of the study of mirror neurons, these were quickly linked to autism spectrum disorders by its deficit in order to, a priori, explain all the symptoms of this disease. However, new standardized and reproducible studies have shown that the mirror neuron system is fully active among the ASD patients. This correlation is consistent with the fact that autism is much more complex than expected after the discovery of mirror neurons.", "title": "UNBREAKING MIRRORS: RELATION BETWEEN AUTISM AND MIRROR NEURONS", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Lieselot Ruysschaert", "P. Warreyn", "J. R. Wiersema", "A. Oostra", "H. Roeyers"], "summary": "Investigating the underlying neural mechanisms of autism spectrum disorder (ASD) has recently been influenced by the discovery of mirror neurons. These neurons, active during both observation and execution of actions, are thought to play a crucial role in imitation and other social\u2010communicative skills that are often impaired in ASD. In the current electroencephalographic study, we investigated mu suppression, indicating neural mirroring in children with ASD between the ages of 24 and 48 months and age\u2010matched typically developing children, during observation of goal\u2010directed actions and non\u2010goal\u2010directed mimicked hand movements, as well as during action execution. Results revealed no significant group differences with significant central mu suppression in the ASD children and control children during both execution and observation of goal\u2010directed actions and during observation of hand movements. Furthermore, no significant correlations between mu suppression on one hand and quality of imitation, age, and social communication questionnaire scores on the other hand were found. These findings challenge the \u201cbroken mirror\u201d hypothesis of ASD, suggesting that impaired neural mirroring is not a distinctive feature of ASD. Autism Res 2014, 7: 197\u2013 206. \u00a9 2014 International Society for Autism Research, Wiley Periodicals, Inc.", "title": "Exploring the Role of Neural Mirroring in Children with Autism Spectrum Disorder", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Bruce K. Dixon"], "summary": "C H I C A G O \u2014 The impaired social interaction and communication characteristic of autistic children is the result of abnormally functioning mirror neurons in the brain, judging from the findings of a novel imaging study. A controlled study of 25 children revealed those with autism have increased gray matter in several areas of the parietal lobes, Manzar Ashtari, Ph.D., said at the annual meeting of the Radiological Society of North America. \u201cWhat we found was that the larger the brain matter, the more restrictive the child\u2019s interest and the more stereotypical his or her behavior, indicating the increased gray matter in autistic children is abnormal,\u201d said Dr. Ashtari, senior neuroscientist at Children\u2019s Hospital of Philadelphia. \u201cThis suggests that the inability of autistic children to relate to people and life situations in an ordinary way may result from an abnormally functioning mirror neuron system,\u201d she said. Mirror neurons are brain cells that are active both when an individual is performing an action and experiencing an emotion or sensation, and when that individual witnesses the same actions, emotions, and sensations in others, Dr. Ashtari explained. \u201cMirror neurons were first discovered in the macaque monkey, and there is a similar system in the human brain,\u201d she said, adding that the mirror neuron system is part of the motor system and plays an essential role in controlling our own actions. The \u201cbroken mirror\u201d theory of autism, which was first proposed about a decade ago, argues that dysfunction of the mirror neuron system is a root cause of social disability in autism. The study led by Dr. Ashtari was conducted at the Fay J. Lindner Center for Autism, North Shore\u2013Long Island Jewish Health System, Bethpage, N.Y., and involved 13 boys diagnosed with high-functioning autism or Asperger syndrome who had IQs greater than 70, and 12 healthy controls. The subjects, average age 11 years, underwent diffusion tensor imaging (DTI), a technique that tracks the movement of water molecules in the brain. Although DTI traditionally is used to study the brain\u2019s white matter and fiber content, Dr. Ashtari\u2019s team applied it to the assessment of gray matter by employing apparent diffusion coefficient based morphometry, which highlights brain regions with changes in gray matter volume. In addition to the gray matter abnormalities linked to the mirror neuron system, the investigators reported that the amount of gray matter in the left parietal area correlated with higher IQs in the control group but not in the autistic children. While this finding was interesting, said Dr. Ashtari, the difference did not reach statistical significance. \u201cHowever, this does suggest that the gray matter in children with autism is dysfunctional.\u201d Dr. Antonia Hamilton doubts the \u201cbroken mirror\u201d theory. \u201cI am skeptical of the mirror neuron\u2013autism link, and the Ashtari study does nothing to change my mind,\u201d she said in an interview. In her own study, Dr. Hamilton reported that children with autism do not suffer general imitation impairment or a global mirror neuron system deficit (Neuropsychologia 2007;45:1859-68). \u201cMirror neurons are active any time you perform an action with your own hand. When you pick up a cup of coffee, or see another person picking up a cup of coffee, the same neurons are involved,\u201d said Dr. Hamilton, a lecturer at the School of Psychology, University of Nottingham (England). \u201cMy experiment found that autistic children do fine when it comes to these practical, goaloriented actions; however, they do not do well with social actions that involve imitation, such as smiling or waving at another person,\u201d she explained. Dr. Hamilton studied 25 children with an independent clinical diagnosis of autism or autism spectrum disorder (ASD). The group had a mean age of 8 years and a mean verbal mental age of just over 4 years and were compared with 29 controls. Children were tested in their ability to copy the experimenter\u2019s hand movement to a target location on a table top, using mirror imitation. The investigators found no evidence for differences in performance between the ASD group and the matched controls. Both showed the typical pattern of hand errors on contralateral trials. \u201cWe can conclude that typical and autistic children have the same tendency to imitate the goal of another person\u2019s action,\u201d the scientists said, noting the concurrency of their results with previous studies. In a second experiment, 23 children with ASD and 31 controls completed a grasp imitation and motor planning task. \u201cMotor planning is known to rely on the frontoparietal circuit which makes up the mirror neuron system, so the [autistic mirror neuron dysfunction, or] AMND predicts poor performance in autism spectrum disorder, which was not found,\u201d they wrote. In another experiment, the children with autism showed no impairment in gesture recognition, and in fact performed better than did the matched controls. The authors concluded that their data are not compatible with the hypothesis of an action representation deficit or mirror neuron deficit in children with autism spectrum disorder. \u201cThe broken-mirror-in-autism idea is a very appealing hypothesis which has received a lot of press in the last few years, despite the fact that there [is a scarcity of] hard data to support it,\u201d Dr. Hamilton said in an interview. \u201cAlso, none of the studies of imitation in autism\u2014 which claim to support the mirror neuron idea\u2014have really shown that the problem is in the mirror neurons themselves, rather than some other social process which controls the mirror neurons,\u201d she added. \u25a0 Yellow areas highlight clusters of increased gray matter in the right and left parietal cortex as seen on DTI. R A D IO L O G IC A L S O C IE T Y O F N O R T H A M E R IC A", "title": "Abnormal Mirror Neurons May Impair Social Skills", "claims": null}, {"metadata": {"year": 2006}, "authors": ["Sarah  Archibald"], "summary": "The social deficits associated with autism, which include difficulty communicating and understanding environmental cues, such as facial expression, have been extensively reported. However, the neural mechanisms behind the disorder remain in question. One proposed culprit is the mirror neuron system, which is thought to be involved in interpreting the emotions of others. Further support for this idea has now been provided by a group of US scientists, who have shown that activation of the mirror neuron system is much reduced in the brains of children with autism. Neuroscientists believe that mirror neurons fire in response to the facial expressions of others, allowing our brains to \u2018mirror\u2019 their actions and so understand their feelings. Mirella Dapretto, who led the research at the University of California, Los Angeles, USA, said, \u201cThe mirroring mechanism may underlie the remarkable ability to read others\u2019 emotional states from a mere glance\u201d (Guardian, 6 December 2005). \u201cOur findings suggest that a dysfunctional mirror neuron system may underlie the social deficits observed in autism. This is exciting because we finally have an account that can explain all core symptoms of this disorder\u201d (Guardian). The researchers observed not only that activation in the brain area containing mirror neurons \u2014 the inferior frontal gyrus pars opercularis \u2014 was reduced in children with autism, but also that the extent of the decrease in activation correlated with the severity of symptoms. Michael Rutter of the Institute of Psychiatry, London, UK, agreed that \u201cThe general notion of linking mirror neurons with the social deficit in autism is quite reasonable\u201d (BBC News Online, 5 December 2005). However, he also observed \u201c...we need more research into the brain systems that might be involved. These might involve mirror neurons, but we need more studies\u201d (BBC News Online). Sarah Archibald R E S E A R C H H I G H L I G H T S", "title": "Mirror image", "claims": null}, {"metadata": {"year": 2013}, "authors": ["C. Andrade", "M. Pond\u00e9"], "summary": "The objective of this study was to conduct a systematic review of experimental studies performed to assess the role of mirror neurons in the pathophysiology of autism. Four papers reported that areas of mirror neurons (MN) were under-active in autistic patients, giving weight to the theory of MN as a cause of ASD. Three papers indicated that MN were activated during the proposed activities, advocating atypical activation of MN, but not necessarily hypoactivation of these areas. One of the articles reported that only part of the MN system or just those areas of interconnected neurons are dysfunctional in autism. Analysis of the selected studies showed a correlation between dysfunction of the mirror neuron system and the main symptoms of autism such as deficits in social cognition, complete absence or a reduction in the individual\u2019s interaction with his/her social environment and a failure in the neural mechanisms of imitation.", "title": "A SYSTEMATIC REVIEW OF THE INFLUENCE OF MIRROR NEURONS IN AUTISM SPECTRUM DISORDERS", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Ruth  Raymaekers", "Jan Roelf Wiersema", "Herbert  Roeyers"], "summary": "Individuals with Autism Spectrum Disorder (ASD) are characterised by an impaired imitation, thought to be critical for early affective, social and communicative development. One neurological system proposed to underlie this function is the mirror neuron system (MNS) and previous research has suggested a dysfunctional MNS in ASD. The EEG mu frequency, more precisely the reduction of the mu power, is considered to be an index for mirror neuron functioning. In this work, EEG registrations are used to evaluate the mirror neuron functioning of twenty children with high functioning autism (HFA) between 8 and 13 years. Their mu suppression to self-executed and observed movement is compared to typically developing peers and related to age, intelligence and symptom severity. Both groups show significant mu suppression to both self and observed hand movements. No group differences are found in either condition. These results do not support the hypothesis that HFA is associated with a dysfunctional MNS. The discrepancy with previous research is discussed in light of the heterogeneity of the ASD population.", "title": "EEG study of the mirror neuron system in children with high functioning autism", "claims": null}, {"metadata": {"year": 2006}, "authors": ["Mirella  Dapretto", "Mari S Davies", "Jennifer H Pfeifer", "Ashley A Scott", "Marian  Sigman", "Susan Y Bookheimer", "Marco  Iacoboni"], "summary": "To examine mirror neuron abnormalities in autism, high-functioning children with autism and matched controls underwent fMRI while imitating and observing emotional expressions. Although both groups performed the tasks equally well, children with autism showed no mirror neuron activity in the inferior frontal gyrus (pars opercularis). Notably, activity in this area was inversely related to symptom severity in the social domain, suggesting that a dysfunctional 'mirror neuron system' may underlie the social deficits observed in autism.", "title": "Understanding emotions in others: mirror neuron dysfunction in children with autism spectrum disorders", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Raphael  Bernier", "Geraldine  Dawson"], "summary": "Autism spectrum disorders are characterized by impairments in the social, communication, and behavioral domains. Impairments in social cognition are considered a core symptom of the disorder. Dysfunction of an observation/execution matching system (the mirror neuron system) has been proposed to serve as a neural mechanism explaining the deficits in social cognition found in autism. Through direct matching of observed and executed behavior an individual can directly experience an internal representation of another\u2019s actions, and therefore, another\u2019s feelings, goals, and intentions. Dysfunction of this system would hinder this process. In this chapter, Bernier and Dawson review findings from behavioral research examining deficits in three areas of social cognition in individuals with autism: imitation, empathy, and theory of mind. They then discuss research implicating mirror neuron dysfunction in autism spectrum disorders; and explore the mirror neuron theory of autism. Autism research utilizing functional and structural brain imaging, electroencephalography, and transcranial magnetic stimulation to study the mirror neuron system is reviewed. Results from these studies suggest dysfunction of the mirror neuron system in both children and adults with autism. The mirror neuron theory of autism proposes that dysfunction of the execution/observation matching system interferes with the acquisition of internal representation of others\u2019 observed behavior, expressions, movements, and emotions. This precludes the individual with autism having an immediate, direct experience of the other through this internal representation. Social impairments, including impairments in imitation, empathy, and theory of mind, are hypothesized to cascade from this lack of immediate, experiential understanding of others in the social world.", "title": "The Role of Mirror Neuron Dysfunction in Autism", "claims": null}], "query": "What is the relationship between mirror neurons and autism in children?", "summary_abstract": "The relationship between mirror neurons and autism in children has been a topic of considerable debate and research. The mirror neuron system (MNS), which is believed to play a crucial role in social cognition, imitation, and empathy, has been linked to the social deficits observed in autism spectrum disorders (ASD). However, findings from various studies present a complex picture.\n\nLuyddy Pires et al. (2015) conducted a systematic review and concluded that while the MNS is often associated with autism due to its role in social interaction and empathy, recent studies indicate that the MNS is active in individuals with ASD. They found no significant differences in the ability to imitate simple actions between autistic and typically developing children, suggesting that the MNS may not be as dysfunctional in autism as previously thought.\n\nSimilarly, Ruysschaert et al. (2014) investigated mu suppression, a measure of neural mirroring, in children with ASD and found no significant differences compared to typically developing peers during both action observation and execution. This challenges the \"broken mirror\" hypothesis, which posits that a dysfunctional MNS is a hallmark of autism.\n\nIn contrast, Dapretto et al. (2006) reported that high-functioning children with autism showed no activity in the inferior frontal gyrus, a key area of the MNS, when imitating emotional expressions. This lack of activation correlated with the severity of social deficits, suggesting that a dysfunctional MNS may underlie some social impairments in autism.\n\nFurther supporting the notion of MNS dysfunction, Archibald (2006) highlighted findings that indicated reduced activation of the MNS in children with autism, which could explain their difficulties in interpreting others' emotions. However, this view is contested by researchers like Antonia Hamilton, who argue that the evidence does not definitively support a global mirror neuron deficit in autism (Dixon, 2008).\n\nOverall, while some studies suggest a link between MNS dysfunction and the social deficits characteristic of autism, others indicate that the MNS may function adequately in autistic individuals. The relationship remains complex, with ongoing research needed to clarify the role of mirror neurons in autism and to understand the underlying neural mechanisms that contribute to the disorder's social challenges.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2021}, "authors": ["Juntao  Shao", "Guilian  Shi", "Zhengqin  Qi", "Jingjing  Zheng", "Shigao  Chen"], "summary": "Ultrasound elastography is a modern imaging technique that has developed rapidly in recent years. It enables objective measurement of tissue stiffness, a physical property intuitive to the human sense of touch. This novel technology has become a hotspot and plays a major role in scientific research and academic practice. Presently, ultrasound elastography has been used in the identification of benign and malignant tumors in superficial organs, such as breast and thyroid, providing clinically accurate diagnosis and treatment. The method has also been widely used for the liver, kidney, prostate, lymph nodes, blood vessels, skin and muscle system. In the application of cervical lesions, ultrasound elastography can distinguish normal cervix from abnormal cervix and differentiate benign from malignant lesions. It can significantly improve the diagnostic specificity for cervical cancer and is also useful for assessing infiltration depth and stage of cervical cancer, as well as predicting chemoradiotherapy treatment response. For cervical evaluation during pregnancy, ultrasound elastography is useful for assessing cervical softening and predicting premature delivery and outcome of induced labor. This article reviews the principles of ultrasound elastography as well as the current status and limitations in its application for cervical lesions and the cervix during pregnancy.", "title": "Advancements in the Application of Ultrasound Elastography in the Cervix.", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Weiren Liang"], "summary": "Elastography is a new technique for noninvasive evaluation of tissue resistance to external hardness and deformation, which plays an increasingly important role in the diagnosis and differential diagnosis of thyroid, breast, liver, prostate and other organs in recent years.This paper reviews the application progress of elastography in various organs. \n \nKey words: \nUltrasonography;\u00a0Elasticity Imaging techniques;\u00a0Thyroid diseases;\u00a0Breast diseases;\u00a0Liver diseases;\u00a0Prostatism;\u00a0Genital diseases, female;\u00a0Review", "title": "Progress of application of real-time ultrasound elastography in clinical", "claims": null}, {"metadata": {"year": 2017}, "authors": ["S. Phatak", "G. Marfani", "Nipun Gupta"], "summary": "Elasticity Imaging, Elastography, Sonoelastography, ABSTRACT Ultrasonic elastography (real-time elastography, sonoelastography) is a new improvement of ultrasound technique and one of the various imaging modalities in the last few years which are being used for characterizing soft tissue masses of breast, thyroid, and lymph nodes. Musculoskeletal pathology was one of the early applications of sonoelastography, but the method is not yet standardized. Other new horizons of applications include Liver, Prostate, obstetrics and gynaecology. We are presenting a pictorial essay of elastography cases in Obstetrics and gynaecology seen in our institution. Dr. Suresh Phatak Professor and Head of department, Dept. of Radio-diagnosis, Jawaharlal Nehru Medical College, Sawangi (Meghe), Wardha, Maharashtra, India 442001", "title": "ROLE OF STRAIN ELASTOGRAPHY IN OBSTETRICS AND GYNECOLOGY: PICTORIAL ESSAY OF OUR INITIAL EXPERIENCE", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Liu Qing-qin"], "summary": "Ultrasound elastography is a new technology imaging according to the different elastic coefficient of different tissues,it makes up the shortage of conventional ultrasound,and become a hot research topic in the field of medical imaging. Ultrasound elastography has been widely used in breast diseases,in recent years,reports about other organs increased gradually,such as thyroid,prostate and liver,and shows the growing advantage in the diagnosis and differential diagnosis of various diseases. This paper review the clinical application of ultrasound elastography in these tissues and organs and related development in recent years.", "title": "Research Progress and Clinical Application of Ultrasound Elasticity Imaging Technology", "claims": null}, {"metadata": {"year": 2006}, "authors": ["A. Basarab", "E. Royer", "P. Boissieu", "P. Delachartre"], "summary": "The purpose of elastography is to characterize from mechanical point of view a material (the soft biological tissues in the medical field). Ultrasound elastography is based on the comparison of ultrasound images, when the material is submitted under axial force (compression). This work presents the advances in the instrumentation of an ultrasound scanner, to acquire a sequence of images which will be processed in the aim of elastography. First, motion estimation between two or more images is processed. The estimated displacement gives the possibility to obtain detailed deformation elastograms. Thus, the presence of a hard inclusion (that simulates a tumor a pathological tissue) within a phantom mimicking soft tissues can be more easily detected, which may lead to a better cancer diagnosis.", "title": "Advances in Ultrasound Elastography", "claims": null}, {"metadata": {"year": 2009}, "authors": ["J. Bonner"], "summary": "Elastography is one of the emerging technologies on display at ECR 2009, reflecting the growing importance of imaging techniques that compare the inherent stiffness of healthy and abnormal tissues in advancing the diagnostic value of ultrasound. Elastography\u2019s clinical applications were discussed at two separate satellite symposia on Saturday and Sunday.", "title": "Emergence of elastography gives renewed impetus and vigor to ultrasound market", "claims": null}, {"metadata": {"year": 2014}, "authors": ["S. Dudea"], "summary": "Definitely, contrast and elastography have been the two words prevailing in the ultrasonography research literature of the last decade. Of these, elastography did cast a special charm over the researchers, due to its ever increasing number of technological approaches. A major role was also played by the adaptability of the method(s) to virtually all applications of ultrasonography and even the opening of novel domains, such as diffuse organ structure assessment. No wonder that dedicated radiologists, mostly breast imaging experts, as well as specialists in gastroenterology, endocrinology, internal medicine etc., were all involved in intensive research and publication. Truly, elastography boosted ultrasound related publications and rapidly expanded the field of knowledge. Recently, solid publications compacted available knowledge into guidelines that represent a stage benchmark. The most notorious are the EFSUMB guidelines on the clinical use of ultrasound elastography [1,2]. So, after a decade of clinical usage, has elastography come to the age of ripening? The answer to this question requires a graded approach. Overall, the answer is, definitely , no. Or, at least, not in all domains. Extensive work and widespread experience allow for expert reviews and statements under the form of guidelines for a specific, well documented application of the technique(s). On the other hand, new, potential applications are under permanent scrutiny. Some of the papers published in this issue of the journal richly substantiate these assertions. The knowledge gathered by four groups in a specific application of elastography-this being liver disease-allows for the work up of national guidelines and recommendations [3]. This ambitious project is justified both by recent technological developments and by the sizeable number of publications of the authors. Even in this field, serendipitous findings may occur [4]. The applications of elastography in the study of the musculoskeletal system and even more superficial structures , such as skin, are surprisingly sparse. Some of the publications in this issue highlight potential applications, although reported results are, insofar, negative [5,6]. So why is elastography angel and demon? It must relate with the way we understand clinical medicine and imaging. Not only does elastography translate information addressing our least developed sense (tact) into coherent data addressing our most evolved sense (sight), but it also provides figures. Therefore, not only do we need to understand and interpret this \" visual tact \" and many physical terms we were not aware of but we must also connect this to the figures provided \u2026", "title": "Has Ultrasound Elastography come to the age of ripening?", "claims": null}, {"metadata": {"year": 2018}, "authors": ["B. Yasmeen"], "summary": "Abstract not available Northern International Medical College Journal Vol.9(1) July 2017: 243-244", "title": "Elastography-One Step Ahead", "claims": null}], "query": "summary the current advance of elastography in obgyn field in the past 3 months", "summary_abstract": "Recent advancements in elastography within the field of obstetrics and gynecology (OBGYN) have highlighted its growing significance as a non-invasive imaging technique for assessing tissue stiffness, particularly in relation to cervical health and pregnancy outcomes. Shao et al. (2021) emphasize that ultrasound elastography has become instrumental in differentiating between normal and abnormal cervical conditions, enhancing diagnostic specificity for cervical cancer. This technique not only aids in identifying malignancies but also plays a crucial role in evaluating the infiltration depth and staging of cervical cancer, as well as predicting responses to chemoradiotherapy. Furthermore, during pregnancy, elastography is valuable for assessing cervical softening, which can help predict premature delivery and the outcomes of induced labor.\n\nLiang (2020) supports this by noting the increasing role of elastography in diagnosing various conditions across multiple organs, including its applications in OBGYN. The technique's ability to provide objective measurements of tissue properties is particularly beneficial in clinical settings, where accurate diagnosis is paramount.\n\nPhatak et al. (2017) also contribute to this discourse by presenting cases that illustrate the application of elastography in OBGYN, reinforcing its relevance in characterizing soft tissue masses and enhancing diagnostic capabilities in this specialty.\n\nOverall, the collective findings from these studies indicate that elastography is rapidly advancing in the OBGYN field, offering promising applications for improving diagnostic accuracy and patient management, particularly concerning cervical health and pregnancy-related assessments.", "summary_extract": null}, {"papers": [{"metadata": {"year": 1993}, "authors": ["H. Brink"], "summary": "Validity and reliability are key aspects of all research. Meticulous attention to these two aspects can make the difference between good research and poor research and can help to assure that fellow scientists accept findings as credible and trustworthy. This is particularly vital in qualitative work, where the researcher\u2019s subjectivity can so readily cloud the interpretation of the data, and where research findings are often questioned or viewed with scepticism by the scientific community.", "title": "Validity and reliability in qualitative research.", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Mohamed Ezzat Khamis Amin", "Lotte Stig N\u00f8rgaard", "Afonso M Cavaco", "Matthew J Witry", "Lisa  Hillman", "Alina  Cernasev", "Shane P Desselle"], "summary": "Spurred by the value it can add, the use of qualitative research methods has been steadily growing by social pharmacy researchers around the globe, either separately or as part of mixed methods research projects. Given this increase, it is important to provide guidance to assist researchers in ensuring quality when employing such methods. This commentary addresses both theoretical fundamentals as well as practical aspects of establishing quality in qualitative social pharmacy research. More specifically, it provides an explanation of each of the criteria of trustworthiness proposed by Lincoln and Guba (credibility, transferability, dependability and confirmability) and different techniques used in establishing them. It also provides a brief overview of authenticity, a more recent and less widely used set of criteria that involve demonstrating fairness, ontological authenticity, educative authenticity, catalytic authenticity, and tactical authenticity. For each of these terms, the commentary provides a definition, how it applies to social pharmacy research, and guidance on when and how to use them. These are accompanied by examples from the pharmacy literature where the criteria have been used. The commentary ends by providing a summary of competing viewpoints of establishing quality in the published literature while inviting the reader to reflect on how the presented criteria would apply to different qualitative research projects.", "title": "Establishing trustworthiness and authenticity in qualitative pharmacy research.", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Susan L. Morrow"], "summary": "This article examines concepts of the trustworthiness, or credibility, of qualitative research. Following a \u201cresearcher-as-instrument,\u201d or self-reflective, statement, the paradigmatic underpinnings of various criteria for judging the quality of qualitative research are explored, setting the stage for a discussion of more transcendent standards (those not associated with specific paradigms) for conducting quality research: social validity, subjectivity and reflexivity, adequacy of data, and adequacy of interpretation. Finally, current guidelines for writing and publishing qualitative research are reviewed, and strategies for conducting and writing qualitative research reports are suggested. Qualitative research, ensuing from a variety of disciplines, paradigms, and epistemologies, embraces multiple standards of quality, known variously as validity, credibility, rigor ,o rtrustworthiness. In addition to some standards that may be thought of as somewhat universal across disciplines and paradigms, the \u201cgoodness\u201d (Morrow & Smith, 2000) of qualitative inquiry is assessed on the basis of the paradigmatic underpinnings of the research and the standards of the discipline. Thus, a grounded theory study or a consensual qualitative research investigation in counseling psychology that is rooted in a postpositivist or constructivist/interpretivist paradigm will look quite different from a critical ethnography in education; and the standards appropriate for evaluating these studies will vary accordingly. I begin this article by addressing the paradigmatic underpinnings of trustworthiness or rigor in qualitative research. Next, I discuss central topics related to trustworthiness or validity that span paradigms and may be thought of as relevant across most research designs. I then provide an overview of guidelines that have been suggested for evaluating qualitative research, particularly in psychology. Finally, I offer recommendations for enhancing the quality of qualitative research in counseling psychology and suggest strategies for writing and publishing. First, however, in keeping with the standard of reflexivity as a way for researchers to inform their audiences about their perspectives as well as to manage their subjectivities, I describe my own assumptions about qualitative research methodology and quality.", "title": "Quality and trustworthiness in qualitative research in counseling psychology.", "claims": null}, {"metadata": {"year": 1999}, "authors": ["J. Cutcliffe", "H. McKenna"], "summary": "Qualitative research is increasingly recognized and valued and its unique place in nursing research is highlighted by many. Despite this, some nurse researchers continue to raise epistemological issues about the problems of objectivity and the validity of qualitative research findings. This paper explores the issues relating to the representativeness or credibility of qualitative research findings. It therefore critiques the existing distinct philosophical and methodological positions concerning the trustworthiness of qualitative research findings, which are described as follows: quantitative studies should be judged using the same criteria and terminology as quantitative studies; it is impossible, in a meaningful way, for any criteria to be used to judge qualitative studies; qualitative studies should be judged using criteria that are developed for and fit the qualitative paradigm; and the credibility of qualitative research findings could be established by testing out the emerging theory by means of conducting a deductive quantitative study. The authors conclude by providing some guidelines for establishing the credibility of qualitative research findings.", "title": "Establishing the credibility of qualitative research findings: the plot thickens.", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Marja Soila-Wadman"], "summary": "The journal is quite a new, international publication, with interests in arts, aesthetics and cultural matters. In this field there have been a lot of creative discussions going on concerning the research methodology. When we authors previously were working with the structure of the article, our belief, at the end of the 2000, was that the scientific status of qualitative research is widely accepted? That the basic arguments concerning the credibility of research would not be required in a specific article in journals which are familiar with qualitative research, and have published research reports where the studies have been conducted by", "title": "The Question of Credibility in Qualitative Research - Once again!", "claims": null}, {"metadata": {"year": 1993}, "authors": ["Cheryl Tatano Beck"], "summary": "The three criteria of credibility, fittingness, and auditability have been focused on in the hope of facilitating the critique of qualitative research. Once criteria that are appropriate to qualitative methodologies are developed, the scientific merit of these research studies can truly be appreciated. If traditional scientific criteria relevant to quantitative studies are used to critique qualitative methods, the development and acceptance of this paradigm-transcending research will be hindered.", "title": "Qualitative Research: The Evaluation of Its Credibility, Fittingness, and Auditability", "claims": null}, {"metadata": {"year": 2008}, "authors": ["W. Kline"], "summary": "The standards used to evaluate qualitative research have been a recurring theme in qualitative research literature for more than 20 years. Included in the discussion of these standards are the qualities of trustworthiness (Y. Lincoln & E. Guba, 1985). More recently, methodological and analytic rigor and coherence have also been emphasized (e.g., M. Poggenpoel & C. P. H. Myburgh, 2005). These factors are discussed and conceptualized as presentational rigor. Implications of presentational rigor for designing and conducting qualitative research and preparing manuscripts that are more likely to be published in Counselor Education and Supervision are discussed.", "title": "Developing and Submitting Credible Qualitative Manuscripts", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Susan Johnson", "Saltanat Rasulova"], "summary": ": Recent developments in impact evaluation recognise the need to go beyond the intense debate over experimental techniques to incorporate theory-based approaches and qualitative research methods. Motivated by an underlying concern that qualitative research in this new wave of qualitative impact evaluation research is appropriately conducted, this paper reviews practical strategies to address rigour deploying Guba and Lincoln\u2019s principles of \u201ctrustworthiness\u201d to do so. In particular we focus on the less discussed principle of \u2018authenticity\u2019 which responds to the demand for research orientations to be more transformative and emancipatory. In development impact evaluation, recent discussion has highlighted the frequent deficit of a transformative orientation and the problematic wider organisational contexts of aid relationships in which commissioned evaluations are conducted. We argue that embracing the authenticity principle offers commissioned researchers with a progressive orientation a rationale for making space for stakeholder interaction and negotiation within the rigour discourse. This in turn creates the scope to incorporate it into checklists of rigour so using the \u2018artefacts\u2019 of evaluation as a means to call commissioning organisations and other stakeholders to be more responsive to concerns for authenticity.", "title": "Qualitative impact evaluation: Incorporating authenticity into the assessment of rigour", "claims": null}], "query": "Write about authenticity and credibility when conducting qualitative research.", "summary_abstract": "The exploration of authenticity and credibility in qualitative research is a multifaceted topic that has garnered significant attention across various studies. The consensus among researchers emphasizes the importance of establishing trustworthiness through specific criteria tailored to qualitative methodologies.\n\nBrink (1993) highlights the critical nature of validity and reliability in qualitative research, noting that the subjective nature of qualitative data can lead to skepticism regarding its findings. This underscores the necessity for researchers to adopt rigorous standards to enhance the credibility of their work. Similarly, Morrow (2005) discusses the concept of trustworthiness, advocating for a self-reflective approach where researchers acknowledge their biases and the paradigmatic foundations of their studies. This reflexivity is essential for ensuring the credibility of qualitative research.\n\nAmin et al. (2020) expand on this by introducing Lincoln and Guba's criteria of trustworthiness, which include credibility, transferability, dependability, and confirmability. They also introduce the concept of authenticity, which encompasses fairness and various forms of authenticity (ontological, educative, catalytic, and tactical). This framework provides a comprehensive approach to assessing the quality of qualitative research, particularly in social pharmacy.\n\nCutcliffe and McKenna (1999) critique the application of quantitative standards to qualitative research, arguing for the development of criteria that are specifically suited to qualitative paradigms. They emphasize that the credibility of qualitative findings should be established through methods that align with qualitative inquiry rather than through traditional quantitative measures.\n\nKline (2008) reinforces the ongoing discourse surrounding the standards for evaluating qualitative research, emphasizing the need for methodological rigor and coherence. This aligns with Johnson and Rasulova's (2016) focus on authenticity in impact evaluation, where they argue for a transformative approach that incorporates stakeholder engagement to enhance the authenticity of qualitative research.\n\nIn summary, the literature collectively underscores the necessity of tailored criteria for assessing authenticity and credibility in qualitative research. Emphasizing reflexivity, the establishment of trustworthiness, and the incorporation of authenticity principles are vital for enhancing the credibility and acceptance of qualitative findings within the scientific community (Brink, 1993; Morrow, 2005; Amin et al., 2020; Cutcliffe & McKenna, 1999; Kline, 2008; Johnson & Rasulova, 2016).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2015}, "authors": ["Daniel H. Kwak"], "summary": "Microbial secondary metabolites are physiologically significant, exhibiting auxiliary functions for the producer and as scaffolds in the developments of new medicines. Advancements in genome sequencing technologies have enabled researchers to access unprecedented amounts of genomic data that can be used to discover the enzymatic machinery necessary to discover novel and biologically-active molecules. This approach has been termed \u201cgenome mining.\u201d In both of the investigations presented herein, genome mining was utilized to discover and characterize biosynthetic pathways of novel molecules. The findings in one study utilize this approach to discover a small molecule virulence factor from the opportunistic human pathogen Acinetobacter baumannii. This virulence factor has been found to be associated with a number of clinically significant phenotypes, and these findings suggest that this can be a target in the developments of next generation antibiotics. In another study, this approach was implemented to discover and characterize the biosynthetic pathway of anticancer compound hapalosin from the cyanobacterial species Hapalosiphon welwitschii. Cloning and expression of this biosynthetic pathway in the surrogate host Escherichia coli enabled its genetic characterization as well as the generation of a small combinatorial library consisting of analogs incorporating natural and unnatural substrates. Collectively, these investigations demonstrate the utility of genome mining to characterize novel molecules important in pathogenesis or in the biosynthesis of clinically-significant compounds.", "title": "MS Thesis (ETD) Dan Kwak", "claims": null}, {"metadata": {"year": 2016}, "authors": ["N. Ziemert", "T. Weber"], "summary": "The computational mining of genomes has become an important part in the discovery of novel natural products as drug leads. Thousands of bacterial genome sequences are publically available these days containing an even larger number and diversity of secondary metabolite gene clusters that await linkage to their encoded natural products. With the development of high-throughput sequencing methods and the wealth of DNA data available, a variety of genome mining methods and tools have been developed to guide discovery and characterisation of these compounds. This article reviews the development of these computational approaches during the last decade and shows how the revolution of next generation sequencing methods has led to an evolution of various genome mining approaches, techniques and tools. After a short introduction and brief overview of important milestones, this article will focus on the different approaches of mining genomes for secondary metabolites, from detecting biosynthetic genes to resistance based methods and \u201cevo-mining\u201d strategies including a short evaluation of the impact of the development of genome mining methods and tools on the field of natural products and microbial ecology.", "title": "genome mining in microbes \u2013 a review", "claims": null}, {"metadata": {"year": 2016}, "authors": ["N. Ziemert", "Mohammad Alanjary", "T. Weber"], "summary": "Covering: 2006 to 2016The computational mining of genomes has become an important part in the discovery of novel natural products as drug leads. Thousands of bacterial genome sequences are publically available these days containing an even larger number and diversity of secondary metabolite gene clusters that await linkage to their encoded natural products. With the development of high-throughput sequencing methods and the wealth of DNA data available, a variety of genome mining methods and tools have been developed to guide discovery and characterisation of these compounds. This article reviews the development of these computational approaches during the last decade and shows how the revolution of next generation sequencing methods has led to an evolution of various genome mining approaches, techniques and tools. After a short introduction and brief overview of important milestones, this article will focus on the different approaches of mining genomes for secondary metabolites, from detecting biosynthetic genes to resistance based methods and \"evo-mining\" strategies including a short evaluation of the impact of the development of genome mining methods and tools on the field of natural products and microbial ecology.", "title": "The evolution of genome mining in microbes - a review.", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Bikash  Baral", "Amir  Akhgari", "Mikko  Mets\u00e4-Ketel\u00e4"], "summary": "Microbial natural products are a tremendous source of new bioactive chemical entities for drug discovery. Next generation sequencing has revealed an unprecedented genomic potential for production of secondary metabolites by diverse micro-organisms found in the environment and in the microbiota. Genome mining has further led to the discovery of numerous uncharacterized \u2018cryptic\u2019 metabolic pathways in the classical producers of natural products such as Actinobacteria and fungi. These biosynthetic gene clusters may code for improved biologically active metabolites, but harnessing the full genetic potential has been hindered by the observation that many of the pathways are \u2018silent\u2019 under laboratory conditions. Here we provide an overview of the various biotechnological methodologies, which can be divided to pleiotropic, biosynthetic gene cluster specific, and targeted genome-wide approaches that have been developed for the awakening of microbial secondary metabolic pathways.", "title": "Activation of microbial secondary metabolic pathways: Avenues and challenges", "claims": null}, {"metadata": {"year": 2020}, "authors": ["K. Hong", "Changsheng Zhang", "A. Dobson"], "summary": "Bioinformatic tools and their application in genome mining for secondary metabolites Genome mining approaches for the identification of novel secondary metabolites Merging ecology (physical, chemical factors or co-culture) with microbial genome mining for secondary metabolites\u2019 discovery Heterologous systems for the expression of gene clusters to identify novel metabolites Unlocking cryptic pathways, employing genomic based approaches Cutting-edge technology in genome editing and novel metabolites identification", "title": "Genome Mining and Marine Microbial Natural Products", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Anne\u2010Catrin Letzel", "S. Pidot", "C. Hertweck"], "summary": "A total of 211 complete and published genomes from anaerobic bacteria are analysed for the presence of secondary metabolite biosynthesis gene clusters, in particular those tentatively coding for polyketide synthases (PKS) and non-ribosomal peptide synthetases (NRPS). We investigate the distribution of these gene clusters according to bacterial phylogeny and, if known, correlate these to the type of metabolic pathways they encode. The potential of anaerobes as secondary metabolite producers is highlighted.", "title": "A genomic approach to the cryptic secondary metabolome of the anaerobic world.", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Quoc-Thai  Nguyen", "Maria E.  Merlo", "Marnix H.  Medema", "Andris  Jankevics", "Rainer  Breitling", "Eriko  Takano"], "summary": "Many microbial secondary metabolites are of high biotechnological value for medicine, agriculture, and the food industry. Bacterial genome mining has revealed numerous novel secondary metabolite biosynthetic gene clusters, which encode the potential to synthesize a large diversity of compounds that have never been observed before. The stimulation or \u201cawakening\u201d of this cryptic microbial secondary metabolism has naturally attracted the attention of synthetic microbiologists, who exploit recent advances in DNA sequencing and synthesis to achieve unprecedented control over metabolic pathways. One of the indispensable tools in the synthetic biology toolbox is metabolomics, the global quantification of small biomolecules. This review illustrates the pivotal role of metabolomics for the synthetic microbiology of secondary metabolism, including its crucial role in novel compound discovery in microbes, the examination of side products of engineered metabolic pathways, as well as the identification of major bottlenecks for the overproduction of compounds of interest, especially in combination with metabolic modeling. We conclude by highlighting remaining challenges and recent technological advances that will drive metabolomics towards fulfilling its potential as a cornerstone technology of synthetic microbiology.", "title": "Metabolomics methods for the synthetic biology of secondary metabolism", "claims": null}, {"metadata": {"year": 2013}, "authors": ["J. Nov\u00e1kov\u00e1", "M. Farka\u0161ovsk\u00fd"], "summary": "Mining of natural sources for new secondary metabolites has a successful history, which is reflected by the fact that over 50% of all drugs, currently on the market, are derived from natural products. Bacteria are one of the most important sources of bioactive natural products destined for drug discovery. However, less than 1% of the microorganisms observed in different habitats have been cultivated and characterized. To explore the genomic and functional diversity of the vast majority of the microbial world, novel methods were introduced, which are based on analysis of a DNA isolated from environmental communities. Metagenomics represents a strategy offering access to the genetic information present in uncultured bacteria by screening of libraries constructed from DNA isolated from different habitats. Functional- and sequence-driven screens are the major approaches employed to mine metagenomic libraries. This review aims to highlight discoveries in this area and discusses the possible future directions of the field.", "title": "Bioprospecting microbial metagenome for natural products", "claims": null}], "query": "Genome mining for bioactive secondary metabolites from microbes", "summary_abstract": "The research question regarding genome mining for bioactive secondary metabolites from microbes is addressed through a variety of studies that highlight the significance of genomic data in discovering novel compounds with potential therapeutic applications. \n\nKwak (2015) emphasizes the role of genome mining in identifying biosynthetic pathways for secondary metabolites, showcasing its application in discovering a virulence factor from *Acinetobacter baumannii* and an anticancer compound, hapalosin, from *Hapalosiphon welwitschii*. This illustrates how genome mining can lead to the development of new antibiotics and anticancer agents.\n\nZiemert and Weber (2016) provide a comprehensive review of the evolution of computational genome mining methods, noting the vast number of bacterial genome sequences available and the diversity of secondary metabolite gene clusters they contain. They discuss various genome mining techniques that have emerged, which facilitate the discovery and characterization of these compounds.\n\nBaral et al. (2018) further elaborate on the potential of microbial natural products as sources for new drugs, highlighting the discovery of previously uncharacterized 'cryptic' metabolic pathways in classical producers like Actinobacteria and fungi. They point out the challenges in harnessing these pathways due to their often silent nature in laboratory conditions.\n\nHong et al. (2020) discuss the integration of bioinformatics tools and ecological factors in genome mining, suggesting that co-culture and other environmental influences can enhance the discovery of novel secondary metabolites. They also mention the use of heterologous systems to express gene clusters, which can help unlock cryptic pathways.\n\nNguyen et al. (2012) focus on the awakening of cryptic microbial secondary metabolism through bacterial genome mining, emphasizing the potential for synthesizing a diverse array of compounds. They highlight the importance of metabolomics in this context, as it aids in the identification and quantification of metabolites produced by engineered pathways.\n\nLastly, Nov\u00e1kov\u00e1 and Farka\u0161ovsk\u00fd (2013) discuss the historical success of mining natural sources for secondary metabolites, noting that bacteria are a crucial source for drug discovery. They advocate for metagenomics as a strategy to access genetic information from uncultured bacteria, which could lead to the discovery of new bioactive compounds.\n\nCollectively, these studies underscore the transformative impact of genome mining on the discovery of bioactive secondary metabolites, revealing the vast potential of microbial genomes in drug development and the innovative methodologies being employed to unlock this potential.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2019}, "authors": ["Charlie Kurth"], "summary": "According to psychological constructivism, emotions result from projecting folk emotion concepts onto felt affective episodes. While constructivists acknowledge there is a biological dimension to emotion, they deny that emotions are (or involve) affect programs. So they also deny emotions are natural kinds. However, the essential role that constructivism gives to felt experience and folk concepts leads to an account that is extensionally inadequate and functionally inaccurate. Moreover, biologically oriented proposals that reject these commitments are not similarly encumbered. Recognizing this has two implications: biological mechanisms are more central to emotion than constructivism allows, and the conclusion that emotions are not natural kinds is premature.", "title": "Are Emotions Psychological Constructions?", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Iris  Berent", "Lisa  Feldman Barrett", "Melanie  Platt"], "summary": "A large literature debates whether emotions are universal and innate. Here, we ask whether reasoning about such matters is shaped by intuitive Essentialist biases that link innateness to the material body. To gauge the perception of innateness, we asked laypeople to evaluate whether emotion categories will be recognized spontaneously by hunter\u2013gatherers who have had no contact with Westerners. Experiment 1 shows that participants believe that emotions are innate and embodied (facially and internally) and these two properties correlate reliably. Experiment 2 demonstrates that the link is causal. When told that emotions are localized in specific brain areas (i.e., embodied), participants concluded that emotions are innate. Experiment 3 shows that this na\u00efve view persists even when participants are explicitly informed that these emotions are acquired. Our results are the first to suggest that laypeople incorrectly believe that, if emotions are embodied, then they must be innate. We suggest that people\u2019s failure to grasp the workings of their psyche arises from the human psyche itself.", "title": "Essentialist Biases in Reasoning About Emotions", "claims": null}, {"metadata": {"year": 2019}, "authors": ["I. Berent", "L. F. Barrett", "Melanie Platt"], "summary": "A large literature debates whether emotions are universal and innate. Here, we ask whether reasoning about such matters is shaped by intuitive Essentialist biases that link innateness to the material body. To gauge the perception of innateness, we asked laypeople to evaluate whether emotion categories will be recognized spontaneously by hunter\u2013gatherers who have had no contact with Westerners. Experiment 1 shows that participants believe that emotions are innate and embodied (facially and internally) and these two properties correlate reliably. Experiment 2 demonstrates that the link is causal. When told that emotions are localized in specific brain areas (i.e., embodied), participants concluded that emotions are innate. Experiment 3 shows that this na\u00efve view persists even when participants are explicitly informed that these emotions are acquired. Our results are the first to suggest that laypeople incorrectly believe that, if emotions are embodied, then they must be innate. We suggest that people\u2019s failure to grasp the workings of their psyche arises from the human psyche itself.", "title": "Essentialist Biases in Reasoning About Emotions", "claims": null}, {"metadata": {"year": 1990}, "authors": ["M. Mascolo", "J. Mancuso"], "summary": "Abstract The theory of emotions outlined in this paper follows a general theory that psychological processes maintain an adaptive equilibrium between one's construction system and sensory input from one's world. Persons build constructions to match any class of sensory data, and also build standards for (construe) varied levels of input which accompany the arousal-related activity associated with standard/input mismatch. Conscious identification of different emotional states reflects the use of constructions assigned to different standard/input relations. A constructivist may regard different emotional states or experiences as idealized configurations of attributes; and these configurations can be treated as prototypes, applying the same analyses which have been developed by cognitive scientists who have offered models for discussions of categorization processes. A model for discussing emotional development is provided, and directions of future constructivist investigations are suggested", "title": "Functioning of Epigenetically Evolved Emotion Systems: A Constructive Analysis", "claims": null}, {"metadata": {"year": 2020}, "authors": ["I. Berent"], "summary": "Can you tell what a stranger feels just by looking at their face? Could you distinguish fear from anger even in a person from an entirely unfamiliar culture (without having the opportunity to learn about it from experience)? Laypeople assume they can, because they believe that emotions are inborn, and they are universally imprinted on the body, both externally, on the face, and internally (I sense anxiety in the rumbling of my gut). In fact, people believe that emotions are innate precisely because they believe that emotions are \u201cin the body.\u201d So strong is their conviction that they will insist on their belief even when told that the emotions in question are in fact acquired. Our tendency to view \u201cwarm\u201d feelings as embodied and innate is the exact mirror image of our tendency to view \u201ccold\u201d concepts as ephemeral and disembodied. A review of the scientific literature reveals that similar presumptions also plague the debate on universal emotions in affective science. Chapter 10 shows how Essentialism (a principle invoked to explain our aversion to innate ideas) also promotes the promiscuous presumption of innate emotions by laypeople and scientists alike.", "title": "Our Big Hearts", "claims": null}, {"metadata": {"year": 2012}, "authors": ["J. de Rivera"], "summary": "Emotions involve transformations of the relationships that exist in the \u201cnatural\u201d world as well as in social worlds. Thus, the emotions that humans construct are based on affective processes that exist independently from human society and are not necessarily constituted by the interplay of culture and genetic or physiological processes.", "title": "Emotions as Transformational Structures", "claims": null}, {"metadata": {"year": 2012}, "authors": ["J. Rivera"], "summary": "Emotions involve transformations of the relationships that exist in the \u201cnatural\u201d world as well as in social worlds. Thus, the emotions that humans construct are based on affective processes that exist independently from human society and are not necessarily constituted by the interplay of culture and genetic or physiological processes.", "title": "Emotions as Transformational Structures", "claims": null}, {"metadata": {"year": 2016}, "authors": ["L. F. Barrett"], "summary": "Abstract The science of emotion has been using folk psychology categories derived from philosophy to search for the brain basis of emotion. The last two decades of neuroscience research have brought us to the brink of a paradigm shift in understanding the workings of the brain, however, setting the stage to revolutionize our understanding of what emotions are and how they work. In this article, we begin with the structure and function of the brain, and from there deduce what the biological basis of emotions might be. The answer is a brain-based, computational account called the theory of constructed emotion.", "title": "The theory of constructed emotion: an active inference account of interoception and categorization", "claims": null}], "query": "Are emotions innate or constructed?", "summary_abstract": "The question of whether emotions are innate or constructed has been explored through various perspectives in the literature. Kurth (2019) argues against the constructivist view that emotions are merely projections of folk concepts onto affective experiences, suggesting instead that biological mechanisms play a more central role in emotions than constructivism acknowledges. This indicates that emotions may not be entirely constructed but rather have innate biological underpinnings.\n\nBerent et al. (2020) provide empirical evidence that laypeople tend to believe emotions are innate and embodied, linking the perception of emotions to physical manifestations in the body. Their experiments reveal that this belief persists even when individuals are informed that emotions can be acquired, suggesting a strong intuitive bias towards viewing emotions as innate. This aligns with the notion that societal and cultural influences may shape our understanding of emotions, yet the underlying belief in their innateness remains robust.\n\nMascolo and Mancuso (1990) present a constructivist perspective, positing that emotional experiences are built from sensory inputs and personal constructions. They argue that emotional states are idealized configurations that arise from the interaction between sensory data and individual cognitive processes, emphasizing the role of personal experience in shaping emotions.\n\nDe Rivera (2012) and Rivera (2012) further elaborate on the idea that emotions are constructed from affective processes that exist independently of societal influences. This suggests that while emotions may be influenced by culture, they also have a foundational basis that is not solely dependent on social constructs.\n\nBarrett (2016) introduces the theory of constructed emotion, which posits that emotions are not fixed entities but rather arise from complex brain processes that integrate various inputs. This perspective supports the idea that emotions are constructed rather than innate, as they are shaped by individual experiences and cognitive interpretations.\n\nIn summary, the literature presents a nuanced view of emotions, suggesting that while there are innate biological components, the construction of emotions is heavily influenced by personal experiences, societal norms, and cognitive processes. The debate continues, with evidence supporting both the innate and constructed aspects of emotional experiences.", "summary_extract": null}, {"papers": [{"metadata": {"year": 1997}, "authors": ["P. Wetzels", "C. Pfeiffer"], "summary": "Police crime statistics show a significant increase in the rates of violent crimes committed by children during the last two years. However, since in Germany children under the age of 14 are not accountable for criminal offences, these statistics are highly selective. On the other hand, police crime statistics also show a huge increase of violent crimes committed by juveniles and young adults which cannot be explained by changes of police intervention strategies. It seems reasonable to attribute this mainly to changed living conditions of young people, particularly the increase proportion of children and adolescents living below the thresholds of poverty. To put the issue of violence and children in perspective, the victimization of children should not be overlooked. Criminological as well as psychological research show the devastating consequences of physical and sexual abuse experiences during childhood. Results of a representative german survey concerning the prevalence of violent victimization experiences during childhood are presented. 13.5% of the male and 16.1% of the female respondents had been victims of severe physical or sexual abuse during childhood. If repeatedly witnessing parental violence is additionally taken into consideration, these rates are 18.3% for male and 20.5% for female respondents. A comparison of age groups failed to identify significant differences of victimization rates, except the rates of minor violence committed by parents. Thus it can be assumed tentatively, that the proportion of children subjected to severe violent acts committed by closely related adults as well as the rate of those witnessing parental violence has remained constant over time.", "title": "[Childhood and violence: perpetrator and victim perspectives from the viewpoint of criminology].", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Chelsea M. Weaver", "J. Borkowski", "T. Whitman"], "summary": "The relationships between childhood exposure to violence and adolescent conduct problems were investigated in a sample of 88 primiparous adolescent mothers and their children. Regression analyses revealed that witnessing violence and victimization prior to age 10 predicted delinquency and violent behaviors, even after controlling for prenatal maternal and early childhood externalizing problems. Social competency and depression during middle childhood moderated the relationship between victimization and violent behaviors for girls, but not boys: Lower levels of social competency and depression served as risk factors for delinquency among teenage girls who experienced victimization during childhood. These findings have important implications for youth violence prevention programs.", "title": "Violence Breeds Violence: Childhood Exposure and Adolescent Conduct Problems.", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Robert A. Murphy", "Robert A. Rosenheck", "Steven J. Berkowitz", "Steven R. Marans"], "summary": "The Child Development Community Policing Program represents a national model of community-based collaboration between police and mental health professionals for violence-exposed and traumatized children. Administrative data from clinical records of a 24-hour consultation service were examined through stepwise multivariate logistic regression to identify child and event characteristics associated with a direct, in-person response at the time of police contact. Of 2361 children, 809 (34.3%) received a direct, in-person response. Relative to Caucasian children, Hispanic youth were more likely to receive this form of response (OR = 1.36). An acute clinical response was more likely for incidents of gang involvement (OR = 8.12), accidents (OR = 5.21), felony assaults (OR = 2.97), property crimes (OR = 2.30), family violence (OR = 1.53) and psychiatric crises (OR = 1.29). Acute response was less likely when juvenile conduct problems (OR = 0.61), fires (OR = 0.59), child maltreatment (OR = 0.57), and domestic violence (OR = 0.44) were involved. Incidents that were more severe or involved a primary mental health component were related to utilization of intensive CDCP resources.", "title": "Acute Service Delivery in a Police-Mental Health Program for Children Exposed to Violence and Trauma", "claims": null}, {"metadata": {"year": 2015}, "authors": ["D. Finkelhor", "H. Turner"], "summary": "A National Profile of Children Exposed to Family Violence: Police Response, Family Response and Individual Impact provides the first nationally representative data on youth contact with law enforcement and victim services \u2013 including best practices and help-seeking obstacles \u2013 for cases of family violence involving exposure to children. These data come from a nationally representative sample of 517 family violence incidents drawn from the 4503 respondents to the Second National Survey of Children\u2019s Exposure to Violence (NatSCEV II). The NatSCEV study, conducted in 2011, involved telephone interviews with parents of children age 0-9 and with the youths themselves if they were age 10-17. Between 13%-58% of police contacts and between 34%-97% of advocate contacts following domestic violence incidents involving a child witness included actions from one or more of 10 best practices. Most police best practices were associated with increased likelihood of arrest. Almost half of children witnessed an arrest when one occurred, though only 1 in 4 youth were spoken to by police responding to the scene. Youth exposed to domestic violence, as a group, have high rates of other victimizations and adversities. Although this group reports elevated trauma symptoms, the characteristics of a specific domestic violence incident and the response to that incident by police were generally unrelated to youth's current trauma symptoms after controlling for history of victimization and other adversities. However, child current trauma symptoms were lowest when perpetrators left the house after the incident, followed by when no one moved out, and were highest when the victim moved out. Child witnesses to family violence are a highly victimized group, and it is recommended that they systematically receive assessment and services when any member of their family enters the system due to family violence. This document is a research report submitted to the U.S. Department of Justice. This report has not been published by the Department. Opinions or points of view expressed are those of the author(s) and do not necessarily reflect the official position or policies of the U.S. Department of Justice. NIJ 2010-IJ-CX-0021 Final Report 3", "title": "A National profile of children exposed to family violence: Police response, family response, and individual impact.", "claims": null}, {"metadata": {"year": 2012}, "authors": ["H. Richardson-Foster", "N. Stanley", "P. Miller", "G. Thomson"], "summary": "The police represent the front line in the service response to children experiencing domestic violence. This paper examines police intervention in domestic violence incidents involving children, drawing on quantitative and qualitative data from police records and interviews with young people and police officers. The quality of police communication with children and young people emerged as key, and police officers evinced reluctance to engage with children at domestic violence incidents. Providing the police with training and information designed to improve their skills and confidence might promote communication with children in this context. Policy that conceptualised children as victims of domestic violence in policy could focus police attention on the needs of children and young people at such incidents.", "title": "Police intervention in domestic violence incidents where children are present: police and children's perspectives", "claims": null}, {"metadata": {"year": 1990}, "authors": ["Beverly  Rivera", "Cathy Spatz Widom"], "summary": "The relationship between childhood victimization and violent offending is examined using a prospective cohorts design. Official criminal histories for a large sample of substantiated and validated cases of physical and sexual abuse and neglect (N = 908) from the years 1967 through 1971 were compared to those of a matched control group (N = 667) of individuals with no official record of abuse or neglect. Sex-specific and race-specific effects of childhood victimization and other characteristics of violent offending (chronicity, age of onset, temporal patterns, and continuity) are assessed. Childhood victimization increased overall risk for violent offending and particularly increased risk for males and blacks. In comparison to controls, abused and neglected children began delinquent careers earlier. Temporal patterns of violent offending were examined and childhood victims did not differ in age of arrest for first violent offense, nor were they more likely to continue offending. The findings and their limitations are discussed, as well as directions for future research.", "title": "Childhood Victimization and Violent Offending", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Jean Dawson", "M. Wells"], "summary": "ABSTRACT This analysis examines child protective service reports made in 494 cases of child victimization known to police. The data were collected from police reports of assaults involving child victims in two rural northeastern towns from 1990 to 1999. Findings suggest that cases reported to child protective services (CPS) were more likely to involve child maltreatment and involve family/caretaker offenders. Cases in which victims were age 13 to 17 years or the perpetrator was male were less likely to be reported to CPS. The data indicate that child protection agencies failed to learn of approximately 35% of the cases involving parent or caretaker offenders. Law enforcement agency policies and mandated reporting guidelines should streamline collaboration between these two types of agencies.", "title": "Crimes Involving Child Victims: Law Enforcement Reporting to Child Protective Services in Rural Communities", "claims": null}, {"metadata": {"year": 2009}, "authors": ["D. Finkelhor", "H. Turner", "Richard K. Ormrod", "S. Hamby"], "summary": "OBJECTIVE: The objective of this research was to obtain national estimates of exposure to the full spectrum of the childhood violence, abuse, and crime victimizations relevant to both clinical practice and public-policy approaches to the problem. METHODS: The study was based on a cross-sectional national telephone survey that involved a target sample of 4549 children aged 0 to 17 years. RESULTS: A clear majority (60.6%) of the children and youth in this nationally representative sample had experienced at least 1 direct or witnessed victimization in the previous year. Almost half (46.3%) had experienced a physical assault in the study year, 1 in 4 (24.6%) had experienced a property offense, 1 in 10 (10.2%) had experienced a form of child maltreatment, 6.1% had experienced a sexual victimization, and more than 1 in 4 (25.3%) had been a witness to violence or experienced another form of indirect victimization in the year, including 9.8% who had witnessed an intrafamily assault. One in 10 (10.2%) had experienced a victimization-related injury. More than one third (38.7%) had been exposed to 2 or more direct victimizations, 10.9% had 5 or more, and 2.4% had 10 or more during the study year. CONCLUSIONS: The scope and diversity of child exposure to victimization is not well recognized. Clinicians and researchers need to inquire about a larger spectrum of victimization types to identify multiply victimized children and tailor prevention and interventions to the full range of threats that children face.", "title": "Violence, Abuse, and Crime Exposure in a National Sample of Children and Youth", "claims": null}], "query": "police violence and child victimization OR adverse childhood experinces", "summary_abstract": "The research question regarding police violence and child victimization, particularly in the context of adverse childhood experiences, is addressed through various studies that highlight the complex interplay between these factors.\n\nWetzels and Pfeiffer (1997) emphasize the alarming rates of violent crimes committed by children and the significant victimization of children, noting that a considerable percentage of children experience severe physical or sexual abuse. Their findings suggest that the prevalence of such victimization has remained constant over time, indicating a persistent issue that requires attention.\n\nWeaver et al. (2008) further explore the consequences of childhood exposure to violence, revealing that witnessing violence and being victimized before the age of ten are strong predictors of delinquent and violent behaviors in adolescence. This underscores the long-term impact of early adverse experiences on youth behavior, particularly among girls who exhibit lower social competency and higher depression levels.\n\nMurphy et al. (2005) discuss the Child Development Community Policing Program, which aims to provide immediate support to children exposed to violence. Their findings indicate that police responses to incidents involving children can vary significantly based on the nature of the event, with certain types of incidents, such as gang involvement or family violence, prompting more direct intervention.\n\nFinkelhor and Turner (2015) provide a national perspective on children exposed to family violence, revealing that a substantial number of children witness police actions during domestic violence incidents. They highlight the need for systematic assessment and services for these children, who often face multiple victimizations and adversities.\n\nRichardson-Foster et al. (2012) examine police interactions with children during domestic violence incidents, noting that police officers often hesitate to engage with child witnesses. They advocate for improved training and policies that recognize children as victims, which could enhance police responses and support for affected youth.\n\nFinally, Finkelhor et al. (2009) present national estimates of childhood victimization, indicating that a significant majority of children experience some form of violence or abuse. Their research calls for a broader understanding of the various types of victimization children face, which is crucial for developing effective prevention and intervention strategies.\n\nCollectively, these studies illustrate the critical link between police violence, child victimization, and adverse childhood experiences, highlighting the urgent need for comprehensive approaches to address these intertwined issues.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2004}, "authors": ["L. Valente"], "summary": "The cucurbitacins are highly oxygenated triterpenoid compounds found in several botanical families that show high toxicity and varied biological activities. This review presents the main cucurbitacins so far isolated and their key structural characteristics. It complements and updates the existing reviews on this subject.", "title": "Cucurbitacinas e suas principais caracter\u00edsticas estruturais", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Chao Zhu", "Jieyu Wang", "Zi-hui Meng", "Zhibin Xu"], "summary": "As a new generation of host molecules, Cucurbitins have made remarkable achievements in host-guest chemistry, molecular recognition, molecular assembly, catalysis, separation detection, energy and so on. It has become a hotspot in supramolecular chemistry research. The advance of cucurbit[n]uril analogues, including glycoluril derivatives, hemicucurbit [n] uril, \u201chandle shape\u201d cucurbit[n]uril and heterocucurbit [n] uril is reviewed. Further researches of cucurbit [n] uril analogues are also discussed.", "title": "Research Progress of Cucurbit[n]uril Analogues", "claims": null}, {"metadata": {"year": 1971}, "authors": ["D  Lavie", "E  Glotter"], "summary": "The potent physiological activity of plants belonging to the Cucurbitaceae family has been known since antiquity. They were feared on account of their high toxicity (Elisha\u2019s Miracle)*, and vet valued because of the medicinal properties ascribed to them (40). Greeks and Romans used them, the doctors of the Middle Ages praised their virtues and some were still described in the British Pharmacopoeia of 1914.", "title": "The cucurbitanes, a group of tetracyclic triterpenes.", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Jian Chao Chen", "M. Chiu", "R. Nie", "G. Cordell", "S. Qiu"], "summary": "The natural cucurbitacins constitute a group of triterpenoid substances which are well-known for their bitterness and toxicity. Structurally, they are characterized by the tetracyclic cucurbitane nucleus skeleton, namely, 19-(10-->9beta)-abeo-10alpha-lanost-5-ene (also known as 9beta-methyl-19-norlanosta-5-ene), with a variety of oxygen substitutions at different positions. According to the characteristics of their structures, cucurbitacins are divided into twelve categories. The biological effects of the cucurbitacins are also covered.", "title": "Cucurbitacins and cucurbitane glycosides: structures and biological activities.", "claims": null}, {"metadata": {"year": 1975}, "authors": ["H.  Ripperger", "K.  Seifert"], "summary": "Zusammenfassung Mit Hilfe von Sephadex-LH-20- bzw. Kieselgel-Chromatographie wurden aus Fruchten von Citrullus lanatus var. citroides (Synonym C. colocynthoides) neben Cucurbitacin E (2) zwei Glykoside isoliert, denen die Strukturen 2-o-\u03b2- d -Glucopyranosyl-cucurbitacin I (3) und 2-o-\u03b2- d -Glucopyranosyl-cucurbitacin E (4) zukommen. 3 ist das dritte Cucurbitacin-Glykosid, das kristallin erhalten wurde.", "title": "Die cucurbitacine von Citrullus lanatus var. citroides", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Jos\u00e9 Luis R\u00edos", "Jos\u00e9 M. Escandell", "M. Carmen Recio"], "summary": "The cucurbitacins are a group of tetracyclic triterpenoids derived from the cucurbitane skeleton and found primarily in the Cucurbitaceae family. These triterpenoids, present in free or glycosidic form, are generally responsible for the bitter taste of the plants that contain them and are probably the principal cause of the antifeedant effects observed for such plants. Several plants used in traditional medicine to treat both inflammatory diseases as well as various types of tumors are rich in cucurbitacins, a fact which has given rise to several studies concerning their potential use as anti-inflammatory and anticancer agents. Nevertheless, since many cucurbitacins are extremely toxic, relatively few papers have dealt with their pharmacological activity. Recently, however, the relationship between the toxicity of a compound and its chemical pattern of substitution has been established, thus allowing for a more in-depth understanding of this class of triterpenes. In the present review, we provide a compilation of all the studies published in the last ten years on the pharmacological and biological effects of cucurbitacins, focusing principally on their pharmacological properties, especially their anti-inflammatory and anticancer effects.", "title": "New insights into the bioactivity of cucurbitacins", "claims": null}, {"metadata": {"year": 1985}, "authors": ["Sun Cj", "J. Ry", "Kyi Zy"], "summary": "Cucurbitine is a novel amino acid with schistosomicidal activity. In an attempt to enhance its activity by modification of its chemical structure, some related compounds were synthesized. 1,2-Dicarbethoxy-pyrrolidin-4-one was converted into the corresponding ketoxime. Catalytic hydrogenation of the latter compound under atmospheric pressure and subsequent hydrolysis gave racemic 4-allo-hydroxyproline. However, reduction under high pressure, 4-aminopyrrolidine-2-carboxylic acid (\u2162_a) was obtained. A number of derivatives of cucurbitine, including some esters and acylamino compounds (Table 1) were also prepared. Pharmacological examinations revealed that none of the compounds prepared was more active than cucurbitine.", "title": "[Synthesis of some cucurbitine-related compounds].", "claims": null}, {"metadata": {"year": 1989}, "authors": ["H. Stuppner1", "H. Wagner2"], "summary": "Picrorhiza kurrooa 4 5 6 7 O O O O", "title": "New Cucurbitacin Glycosides from Picrorhiza kurrooa", "claims": null}], "query": "cucurbitins", "summary_abstract": "The research on cucurbitins, particularly focusing on their chemical properties and biological activities, reveals a complex interplay of toxicity and potential therapeutic benefits. Cucurbitins, which are highly oxygenated triterpenoid compounds primarily found in the Cucurbitaceae family, are known for their bitterness and toxicity (Valente, 2004; Chen et al., 2005). These compounds are characterized by a tetracyclic cucurbitane skeleton and can exist in free or glycosidic forms, contributing to their antifeedant properties (R\u00edos et al., 2005).\n\nRecent studies highlight the significant role of cucurbitins in supramolecular chemistry, where they serve as host molecules in various applications, including molecular recognition and catalysis (Zhu et al., 2018). This emerging interest underscores their versatility beyond traditional uses.\n\nDespite their toxic nature, cucurbitins have garnered attention for their potential medicinal properties, particularly in anti-inflammatory and anticancer applications. Research indicates that the toxicity of these compounds is closely linked to their chemical structure, which may inform future pharmacological studies (R\u00edos et al., 2005). However, the high toxicity of many cucurbitacins has limited the extent of research into their pharmacological activities (R\u00edos et al., 2005).\n\nAdditionally, cucurbitine, a novel amino acid derived from cucurbitins, has shown schistosomicidal activity, although attempts to enhance its efficacy through structural modifications have not yielded more potent derivatives (Sun et al., 1985).\n\nIn summary, while cucurbitins are recognized for their toxicity and bitter taste, they also present promising avenues for research in medicinal chemistry, particularly in the context of their anti-inflammatory and anticancer properties, as well as their applications in supramolecular chemistry (Valente, 2004; Zhu et al., 2018; R\u00edos et al., 2005).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2005}, "authors": ["A. Branch"], "summary": "Policy for Flexible Work Practices , Outlines the flexible work practices within the NSW Health System.", "title": "Flexible Work Practices - Policy - NSW Department of Health", "claims": null}, {"metadata": {"year": 2012}, "authors": ["M. Grabowska"], "summary": "Flexible employment forms are one of the elements of active labour market policy which is connected with fl exicurity system. Th e new option on labour market in the present times. Results of the global economic crisis and the demographic situation related to ageing societies cause a need to implement solutions on the labour market which shall be both fl exible and protective. Flexible forms of work are forms of work which deviate from the typical form of employment such as a permanent employment contract. Th ese are atypical forms of employment whose characteristics include fl exible working time and place, the form of employment, other relations between the employee and the employer. Th ey are a truly new approach in the labour law as they more and more depart from the classical job under a permanent employment contract.", "title": "Flexible Employment Forms as an Element of Flexicurity", "claims": null}, {"metadata": {"year": 2022}, "authors": ["G. Nazarova", "V. O. Baliasnyi"], "summary": "The article is aimed at studying theoretical approaches to understanding the essence of flexible employment policy in modern enterprises. The article analyzes approaches to understanding the essence of the definition of \u00abemployment of the population\u00bb by domestic and foreign scholars. Modern trends in the field of employment of workers are identified, interpretations of the terms \u00abnon-standard employment\u00bb and \u00abflexible employment\u00bb are studied, their common features and differences are determined. The main trends in the use of labor force are aimed at reducing the costs of structural restructuring of the economy by: the use of various forms of hiring employees and their labor engagement, flexibility in regulating working hours (ensuring a flexible schedule with irregular working hours), flexibility in matters of remuneration (using a differentiated and individual approach), flexibility in the use of forms and methods of social protection of workers. The article analyzes the peculiarities of using the flexibility model in the context of employment problems. Based on the analyzed theoretical approaches, the definition of a flexible employment policy at the State level and the level of individual enterprises is provided. At the State level, flexible employment is defined as a form of labor market regulation, which takes into account and does not oppose the interests of both subjects of social and labor relations, i. e.: employers and hired employees. At the level of individual enterprises, flexible employment policy is the company\u2019s policy towards employees, which gives the latter the opportunity to choose between working and free time \u2013 both in terms of the amount of time and the mode of its use. Thus, the introduction of a flexible employment policy both at the level of individual enterprises and at the State level as a whole will make business more responsible, increase the level of economic activity of the population, make enterprises and organizations more competitive in the labor market in the context of informatization and digitalization of modern society.", "title": "The Theoretical Foundations of Flexible and Non-Standard Employment", "claims": null}, {"metadata": {"year": 1989}, "authors": ["T. Walsh"], "summary": "Drawing on publicly available statistics and data on employment structures, hours and labour utilisation practices from detailed company case\u2010studies in the retail and catering sectors, this paper considers the implications for labour markets of the rise in part\u2010time, temporary and casual working (variously described as \u2018flexible\u2019 labour). Throughout the paper, emphasis will be given to part\u2010time employment and temporary and casual working in their own right, not simply in relation to full\u2010time work.", "title": "\u2018Flexible\u2019 Labour: Some Policy Perspectives", "claims": null}, {"metadata": {"year": 2003}, "authors": ["Li Hui"], "summary": "Flexible employment has been developing rapidly in developed countries and playing an important role in improving labor market and economy since 1970s. There are many reasons for this. An important reason is that governments in the developed countries have made some special policies and measures for the flexible employment, including subsidies for employers and enterprises, assistance to up - starters, support for special groups in poverty as well as increasingly perfect and complete laws and legislations on flexible employment. These policies and measures can definitely give some enlightenment for the flexible employment in Chi-", "title": "Policy of the Flexible Employment in Developed Countries and Its Enlightenment", "claims": null}, {"metadata": {"year": 2001}, "authors": ["S. Ja\u0161arevi\u0107"], "summary": "In today's complex operating conditions, characterized by stricter and stricter competition in the market, there must be a more flexible approach to the organization and continuation of work process, as compared to the years when unemployment and economic crisis did not represent a chronic problem. For our country, with particularly weakened economy and one of the highest rates of actual unemployment in the world, such an approach shall be inevitable. In order to commence more successfully the flexibilization of work, besides reforms leading to originating and open market economy, we shall have to effect significant changes in the area of education (especially expertise), labor legislation and system of social security. However, the experiences of other countries clearly point that amending the regulations shall not be sufficient without reorienting the whole society. especially the state, the unions and employers' associations, towards more active incentive for flexible work in all the operating levels.", "title": "'Flexible work': A necessity of modern economy", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Sla\u0111ana Gligori\u0107", "Sanja \u0160kori\u0107"], "summary": "Global trends in the working world clearly show the changes in the character of labor relations with a significant representation of flexible forms of work. It is an atypical form of work organization that arises as a consequence of strong globalization flows, economic crisis, information and technological revolution, and which stands against the labor law standard of a classical employment, being represented in the form of the employment contract for an indefinite period of time with full time employment. Starting from the concept of flexicurity as the dominant concept in the EU, flexicurity seeks to establish a balance between flexibility in the organization of work in order to preserve stability. Through the flexible organization of work and working hours, there establishes a kind of balance between working hours, the rest time and time for socialization and social activity. However, on the other hand, a flexible organization of work can reduce the rights of the employees and workers outside the employment relationship, especially if this form of work is abused in practice by the contracts concluded contrary to their essence or legal norm. The authors use the historical, comparative-legal and sociological method, as well as the statistical data in the analysis of research subjects with the aim of providing the concrete proposals for the improvement of the existing normative framework.", "title": "Flexible forms of work and work engagement", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Elke J. Jahn", "R. Riphahn", "C. Schnabel"], "summary": "In recent decades, economic policy makers across Europe have sought to increase labour market flexibility by promoting the use of temporary employment. The articles in this Feature provide new results on how fixed\u2010term and agency work contracts affect firm productivity and how the segments of two\u2010tier labour markets interact. This article points to a possible trade\u2010off between efficiency and equity when deregulating labour markets. Taken together, the evidence presented in this Feature suggests that flexible forms of employment can be both a boon and a bane for labour markets and for society as a whole.", "title": "Feature: Flexible Forms of Employment: Boon and Bane", "claims": null}], "query": "what is flexible work policy", "summary_abstract": "Flexible work policies encompass a range of employment practices that deviate from traditional, permanent employment contracts, allowing for greater adaptability in work arrangements. These policies are increasingly recognized as essential in modern labor markets, particularly in response to economic changes and demographic shifts.\n\nAccording to Grabowska (2012), flexible employment forms are characterized by variations in working time, location, and the nature of the employment relationship itself. This flexibility is seen as a necessary adaptation to the evolving labor market, especially in light of the global economic crisis and the aging population. Nazarova and Baliasnyi (2022) further elaborate on this by defining flexible employment policy at both the state and enterprise levels. At the state level, it serves as a regulatory framework that balances the interests of employers and employees, while at the enterprise level, it allows employees to choose how they manage their work and free time.\n\nThe rise of flexible work arrangements, such as part-time, temporary, and casual employment, has been noted by Walsh (1989), who emphasizes the implications of these practices for labor markets. These forms of employment are increasingly prevalent and represent a shift away from the traditional full-time, indefinite contracts that have historically dominated the labor landscape.\n\nMoreover, the concept of \"flexicurity,\" which aims to balance flexibility in work organization with job security, is highlighted by Gligori\u0107 and \u0160kori\u0107 (2021). This approach seeks to ensure that while work arrangements become more adaptable, employees' rights and protections are not compromised. The authors caution that without careful implementation, flexible work policies could lead to a reduction in employee rights, particularly if misused.\n\nIn summary, flexible work policies represent a significant shift in employment practices, characterized by adaptability in work arrangements and a focus on balancing the needs of both employers and employees. These policies are increasingly recognized as vital for enhancing labor market responsiveness and competitiveness in a rapidly changing economic environment (Li Hui, 2003; Ja\u0161arevi\u0107, 2001).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2011}, "authors": ["Eyal Yaniv"], "summary": "Organizational attention is an underdeveloped construct that can account for a variety of organizational phenomena. Attention is the means by which individuals select and process a limited amount of input from the enormous amount of internal and environmental inputs bombarding the senses, memories and other cognitive processes. This article develops a coherent theory of organizational attention, drawing on Cornelissen\u0219s domain-interactive metaphor model. Topics that form the building blocks of individual attention research, including selective and divided attention, automatic versus controlled processes, attention and memory, attention and learning, are examined in terms of their applicability to the organizational context.", "title": "ORGANIZATIONAL ATTENTION: A METAPHOR FOR A CORE COGNITIVE PROCESS", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Eyal Yaniv", "D. Schwartz"], "summary": "INTRODUCTION Attention is a term commonly used in education, psychiatry, and psychology. Attention can be defined as an internal cognitive process by which one actively selects environmental information (i.e., sensation) or actively processes information from internal sources (i.e., stored memories and thoughts; Sternberg, 1996). In more general terms, attention can be defined as an ability to focus and maintain interest in a given task or idea, including managing distractions. Attention is selective by its nature. According to Pashler (1998, p. 37), \" The process of selecting from among the many potentially available stimuli is the clearest manifestation of selective attention. \" Why do firms respond to certain events or stimuli in their environment while neglecting others? It seems that organizations, just like individuals , have limited attention capacity. Hence, they must select from among the many potentially available stimuli and respond to these selected stimuli only. Organizational attention is defined as the socially structured pattern of attention by decision makers within the organization (Ocasio, 1997). Organizational attention, like human attention , is a limited resource: \" Attentional limits filter or screen incoming information such that a great deal of data pertinent to strategic decision may never get processed \" (2003) show that the extent to which CEOs (chief executive officers) are selective in their attention to sectors of the environment is a significant predictor of performance. Knowledge management (KM) models and process theories, almost without exception, incorporate a stage or phase in which a given knowledge item is brought to bear on a current decision or action. This stage, referred to alternatively as, is of crucial importance in any knowledge-management cycle. The flow of knowledge in and out of an awareness stage is not merely a function of the universe of available organizational memory or the technological tools available to filter and identify such knowledge. It is influenced to a large degree by organizational attention. The second area in which organizational attention is key is knowledge acquisition and creation as discussed by Ocasio (1997), and Yaniv and Elizur (2003). Successful knowledge management requires attention. Davenport and Volpel (2001) argues that attention is the currency of the information age. Knowledge consumers must pay attention to knowledge and become actively involved in the knowledge-transfer processes. This is particularly important when the knowledge to be received is tacit (Nonaka, 1994). Knowledge can be part of the organization's repository, however, if it does not \u2026", "title": "Organizational Attention", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Eyal Yaniv"], "summary": "Organizational attention is an underdeveloped construct that can account for a variety of organizational phenomena. Attention is the means by which individuals select and process a limited amount of input from the enormous amount of internal and environmental inputs bombarding the senses, memories and other cognitive processes. This article develops a coherent theory of organizational attention, drawing on Cornelissen's domain-interactive metaphor model. Topics that form the building blocks of individual attention research, including selective and divided attention, automatic versus controlled processes, attention and memory, attention and learning, are examined in terms of their applicability to the organizational context.", "title": "A METAPHOR FOR A CORE COGNITIVE PROCESS", "claims": null}, {"metadata": {"year": 2010}, "authors": ["L. C. D. M. Ferreira"], "summary": "textabstractOrganizational studies emphasizing the role of attention in organizational behavior depart from the idea that organizations, like individuals, have limited capacity to attend to environmental stimuli. The bounded capacity of the organizations to respond to stimuli is conditioned by the limited cognitions of individuals and by the limited capability of organizations to distribute, coordinate and integrate those cognitions. The cross-level nature of organizational attention, its dual character as both a process and an output, means that theories of attention afford interesting insights to explain organizational behavior.\nThis dissertation presents one conceptual and two empirical studies about organizational attention. In the conceptual study entitled \u201cAttention span: expanding the attention-based view to team, organizational and social movements levels\u201d, it is argued that attentional processes have functional equivalence at the team, organizational and social movements level. The study entitled \u201cWhen a thousand words are (not) enough: an empirical study of the relationship between firm performance and attention to shareholders\u201d, tests the power of the attention-based view combined with resource dependence theory to explain the relationship between financial performance and attention to shareholders. Finally, the study \u201cSense and sensibility: testing the effects of attention structures and organizational attention on financial performance\u201d tests the process model of situated attention by examining the effects of attention structures and the allocation of attention on organizational social responses and performance/\nTogether, these studies deepen and expand attentional perspectives on organizational behavior. Moreover, they renew scholars\u2019 interest in organizational attention, indicating some of the strengths and limitations of theories of attention and also revealing a prolific research stream.", "title": "Attention Mosaics: Studies of Organizational Attention", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Rex Wang Renjie", "P. Verwijmeren"], "summary": "This paper shows that exogenous director distraction affects board monitoring intensity and leads to a higher level of inactivity by management. We construct a firm-level director \"distraction'' measure by exploiting shocks to unrelated industries in which directors have additional directorships. Directors attend significantly fewer board meetings when they are distracted. \nFirms with distracted board members tend to be inactive and experience a significant decline in firm value. Overall, this paper highlights the impact of limited director attention on the effectiveness of corporate governance and the importance of directors in keeping management active.", "title": "Director Attention and Firm Value", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Rex Wang  Renjie", "Patrick  Verwijmeren"], "summary": "This paper shows that exogenous director distraction affects board monitoring intensity and leads to a higher level of inactivity by management. We construct a firm-level director \"distraction'' measure by exploiting shocks to unrelated industries in which directors have additional directorships. Directors attend significantly fewer board meetings when they are distracted. \nFirms with distracted board members tend to be inactive and experience a significant decline in firm value. Overall, this paper highlights the impact of limited director attention on the effectiveness of corporate governance and the importance of directors in keeping management active.", "title": "Director Attention and Firm Value", "claims": null}, {"metadata": {"year": 2011}, "authors": ["OcasioWilliam"], "summary": "Organizational theory and research has increased attention to the determinants and consequences of attention in organizations. Attention is not, however, a unitary concept but is used differently i...", "title": "Attention to Attention", "claims": null}, {"metadata": {"year": 2005}, "authors": ["M. John"], "summary": "Interruptions and distractions are a serious problem for many tasks, from programming a device to forming a battle plan, and from driving a car to monitoring airspaces. Interruptions and distractions can interfere with short term memory in planning and executing plans, and they can divert attention and reduce processing capacities for the detection and interpretation of significant events in dynamic situations. Attention management across tasks, and even within complex tasks, is a multi-faceted cognitive, perceptual, and social problem for users and display designers alike. The key to improving attention management is understanding the human, task, and environment issues and designing technologies that dovetail with and exploit that understanding. The research presentations in this session delve into several different facets of attention management and interruption. They analyze the issues, develop design principles, and evaluate alternative interface designs.", "title": "Interruptions, Distractions, and Attention Management: A Multi-Faceted Problem for Human Factors", "claims": null}], "query": "organizational attention diversion", "summary_abstract": "The research on organizational attention diversion highlights the critical role that attention plays in shaping organizational behavior and decision-making processes. A central theme across the studies is that organizations, much like individuals, possess a limited capacity for attention, which necessitates selective focus on certain stimuli while neglecting others (Yaniv & Schwartz, 2011; Ferreira, 2010). This selective attention is crucial for effective knowledge management, as it influences how organizations process information and respond to environmental changes (Yaniv, 2011).\n\nYaniv (2011) elaborates on the concept of organizational attention, suggesting that it is a socially structured pattern among decision-makers that filters incoming information, thereby affecting strategic decisions. This filtering process can lead to significant gaps in information processing, where relevant data may be overlooked, ultimately impacting organizational performance (Yaniv & Schwartz, 2011).\n\nMoreover, the impact of attention diversion is particularly evident in the context of corporate governance. Research by Wang and Verwijmeren (2017, 2018) demonstrates that distractions faced by board members\u2014such as commitments to other directorships\u2014can lead to decreased monitoring intensity and increased managerial inactivity. This diversion of attention not only hampers the effectiveness of governance but also correlates with declines in firm value, underscoring the importance of focused attention at the board level.\n\nFerreira (2010) further expands on the implications of attention diversion by examining its effects across different organizational levels, suggesting that attentional processes are vital for understanding organizational behavior and performance. The studies collectively indicate that managing attention within organizations is essential for maintaining operational effectiveness and achieving strategic goals.\n\nIn summary, the literature emphasizes that organizational attention is a limited resource that, when diverted, can lead to significant negative consequences for decision-making and overall performance (Yaniv, 2011; Wang & Verwijmeren, 2017). Understanding and managing attention is therefore crucial for organizations aiming to navigate complex environments effectively.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2011}, "authors": ["Eyal Yaniv", "D. Schwartz"], "summary": "INTRODUCTION Attention is a term commonly used in education, psychiatry, and psychology. Attention can be defined as an internal cognitive process by which one actively selects environmental information (i.e., sensation) or actively processes information from internal sources (i.e., stored memories and thoughts; Sternberg, 1996). In more general terms, attention can be defined as an ability to focus and maintain interest in a given task or idea, including managing distractions. Attention is selective by its nature. According to Pashler (1998, p. 37), \" The process of selecting from among the many potentially available stimuli is the clearest manifestation of selective attention. \" Why do firms respond to certain events or stimuli in their environment while neglecting others? It seems that organizations, just like individuals , have limited attention capacity. Hence, they must select from among the many potentially available stimuli and respond to these selected stimuli only. Organizational attention is defined as the socially structured pattern of attention by decision makers within the organization (Ocasio, 1997). Organizational attention, like human attention , is a limited resource: \" Attentional limits filter or screen incoming information such that a great deal of data pertinent to strategic decision may never get processed \" (2003) show that the extent to which CEOs (chief executive officers) are selective in their attention to sectors of the environment is a significant predictor of performance. Knowledge management (KM) models and process theories, almost without exception, incorporate a stage or phase in which a given knowledge item is brought to bear on a current decision or action. This stage, referred to alternatively as, is of crucial importance in any knowledge-management cycle. The flow of knowledge in and out of an awareness stage is not merely a function of the universe of available organizational memory or the technological tools available to filter and identify such knowledge. It is influenced to a large degree by organizational attention. The second area in which organizational attention is key is knowledge acquisition and creation as discussed by Ocasio (1997), and Yaniv and Elizur (2003). Successful knowledge management requires attention. Davenport and Volpel (2001) argues that attention is the currency of the information age. Knowledge consumers must pay attention to knowledge and become actively involved in the knowledge-transfer processes. This is particularly important when the knowledge to be received is tacit (Nonaka, 1994). Knowledge can be part of the organization's repository, however, if it does not \u2026", "title": "Organizational Attention", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Eyal Yaniv"], "summary": "Organizational attention is an underdeveloped construct that can account for a variety of organizational phenomena. Attention is the means by which individuals select and process a limited amount of input from the enormous amount of internal and environmental inputs bombarding the senses, memories and other cognitive processes. This article develops a coherent theory of organizational attention, drawing on Cornelissen\u0219s domain-interactive metaphor model. Topics that form the building blocks of individual attention research, including selective and divided attention, automatic versus controlled processes, attention and memory, attention and learning, are examined in terms of their applicability to the organizational context.", "title": "ORGANIZATIONAL ATTENTION: A METAPHOR FOR A CORE COGNITIVE PROCESS", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Eyal Yaniv"], "summary": "Organizational attention is an underdeveloped construct that can account for a variety of organizational phenomena. Attention is the means by which individuals select and process a limited amount of input from the enormous amount of internal and environmental inputs bombarding the senses, memories and other cognitive processes. This article develops a coherent theory of organizational attention, drawing on Cornelissen's domain-interactive metaphor model. Topics that form the building blocks of individual attention research, including selective and divided attention, automatic versus controlled processes, attention and memory, attention and learning, are examined in terms of their applicability to the organizational context.", "title": "A METAPHOR FOR A CORE COGNITIVE PROCESS", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Rian Drogendijk", "H. Haq"], "summary": "In 1997, Ocasio attempted to highlight and revive research on attention in organizations by introducing the attention-based view of the firm (ABV), which specifically emphasizes the central role of...", "title": "Has the Attention-Based View Changed Research on Attention in Organizations? A Systematic Review", "claims": null}, {"metadata": {"year": 2011}, "authors": ["OcasioWilliam"], "summary": "Organizational theory and research has increased attention to the determinants and consequences of attention in organizations. Attention is not, however, a unitary concept but is used differently i...", "title": "Attention to Attention", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Longbing Cao", "Chengqi Zhang", "Ruwei Dai"], "summary": "Organization-oriented analysis acts as the key step and foundation in building organization-oriented methodology (OOM) to engineer multi-agent systems especially open complex agent systems (OCAS). A number of existing approaches target OOM, while they are incompatible with each other, and none of them is available as a solid and practical tool for engineering OCAS. This paper summarizes our investigation in building a unified framework for abstracting and analyzing OCAS organizations. Our organizationoriented framework, referred to as ORGANISED, integrating and expanding existing approaches, explicitly captures the main attributes in an OCAS. Following this framework, individual modelbuilding blocks are developed for all ORGANISED members; both visual and formal specifications are utilized to present an intuitive and precise analysis . The above techniques have been deployed in developing an agent service-based trading and mining support infrastructure.", "title": "Organization-Oriented Analysis of Open Complex Agent Systems", "claims": null}, {"metadata": {"year": 2013}, "authors": ["\u00c9. Rouby", "Catherine Thomas"], "summary": "We use longitudinal, qualitative data from a cement plant to develop a model of organizational attention elasticity (OAE). The model shows how coherent attention to emerging threats varies horizontally across different teams and vertically across different levels in the hierarchy. The data reveal the micro-processes of attention through which OAE develops, and how coherent attention is accomplished horizontally and vertically. By showing 'how' OAE and coherent attention intersect, this work identifies the underlying mechanisms of attention that are important to balance the tension between attention elasticity and attention coherence. These findings have implications for theories of organizational attention.", "title": "Individual and collective sensemaking in a complex environment: an attentional perspective", "claims": null}, {"metadata": {"year": 2013}, "authors": ["\u00c9. Rouby", "Catherine Thomas"], "summary": "We use longitudinal, qualitative data from a cement plant to develop a model of Organizational Attention Elasticity (OAE). The model shows how coherent attention to emerging threats varies horizontally across different teams and vertically across different levels in the hierarchy. The data reveal the micro-processes of attention through which OAE develops, and how coherent attention is accomplished horizontally and vertically. By showing \u2018how\u2019 OAE and coherent attention intersect, this work identifies the underlying mechanisms of attention that are important to balance the tension between attention elasticity and attention coherence. These findings have implications for theories of organizational attention.", "title": "Organizational Attention Elasticity: An Exploratory Case of Cement Production", "claims": null}], "query": "organizational attention definition Ocasio", "summary_abstract": "The concept of organizational attention, as defined by Ocasio (1997), refers to the socially structured patterns of attention that decision-makers within an organization exhibit. This construct emphasizes that organizations, much like individuals, have limited attention capacities, necessitating the selection of specific stimuli from a vast array of potential inputs (Yaniv & Schwartz, 2011). Ocasio's attention-based view of the firm (ABV) highlights the critical role that attention plays in determining how organizations respond to environmental stimuli, which can significantly influence their performance (Drogendijk & Haq, 2020).\n\nYaniv (2011) further elaborates on this definition by framing organizational attention as a means through which organizations process limited inputs from both internal and external environments. This processing is influenced by various factors, including selective and divided attention, as well as the interplay between attention and memory. The theory posits that successful knowledge management within organizations hinges on the effective allocation of attention, underscoring its importance in decision-making processes (Yaniv, 2011).\n\nMoreover, the concept of Organizational Attention Elasticity (OAE) has been introduced to explore how attention can vary across different teams and hierarchical levels within an organization. This model illustrates the micro-processes that contribute to coherent attention in response to emerging threats, highlighting the dynamic nature of organizational attention (Rouby & Thomas, 2013). \n\nIn summary, organizational attention is a multifaceted construct that plays a pivotal role in how organizations navigate their environments, make decisions, and manage knowledge. It is characterized by selective focus, limited capacity, and variability across different organizational contexts, as articulated by Ocasio and further developed by subsequent researchers (Yaniv, 2011; Rouby & Thomas, 2013).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2018}, "authors": ["Alka Ekka", "Neha Namdeo"], "summary": "Amylase is (E.C.3.2.1.1-1,4-alpha D-glucanohydrolase) an extracellular enzyme, which is involved in the starch processing industries where it breaks starch into simple sugar constituents.Amylase has extensive application in starch processing, brewing and sugar production, in textile industries and in detergent manufacturing processes. Interestingly, the first enzyme produced industrially was an amylase.In the present study, amylase producing bacteria were isolated from rice field, sugarcane field and sugarcane dump area and characterized for their morphological and biochemical properties. Then amylase activity of isolated bacterial cultures were determined and it was concluded that 3 (NN1, NN2, NN5)out of 6 bacterial colonies(NN1, NN2, NN3, NN4, NN5, NN6) were potent and their enzyme activity was more than other colonies. The potent colonies were also optimized for enzyme activity under certain conditions like different carbon sources, nitrogen sources, pH, incubation time and chlorides.Agro-industrial wastes were used as substrate for amylase production by Solid-State FermentationSSF) and we have found that wheat bran was the suitable substrate for amylase production.", "title": "Screening , Isolation and Characterization of Amylase Producing Bacteria and optimization for Production of Amylase", "claims": null}, {"metadata": {"year": 2021}, "authors": ["L. Garba", "M. Ibrahim", "E. Sahara", "M. T. Adamu", "S. Isa", "A. Yarma"], "summary": "Amylases are enzymes that are able to hydrolyse starch or glycogen molecules into polymers of glucose units. They have great potential applications in various industrial processes like in pharmaceutical, fermentation and food industries. Research on starch degrading enzymes has resulted into increased applications of amylases in different industrial processes. These enzymes occupy a greater space in the current biotechnological processes such as detergent, starch degradation, pharmaceutical, foodstuff, textile, and paper manufacturing. In fact, amylases constitute nearly 25% of the total sale of global enzymes. Amylases have been screened and identified from various sources, both eukaryotic and prokaryotic organisms such as animals, plants, fungi and bacteria, respectively. To further isolate novel amylases with enhanced desirable properties for such diverse industrial application, more organisms need to be screened. In this study, a total of 27 bacterial isolates were isolated from soil samples in Gombe metropolis. The bacteria were screened for amylase production using plate screening method. Each isolate was streaked onto a 1% starch agar plate and incubated for 24h at 37 \u00c2\u00b0C. The plates were covered with iodine solution and observed for positive amylase isolates based on the formation of clearing zones against the blue black background. The results confirmed eight (8) isolates of amylase-producing bacteria which include Bacillus subtilis, Escherichia coli, Streptococcus spp., Salmonella spp., Pseudomonas spp., Serratia spp., Proteus vulgaris, and Klebsiella spp. In conclusion, bacterial isolates capable of amylase production have been successfully screened and identified. This research may serve as a stepping stone to isolating functional amylase enzymes from these bacteria for promising industrial applications.", "title": "Preliminary Investigation of Amylase Producing-Bacteria from Soil in Gombe Metropolis", "claims": null}, {"metadata": {"year": 2012}, "authors": ["C. Shah"], "summary": "Amylases are among the most important industrial enzymes and also have great significance in Biotechnological studies. In this study cultural, morphological, and metabolic characteristics of the bacterial isolates were studied. Total 18 bacterial cultures were isolated from collected soil samples. Among 18 bacterial isolates, 14 isolates showed the amylolytic activity. These 18 isolate was identified according to Bergey\u2019s manual of systemic Bacteriology .These isolates related to Bacillus sp. The optimum pH for the growth of all the cultures was observed at pH 7. Submerged fermentation was carried out for the production of amylase was observed in the range of 0.045-1.35 U/min/mL. The maximum activity of amylase was 1.35 (U/min/mL) after 48 hours was recorded, have great significance.", "title": "Characterization of Amylase Producing Bacterial Isolates", "claims": null}, {"metadata": {"year": 2012}, "authors": ["L. Bin"], "summary": "Three bacterial strains Jz1\u3001Jz2\u3001Jz3 show high amylase activity,which were achieved from the soil near the rice and starch factories.The bacteria strains Jz1 and Jz3 are Bacillus after preliminary identification,while the bacterial strain Jz2 is Curtobacterium.The amylase activities of the three bacterial strains are 47.29 U /mL,48.48 U /mL and 49.74 U /mL respectively,determined by YoungJ.Y00 modified method.They will be well applied in the future.", "title": "The isolation of amylase-producing bacterial strains", "claims": null}, {"metadata": {"year": 1957}, "authors": ["C. Chen"], "summary": "From different sources, we have isolated 110 cultures belonging to the species Bacillus subtilisand Bacillus cereus. Of these, 2 strains formed considerably more a-amylase (S_(17) and S_(56) thanthe other strains. The optimum conditions for the cultivation of strains S_(17) were 35-37\u2103 and pH 4.4-8.Cultivated in the extract of peanutseed cake liquid medium, the strain formed amylase morethan that in the extracts of soyabean cake, cotton seed cake and wheat brain. The amylase wasmost active in pH 5-5.6 and at 60-65\u2103.", "title": "STUDIES ON THE AMYLASE-PRODUCING BACTERIA", "claims": null}, {"metadata": {"year": 2021}, "authors": ["T. Okunwaye", "P. Uadia", "B.O. Okogbenin", "E. Okogbenin", "D.C. Onyia", "J. U. Obibuzor"], "summary": "Amylases are enzymes that catalyze the hydrolysis of glycosidic bonds present in starch to release simple sugars. They are one of the most important enzymes in numerous commercial processes. In this investigation, fungal and bacterial strains from the following agro-industrial wastes were isolated and screened for amylolytic ability: soil from oil palm plantation, shea seed, date fruit, coconut meat, cassava effluent, cassava peel, cassava tubers, yam and potato tubers, starch medium, parboiled water from noodles and rice. The results revealed the presence of Geotrichum, Aspergillus, Penicillium, Trichoderma, Rhizopus and Fusarium spp. Five major genera of bacterial species namely Corynebacterium, Pseudomonas, Lactobacillus, Micrococcus and Bacillus were isolated and screened for amylase activity. Cassava soil had the highest heterotrophic bacterial count of 5.7 x105cfu/g and coconut meat waste had the lowest heterotrophic bacterial count of 1.3 x105cfu/g. All isolated microorganisms had the amylolytic ability. The fungal isolates had higher amylase activity when compared with the bacterial isolates. This investigation reveals organisms with high amylase activity.", "title": "Amylase-Producing Fungi and Bacteria Associated with Some Food Processing Wastes", "claims": null}, {"metadata": {"year": 2013}, "authors": ["S. Shivaji", "N. Sonune"], "summary": "Amylase is the most important industrial enzymes and having great significance in Biotechnological studies. Amylase production from bacteria is economical because the enzyme production rate is higher in bacteria as compared to other microorganism. The aim of this study was to isolate amylase producing bacteria from domestic wastewater. In this study cultural, morphological and biochemical characteristics of the bacterial isolates were studied. Total 40 bacterial cultures were isolated from collected domestic wastewater samples on nutrient agar medium. All isolates were screened for amylolytic activity by starch agar plate method. Among 40 bacterial isolates, only 8 isolates showed the best amylolytic activity. These isolates related to the genera Bacillus sp. and Paenibacillus sp.", "title": "Garode A. M.", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Ayalew Damte"], "summary": "A Thesis submitted to the School of Graduate Studies of Addis Ababa University \nin Partial Fulfillment of the Requirements for the Degree of Master of Science in \nBiotechnology.", "title": "Production of Alkaline Amylase from A Facultatively Anaerobic Alkaliphilic Bacteria.", "claims": null}], "query": "Amylase-Producing Bacteria", "summary_abstract": "The research on amylase-producing bacteria highlights their significant potential in various industrial applications, particularly in starch processing, food production, and biotechnology. Several studies have isolated and characterized bacterial strains capable of producing amylase, emphasizing the importance of these enzymes in commercial processes.\n\nEkka and Namdeo (2018) isolated amylase-producing bacteria from agricultural environments, identifying three potent strains with high enzyme activity. They optimized conditions for amylase production, finding that wheat bran served as an effective substrate for solid-state fermentation. Similarly, Garba et al. (2021) screened 27 bacterial isolates from soil, identifying eight strains, including Bacillus subtilis and Escherichia coli, that exhibited significant amylase production. This study underscores the diverse sources from which amylase-producing bacteria can be obtained.\n\nShah (2012) also focused on bacterial isolates, reporting that 14 out of 18 strains demonstrated amylolytic activity, predominantly from the Bacillus genus. The study highlighted the optimal conditions for enzyme production, which were found to be favorable at neutral pH levels. In another investigation, Bin (2012) identified three bacterial strains with high amylase activity from soil near rice and starch factories, further confirming the potential of Bacillus species in enzyme production.\n\nOkunwaye et al. (2021) expanded the scope by isolating both fungal and bacterial strains from various agro-industrial wastes, revealing that several bacterial genera, including Bacillus, exhibited amylolytic capabilities. This study indicated that while fungi showed higher amylase activity, the bacterial isolates still hold promise for industrial applications.\n\nShivaji and Sonune (2013) focused on domestic wastewater as a source for amylase-producing bacteria, successfully isolating eight strains related to Bacillus and Paenibacillus, which demonstrated significant enzyme activity. This finding suggests that unconventional sources, such as wastewater, can also yield valuable amylase-producing bacteria.\n\nOverall, the collective findings from these studies illustrate the diverse sources and significant potential of amylase-producing bacteria, particularly from the Bacillus genus, for industrial applications. The optimization of growth conditions and substrate selection further enhances the viability of these bacteria in commercial enzyme production.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2006}, "authors": ["Junfeng Qiu"], "summary": "This paper extends the recent literature of \u201cliquidity and asset prices\u201d into monetary models by adding money-creating banks. We explain why the money creation function of banks is important to financial stability. We study an economy in which not all assets can be used to make payments, agents may have to sell assets when they need cash. Sale of assets can lead to low asset price because buyers have limited ability to buy assets. Banks can provide liquidity by creating and lending out new deposit. This will reduce the sale of assets and stabilize asset prices. We also compare two types of liquidity provision mechanisms: liquidity-risk sharing through coalitions and liquidity provision through money creation. We show that if people use mutual-fund-like non-bank coalitions to share liquidity risks, then the function of banks to relax the aggregate money constraint is important. Without banks, non-bank coalitions will not be able to insure against aggregate liquidity risks, they will only add more endogenous volatility to asset prices. We also model how the interest rate policy of the central bank is transmitted through the", "title": "Bank money, asset prices and the financial liquidity channel of monetary policy", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Pierpaolo Benigno", "S. Nistic\u00f2"], "summary": "This paper studies monetary policy in models where multiple assets have different liquidity properties: safe and \"pseudo-safe\" assets coexist. A shock worsening the liquidity properties of the pseudo-safe assets raises interest-rate spreads and can cause a deep recession cum deflation. Expanding the central bank's balance sheet fills the shortage of safe assets and counteracts the recession. Lowering the interest rate on reserves insulates market interest rates from the liquidity shock and improves risk sharing between borrowers and savers.", "title": "Safe Assets, Liquidity and Monetary Policy", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Javier Bianchi", "Saki Bigio"], "summary": "We develop a new framework for studying the implementation of monetary policy through the banking sector. Banks are subject to a maturity mismatch problem leading to precautionary holdings of reserves. Through various instruments, monetary policy alters tradeos banks face between lending, holding reserves, holding deposits and paying dividends. This translates into the real economy via eects on real interests and lending. We study how these instruments interact with shocks to the volatility in the payments system, bank losses, the demand for loans and with capital requirements. We use a calibrated version of the model to answer, quantitatively, why have banks held onto a substantial increase in reserves while not increasing lending since 2008.", "title": "Banks, Liquidity Management and Monetary Policy", "claims": null}, {"metadata": {"year": 2006}, "authors": ["P. He", "Lixin Huang", "Randall Wright"], "summary": "One important function of banks is to issue liabilities, like demand deposits, that are relatively safe and also liquid (usable as means of payment). We introduce risk of theft and a safe-keeping role for banks into monetary theory. This provides a general equilibrium framework for analyzing banking in historical and contemporary contexts. The model can generate concurrent circulation of cash and bank liabilities as media of exchange (inside and outside money), and yields novel policy implications. For example, negative nominal interest rates are feasible, and for some parameters optimal; for other parameters, strictly positive rates (inflation above the Friedman Rule) are optimal.", "title": "Money, Banking, and Monetary Policy", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Jin Cao", "G. Illing"], "summary": "In most banking models, money is merely modeled as medium for transaction, but in reality, money is also the most liquid asset for banks. Central banks do not only passively supply money to meet demand for transaction, as often assumed in these models, instead they also actively inject liquidity into market, taking banks\u2019 illiquid assets as collateral. We examine both roles of money in an integrated framework, in which banks are subject to aggregate illiquidity risk. With fixed nominal deposit contracts, the monetary economy with active central bank can replicate constrained efficient allocation. This allocation, however, cannot be implemented in market equilibrium without additional regulation: Due to moral hazard problems, banks invest excessively in illiquid assets, forcing the central bank to provide liquidity at low interest rates. We show that interest rate policy to reduce systemic liquidity risk on its own is dynamically inconsistent. Instead, the constrained efficient solution can be achieved by imposing ex ante liquidity coverage requirement.", "title": "Money in the Equilibrium of Banking1", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Jin Cao", "G. Illing"], "summary": "In most banking models, money is merely modeled as a medium of transactions, but in reality, money is also the most liquid asset for banks. Central banks do not only passively supply money to meet demand for transactions, as often assumed in these models, instead they also actively inject liquidity into market, taking banks\u2019 illiquid assets as collateral. We examine both roles of money in an integrated framework, in which banks are subject to aggregate illiquidity risk. With fixed nominal deposit contracts, the monetary economy with an active central bank can replicate constrained efficient allocation. This allocation, however, cannot be implemented in market equilibrium without additional regulation: Due to moral hazard problems, banks invest excessively in illiquid assets, forcing the central bank to provide liquidity at low interest rates. We show that interest rate policy to reduce systemic liquidity risk on its own is dynamically inconsistent. Instead, the constrained efficient solution can be achieved by imposing an ex ante liquidity coverage requirement.", "title": "Money in the Equilibrium of Banking", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Jin Cao"], "summary": "In most banking models, money is merely modeled as a medium of transactions, but in reality, money is also the most liquid asset for banks. Central banks do not only passively supply money to meet demand for transactions, as often assumed in these models, instead they also actively inject liquidity into market, taking banks\u2019 illiquid assets as collateral. We examine both roles of money in an integrated framework, in which banks are subject to aggregate illiquidity risk. With fixed nominal deposit contracts, the monetary economy with an active central bank can replicate constrained efficient allocation. This allocation, however, cannot be implemented in market equilibrium without additional regulation: Due to moral hazard problems, banks invest excessively in illiquid assets, forcing the central bank to provide liquidity at low interest rates. We show that interest rate policy to reduce systemic liquidity risk on its own is dynamically inconsistent. Instead, the constrained efficient solution can be achieved by imposing an ex ante liquidity coverage requirement.", "title": "Money in the Equilibrium of Banking 1", "claims": null}, {"metadata": {"year": 2007}, "authors": ["Junfeng Qiu"], "summary": "This thesis studies the role of the banking system in several aspects of the macroeconomy, including the likelihood of financial crises, volatility of asset prices and the transmission of monetary policy. In chapter 2, I analyze the accumulation of international reserves by central banks as insurance against financial crises. In the model, private banks borrow from foreign creditors to invest in domestic projects. By lending to banks in response to liquidity shocks, the central bank can reduce the liquidation of bank assets and lower the probability of bank runs. I show that the central bank will hold more reserves when private banks hold lower reserves. I also find that if the central bank can borrow additional loans from external sources, then domestic banks will hold fewer reserves by themselves. If the borrowing cost of external loan is very high, then the central bank may actually want to accumulate more reserves in order to avoid borrowing from external sources at high costs. In chapter 3, I show that the ability of banks to supply liquidity through money creation is important for financial stability. By supplying liquidity, banks can smooth the sale of assets and stabilize asset prices. I find that without elastic money, the attempt of non-bank mutual funds to raise cash by selling assets will only add more volatility into the market. Elastic money provided by banks can help mutual funds better smooth the consumption of their shareholders. In chapter 4, we consider the role of elastic money in an different environment where liquidity shocks affect agents asymmetrically. We show how money growth and interest rate policy can be used to adjust the consumption level of households. We find that the optimal policy is affected by the sensitivity of the supply price to the interest rate. When the supply price is more sensitive to the interest rate, it would be better to adopt a higher inflation rate, and to make the zero-bound of nominal interest rate less likely to be binding.", "title": "Essays on the financial system and the transmission of monetary policy", "claims": null}], "query": "monetary policy, bank assets", "summary_abstract": "The research question regarding the interplay between monetary policy and bank assets is addressed through various perspectives in the provided papers. \n\nQiu (2006) emphasizes the critical role of banks in creating liquidity through money creation, which is essential for stabilizing asset prices. The paper argues that when banks provide liquidity, they can mitigate the adverse effects of asset sales during cash shortages, thereby reducing volatility in asset prices. This highlights the importance of banks in the transmission of monetary policy, particularly in times of liquidity crises.\n\nBenigno and Nistic\u00f2 (2013) explore the implications of different asset liquidity properties on monetary policy. They find that a deterioration in the liquidity of certain assets can lead to increased interest-rate spreads and potentially trigger a recession. The authors suggest that expanding the central bank's balance sheet can alleviate shortages of safe assets, thus counteracting economic downturns.\n\nBianchi and Bigio (2014) introduce a framework that examines how monetary policy influences banks' decisions regarding reserves and lending. They note that banks face maturity mismatches, which affect their liquidity management. The study indicates that monetary policy can significantly impact real interest rates and lending behaviors, particularly in response to shocks in the banking sector.\n\nCao and Illing (2021) further elaborate on the dual role of money as both a medium of exchange and a liquid asset for banks. They argue that central banks actively inject liquidity into the market, which is crucial for maintaining financial stability. Their findings suggest that without proper regulation, banks may over-invest in illiquid assets, necessitating central bank intervention to provide liquidity at lower interest rates.\n\nOverall, these studies collectively underscore the vital function of banks in the monetary system, particularly in terms of liquidity provision and asset price stabilization. They highlight the necessity for effective monetary policy that considers the liquidity characteristics of bank assets and the broader implications for financial stability (Qiu, 2006; Benigno & Nistic\u00f2, 2013; Bianchi & Bigio, 2014; Cao & Illing, 2021).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2010}, "authors": ["L. Puig", "J. J. Guerrero", "Kostas Daniilidis"], "summary": "In this paper we present a new way to compute a topological map using only orientation information. We exploit the natural presence of lines in man-made environments in dominant directions. We extract all the image lines present in the scene acquired by an omnidirectional system composed of 6 aligned cameras. From the parallel lines we robustly compute the three dominant directions using vanishing points. With this information we are able to align the camera with respect to the scene and to identify the turns in the trajectory. Assuming a Manhattan world where the changes of heading in the navigation are related by multiples 90 degrees. We also use geometrical image-pair constraints as a tool to identify the visual traversable nodes that compose our topological map. Experiments with an indoor sequence have been performed to validate this approach.", "title": "Topological map from only visual orientation information using omnidirectional cameras", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Hemanth  Korrapati", "Jonathan  Courbon", "Youcef  Mezouar"], "summary": "Topological maps are vital for fast and accurate localization in large environments. Sparse topological maps can be constructed by partitioning a sequence of images acquired by a robot, according to their appearance. All images in a partition have similar appearance and are represented by a node in a topological map. In this paper, we present a topological mapping framework which makes use of image sequence partitioning (ISP) to produce sparse maps. The framework facilitates coarse loop closure at node level and a finer loop closure at image level. Hierarchical inverted files (HIF) are proposed which are naturally adaptable to our sparse topological mapping framework and enable efficient loop closure. Computational gain attained in loop closure with HIF over sparse topological maps is demonstrated. Experiments are performed on outdoor environments using an omni-directional camera.", "title": "Topological Mapping with Image Sequence Partitioning", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Hemanth Korrapati", "Y. Mezouar", "P. Martinet"], "summary": "Topological maps are vital for fast and accurate localization in large environments. Sparse topological maps can be constructed by partitioning a sequence of images acquired by a robot, according to their appearance. All images in a partition have similar appearance and are represented by a node in a topological map. In this paper, we present a topological mapping framework which makes use of image sequence partitioning (ISP) to produce sparse maps. The framework facilitates coarse loop closure at node level and a finer loop closure at image level. Hierarchical inverted files (HIF) are proposed which are naturally adaptable to our sparse topological mapping framework and enable efficient loop closure. Computational gain attained in loop closure with HIF over sparse topological maps is demonstrated. Experiments are performed on outdoor environments using an omnidirectional camera.", "title": "Efficient Topological Mapping with Image Sequence Partitioning", "claims": null}, {"metadata": {"year": 2000}, "authors": ["Yves  Bertrand", "Guillaume  Damiand", "Christophe  Fiorio"], "summary": "In this paper we define the 3d topological map and give an optimal algorithm which computes it from a segmented image. This data structure encodes totally all the information given by the segmentation. More, it allows to continue segmentation either algorithmically or interactively. We propose an original approach which uses several levels of maps. This allows us to propose a reasonable and implementable solution where other approaches don't allow suitable solutions. Moreover our solution has been implemented and the theoretical results translate very well in practical applications.", "title": "Topological Encoding of 3D Segmented Images", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Yong Li", "Hongzhen Jin", "Hui Wang"], "summary": "A new approach for three-dimensional shape measurement is proposed. The corresponding point pairs between the projector and camera are identified by projecting a special pseudorandom sequence onto the surface of measured objects. The important properties of this sequence are expressed as follows: (1) any subsequence with a length of four symbols is unique; (2) there are no repeated symbols in any subsequence. The pseudorandom sequence is constructed with an alphabet of six symbols. These symbols are encoded with local spatial and temporal information by pixels in vertical strips of the projector plane. During measurement, the patterns for encoding are projected in turn and are captured with a camera. The projected pseudorandom sequence is retrieved by analysing the captured images. Then, the corresponding point pairs are worked out. Finally, the shapes of objects are reconstructed with triangulation. The experimental results reveal that a dense depth image with high resolution can be obtained. However, texture can be acquired simultaneously. The proposed method is robust.", "title": "Three-dimensional shape measurement using binary spatio-temporal encoded illumination", "claims": null}, {"metadata": {"year": 2008}, "authors": ["F. Werner", "J. Sitte", "F. Maire"], "summary": "In this paper we present a system for appearance-based topological mapping and localisation using vision data. The algorithms are designed for robots which are equipped with FPGA cameras. Such cameras do not provide the entire image to the robot but simple image features like colour histograms.", "title": "Visual topological mapping and localisation using colour histograms", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Ishit Mehta", "Manmohan Chandraker", "R. Ramamoorthi"], "summary": "We introduce a theoretical framework for differentiable surface evolution that allows discrete topology changes through the use of topological derivatives for variational optimization of image functionals. While prior methods for inverse rendering of geometry rely on silhouette gradients for topology changes, such signals are sparse. In contrast, our theory derives topological derivatives that relate the introduction of vanishing holes and phases to changes in image intensity. As a result, we enable differentiable shape perturbations in the form of hole or phase nucleation. We validate the proposed theory with optimization of closed curves in 2D and surfaces in 3D to lend insights into limitations of current methods and enable improved applications such as image vectorization, vector-graphics generation from text prompts, single-image reconstruction of shape ambigrams and multi-view 3D reconstruction.", "title": "A Theory of Topological Derivatives for Inverse Rendering of Geometry", "claims": null}, {"metadata": {"year": 2006}, "authors": ["Xuming He", "R. Zemel", "Volodymyr Mnih"], "summary": "We propose an approach to building topological maps of environments based on image sequences. The central idea is to use manifold constraints to find representative feature prototypes, so that images can be related to each other, and thereby to camera poses in the environment. Our topological map is built incrementally, performing well after only a few visits to a location. We compare our method to several other approaches to representing images. During tests on novel images from the same environment, our method attains the highest accuracy in finding images depicting similar camera poses, including generalizing across considerable seasonal variations. \u00a9 2007 Wiley Periodicals, Inc.", "title": "Topological map learning from outdoor image sequences", "claims": null}], "query": "topological mapping with encoded latent vectors from inverse camera projections", "summary_abstract": "The research question regarding topological mapping with encoded latent vectors from inverse camera projections is addressed through various innovative approaches in the provided abstracts. \n\nPuig et al. (2010) introduce a method for constructing topological maps using orientation information derived from image lines in man-made environments. Their approach leverages vanishing points to identify dominant directions and align the camera with the scene, which is crucial for mapping trajectories in a structured environment. This method emphasizes the importance of geometric constraints in identifying visual nodes for the topological map.\n\nKorrapati et al. (2011, 2012) focus on creating sparse topological maps by partitioning sequences of images based on their appearance. Their framework allows for efficient loop closure at both node and image levels, utilizing hierarchical inverted files (HIF) to enhance computational efficiency. This approach is particularly beneficial for localization in large environments, demonstrating the effectiveness of image sequence partitioning in topological mapping.\n\nBertrand et al. (2000) present a 3D topological mapping algorithm that computes maps from segmented images, encoding all segmentation information. Their multi-level mapping strategy offers a practical solution for topological mapping, which can be further refined through interactive segmentation.\n\nLi et al. (2009) propose a method for 3D shape measurement that involves projecting a pseudorandom sequence onto objects and capturing the resulting images. This technique, while primarily focused on shape reconstruction, highlights the potential for encoding spatial information that could be relevant for topological mapping.\n\nMehta et al. (2023) introduce a theoretical framework for differentiable surface evolution, which allows for topology changes through topological derivatives. This framework could enhance the understanding of how changes in image intensity relate to topological modifications, potentially informing future methods in topological mapping.\n\nLastly, He et al. (2006) discuss building topological maps based on image sequences using manifold constraints to relate images to camera poses. Their incremental approach shows promise in accurately mapping environments with minimal visits, which is essential for efficient topological mapping.\n\nCollectively, these papers illustrate a range of methodologies for topological mapping that incorporate encoded latent vectors and inverse camera projections, emphasizing the significance of image processing, geometric constraints, and efficient data structures in the development of robust mapping systems.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2004}, "authors": ["LUShan"], "summary": "Anew method of immunization was discovered in the early 1990s. Several research groups independently demonstrated that direct inoculation of DNA plasmids coding for a specific protein antigen could elicit immune responses against that antigen[1-4].Since in theory the mRNA molecules also have the potential to be translated into the protein antigen, this vaccination approach was officially named by WHO as the nucleic acid vaccination even though the term DNA vaccine has been used more commonly in the literature. This novel approach is considered the fourth generation of vaccines after live attenuated vaccines, killed or inactivated vaccines and recombinant protein based subunit vaccines.", "title": "Nucleic Acid Vaccines", "claims": null}, {"metadata": {"year": 1996}, "authors": ["J B Ulmer", "D L Montgomery", "J J Donnelly", "M A Liu"], "summary": "The use of DNA and mRNA as vectors for immunization is a relatively recent development in the field of vaccines. The first paper demonstrating the efficacy of a DNA vaccine in an animal model of viral disease was published in 1993 (1). The rationale for using nucleic acids as vaccines came from the Initial observations that mtramuscular (im) injection of nonrephcating plasmid DNA expression vectors or mFWA-encoding reporter genes could result in the in vivo expression of proteins in mouse muscle cells (2). This ability to express proteins in vivo offers the opportunity to generate immune responses against foreign antigens encoded by the nucleic acid. In addition, both humoral and cell-mediated immune (CMI) responses, such as cytotoxic T-lymphocytes (CTL), can be induced. In general, CTL responses require endogenous expression of the antigen, such as during immunization with live viruses or replicating vectors, whereas subunit protein, polysaccharide conjugate, or inactivated virus vaccines generate humoral immune responses, but not CTL. Therefore, the technique of DNA injection has potential advantages over certain other vaccine technologies.", "title": "DNA vaccines.", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Liu Guoxia", "Jiang Yu", "Zhang Bin", "Zhang Yi", "Liu Xu", "Fan Zhongxue", "Qian Feng-qin"], "summary": "Nucleic acid vaccine,also called gene vaccine or DNA vaccine,is a newly-developed vaccine,which derived from gene therapy in the early 1990s.From then on it absorbs researchers' attention because of so many advantages,including its ability to induce both cellular and humoral immune responses,no way to spread virus and easy to deposit and transportation.Researchers have designed various strategies to enhance the DNA vaccine potency,including the selection of different DNA encoding-Ag,constructing different plasmid vectors(including selecting different promoter,enhancer and intron),using various adjuvants and different delivery strategies,which provide useful method to produce efficient and practical DNA vaccine.Some main animal infectious diseases,such as influenza,Newcastle disease and foot-and-mouth disease,had been studied deeply,which provide worthy candidate to study other diseases.", "title": "Progress on Nucleic Acid Vaccine in Animal", "claims": null}, {"metadata": {"year": 1994}, "authors": [], "summary": "\n One of the most active areas of vaccine research, nucleic acid vaccination is a new method of vaccination which involves taking a gene from a disease-causing virus or bacterium and injecting it into the person to be vaccinated in such a way that the person's cells produce vaccinating molecules (antigen) which provoke a protective immune response in the host against future infection by the pathogen. In effect, the foreign gene instructs the human body how to produce its own protective antigens against a given disease. This technique has been successfully tested in animal models, but is not yet ready for general use in humans. In principle, a single injection of many different foreign genes could be used to confer immunity against many different diseases. A meeting held last May at the Geneva headquarters of the World Health Organization confirmed current interest in the subject among both vaccine researchers and manufacturers. The following subjects are discussed: how the vaccination procedure is conducted, why bacterial DNA is used, why the DNA is injected into muscle tissue rather than into other tissues, what is revolutionary about nucleic acid vaccination, and potential drawbacks and concerns about the technique.\n", "title": "A Forum brief on nucleic acid vaccines.", "claims": null}, {"metadata": {"year": 1995}, "authors": ["F. Vogel", "N. Sarver"], "summary": "The use of nucleic acid-based vaccines is a novel approach to immunization that elicits immune responses similar to those induced by live, attenuated vaccines. Administration of nucleic acid vaccines results in the endogenous generation of viral proteins with native conformation, glycosylation profiles, and other posttranslational modifications that mimic antigen produced during natural viral infection. Nucleic acid vaccines have been shown to elicit both antibody and cytotoxic T-lymphocyte responses to diverse protein antigens. Advantages of nucleic acid-based vaccines include the simplicity of the vector, the ease of delivery, the duration of expression, and, to date, the lack of evidence of integration. Further studies are needed to assess the feasibility, safety, and efficacy of this new and promising technology.", "title": "Nucleic acid vaccines", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Weng Jian-xin"], "summary": "The nucleic acid vaccine not only causes the immune response of body fluid but also leads the high level cell immun to reply, cytotoxic T lymphocyte react especially.It has heavy advantage in the prevention and cure of the infection with virus,bacterium,parasite,etc. The nucleic acid vaccine is as a new biotechnology of developing in recent years, has already become one of the focuses of the vaccine research field, and get fast development. The experimental results indicates that the nucleic acid vaccine can be regarded as the prevention vaccine of the virus, bacterium or parasite and as treatment vaccine of infectious disease and non-infectious disease. Through introduction of the progress of studying nucleic acid vaccine in preventing and treating infectious diseases, such as virus, bacterium, parasite in recent years,and the constant development with molecule biotechnology, the further research and practice of the nucleic acid vaccine will demonstrate the new hope in improving mankind and animal health.", "title": "Progress on the Application of Nucleic Acid Vaccine", "claims": null}, {"metadata": {"year": 1995}, "authors": ["G J Waine", "D P McManus"], "summary": "The recent successful immunization of experimental animals using nucleic acids has provided a revolutionary new approach in vaccinology. In this article, Gary Waine and Don McManus examine the potential of nucleic acid vaccines for their effectiveness not only against infectious and parasitic organisms exhibiting an intracellular phase during their life cycle, but also against parasitic helminths, whose life cycle stages are either predominantly or completely extracellular.", "title": "Nucleic acids: vaccines of the future.", "claims": null}, {"metadata": {"year": 2022}, "authors": ["W. Devi", "Supriya Kammar", "S. Logesh", "Gareth Dsouza", "Thotegowdanapalya Mohan", "Charukesi Rajulu"], "summary": "DNA vaccines, a type of nucleic acid vaccine, have emerged as one of the recent developments in immunology and recombinant DNA technology, offering great potential in terms of ease of manufacture, maintenance, and safety compared to conventional vaccines. Since their discovery, DNA vaccines have evolved immensely, resulting in the employment of new techniques such as gene guns, in vivo electroporation, and nanoparticle-based carriers to enhance the delivery of vaccines into the cells. Starting from the failures of the first-generation DNA vaccines to the near-success second-generation vaccines, several strategies including codon optimization, antigen design, and heterologous prime-boost have greatly helped in strengthening the vaccine's immunogenicity. The purpose of developing these third-generation vaccines is primarily to solve existing medical complications like cancer, along with therapeutic uses, to address health problems, and to aid the rapid eradication of sudden global outbreaks of infectious diseases including Ebola and COVID-19. In this review, we focus on the recent developments and strategies employed to improve the efficacy of DNA vaccines and discuss their mechanism of action, potential concerns, progress achieved, and a brief update on its clinical applications.", "title": "Nucleic acid vaccines: A rising antidote for the future", "claims": null}], "query": "History of Nucleic Acid Vaccines", "summary_abstract": "The history of nucleic acid vaccines, which include DNA and mRNA vaccines, traces back to the early 1990s when researchers began exploring the potential of using genetic material to elicit immune responses. The concept was first demonstrated in 1993, marking a significant shift in vaccine technology (Ulmer et al., 1996). Nucleic acid vaccines represent the fourth generation of vaccines, following traditional methods such as live attenuated and inactivated vaccines (Lu, 2004).\n\nThe fundamental principle behind nucleic acid vaccines is the introduction of plasmid DNA or mRNA encoding specific antigens into the host, prompting the body to produce these antigens and subsequently mount an immune response (Vogel & Sarver, 1995). This method has shown promise in generating both humoral and cell-mediated immunity, including cytotoxic T-lymphocyte responses, which are crucial for combating viral infections (Liu et al., 2005; Weng, 2005).\n\nOver the years, various strategies have been developed to enhance the efficacy of nucleic acid vaccines, such as optimizing the design of the DNA constructs, employing different delivery methods, and utilizing adjuvants (Devi et al., 2022). These advancements have led to improved immunogenicity and the potential for these vaccines to address a wide range of diseases, including infectious diseases and even cancer (Devi et al., 2022).\n\nDespite their advantages, including safety and ease of production, nucleic acid vaccines were initially met with skepticism and faced challenges in early trials (Ulmer et al., 1996). However, ongoing research and technological innovations have revitalized interest in this field, leading to significant progress in both animal models and clinical applications (Waine & McManus, 1995; Devi et al., 2022).\n\nIn summary, nucleic acid vaccines have evolved from a novel concept in the 1990s to a promising area of research with the potential to revolutionize immunization strategies. Their ability to induce robust immune responses and adaptability for various diseases positions them as a critical component of modern vaccinology.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2004}, "authors": ["Brian  Fynes", "Se\u00e1n de B\u00farca", "Donna  Marshall"], "summary": "Abstract Environmental uncertainty is a fact of life in today's supply chains. In this paper we develop a model of environmental uncertainty, supply chain (SC) relationship quality and SC performance. We use data from the electronics sector in Ireland to test our model. Our results provide mixed support for the model, with the moderating role of both demand and supply uncertainty being supported, but technological uncertainty not supported. We reflect on these findings and suggest a research agenda based on our results.", "title": "Environmental uncertainty, supply chain relationship quality and performance", "claims": null}, {"metadata": {"year": 2017}, "authors": ["R.  Sreedevi", "Haritha  Saranga"], "summary": "Abstract In order to remain competitive in the market, firms are forced to expand their product offerings and offer high levels of customization, bringing about high uncertainty in their supply chain. Firms that face high environmental uncertainty are increasingly facing higher risks in terms of supply disruptions, production and delivery delays that ultimately result in poor operational performance. This study aims at understanding the antecedents of supply chain operational risk faced by firms and the conditions under which such risks can be mitigated. Using Indian data from the sixth edition of International Manufacturing Strategy Survey (IMSS) and structural equation modeling, we investigate the relationships between environmental uncertainty and supply chain risk and the moderating effect of supply chain flexibility. We identify appropriate types of flexibility to mitigate the three major aspects of supply chain risk: supply risk, manufacturing process risk and delivery risk. Our empirical investigation reveals that uncertainty in the supply chain leads to high supply chain risk; and in uncertain environments, supply and manufacturing flexibility help in reducing the supply and manufacturing process risks respectively. However, our results also indicate that, in emerging markets such as India where logistic infrastructure is less developed, internal capabilities alone may not be sufficient in reducing supply chain delivery risk. Our findings not only contribute towards filling certain gaps in the supply chain risk management literature, but also provide practicing managers and researchers a better understanding of the types of flexibility that can mitigate supply chain risk in different business environments.", "title": "Uncertainty and supply chain risk: The moderating role of supply chain flexibility in risk mitigation", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Nitya P. Singh"], "summary": "ABSTRACT With increasing global exposure, organisations have started to witness supply chain risks that they traditionally were not exposed to. This article therefore attempts to answer the research question of how environmental uncertainty within a firm\u2019s supply chain, impacts firm financial performance. We further evaluate the role of supply chain risk management practices in mitigating the negative impact of such disruption events. To answer the research question, we conduct a literature review and develop the corresponding hypothesis. We test our hypothesis using both primary and secondary data. The results show that environmental uncertainty has a negative impact on firm financial performance, with entrepreneurial managerial capitalism mediating the impact. However, organisations that adopt macro and network supply chain risk management practices are able to improve managerial decision making frame, and mitigate the negative impact of environmental uncertainty on firm financial performance. The article concludes with our findings, along with managerial and practitioner implications of the research.", "title": "Managing environmental uncertainty for improved firm financial performance: the moderating role of supply chain risk management practices on managerial decision making", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Odkhishig Ganbold", "Y. Matsui"], "summary": "Today\u2019s ever-changing business environment is often described to be highly competitive, dynamic and complex. Customers are demanding more variability, better quality, higher reliability and faster delivery. Organizations are being faced with more uncertainties from its task environment than before. In order to respond to the uncertainties, organizations are internalizing fewer resources and capabilities, while increasing their integration with partners in the supply chain. Drawing on the resource-dependence theory, this study aims to examine the impact of environmental uncertainty on supply chain integration initiatives. Environmental uncertainty is considered in terms of three types, namely, supply uncertainty, demand or customer uncertainty, and technology uncertainty, based on its sources. Supply chain integration is comprised of internal integration, customer integration, and supplier integration. Based on the empirical study with 108 Japanese manufacturing firms, this study makes significant contributions to the knowledge base and provides theoretical and practical implications.", "title": "IMPACT OF ENVIRONMENTAL UNCERTAINTY ON SUPPLY CHAIN INTEGRATION", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Stephan  Vachon", "Sara  Hajmohammad"], "summary": "This manuscript examines the impact of supply chain uncertainty on environmental management spending in manufacturing plants. Building on the attention-based view of the firm (ABV), the basic premise is that with increased uncertainty in the supply chain, managers\u2019 attention to environmental management lessens which in turn leads to (i) fewer resources devoted to green issues within the plant and (ii) a bias to use resources toward less disruptive pollution control approaches rather than pollution prevention approaches. Data from a survey of 251 Canadian manufacturing plants was used to test the link between the level of uncertainty in the supply chain and environmental management decisions. The results indicate that supply chain uncertainty does not have a substantial impact on the level of environmental spending in a plant but has a substantial and significant impact on the allocation of the spending between pollution prevention and pollution control. More particularly, as supply chain uncertainty increases, organizations shift their resources away from pollution prevention to favor pollution control approaches.", "title": "Supply chain uncertainty and environmental management", "claims": null}, {"metadata": {"year": 2021}, "authors": ["R. A. Inman", "K. Green"], "summary": "PurposeToday's businesses are facing a world that is more complex, turbulent and unpredictable than in the past with increasing levels of environmental complexity. Rather than proposing environmental uncertainty as a mediator/moderator of the relationship between agility and performance as others have done, the authors offer an alternative view where supply chain agility is seen as mediating the relationship between environmental uncertainty and supply chain performance.Design/methodology/approachThe authors propose that supply chain agility is a response to the effects of environmental uncertainty and, as such, environmental uncertainty should be seen as a driver of supply chain agility. Few studies test the direct relationship between uncertainty and supply chain performance, and none simultaneously test for agility's mediation and moderation effect between environmental uncertainty and agility.FindingsThe model was statistically assessed using partial-least-squares structural equation modeling (PLS/SEM) by analyzing survey data from manufacturing managers in 136 US firms. The study results did not indicate a significant relationship between environmental uncertainty and supply chain performance. However, the authors did find a significant positive relationship between agile manufacturing and supply chain performance using measures that were primarily operations-centered rather than financial. Additionally, the authors found that agile manufacturing fully mediates the relationship between environmental uncertainty and supply chain performance.Originality/valueThe authors\u2019 model, though simple, provides a base for future research for them and other researchers who can incorporate other impacting variables into the model. The study results show that uncertainty can be a force for good and that utilizing agile manufacturing can be a new source of opportunity.", "title": "Environmental uncertainty and supply chain performance: the effect of agility", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Erdin\u00e7 Ko\u00e7", "Muhammet Burak Deliba\u015f", "Yaprak Anadol"], "summary": "In this study, the direct effect of environmental uncertainty on competitive advantage and its indirect effect through the sequential mediator variables of supply chain integration and supply chain agility were investigated. The sample of the study consists of company managers operating in the manufacturing sector in Turkey. An online survey was sent to company managers through connections established on LinkedIn and an analysis was carried out with the data collected from 414 participants. As a result of the analysis, it has been determined that environmental uncertainty has a direct, significant and positive effect on competitive advantage. In addition, the results of the research show that supply chain integration and supply chain agility have a partial mediating role in the relationship between environmental uncertainty and competitive advantage. According to the results of this study, in conditions of high environmental uncertainty, companies can increase their supply chain agility capabilities by establishing a more integrated structure with their supply chain partners, and thus gain a unique competitive advantage over their competitors. It has been observed that the relationships between the concepts, which are the subject of the study, have been investigated separately in different studies in the literature. This study will contribute to the literature by investigating the relationships between concepts in a holistic way.", "title": "Environmental Uncertainties and Competitive Advantage: A Sequential Mediation Model of Supply Chain Integration and Supply Chain Agility", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Dehui Xu", "Li Zhao", "Gang Li", "Linyan Sun"], "summary": "Supply chain integration nowadays is considered as an important approach to building and sustaining advantages. Many previous empirical researches focus on the effect of supply chain integration on the firms' performance, yet, the antecedence of supply chain integration is still largely unknown. This paper investigates the relationship between environmental uncertainty (including demand uncertainty, supply uncertainty and technology uncertainty) and supply chain integration (including customer integration, supplier integration and internal integration) with 139 samples from Chinese manufacturing industry. The results reveal both supply uncertainty and technology uncertainty significantly influence supply chain integration; the effect of supply uncertainty is negative, while that of technology uncertainty is positive. However, demand uncertainty just has a significant effect on internal integration. The relationship between demand uncertainty and external integration (including customer integration and supplier integration) is mediated by the internal integration.", "title": "The effect of environmental uncertainty on supply chain integration in Chinese manufacturing industry", "claims": null}], "query": "\"Environmental uncertainty would moderate the relationship between demand risk and supply chain disruption impact such that the relationship would become stronger when environmental uncertainty is high.\"", "summary_abstract": "The research question regarding the moderating role of environmental uncertainty on the relationship between demand risk and supply chain disruption impact is addressed in several studies, which collectively highlight the complexities of this interaction.\n\nFynes et al. (2004) provide evidence that environmental uncertainty significantly influences supply chain dynamics, supporting the notion that both demand and supply uncertainties can exacerbate supply chain disruptions. Their findings suggest that as environmental uncertainty increases, the risks associated with supply chain disruptions also heighten.\n\nSreedevi and Saranga (2017) further elaborate on this relationship by demonstrating that firms operating under high environmental uncertainty face increased risks of supply disruptions and operational delays. Their research indicates that in such uncertain environments, the flexibility of supply chains becomes crucial in mitigating these risks, thereby reinforcing the idea that environmental uncertainty intensifies the impact of demand risk on supply chain disruptions.\n\nSingh (2020) adds another layer by examining how environmental uncertainty negatively affects firm financial performance, emphasizing the importance of effective supply chain risk management practices. This suggests that organizations that can navigate environmental uncertainties are better positioned to manage the adverse effects of demand risks.\n\nGanbold and Matsui (2017) explore the broader implications of environmental uncertainty on supply chain integration, indicating that increased uncertainty necessitates greater integration among supply chain partners. This integration can help firms respond more effectively to demand risks, thereby potentially reducing the impact of disruptions.\n\nInman and Green (2021) propose that supply chain agility mediates the relationship between environmental uncertainty and supply chain performance. Their findings imply that while environmental uncertainty does not directly correlate with performance, it drives the need for agility, which in turn can mitigate the effects of demand risks.\n\nLastly, Ko\u00e7 et al. (2022) confirm that environmental uncertainty has a direct and positive effect on competitive advantage, with supply chain integration and agility serving as mediators. This suggests that in high-uncertainty environments, firms that enhance their supply chain capabilities can better manage demand risks and minimize disruption impacts.\n\nIn summary, the collective findings from these studies indicate that environmental uncertainty indeed moderates the relationship between demand risk and supply chain disruption impact, with higher uncertainty leading to stronger adverse effects. Flexibility, integration, and agility within supply chains emerge as critical factors in mitigating these risks (Fynes et al., 2004; Sreedevi & Saranga, 2017; Singh, 2020; Ganbold & Matsui, 2017; Inman & Green, 2021; Ko\u00e7 et al., 2022).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2022}, "authors": ["Karolina A. Serhan", "Tamara L. Kemp"], "summary": "Summary: Phyllodes tumors are rare fibroepithelial breast tumors representing less than 1% of all breast malignancies, with an extremely uncommon presence in the pediatric population.1 Although prognosis is favorable following excision given their indolent course, they often grow rapidly and frequently recur. As such, they can present unique oncologic and reconstructive challenges. Herein we present a case of a malignant phyllodes tumor in an 11-year-old girl treated with total skin-sparing mastectomy and adjustable saline implant, and explore the reconstructive challenges of this unique case.", "title": "Immediate Breast Reconstruction in an 11-year-old Girl with a Large Malignant Phyllodes Tumor", "claims": null}, {"metadata": {"year": 2014}, "authors": ["E. Houseman", "T. Ince"], "summary": "Historically, breast cancer classification has relied on prognostic subtypes. Thus, unlike hematopoietic cancers, breast tumor classification lacks phylogenetic rationale. The feasibility of phylogenetic classification of breast tumors has recently been demonstrated based on estrogen receptor (ER), androgen receptor (AR), vitamin D receptor (VDR) and Keratin 5 expression. Four hormonal states (HR0\u20133) comprising 11 cellular subtypes of breast cells have been proposed. This classification scheme has been shown to have relevance to clinical prognosis. We examine the implications of such phylogenetic classification on DNA methylation of both breast tumors and normal breast tissues by applying recently developed deconvolution algorithms to three DNA methylation data sets archived on Gene Expression Omnibus. We propose that breast tumors arising from a particular cell-of-origin essentially magnify the epigenetic state of their original cell type. We demonstrate that DNA methylation of tumors manifests patterns consistent with cell-specific epigenetic states, that these states correspond roughly to previously posited normal breast cell types, and that estimates of proportions of the underlying cell types are predictive of tumor phenotypes. Taken together, these findings suggest that the epigenetics of breast tumors is ultimately based on the underlying phylogeny of normal breast tissue.", "title": "Normal Cell-Type Epigenetics and Breast Cancer Classification: A Case Study of Cell Mixture\u2013Adjusted Analysis of DNA Methylation Data from Tumors", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Meher Charfi", "S. Ka", "A. Dem"], "summary": "No abstract.", "title": "Breast reconstruction after breast cancer", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Benjamin Yongcheng Tan", "Nur Diyana Md Nasir", "Huan Ying Chang", "Cedric Chuan Young Ng", "Peiyong  Guan", "Sanjanaa  Nagarajan", "Vikneswari  Rajasegaran", "Jing Yi Lee", "Jing Quan Lim", "Aye Aye Thike", "Bin Tean Teh", "Puay Hoon Tan"], "summary": "Breast fibroepithelial lesions (FELs) encompass the common fibroadenoma (FA) and relatively rare phyllodes tumour (PT); the latter entity is usually classified as benign, borderline or malignant. Intratumoural heterogeneity is frequently present in these tumours, making accurate histologic evaluation challenging. Despite their rarity, PTs are an important clinical problem due to their propensity for recurrence and, in the case of malignant PT, metastasis. Surgical excision is the mainstay of management. Recent work has uncovered myriad genetic alterations in breast FELs. In this study, exome sequencing was performed on seven cases of morphologically heterogeneous breast FELs, including FAs, PTs of all grades, and a case of metaplastic spindle cell carcinoma arising in PT, in order to elucidate their intratumoural genetic repertoire. Gene mutations identified encompassed cell signalling, tumour suppressor, DNA repair and cell cycle regulating pathways. Mutations common to multiple tumour regions generally showed higher variant allele frequency. Frequent mutations included MED12 , TP53 , RARA and PIK3CA . Histological observations of increased cellular density and pleomorphism correlated with mutational burden. Phylogenetic analyses revealed disparate pathways of possible tumour progression. In summary, histological heterogeneity correlated with genetic changes in breast FELs.", "title": "Morphologic and genetic heterogeneity in breast fibroepithelial lesions\u2014a comprehensive mapping study", "claims": null}, {"metadata": {"year": 2020}, "authors": ["J. Kutasovic", "A. M. McCart Reed", "A. Sokolova", "S. Lakhani", "P. Simpson"], "summary": "Breast cancer is a remarkably complex and diverse disease. Subtyping based on morphology, genomics, biomarkers and/or clinical parameters seeks to stratify optimal approaches for management, but it is clear that every breast cancer is fundamentally unique. Intra-tumour heterogeneity adds further complexity and impacts a patient\u2019s response to neoadjuvant or adjuvant therapy. Here, we review some established and more recent evidence related to the complex nature of breast cancer evolution. We describe morphologic and genomic diversity as it arises spontaneously during the early stages of tumour evolution, and also in the context of treatment where the changing subclonal architecture of a tumour is driven by the inherent adaptability of tumour cells to evolve and resist the selective pressures of therapy.", "title": "Morphologic and Genomic Heterogeneity in the Evolution and Progression of Breast Cancer", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Kathleen  Sprouffske", "Grainne  Kerr", "Cheng  Li", "Anirudh  Prahallad", "Ramona  Rebmann", "Verena  Waehle", "Ulrike  Naumann", "Hans  Bitter", "Michael R Jensen", "Francesco  Hofmann", "Saskia M Brachmann", "St\u00e9phane  Ferretti", "Audrey  Kauffmann"], "summary": "Graphical abstract", "title": "Genetic heterogeneity and clonal evolution during metastasis in breast cancer patient-derived tumor xenograft models", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Agla Jael Rubner Fridriksdottir", "Ren\u00e9  Villadsen", "Thorarinn  Gudjonsson", "Ole William Petersen"], "summary": "Recent genome-wide expression analysis of breast cancer has brought new life to the classical idea of tumors as caricatures of the process of tissue renewal as envisioned by Pierce and Speers (Cancer Res 1988;48:1996\u20132004) more than a decade ago. The search for a cancer founder cell or different cancer founder cells is only possible if a hierarchy of differentiation has been established for the particular tissue in question. In the human breast, the luminal epithelial and myoepithelial lineages have been characterized extensively in situ by increasingly elaborate panel of markers, and methods to isolate, culture, and clone different subpopulations have improved dramatically. Comparisons have been made with the mouse mammary gland in physiological three-dimensional culture assays of morphogenesis, and the plasticity of breast epithelial cells has been challenged by immortalization and transformation. As a result of these efforts, several candidate progenitor cells have been proposed independently of each other, and some of their features have been compared. This research has all been done to better understand breast tissue homeostasis, cell-type diversification in general and breast cancer evolution in particular. The present review discusses the current approaches to address these issues and the measures taken to unravel and maintain cell type diversification for further investigation.", "title": "Maintenance of Cell Type Diversification in the Human Breast", "claims": null}, {"metadata": {"year": 2018}, "authors": ["M. Ejjiyar", "M. Elgueouatri", "Mehdi Sahibi", "Abdelkoddous Bhihi", "I. Yafi", "M. Mahrouch", "Moulay", "D. Elamrani", "Y. Benchamkha"], "summary": "Article History Received: 19.11.2018 Accepted: 28.11.2018 Published: 30.12.2018 Abstract: Phyllodes tumors of the breast are rare fibroepithelial tumors in their malignant form, the gravity lies in high risk of local recurrence. We report two observations of patients admitted for the management of local recurrence of phyllode breast tumor. The average age of our patients was 39 years old. The size of the tumor ranged from 10 cm to 30 cm in diameter, with a localization exclusively on the right. Surgical management consisted of a enlarged tumor resection, followed by radiotherapeutic complement. Histologically, it was a phyllode tumor initially benign with slowly progressive evolution, to a phyllode sarcoma at one patient, and to a liposarcoma in the other. The evolution towards a malignant transformation can be explained in both cases by incomplete previous excisional gestures. This underlines the importance of early diagnosis and surgical management responding to oncological rules.", "title": "Phyllode Tumors: Breast Tumors with a High Potential for Recurrence- About Two Cases", "claims": null}], "query": "phylogeny reconstruction in breast cancer", "summary_abstract": "The exploration of phylogeny reconstruction in breast cancer reveals significant insights into the genetic and epigenetic underpinnings of this complex disease. Houseman and Ince (2014) highlight the potential for a phylogenetic classification of breast tumors based on hormonal receptor expression, suggesting that the epigenetic states of tumors reflect their cell-of-origin. This classification not only aids in understanding tumor biology but also has implications for clinical prognosis, as it correlates with DNA methylation patterns in both tumors and normal breast tissues.\n\nFurther supporting the importance of phylogenetic analysis, Tan et al. (2020) conducted exome sequencing on various breast fibroepithelial lesions, including phyllodes tumors. Their findings indicate that intratumoral heterogeneity is linked to genetic alterations, with specific mutations associated with tumor progression pathways. This suggests that phylogenetic relationships can elucidate the evolutionary trajectories of these tumors, providing a clearer understanding of their behavior and potential treatment responses.\n\nKutasovic et al. (2020) emphasize the inherent complexity of breast cancer, noting that intra-tumor heterogeneity complicates treatment strategies. They discuss how the evolutionary dynamics of tumors can lead to diverse morphologic and genomic characteristics, which are crucial for developing effective management approaches.\n\nIn summary, the collective research underscores the significance of phylogenetic reconstruction in breast cancer, revealing how genetic and epigenetic factors contribute to tumor diversity and behavior. This understanding is essential for improving diagnostic and therapeutic strategies in managing breast cancer (Houseman & Ince, 2014; Tan et al., 2020; Kutasovic et al., 2020).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2019}, "authors": ["Tarmo  Tuisk", "Gunnar  Prause"], "summary": "Since the 1st of January 2013 the public transport is free for residents of the City of Tallinn. People registered outside Tallinn who want to use the public transport have to pay for their trips. Parallel, Tallinn City Governance raised the parking tariffs in the city centre which caused an increase of using of the public transport by 10% and a decrease of car traffic in the city centre by 6%. Currently, the greening of urban transport enjoys a high rank on the political agenda in a lot of European and even some cities are thinking to follow the example of Tallinn. Literature review reveals that scientific papers are focussing mainly on ecological aspects whereas socio-economic studies are neglected. The paper will present insights in social, economic and political aspects which are related to the experience of free public transport in Tallinn. Furthermore, the paper will highlight the role of digitalisation in public transport and discuss the influence of the Estonian e-governance system for the success of the Tallinn case.", "title": "Socio-Economic Aspects of Free Public Transport", "claims": null}, {"metadata": {"year": 2016}, "authors": ["D. Gabald\u00f3n-Estevan"], "summary": "Urban areas are of increasing relevance when it comes to sustainability. \u2022\u00a0\u00a0\u00a0\u00a0\u00a0 First, about half of the world\u2019s population now lives in cities (increasing to 60% by 2030). \u2022\u00a0\u00a0\u00a0\u00a0\u00a0 Second, cities are nowadays responsible for levels of resource consumption and waste generation that are higher beyond their share on world population. \u2022\u00a0\u00a0\u00a0\u00a0\u00a0 Third, cities are more vulnerable to disruptive events that can lead to restrictions on the provision of resources and to changes on the environment caused by climate change. \u2022\u00a0\u00a0\u00a0\u00a0\u00a0 And fourth, because they concentrate key resources (political, social, cultural\u2026), cities are seen as strategic scenarios where to experiment and develop solutions to cope with the prevailing sustainability challenges driven by the major social and environmental transformations. Urban agglomerations can be seen as complex innovation systems where human activities are shaped in order to transform societies towards sustainable development. For this paper, we focus on the case of an environmental innovation regarding transport policy, the implementation of the fare-free policy on public transport for all inhabitants of Tallinn, Estonia. Tallinn, with 414,000 inhabitants in 2015, is the capital of Estonia and the largest city in the country. Over the last two decades the share of public transport trips decreased dramatically. After a public opinion poll in 2012, in which over 75% of the participants voted for a fare-free public transportation system (FFPTS) in Tallinn, the new policy was implemented on 1st January 2013. From that date on inhabitants of Tallinn could use all public transport services (busses, trams, trolly-busses) operated by city-run operators for free. Later the fare-free system was implemented also on trains within Tallinn. In this paper we analyze the context, in which this policy was implemented, the main characteristics of its implementation and its actual situation. DOI: http://dx.doi.org/10.4995/CIT2016.2016.3532", "title": "Environmental innovation through transport policy. The implementation of the free fare policy on public transport in Tallinn, Estonia", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Daniel Baldwin Hess"], "summary": "Abstract Among many possible interventions in public transport finance and policy designed to enhance the attractiveness of riding public transport, one of the most extreme, which is seldom implemented, is the elimination of passenger fares, effectively making public transport \u201cfree\u201d for riders (with operating costs paid from other funding sources). This article describes a fare-free public transport program in Tallinn, Estonia, launched in 2013, which has exhibited lower-than-expected increases in ridership. Evaluations of Tallinn\u2019s fare-free public transport program are presented and synthesized, with a focus on program goals and how goals are met through program performance. Findings suggest certain flaws limit the program\u2019s potential success since the program design is misaligned with its primary stated goals, and several program goals relating to external effects of fare reform cannot be evaluated. Although it would be valuable for transport managers in other cities to learn about this experience, the Tallinn fare-free public transport program provides scant transferable evidence about how such a program can operate outside of a politicized context, which was crucial to its implementation in Estonia.", "title": "Decrypting fare-free public transport in Tallinn, Estonia", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Derek Galey"], "summary": "The City of Tallinn, capital of Estonia, with a population of 420,000, recently became the world's largest municipality offering free public transportation. Tourists still have to pay to ride the city's bus, trolley, and tram network, but registered residents\u2014including a large population of Russian-speaking non-citizens\u2014only have to tap their municipal transit cards once onboard. Tallinn's leadership has justi\ufffd\u8000ed the policy on environmental and social grounds\u2014namely, reducing carbon dioxide emissions and providing equal rights to freedom of movement. Although only 26% of trips in Tallinn utilize private cars, private transportation accounts for 60% of the city's carbon dioxide emissions. Public transportation, which provides 40% of trips in Tallinn, accounts for only 6-7% of the city's total emissions. On an annual municipal public transport satisfaction survey from 2010, 49% of the respondents were most unsatis- \ufffd\u8000ed with fares, followed by crowding (29%) and frequency (21%). (Cats, Susilo, and Eliasson 2012, 3-4) The city's government responded by calling a March 2012 referendum, in which 75% of voters supported free public transportation. In contrast with past experiences with free public transportation in other cities, preliminary results indicate a \"relatively small increase in passenger demand\" of only 3% citywide in the three-month period after implementation (Cats, Susilo, and Reimal 2014, 5). Notably, however, passenger counts increased 10% in Lasnamae, a populous and dense housing district with a price-sensitive population and many Russian-speaking residents.", "title": "Free Public Transportation for Residents of Tallinn", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Derek Galey"], "summary": "The City of Tallinn, capital of Estonia, with a population of 420,000, recently became the world\u2019s largest municipality offering free public transportation. Tourists still have to pay to ride the city\u2019s bus, trolley, and tram network, but registered residents\u2014including a large population of Russian-speaking non-citizens\u2014only have to tap their municipal transit cards once onboard. This article presents a qualitative account of the world\u2019s largest free public transporta- tion experiment to date. The results challenge and inform the conventional measures and objectives of transportation experts. The analysis is meant to complement the existing literature surveying free public transportation experiments and evaluating transportation pricing schemes.", "title": "License to Ride: Free Public Transportation for Residents of Tallinn", "claims": null}, {"metadata": {"year": 2019}, "authors": ["D. Gabald\u00f3n-Estevan", "K. Orru", "C. Kaufmann", "H. Orru"], "summary": "ABSTRACT In this paper, we focus on the rationale for implementing the fare-free public transportation system (FFPTS) in Tallinn, Estonia, that took place on 1 January 2013. Through a series of interviews with relevant informants, we identify the main enablers and the FFPTS in Tallinn faced and the potential of such a system to contribute to the sustainable city development. Our analysis shows that the interlinking between local and national politics determines not only the type of initiatives implemented and the support they receive but also the degree of their success and their stability. We conclude that to be even more effective, it should be extended to the all potential users, not just to local registered residents as it has been recently applied in state-run bus travels in rural municipalities in Estonia. Finally, more restrictive private car policies should be considered to fuel a sustainable mobility transition and increase cities life quality.", "title": "Broader impacts of the fare-free public transportation system in Tallinn", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Wojciech K\u0119b\u0142owski", "T. Tuvikene", "T. Pikner", "J. Jauhiainen"], "summary": "In this article, we study the largest existing fare-free public transport (FFPT) programme, launched in 2013 in Tallinn, Estonia. Instead of focusing solely on the rationale and impact of fare-free public transport in terms of finances and travel patterns, we propose to analyse FFPT from the perspective of urban political geography, and to inquire into its political and scalar dynamics. We analyse how Tallinn\u2019s fare-free programme was developed, and demonstrate the politics of its conception and implementation. We observe who has access to free travel and we reveal how FFPT is embedded in Estonia\u2019s place-of-residence-based taxation system. Finally, we identify where lies the impact of territorial competition exacerbated by FFPT. Therefore, we argue that transport policies \u2013 of which FFPT is but an example \u2013 should be understood as much more than strategies dealing with transport issues per se. Instead, we propose to approach them as political and spatial projects, whose processual, cross-sectorial and scalar dimensions help to reveal the embeddedness of transport in inherently urban questions of metropolitan governance, electoral strategies, territorial competition and socio-spatial inequalities.", "title": "Towards an urban political geography of transport: Unpacking the political and scalar dynamics of fare-free public transport in Tallinn, Estonia", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Gunnar  Prause", "Tarmo  Tuisk"], "summary": "Abstract Tallinn's fare-free public transportation (FFPT) for the residents of the city has been vital for over 6\u00a0years, and there is no evidence of major setbacks. While expectations about political impact were high, the implementation if FFPT still did not demonstrate any significant change in this respect. The usage of public transport rose from 55% to 63%. Within Estonia's developed e-governance framework, the optimization of transport lines and costs has been taken place effectively. Pollution and emission levels in the city have not increased due to the stricter environmental requirements for vehicles and fuels, although the number of cars is continuously rising. As the satisfaction with FFPT transport services in Tallinn is rather high, also other municipalities of Estonia have started to implement the same concept in their transportation.", "title": "Case study", "claims": null}], "query": "Tallinn emerged as a pioneer by introducing free public transport in 2013.", "summary_abstract": "The introduction of free public transport in Tallinn, Estonia, on January 1, 2013, marked a significant shift in urban transport policy, positioning the city as a pioneer in this area. The initiative was primarily aimed at enhancing social equity and reducing environmental impact, as evidenced by the substantial public support it garnered\u2014over 75% of voters favored the fare-free system in a 2012 referendum (Gabald\u00f3n-Estevan, 2016; Galey, 2014).\n\nThe policy allowed registered residents to use all city-operated public transport services without charge, which was expected to increase ridership and decrease reliance on private vehicles. Initial evaluations indicated a modest increase in public transport usage, with a 10% rise in certain districts, although overall citywide ridership only saw a 3% increase shortly after implementation (Galey, 2014; Hess, 2017). Despite these figures, the program has been credited with contributing to a reduction in car traffic and associated emissions, as public transport accounts for a smaller percentage of the city's total carbon footprint compared to private vehicles (Tuisk & Prause, 2019).\n\nThe success of Tallinn's fare-free public transport system has prompted discussions about its potential replication in other cities, particularly in the context of urban sustainability and governance (Gabald\u00f3n-Estevan et al., 2019; K\u0119b\u0142owski et al., 2019). However, some studies suggest that the program's design may not fully align with its goals, indicating that while it has been beneficial, there are areas for improvement, particularly in expanding access beyond just registered residents (Hess, 2017; Prause & Tuisk, 2020).\n\nOverall, Tallinn's initiative has not only served as a case study in urban transport reform but has also highlighted the complex interplay between local governance, public policy, and environmental sustainability, making it a noteworthy example for other municipalities considering similar measures (Tuisk & Prause, 2019; K\u0119b\u0142owski et al., 2019).", "summary_extract": null}, {"papers": [{"metadata": {"year": 1993}, "authors": ["Markus  See\u03b2elberg", "Francesco  Petruccione"], "summary": "Abstract The solution of stochastic partial differential equations generally relies on numerical tools. However, conventional numerical procedures are not appropriate to solve such problems. In this paper an algorithm is proposed which allows the numerical treatment of a large class of stochastic partial differential equations. To this end we reduce stochastic partial differential equations to a system of stochastic ordinary differential equations which can be solved numerically by a well-known stochastic Euler-procedure. We apply our algorithm to two stochastic partial differential equations which are special examples because their stationary two-point correlation functions can be determined analytically. Our algorithm proves to work out very well when numerical results are compared with the analytic correlation function.", "title": "Numerical integration of stochastic partial differential equations", "claims": null}, {"metadata": {"year": 1992}, "authors": ["Tadahisa  Funaki"], "summary": "Abstract We investigate a certain stochastic partial differential equation which is defined on the unit interval with periodic boundary condition and takes values in a manifold. Such equation has particularly two different applications. Namely, it determines the evolution law of an interacting constrained system of continuum distributed over the unit circle, while it defines a diffusive motion of loops on a manifold. We establish the existence and uniqueness results and then show the smoothness property of the solutions. Some examples are given in the final section.", "title": "A stochastic partial differential equation with values in a manifold", "claims": null}, {"metadata": {"year": 1987}, "authors": ["G. Prato", "L. Tubaro"], "summary": "Existence and uniqueness results for a non linear stochastic partial differential equation.- Continuity in non linear filtering some different approacees.- Expectation functionals associated with some stochastic evolution equations.- Dirichlet boundary value problem and optimal control for a stochastic distributed parameter system.- Stochastic product integration and stochastic equations.- Some remarks on a problem in stochastic optimal control.- Passage from two-parameters to infinite dimension.- The heat equation and fourier transforms of generalized brownian functionals.- The separation principle for stochastic differential equations with unbounded coefficients.- Weak convergence of measure valued processes using sobolev-imbedding techniques.- Probability distributions of solutions to some stochastic partial differential equations.- Two-sided stochastic calculus for spdes.- Convergence of implicit discretization schemes for linear differential equations with application to filtering.- Some applications of the Malliavin calculus to stochastic analysis.- Exit problem for infinite dimensional systems.", "title": "Stochastic partial differential equations and applications : proceedings of a conference held in Trento, Italy, Sept. 30-Oct. 5, 1985", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Yijun Li", "Guanggan Chen", "Ting Lei"], "summary": "This work is concerned with a class of stochastic partial differential equations with a fast random dynamical boundary condition. In the limit of fast diffusion, it derives an effective stochastic partial differential equation to describe the evolution of the dominant pattern. Using the multiscale analysis and the averaging principle, it then establishes deviation estimates of the original stochastic system towards the effective approximating system. A concrete example further illustrates the result on a large time scale.", "title": "Approximate dynamics of stochastic partial differential equations under fast dynamical boundary conditions", "claims": null}, {"metadata": {"year": 2013}, "authors": ["N. Hema", "A. Jeyalakshmi"], "summary": "In this paper, we propose a stochastic differential equation model where the underlying stochastic process is a jumpdiffusion process.The stochastic differential equation is represented as a Partial Integro Differential Equation(PIDE) using the Fokker Planck equation. The solution of the PIDE is obtained by the method of finite differences. The consistency, the convergence of the solution and the stability of the finite difference scheme are discussed. The model is applied to forecast the daily price changes in a commodity derivative. The observed values are compared graphically with the values expected from the proposed model.", "title": "A Stochastic Differential Equation Model", "claims": null}, {"metadata": {"year": 1996}, "authors": ["H. Holden", "B. \u00d8ksendal", "J. Ub\u00f8e", "Tusheng Zhang"], "summary": "Framework.- Applications to Stochastic Ordinary Differential Equations.- Stochastic Partial Differential Equations Driven by Brownian White Noise.- Stochastic Partial Differential Equations Driven by L#x00E9 vy Processes.", "title": "Stochastic Partial Differential Equations: A Modeling, White Noise Functional Approach", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Chi Hong Wong", "X. Yang", "Jing Zhang"], "summary": "We study a class of stochastic partial integral-differential equations with an asymmetrical non-local operator 1 2 \u2206+a\u2206 \u03b1 2 +b \u00b7\u2207 and a distribution expressed as divergence of a measurable field. For 0 < \u03b1 < 2, the existence and uniqueness of solution is proved by analytical method, and a probabilistic interpretation, similar to the Feynman-Kac formula, is presented for 0 < \u03b1 < 1. The method of backward doubly stochastic differential equations is also extended in this work.", "title": "Stochastic partial integral-differential equations with divergence terms", "claims": null}, {"metadata": {"year": 2007}, "authors": ["Wei Wang", "Jinqiao Duan"], "summary": "Abstract In order to understand the impact of random influences at physical boundary on the evolution of multiscale systems, a stochastic partial differential equation model under a fast random dynamical boundary condition is investigated. The noises in the model and in the boundary condition are both additive. An effective equation is derived and justified by reducing the random dynamical boundary condition to a simpler one. The effective system is still a stochastic partial differential equation. Furthermore, the quantitative comparison between the solution of the original stochastic system and the effective solution is provided by establishing normal deviations and large deviations principles. Namely, the normal deviations are asymptotically characterized, while the rate and speed of the large deviations are estimated.", "title": "Reductions and Deviations for Stochastic Partial Differential Equations Under Fast Dynamical Boundary Conditions", "claims": null}], "query": "application of algebraic analysis for stochastic partial differential equation", "summary_abstract": "The application of algebraic analysis to stochastic partial differential equations (SPDEs) is explored through various methodologies and frameworks in the literature. See\u00dfelberg and Petruccione (1993) propose an innovative algorithm that transforms SPDEs into a system of stochastic ordinary differential equations, which can be effectively solved using a stochastic Euler procedure. Their approach demonstrates strong numerical performance when compared to analytic solutions, indicating a successful application of algebraic techniques in this context.\n\nFunaki (1992) investigates a specific SPDE defined on a manifold, establishing existence and uniqueness results for its solutions. This work highlights the algebraic structure inherent in the equations governing the evolution of interacting systems, showcasing the potential for algebraic analysis to elucidate the properties of solutions.\n\nLi et al. (2021) delve into SPDEs with fast random dynamical boundary conditions, deriving effective equations that describe the dominant patterns in the system. Their use of multiscale analysis and averaging principles illustrates how algebraic methods can be employed to simplify complex stochastic systems and derive meaningful approximations.\n\nWang and Duan (2007) also focus on the impact of random influences at physical boundaries, deriving effective SPDEs from more complex models. Their work emphasizes the importance of algebraic techniques in understanding the behavior of solutions under varying conditions, further supporting the utility of algebraic analysis in this field.\n\nOverall, the collective findings from these studies underscore the significant role that algebraic analysis plays in the understanding and solution of stochastic partial differential equations, providing both theoretical insights and practical algorithms for their application.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2010}, "authors": ["Alexandros  Flamos", "Katherine  Begg"], "summary": "The purpose of technology transfer under the UNFCCC Article 4.5 is to \u201c\u2026promote, facilitate, and finance as appropriate the transfer of, or access to, environmentally sound technologies and know how to other Parties particularly Developing Country parties to enable them to implement the provisions of the Convention.\u201d The key challenge in this respect is that low-carbon sustainable technologies need to be adopted both by developed as well as developing countries. However, this paper focuses on the process of technology transfer to developing countries to allow them to move quickly to environmentally sound and sustainable practices, institutions and technologies. In the above framework, this paper reviews key aspects of technology transfer from a range of perspectives in the literature and discusses insights from this literature for the transfer and innovation process needed to reduce global vulnerability to climate change in the context of current international activities based on the research undertaken by the EU sponsored ENTTRANS project.", "title": "Technology transfer insights for new climate regime", "claims": null}, {"metadata": {"year": 1979}, "authors": ["K. H. Veldhuis"], "summary": "It is said nowadays that commercial companies, and particularly multinational ones, are one of the most important routes via which technology is transferred to developing countries. What is often called into question, however, is how appropriate is a particular technology to the country concerned. What, in fact, is meant by \u2018appropriate\u2019 in this context?", "title": "Transfer and Adaptation of Technology: Unilever as a Case Study", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Charikleia  Karakosta", "Haris  Doukas", "John  Psarras"], "summary": "Climate change mitigation is considered as a high priority internationally and is placed in the top of the agenda for most politicians and decision makers. The key challenge is that low-carbon sustainable technologies need to be adopted both by developed as well as developing countries, in an effort to avoid past unsustainable practices and being locked into old, less sustainable technologies. Technology transfer (TT), as an important feature of both the United Nations Framework Convention on Climate Change (UNFCCC) and its Kyoto Protocol can play a key role. TT can allow countries to move quickly to environmentally sound and sustainable practices, institutions and technologies. Indeed, the transfer or innovation process must be fast enough, to reduce global vulnerability to climate change. The aim of this paper is to analyse the TT challenges and emerging opportunities under the new climate regime, in terms of the process of innovation into an existing energy system, the related barriers and the supporting diffusion mechanisms. Good practices for renewable energy are also presented and discussed by both the developed and the developing countries in this respect.", "title": "Technology transfer through climate change: Setting a sustainable energy pattern", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Nicol\u00e1s M. Perrone"], "summary": "The role of technology transfer in climate change negotiations is vital. If technology is to help us mitigate and adapt to climate change, the international community needs to ensure sufficient innovation and technology transfer. One of the main challenges of the technology transfer regime for environmentally sound technologies is that a private and market-led model may not meet global technology transfer needs. This policy brief suggests that governments should explore market, hybrid and non-market approaches to accelerate the transfer of environmentally sound technologies. Developing countries\u2019 governments should also explore cooperative approaches to improve their bargaining power, reduce costs and ensure adaptation and innovation capacity in the developing world", "title": "Technology Transfer and Climate Change: A developing country perspective", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Chen Zhou"], "summary": "Climate friendly technologies contribute to tackling global climate crisis and the dynamic transfer of these technologies is important to achieve universal climate actions. The UNFCCC, and its recent Paris Agreement, have introduced international assistance to promote climate related-technology transfer. They call for collaborative actions from both technology supplier and demander sides in order to enable environments for a meaningful and effective technology transfer. According to the UNFCCC, the international technology assistance is unlikely to work in a desired way with the absence of indigenous enabling environments. Therefore, it is crucial to identify, assess and overcome potential barriers potentially confronted by host countries in their acquisition of climate technologies, which helps prepare these countries for climate resilience economy and sustainable development. This paper attempts to provide a deep and comprehensive analysis on enabling policy/law environments in host countries and uses Asian countries as examples in most occasions.", "title": "Enabling Law and Policy Environment for Climate Technology Transfer: From Perspectives of Host Countries", "claims": null}, {"metadata": {"year": 2012}, "authors": ["May  Elsayyad", "Florian  Morath"], "summary": "This paper considers investments in cost-reducing technology in the context of contributions to climate protection. Contributions to mitigating climate change are analyzed in a two-period public goods framework where later contributions can be based on better information, but delaying the contribution to the public good is costly because of irreversible damages. We show that, when all countries have access to the new technology, countries have an incentive to invest in technology because this can lead to an earlier contribution of other countries and therefore reduce a country's burden of contributing to the public good. Our results provide a rationale for the support of technology sharing initiatives.", "title": "Technology Transfers for Climate Change", "claims": null}, {"metadata": {"year": 2012}, "authors": ["J. Lovett", "P. Hofman", "K. Morsink", "J. Clancy"], "summary": "A key element of climate change mitigation and adaptation is the transfer of more effective and efficient low-carbon technologies between developed and developing countries. Although several policy mechanisms for technology transfer are in place, most observers agree that these have not been very effective in accelerating the rate of diffusion of energy-efficient and renewable-based technologies. There is a need for market-oriented approaches in order to diffuse efficient technologies more rapidly and to reduce high transaction costs, which are a major factor explaining the low effectiveness of existing mechanisms (Michaelowa and Jotzo, 2005; Jung, 2006; Hofman et al, 2008; Lovett et al, 2009; Byigero et al, 2010; Timilsina et al, 2010). At the 2007 G8 Summit in Heiligendamm, it was recognized that an \u2018expanded approach to collaboratively accelerate the widespread adoption of clean energy and climate friendly technology\u2019 was needed (G8, 2007). In successive outputs from the United Nations Framework Convention on Climate Change (UNFCCC) negotiations, such as the 2007 Bali Action Plan, the 2008 Poznan Strategic Programme on Technology Transfer, the 2009 Copenhagen Accord and the 2010 Cancun Technology Mechanism, the requirement for scaling up technology transfer features prominently. The problem is that project-based funding mechanisms, such as those under the Global Environment Facility (GEF) and Clean Development Mechanism (CDM), can never do more than provide a fraction of the resources needed to transfer sufficient environmentally sound technologies to permit economic advancement of developing countries while minimizing greenhouse gas emissions;", "title": "Technology Transfer and Global Markets", "claims": null}, {"metadata": {"year": 2011}, "authors": ["D. Popp"], "summary": "As the developed world begins efforts to limit its emissions of greenhouse gases, economic growth in developing countries is causing increased emissions from the developing world. Reducing these emissions while still enabling developing countries to grow requires the use of climate-friendly technologies in these countries. In most cases, these technologies are first created in high-income countries. Thus, the challenge for climate policy is to encourage the transfer of these climate-friendly technologies to the developing world. This article reviews the economic literature on environmental technology transfer and discusses the implications of this literature for climate policy, focusing on the Clean Development Mechanism (CDM). A key point is that technology diffusion is gradual. Early adoption of policy by developed countries leads to the development of new technologies that make it easier for developing countries to reduce pollution as well. Since clean technologies are first developed in the world's leading economies, international trade and foreign investments provide access to these technologies. Moreover, evidence suggests that some technologies, such as those enhancing energy efficiency, will diffuse to developing countries even without the aid of policy prescriptions, such as the CDM. This is important for assessing the potential emissions reductions of proposed CDM projects.", "title": "International Technology Transfer, Climate Change, and the Clean Development Mechanism", "claims": null}], "query": "Technology Transfer: COP26 emphasized the importance of technology transfer from developed to developing countries to foster climate adaptation and mitigation. This entails sharing and facilitating access to clean and sustainable technologies that can support low-carbon pathways for development. Unilever, as a global player in the consumer goods industry, has made significant contributions through technology transfer. For instance, the company's Sustainable Living Plan includes initiatives such as sharing renewable energy solutions, water-saving technologies, and sustainable packaging innovations with suppliers and partners across its value chain. Unilever's commitment to technology transfer supports the agenda set at COP26, promoting the equitable distribution of climate-friendly technologies for a more sustainable future.", "summary_abstract": "The collective findings from the reviewed papers underscore the critical role of technology transfer in addressing climate change, particularly in the context of the commitments made at COP26. The emphasis is on the necessity for developed countries to share environmentally sound technologies with developing nations to facilitate their transition to low-carbon pathways.\n\nFlamos and Begg (2010) highlight that technology transfer is essential for enabling developing countries to adopt sustainable practices and technologies, which is a key aspect of the UNFCCC framework. They argue that the process must be efficient to effectively reduce global vulnerability to climate change. Similarly, Karakosta et al. (2010) stress that technology transfer can help countries avoid past unsustainable practices and quickly adopt environmentally sound technologies.\n\nPerrone (2022) points out that the current technology transfer regime may not adequately meet the needs of developing countries, suggesting that a mix of market and non-market approaches should be explored to enhance the transfer of clean technologies. Zhou (2019) further emphasizes the importance of creating enabling environments in host countries to facilitate the acquisition of climate technologies, which is crucial for building climate resilience.\n\nMoreover, Lovett et al. (2012) note that while various policy mechanisms for technology transfer exist, their effectiveness has been limited. They advocate for market-oriented approaches to accelerate the diffusion of energy-efficient technologies, which aligns with the need for collaborative efforts highlighted in the COP26 agenda. Popp (2011) adds that the gradual diffusion of climate-friendly technologies from developed to developing countries is essential for reducing emissions while allowing for economic growth in the latter.\n\nIn summary, the literature collectively supports the notion that effective technology transfer is vital for fostering climate adaptation and mitigation in developing countries. It calls for innovative approaches and collaborative efforts to ensure that these nations can access and implement sustainable technologies, thereby contributing to a more equitable and sustainable future as envisioned in the COP26 discussions.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2016}, "authors": ["Steven Arnocky"], "summary": "Men, more than women, prefer physically attractive and sexually faithful mates. Women and girls who are high on these desired traits might also be more popular within their same-sex peer networks. Accordingly, women, more than men, are motivated to display and accentuate these matevalue and peer-valued characteristics, which are, in part, signals of reproductively and socially relevant information. Social-cognitive mechanisms such as social comparison, as well as emotions such as envy, appear to underlie some behaviors associated with women\u2019s desire to be considered among desirable women. Across diverse human cultures, men\u2019s and women\u2019s mate preferences have been shown to diverge in reproductively important ways. One of the most consistent sex differences is men\u2019s greater expressed preference for physically attractive partners. For instance, Buss (1989) found that in each of 37 cultures studied, men more than women desired partners who were youthful and physically attractive. Moreover, men are remarkably consistent in the specific features that they find attractive in a mate. These features typically include lustrous hair, clear skin, full breasts, feminine and symmetrical facial features, and a low waist-to-hip ratio (i.e., an hourglass figure) (Buss 1989; see Arnocky et al. 2014 for review). A growing body of evidence suggests that such physical features may have evolved, in part, as relatively honest cues to reproductively important aspects of women\u2019s phenotypic condition, including youth (i.e., being of reproductive age), underlying health, and/or fertility (Arnocky et al. 2014). Ancestral men who happened to be attracted to these features would therefore have out-reproduced those who did not share their preferences (Arnocky et al. 2014). To the extent that mate preferences for condition-linked physical features are heritable, such preferences would proliferate and become prevalent in the population.", "title": "D Desire to Be Included Among Desirable Women", "claims": null}, {"metadata": {"year": 2013}, "authors": ["A. Little", "B. Jones", "D. Feinberg", "D. Perrett"], "summary": "Several evolutionarily relevant sources of individual differences in face preference have been documented for women. Here, we examine three such sources of individual variationinmen\u2019spreferenceforfemalefacialfemininity:termofrelationship,partnership status and self-perceived attractiveness. We show that men prefer more feminine female faces when rating for a short-term relationship and when they have a partner (Study 1). These variables were found to interact in a follow-up study (Study 2). Men who thought themselves attractive also preferred more feminized female faces for short-term relationships than men who thought themselves less attractive (Study 1 and Study 2). In women, similar findings for masculine preferences in male faces have been interpreted as adaptive. In men, such preferences potentially reflect that attractive males are able to compete for high-quality female partners in short-term contexts. When a man has secured a mate, the potential cost of being discovered may increase his choosiness regardingshort-termpartnersrelativetounpartneredmen,whocanbetterincreasetheir short-term mating success by relaxing their standards. Such potentially strategic preferences imply that men also face trade-offs when choosing relatively masculine or feminine faced partners. In line with a trade-off, women with feminine faces were seen as more likely to be unfaithful and more likely to pursue short-term relationships (Study 3), suggesting that risk of cuckoldry is one factor that may limit men\u2019s preferences for femininity in women and could additionally lead to preferences for femininity in short-term mates. Mature features in adult human faces reflect the masculinization or feminization of secondary sexual characteristics that occurs at puberty. These face shape differences in part arise because of the action of hormones such as testosterone. For example, smaller jawbones and fatter cheeks are features of female faces that differentiate them from male faces (e.g., Enlow, 1982). In terms of women\u2019s attraction to masculinity in male faces, the direction of relationship varies across studies and researchers have documented differences in the attractiveness of masculinity according to short-term versus long-term matingcontextsandvarioussourcesofindividualdifferencesinpreferences(Little,Jones,", "title": "Men'sstrategicpreferencesforfemininityinfemale faces", "claims": null}, {"metadata": {"year": 2020}, "authors": ["L. Lamy"], "summary": "When asked to state their ideal romantic-partner preferences, men tend to overestimate women\u2019s physical attractiveness, whereas women tend to overestimate men\u2019s earning prospects [1,2]. Beyond this gender-related difference, however, both men and women prefer attractive, as compared to non-attractive partners. Attractive individuals are more popular, at least among members of the opposite gender, and they are more successful in dating relationships [3]. In line with these findings, it could be expected that feelings of love and affection would be more readily directed towards attractive individuals. But it is striking that this hypothesis was not supported by empirical studies. Among dating partners, neither independent observer, self, nor partner ratings of attractiveness are linked with the level of love an individual receives from his/her partner [4]. For men and women alike, beauty has no advantage in terms of love received, or given. Attractive women are more desired as romantic partners. Attractive men have more cross-gender interactions. Thus, it could be stated that physical attractiveness creates attraction, but it is not enough to create love.", "title": "Physical Attractiveness and Romantic Relationships: A Review", "claims": null}, {"metadata": {"year": 1994}, "authors": ["D. Kenrick", "S. Neuberg", "Kristin L. Zierk", "J. Krones"], "summary": "Previous research indicates that males, compared with females, evaluate their relationships less favorably after exposure to physically attractive members of the other sex. An evolutionary model predicts a converse effect after exposure to opposite-sex individuals high in dominance, which should lead females to evaluate their current relationships less favorably than males. Women and men rated their current relationships after being exposed to opposite-sex targets varying in both dominance and physical attractiveness. Consistent with earlier research, males exposed to physically attractive, as compared with average, targets rated their current relationships less favorably. Males' relationship evaluations were not directly influenced by the targets' dominance, although the effect of physical attractiveness was significant only for men exposed to women low in dominance. However; females' evaluations of their relationships were unaffected by exposure to physically attractive males but were lower after exposure to targets high in dominance. These data support predictions derived from an evolutionary model and suggest that such models can be used to generate testable hypotheses about ongoing social cognition.", "title": "Evolution and Social Cognition: Contrast Effects as a Function of Sex, Dominance, and Physical Attractiveness", "claims": null}, {"metadata": {"year": 2021}, "authors": ["S. Whyte", "R. Brooks", "H. F. Chan", "B. Torgler"], "summary": "Because sexual attraction is a key driver of human mate choice and reproduction, we descriptively assess relative sex differences in the level of attraction individuals expect in the aesthetic, resource, and personality characteristics of potential mates. As a novelty we explore how male and female sexual attractiveness preference changes across age, using a dataset comprising online survey data for over 7,000 respondents across a broad age distribution of individuals between 18 and 65 years. In general, we find that both males and females show similar distribution patterns in their preference responses, with statistically significant sex differences within most of the traits. On average, females rate age, education, intelligence, income, trust, and emotional connection around 9 to 14 points higher than males on our 0\u2013100 scale range. Our relative importance analysis shows greater male priority for attractiveness and physical build, compared to females, relative to all other traits. Using multiple regression analysis, we find a consistent statistical sex difference (males relative to females) that decreases linearly with age for aesthetics, while the opposite is true for resources and personality, with females exhibiting a stronger relative preference, particularly in the younger aged cohort. Exploring non-linearity in sex difference with contour plots for intelligence and attractiveness across age (mediated by age) indicates that sex differences in attractiveness preferences are driven by the male cohort (particularly age 30 to 40) for those who care about the importance of age, while intelligence is driven by females caring relatively more about intelligence for those who see age as very important (age cohort 40 to 55). Overall, many of our results indicate distinct variations within sex at key life stages, which is consistent with theories of selection pressure. Moreover, results also align with theories of parental investment, the gender similarities hypothesis, and mutual mate choice\u2013which speaks to the fact that the broader discipline of evolutionary mate choice research in humans still contains considerable scope for further inquiry towards a unified theory, particularly when exploring sex-difference across age.", "title": "Sex differences in sexual attraction for aesthetics, resources and personality across age", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Guanlin  Wang", "Minxuan  Cao", "Justina  Sauciuvenaite", "Ruth  Bissland", "Megan  Hacker", "Catherine  Hambly", "Lobke M. Vaanholt", "Chaoqun  Niu", "Mark D. Faries", "John R. Speakman"], "summary": "Abstract Parental investment hypotheses regarding mate selection suggest that human males should seek partners featured by youth and high fertility. However, females should be more sensitive to resources that can be invested on themselves and their offspring. Previous studies indicate that economic status is indeed important in male attractiveness. However, no previous study has quantified and compared the impact of equivalent resources on male and female attractiveness. Annual salary is a direct way to evaluate economic status. Here, we combined images of male and female body shape with information on annual salary to elucidate the influence of economic status on the attractiveness ratings by opposite sex raters in American, Chinese and European populations. We found that ratings of attractiveness were around 1000 times more sensitive to salary for females rating males, compared to males rating females. These results indicate that higher economic status can offset lower physical attractiveness in men much more easily than in women. Neither raters' BMI nor age influenced this effect for females rating male attractiveness. This difference explains many features of human mating behavior and may pose a barrier for male engagement in low-consumption lifestyles.", "title": "Different impacts of resources on opposite sex ratings of physical attractiveness by males and females", "claims": null}, {"metadata": {"year": 2022}, "authors": ["M. Kowal", "P. Sorokowski"], "summary": "Background: Public opinion on who performs more beauty-enhancing behaviors (men or women) seems unanimous. Women are often depicted as primarily interested in how they look, opposed to men, who are presumably less focused on their appearance. However, previous studies might have overlooked how masculinity relates to self-modification among men. Methods: We explored this issue in depth by conducting a qualitative Study 1 aimed to establish how men and women enhance their attractiveness (N = 121) and a quantitative Study 2 aimed to test time spent on activities that increase one\u2019s attractiveness in a longitudinal design (with seven repeated measures from 62 participants; N(total) = 367). Results: We observed no sex differences in beauty investments. Although women spent more time on make-up and cosmetics usage, men caught up with women in exercising and bodybuilding. Conclusion: Our study provides evidence that there may not be such wide sex differences in the intensity of enhancing one\u2019s appearance as has been previously thought. We hypothesize that this might partly stem from changes in gender roles regarding masculinity.", "title": "Sex Differences in Physical Attractiveness Investments: Overlooked Side of Masculinity", "claims": null}, {"metadata": {"year": 1995}, "authors": ["Nigel  Barber"], "summary": "Abstract Psychological evidence suggests that sex differences in morphology have been modified by sexual selection so as to attract mates (intersexual selection) or intimidate rivals (intrasexual selection). Women compete with each other for high quality husbands by advertising reproductive value in terms of the distribution of fat reserves and by exaggerating morphological indicators of youthfulness such as a small nose and small feet and pale, hairless skin. Men's physical appearance tends to communicate social dominance, which has the combined effects of intimidating reproductive rivals and attracting mates. In addition to their attractiveness and intimidatory effects, human secondary sexual characters also provide cues to hormonal status and phenotypic quality consistent with the good genes model of sexual selection (which includes parasite resistance). Low waist-hip ratio is sexually attractive in women and indicates a high estrogen/testosterone ratio (which favors reproductive function). Facial attractiveness provides honest cues to health and mate value. The permanently enlarged female breast appears to have evolved under the influence of both the good genes and the runaway selection mechanisms. The male beard is not obviously related to phenotypic quality and may have evolved through a process of runaway intersexual selection.", "title": "The evolutionary psychology of physical attractiveness: Sexual selection and human morphology", "claims": null}], "query": "Female attractiveness evolves compared to male attractiveness because men are more interested in attractiveness?", "summary_abstract": "The question of how female attractiveness evolves in comparison to male attractiveness, particularly in light of men's greater interest in attractiveness, is addressed through various studies that highlight distinct evolutionary pressures and preferences.\n\nArnocky (2016) emphasizes that men exhibit a stronger preference for physical attractiveness in potential mates, which has evolutionary roots. This preference is linked to traits that signal reproductive health and fertility, such as youthfulness and specific physical features. The consistent male preference for these traits suggests that such preferences have been naturally selected over time, leading to a pronounced focus on female attractiveness.\n\nIn contrast, Wang et al. (2017) found that while economic status significantly influences male attractiveness ratings, it has a much lesser effect on female attractiveness. This indicates that men can compensate for lower physical attractiveness with higher economic status, a dynamic that does not apply as strongly to women. This disparity suggests that female attractiveness is more rigidly tied to physical traits than male attractiveness, which can be influenced by other factors.\n\nLittle et al. (2013) further explore the nuances of male preferences, revealing that men's attraction to feminine features varies based on relationship context and their self-perceived attractiveness. This indicates that while men may prioritize physical attractiveness, their preferences are also shaped by social and relational dynamics.\n\nMoreover, Kowal and Sorokowski (2022) challenge the notion that women are the primary investors in beauty-enhancing behaviors. Their findings suggest that men are increasingly engaging in activities to enhance their attractiveness, indicating a shift in gender roles and perceptions of masculinity.\n\nOverall, the collective findings suggest that while both sexes value attractiveness, the evolutionary pressures on female attractiveness are more pronounced and directly linked to physical traits, whereas male attractiveness can be influenced by a broader range of factors, including economic status and social dynamics. This reflects a complex interplay of evolutionary strategies that shape how attractiveness is perceived and prioritized across genders.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2021}, "authors": ["P. Robertson"], "summary": ": Misinformation related to climate change has been around for decades, mostly in the form of denying the existence of global warming. Today, misinformation is manifesting in different ways, but it still has the same outcome: undermining science and delaying climate action.", "title": "Introduction and context", "claims": null}, {"metadata": {"year": 2015}, "authors": ["LaRue Allen", "Bridget B. Kelly", "Youth Board on Children", "Families."], "summary": ": Misinformation related to climate change has been around for decades, mostly in the form of denying the existence of global warming. Today, misinformation is manifesting in different ways, but it still has the same outcome: undermining science and delaying climate action. Social can undermine and have", "title": "Introduction and Context", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Tiffany E. Sill", "Jaime R Ayala", "Julianne Rolf", "Spencer Smith", "Shelby Dye"], "summary": "Despite the existence of a substantial amount of climate-related scientific data, misconceptions about climate change are still prevalent within public opinion. Dissemination of misinformation to the public through subjective media sources is a major challenge that climate scientists face. Implementation of climate policy is crucial for mitigation and adaptation measures required to curtail anthropogenic rooted climate change. This paper will discuss student perspectives on the 2022 United Nations climate summit in Egypt (COP27) related to climate literacy and public opinion as the driving forces behind the enactment and execution of important climate-based policy.", "title": "How Climate Literacy and Public Opinion Are the Driving Forces Behind Climate-Based Policy: A Student Perspective on COP27", "claims": null}, {"metadata": {"year": 2019}, "authors": ["J. Cook"], "summary": "While there is overwhelming scientific agreement on climate change, the public has become polarized over fundamental questions such as human-caused global warming. Communication strategies to reduce polarization rarely address the underlying cause: ideologically-driven misinformation. In order to effectively counter misinformation campaigns, scientists, communicators, and educators need to understand the arguments and techniques in climate science denial, as well as adopt evidence-based approaches to neutralizing misinforming content. This chapter reviews analyses of climate misinformation, outlining a range of denialist arguments and fallacies. Identifying and deconstructing these different types of arguments is necessary to design appropriate interventions that effectively neutralize the misinformation. This chapter also reviews research into how to counter misinformation using communication interventions such as inoculation, educational approaches such as misconception-based learning, and the interdisciplinary combination of technology and psychology known as technocognition.", "title": "Understanding and Countering Misinformation About Climate Change", "claims": null}, {"metadata": {"year": 2022}, "authors": ["John  Cook"], "summary": "While there is overwhelming scientific agreement on climate change, the public has become polarized over fundamental questions such as human-caused global warming. Communication strategies to reduce polarization rarely address the underlying cause: ideologically-driven misinformation. In order to effectively counter misinformation campaigns, scientists, communicators, and educators need to understand the arguments and techniques in climate science denial, as well as adopt evidence-based approaches to neutralizing misinforming content. This chapter reviews analyses of climate misinformation, outlining a range of denialist arguments and fallacies. Identifying and deconstructing these different types of arguments is necessary to design appropriate interventions that effectively neutralize the misinformation. This chapter also reviews research into how to counter misinformation using communication interventions such as inoculation, educational approaches such as misconception-based learning, and the interdisciplinary combination of technology and psychology known as technocognition.", "title": "Understanding and Countering Misinformation About Climate Change", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Justin  Farrell", "Kathryn  McConnell", "Robert  Brulle"], "summary": "Nowhere has the impact of scientific misinformation been more profound than on the issue of climate change in the United States. Effective responses to this multifaceted problem have been slow to develop, in large part because many experts have not only underestimated its impact, but have also overlooked the underlying institutional structure, organizational power and financial roots of misinformation. Fortunately, a growing body of sophisticated research has emerged that can help us to better understand these dynamics and provide the basis for developing a coordinated set of strategies across four related areas (public inoculation, legal strategies, political mechanisms and financial transparency) to thwart large-scale misinformation campaigns before they begin, or after they have taken root.This Perspective synthesizes research on the origins and impacts of scientific misinformation campaigns, pointing to public inoculation, legal, political and financial strategies for countering climate change misinformation and limiting its dissemination.", "title": "Evidence-based strategies to combat scientific misinformation", "claims": null}, {"metadata": {"year": 2017}, "authors": ["J. Cook", "S. Lewandowsky", "Ullrich K. H. Ecker"], "summary": "Misinformation can undermine a well-functioning democracy. For example, public misconceptions about climate change can lead to lowered acceptance of the reality of climate change and lowered support for mitigation policies. This study experimentally explored the impact of misinformation about climate change and tested several pre-emptive interventions designed to reduce the influence of misinformation. We found that false-balance media coverage (giving contrarian views equal voice with climate scientists) lowered perceived consensus overall, although the effect was greater among free-market supporters. Likewise, misinformation that confuses people about the level of scientific agreement regarding anthropogenic global warming (AGW) had a polarizing effect, with free-market supporters reducing their acceptance of AGW and those with low free-market support increasing their acceptance of AGW. However, we found that inoculating messages that (1) explain the flawed argumentation technique used in the misinformation or that (2) highlight the scientific consensus on climate change were effective in neutralizing those adverse effects of misinformation. We recommend that climate communication messages should take into account ways in which scientific content can be distorted, and include pre-emptive inoculation messages.", "title": "Neutralizing misinformation through inoculation: Exposing misleading argumentation techniques reduces their influence", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Isyaku Hassan", "R. Musa", "Mohd Nazri Latiff Azmi", "Mohamad Razali Abdullah", "Siti Zanariah Yusoff"], "summary": "Disinformation represents one of the major obstacles to meaningful actions against climate change skepticism. This study, therefore, aimed to investigate climate change disinformation across types, agents, and media platforms as reported in selected online newspapers. A total of 124 news articles gathered from Malaysia's The Star and New Straits Times were subjected to content analysis. The articles were collected from the newspapers\u2019 digital archives between August 2015 and October 2021 using \u201cclimate change\u201d and \u201cdisinformation\u201d as keywords. Subsequently, data were analyzed using the Chi-square test and descriptive statistics. The study found that disinformation about climate change is more likely to be spread by politicians, organizations, and anonymous agencies compared with business tycoons, celebrities, and academics. Also, misleading and fabricated contents represent the most common types of disinformation spread by politicians. The findings suggest that politics and social media represent the most critical factors influencing climate change. The study, therefore, calls for the joint efforts of activists, media practitioners, and governments to mitigate disinformation about climate change.", "title": "Analysis of climate change disinformation across types, agents and media platforms", "claims": null}], "query": "laws and policies governing climate change disinformation or misinformation", "summary_abstract": "The body of research on laws and policies governing climate change disinformation reveals a complex landscape where misinformation significantly undermines public understanding and action on climate issues. A consistent theme across the studies is the pervasive nature of climate change misinformation, which has evolved over decades, primarily manifesting as denial of global warming and other misleading narratives (Robertson, 2021; Allen et al., 2015). This misinformation not only distorts public perception but also hampers the implementation of effective climate policies (Sill et al., 2023).\n\nCook (2019; 2022) emphasizes the need for targeted communication strategies to counteract the ideological roots of misinformation. He argues that understanding the arguments and techniques used in climate science denial is crucial for developing effective interventions. This includes educational approaches and communication strategies that can inoculate the public against misinformation (Cook et al., 2017). The research highlights that misinformation can polarize public opinion, leading to decreased support for climate action, particularly among certain ideological groups (Cook et al., 2017).\n\nFarrell et al. (2019) further elaborate on the institutional and financial underpinnings of misinformation campaigns, suggesting that a coordinated response involving public inoculation, legal strategies, and political mechanisms is essential to combat these challenges. They advocate for transparency and proactive measures to limit the spread of misinformation before it takes root.\n\nAdditionally, Hassan et al. (2023) provide empirical evidence from Malaysia, showing that politicians and organizations are significant sources of climate change disinformation, often disseminating misleading content through social media and traditional media platforms. Their findings underscore the critical role of political and media landscapes in shaping public discourse on climate change.\n\nIn summary, the collective research underscores the urgent need for comprehensive strategies that encompass legal, educational, and communicative approaches to effectively combat climate change misinformation and bolster public support for necessary climate policies.", "summary_extract": null}, {"papers": [{"metadata": {"year": 1993}, "authors": ["Anjali  Kumar"], "summary": "Concerns about public enterprise management, especially since the 1980s, have been closely associated with more generalized concerns about the role of the state in processes of production. Whether in specific enterprises or industries in the context of a mixed market economy, or across the entire economy, in a system of centralized planning, state owned and operated production units have been associated with economic inefficiencies leading to lower relative levels of output, a diversion of resources from their most productive uses and internal economic inefficiencies in management and motivation within the productive unit.1 A major cause of the economic inefficiencies of public enterprises is the difficulty of reconciling the multiple political, social and ideological aims of governments and government-appointed managers guiding the operations of these enterprises, with the achievement of economic efficiency. These concerns have led to the adoption of a wide spectrum of mechanisms to modify or alter the controls and obligations between governments and state owned enterprises to better achieve these multiple and sometimes conflicting objectives.", "title": "Public Enterprise Management Through State Holding Companies", "claims": null}, {"metadata": {"year": 2002}, "authors": ["Suthathip  Yaisawarng"], "summary": "A number of studies have documented that public and private sector organisations do not use their limited resources efficiently. A possible implication is that reallocation of resources from the provision of goods and services that have relatively low marginal social benefits to those that have relatively high marginal social benefits would enhance overall social welfare. Another implication is that resources have not been used by the most productive means; ie, it is possible to produce more goods and services without additional resources. Suggested remedies range from industry reforms such as restructuring and deregulation to promote competition, to institutional changes such as adoption of efficient management practices.", "title": "Performance Measurement and Resource Allocation", "claims": null}, {"metadata": {"year": 2002}, "authors": ["Go  Yano", "Maho  Shiraishi"], "summary": "In this paper we attempt to investigate empirically the cause of inefficiency of Chinese state-owned enterprises in 1989\u201395, in the contract relation between Chinese government and state-owned enterprises. It is found that several moral hazards arose, in 1989\u201395 Chinese state-owned enterprises, in textile industry. To put it another way, the enterprises chose less private effort and risk than the first-best levels, because of the incentive structure designed by the government. Especially, concave payoff function to the enterprises designed by the government made them institutional risk averters and choose too little risks.", "title": "State-Owned Enterprises and Their Contract with Government in China: An Econometric Analysis", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Xiong Qing-guo"], "summary": "The state-owned enterprises almost have no room to make their own decision in changing their own fate and have always played a relatively passive role and been in a submissive position. That is the key reason why the state-owned enterprises are confronted with difficulty in the reform. As a result , effective measures should been taken to eliminate the government\u2032s substitution for the state-owned enterprises in making decision, and return the right to conduct self-reform , thus these enterprises can decide their own future.", "title": "The Consequence of Government\u2032s Substitution in Decision-making and its Rectification \u2014\u2014Reflections on the reform of the state-owned enterprises", "claims": null}, {"metadata": {"year": 1974}, "authors": ["Mancur  Olson"], "summary": "Many of those who write about the corporation and the role of government take it for granted that the fundamental problem is the prevalence of monopoly in the corporate sector and that one of the crucial functions of government is to deal with this problem through anti-trust laws, or regulation of uncompetitive industries, or public ownership of firms with monopoly power. Though there is some merit in this commonplace perspective, this paper will attempt to show that there is another way of looking at the corporate and public sectors that is both better balanced and more likely to bring progress in dealing with the major problems modern societies face. More specifically, the first section of this paper will contend that the problem of monopoly, as that term is characteristically understood in economic theory, is of only modest importance, at least in the United States, both in terms of economic efficiency and distributive justice; the second section will argue that the concepts of collective goods and externalities provide a more relevant insight into the major economic problems of the time; the third will conclude that because of the problems involved in dealing with externalities and producing collective goods, and the existing mechanisms in the public sector for dealing with these problems, inefficiencies in this area are probably of extraordinary importance, and in the aggregate presumably dwarf all of those resulting from monopoly; and the fourth will suggest that social efficiency could be substantially increased through certain changes in existing mechanisms in the public sector.", "title": "On the Priority of Public Problems", "claims": null}, {"metadata": {"year": 2017}, "authors": ["A. Tatuev", "Murat A. Kerefov", "N. I. Ovcharova", "Maria L. Vilisova", "Renata A. Shibzuhova"], "summary": "Nowadays, the implementation of state's social policies becomes critically urgent in the context of social development for all countries of the world. For a number of objective reasons, the state is forced to reduce its obligations to a certain extent in the social sphere in such a way as to avoid hampering the markets development. The objective of the current study is to identify the priorities of modern public administration. The structure of budget expenditures demonstrates shifting to national defense, social policy and national economy with lower health-care and education spending. At the same time, the use of tax system as a resource for meeting the State's social obligations is increasingly constrained.", "title": "Budget Constraints and Expanding of Non-Governmental Organizations Participation in the Implementation of State's Social Policies", "claims": null}, {"metadata": {"year": 1985}, "authors": ["T. Goodale"], "summary": "Limited resources and growing needs continue to challenge providers of local government services. Equitable distribution of resources is both more important and more difficult when the local economy is growing slowly if at all. The need to increase revenue and reduce costs leads us to focus on efficiency rather than effectiveness, and market equity rather than equal opportunity. This can lead to a distortion of mandate and objectives. Public purpose and philosophy should join the dialogue of business and finance.", "title": "The Political Economics of Local Leisure Services", "claims": null}, {"metadata": {"year": 2011}, "authors": ["E. Apriatni"], "summary": "Abstract : The role of private enterprise in poverty reduction is still questionable because the company's concern toward social problems is still half-heartedly. Coverage in the media, associated with forest destruction, pollution of river water, as well as other forms of injustice felt by the community is still often heard. This proves the corporate social responsibility in upholding justice in the world of business is still low. Injustices in the economic and social development create a gap, especially in public access to utilize existing resources. On the other hand, companies show their existence in the business world, with their ability to utilize existing resources in the country. Embodiment of business ethics within the context of corporate social responsibility should be accommodated in company policies with the aim to achieve corporate objectives, but do not ignore the social interests of surrounding communities. With good relationship, through the CSR labeled activity, will eventually be able to maintain the existence of the company in the business world.\u00a0 Key words: Social Responsibility: Business Ethics; justice permalink :\u00a0 http://ejournal.undip.ac.id/index.php/forum/article/view/3150", "title": "TANGGUNG JAWAB SOSIAL PERUSAHAAN DALAM MEWUJUDKAN KEADILAN DALAM BISNIS", "claims": null}], "query": "Curry (2005) and Seip (2013) highlight that government-owned businesses often prioritize meeting social obligations over commercial objectives, leading to inefficient resource allocation and financial struggles for these enterprises.", "summary_abstract": "The research question regarding the prioritization of social obligations over commercial objectives by government-owned businesses, leading to inefficiencies and financial struggles, is supported by various studies. Kumar (1993) emphasizes that public enterprises often grapple with reconciling political, social, and ideological aims with the need for economic efficiency, resulting in resource misallocation and internal inefficiencies. This aligns with Yaisawarng (2002), who notes that both public and private organizations frequently fail to utilize their limited resources effectively, suggesting that a reallocation towards higher marginal social benefits could enhance overall welfare.\n\nFurther, Yano and Shiraishi (2002) investigate the inefficiencies in Chinese state-owned enterprises, attributing them to a flawed incentive structure imposed by the government, which discourages risk-taking and optimal effort. This reflects a broader theme identified by Xiong (2005), who argues that state-owned enterprises often lack decision-making autonomy, which hinders their ability to reform and adapt, thereby perpetuating their financial difficulties.\n\nOlson (1974) adds another layer to this discussion by suggesting that inefficiencies in the public sector, particularly in addressing externalities and collective goods, may be more significant than those arising from monopolistic practices. This perspective underscores the complexity of the challenges faced by government-owned enterprises in balancing social responsibilities with the need for economic viability.\n\nIn summary, the collective findings from these studies illustrate that government-owned businesses often prioritize social obligations at the expense of commercial objectives, leading to inefficiencies and financial struggles. The interplay of political pressures, inadequate incentive structures, and limited decision-making autonomy are critical factors contributing to these challenges (Kumar, 1993; Yaisawarng, 2002; Yano & Shiraishi, 2002; Xiong, 2005; Olson, 1974).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2013}, "authors": ["Bj\u00f6rn Bartling", "F. Engl", "Roberto A. Weber"], "summary": "This paper studies whether people can avoid punishment by remaining willfully ignorant about possible negative consequences of their actions for others. We employ a laboratory experiment, using modified dictator games in which a dictator can remain willfully ignorant about the payoff consequences of his decision for a receiver. A third party can punish the dictator after observing the dictator\u2019s decision and the resulting payoffs. On the one hand, willfully ignorant dictators are punished less if their actions lead to unfair outcomes than dictators who reveal the consequences before implementing the same outcome. On the other hand, willfully ignorant dictators are punished more than revealing dictators if their actions do not lead to unfair outcomes. We conclude that willful ignorance can circumvent blame when unfair outcomes result, but that the act of remaining willfully ignorant is itself punished, regardless of the outcome.", "title": "Does Willful Ignorance Deflect Punishment? - An Experimental Study", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Robert  St\u00fcber"], "summary": "Altruistic punishment is often thought to be a major enforcement mechanism of social norms. I present experimental results from a modified version of the dictator game with third-party punishment, in which third parties can remain ignorant about the choice of the dictator. I find that a substantial fraction of subjects choose not to reveal the dictator\u2019s choice and not to punish the dictator. I show that this behavior is in line with the social norms that prevail in a situation of initial ignorance. Remaining ignorant and choosing not to punish is not inappropriate. As a result, altruistic punishment is significantly lower when the dictator\u2019s choice is initially hidden. The decrease in altruistic punishment leads to more selfish dictator behavior only if dictators are explicitly informed about the effect of willful ignorance on punishment rates. Hence, in scenarios in which third parties can ignore information and dictators know what this implies, third-party punishment may only ineffectively enforce social norms.", "title": "The benefit of the doubt: willful ignorance and altruistic punishment", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Rainer Michael Rilke"], "summary": "Organizations aim to influence\u2014via their internal guidelines and corporate culture\u2014how unfair treatment of other stakeholders is perceived and condemned by employees. To understand how different frames and forms of publicity influence moralistic punishment, that is, the willingness of employees to take costs in order to foster norm compliance, we employ a modified version of a dictator game. In our dictator game, a bystander observes a dictator\u2019s behavior towards a recipient and can punish the dictator. We vary how the dictator\u2019s action is framed (either as giving money to the recipient or taking money from the recipient) and whether or not the recipient, as a victim of unfair behavior, is informed about the punishment. Our results suggest that bystanders are more likely to punish dictators when their action is framed as giving rather than taking, although both lead to the same consequences. When bystanders cannot inform recipients about their punishment, less punishment can be observed. On average, dictators partially anticipate this effect and behave more generously when recipients are informed about the bystanders\u2019 punishment.", "title": "On the duty to give (and not to take): An experiment on moralistic punishment", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Stefania Ottone", "Ferruccio Ponzano", "L. Zarri"], "summary": "Social norms are ubiquitous in human life. Their role is essential in allowing cooperation to prevail, despite the presence of incentives to free ride. As far as norm enforcement devices are concerned, it would be impossible to have widespread social norms if second parties only enforced them. However, both the quantitative relevance and the motivations underlying altruistic punishment on the part of \u2018unaffected\u2019 third parties are still largely unexplored. This paper contributes to shed light on the issue, by means of an experimental design consisting of three treatments: a Dictator Game Treatment, a Third-Party Punishment Game Treatment (Fehr and Fischbacher, 2004) and a Metanorm Treatment, that is a variant of the Third-party Punishment Game where the Recipient can punish the third party. We find that third parties are willing to punish dictators (Fehr and Fischbacher, 2004; Ottone, 2008) and, in doing so, they are affected by \u2018reference-dependent fairness\u2019, rather than by the \u2018egalitarian distribution norm\u2019. By eliciting players\u2019 normative expectations, it turns out that all of them expect a Dictator to transfer something \u2013 not half of the endowment. Consequently, the Observers\u2019 levels of punishment are sensitive to their subjective sense of fairness. A positive relation between the level of punishment and the degree of negative subjective unfairness emerges. Subjective unfairness also affects Dictators\u2019 behaviour: their actual transfers and their ideal transfer are not significantly different. Finally, we interestingly find that third parties are also sensitive to the receivers\u2019 (credible) threat to punish them: as the Dictator\u2019s transfer becomes lower and lower than the Observer\u2019s ideal transfer, the Observer\u2019s reaction is \u2013 other things being equal \u2013 significantly stronger in the Metanorm Treatment than in the Third-Party Punishment Game Treatment. Hence, despite their being to some extent genuinely nonstrategically motivated, also third parties \u2013 like second parties \u2013 are sensitive to the costs of punishing.", "title": "Moral Sentiments and Material Interests behind Altruistic Third-Party Punishment", "claims": null}, {"metadata": {"year": 2019}, "authors": ["R\u00e9ka  Heim", "J\u00fcrgen  Huber"], "summary": "Abstract In this paper we explore how decisions in a sequential three-person game are influenced by either dynamics of roles or group composition. In the game a dictator decides how much of his endowment to transfer to the recipient. The supervisor can punish the dictator and/or transfer to the recipient after learning about the dictator\u2019s decision. We find that transfers by a dictator are highest and stable when the group of three is fixed, no matter whether roles change or not. There is limited support of a leading-by-example effect, that is, only when both role and group composition of subjects are fixed, supervisors give more the more dictators gave, and dictators transfer more in the next period the more supervisors gave in a period. Punishment partially has a disciplining effect on dictators. Finally, we observe that subjects\u2019 actual actions are consistent with their beliefs and expectations.", "title": "Leading-by-example and third-party punishment: Experimental evidence", "claims": null}, {"metadata": {"year": 2014}, "authors": ["E. Winschel", "Philipp Zahn"], "summary": "In most laboratory experiments concerning prosocial behavior subjects are fully informed how their decision influences the payoff of other players. Outside the laboratory, however, individuals typically have to decide without such detailed knowledge. To assess the effect of information asymmetries on prosocial behavior, we conduct a laboratory experiment with a simple non-strategic interaction. A dictator has only limited knowledge about the benefits his prosocial action generates for a recipient. We observe subjects with heterogenous social preferences. While under symmetric information only individuals with the same type of preferences transfer, under asymmetric information different types transfer at the same time. As a consequence and the main finding of our experiment, uninformed dictators behave more prosocially than informed dictators.", "title": "When Ignorance is Bliss - Information Asymmetries Enhance Prosocial Behavior in Dictator Games", "claims": null}, {"metadata": {"year": 2016}, "authors": ["A. Murata"], "summary": "The aim of this study was to demonstrate that indirect mediation in behaviors leads to insensitivity to unethical behavior through a dictator game and to give some implications for safety management. The indirect involvement in the unethical behavior such as the violation of regulation is believed to lessen the responsibility and the criticism from others for the unethical behavior as compared to the direct involvement in it. The instruction condition for the evaluator of behavior in a dictator game was taken up as an experimental variable. Instruction condition 1 was to pay attention to the behavior of only a dictator. In instruction condition 2, the participant (evaluator) was required to review all players\u2019 behavior and evaluate a dictator. It has been investigated whether allowing indirect actions (mediations) leads to reduced punishment as a function of the instruction condition. While the punishment to the indirectness did not get smaller for instruction condition 2, the punishment to the indirectness tended to get smaller only for instruction condition 1.", "title": "Insensitivity to Unethical Behavior in Dictator Game When Indirectly Intermediated-Implications of Indirect Blindness for Safety Management", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Jo Thori Lind", "Karine  Nyborg", "Anna  Pauls"], "summary": "Our lab experiment tests for strategic ignorance about the environmental consequences of one\u2019s actions. In a binary dictator situation based on the design by Dana, Weber, and Kuang (2007), we test whether the option to remain ignorant about the receiver\u2019s payoffs reduces generosity. Our receiver is a charity that engages in carbon offset. Contrary to previous \ufb01ndings by Dana, Weber, and Kuang (2007) and replications, the option to remain ignorant does not decrease generosity. Only 22% of dictators choose ignorance. We test social interaction by allowing another subject to force the dictator to learn the receiver\u2019s payoff, and by allowing the dictator to sanction that subject. When information can be imposed by another subject, almost all dictators choose information themselves, but this does not increase generosity. The possibility of sanctions does not discourage subjects from providing information to dictators.", "title": "Save the planet or close your eyes? Testing strategic ignorance in a charity context", "claims": null}], "query": "how does deliberate ignorance affect punishment in the dictator game?", "summary_abstract": "The research question regarding how deliberate ignorance affects punishment in the dictator game has been explored through various experimental studies, revealing nuanced insights into the dynamics of willful ignorance and its implications for social norms and punishment.\n\nBartling, Engl, and Weber (2013) found that dictators who remain willfully ignorant about the consequences of their actions tend to face less punishment when their decisions lead to unfair outcomes. Conversely, if their actions do not result in unfairness, these ignorant dictators are punished more than those who are aware of the consequences. This suggests that while willful ignorance can shield individuals from blame in certain contexts, the act of choosing ignorance itself can attract punishment.\n\nSt\u00fcber (2019) further elaborated on this by demonstrating that when third parties can remain ignorant of the dictator's choices, altruistic punishment diminishes significantly. This reduction in punishment occurs because individuals often adhere to social norms that permit ignorance, leading to less enforcement of fairness. The study indicates that if dictators are aware of the implications of willful ignorance on punishment, they may exploit this to behave more selfishly.\n\nRilke (2017) examined how framing and the visibility of punishment affect moralistic responses. The findings suggest that when bystanders cannot inform victims about their punitive actions, the level of punishment decreases. This implies that the visibility of punishment plays a crucial role in how ignorance is perceived and acted upon.\n\nIn contrast, Heim and Huber (2019) found that when group dynamics are fixed, dictators tend to behave more generously, indicating that the context of the game can influence the relationship between ignorance and punishment. This suggests that the social environment may mitigate the effects of willful ignorance.\n\nWinschel and Zahn (2014) highlighted that under conditions of asymmetric information, uninformed dictators may act more prosocially than their informed counterparts. This finding challenges the assumption that knowledge always leads to more responsible behavior, suggesting that ignorance can sometimes foster greater generosity.\n\nOverall, the collective findings indicate that deliberate ignorance can have a dual effect on punishment in the dictator game: it can reduce punishment in cases of unfair outcomes while simultaneously attracting punishment for the act of being ignorant. The context, visibility of actions, and social norms significantly influence how ignorance is perceived and the subsequent responses from third parties.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2018}, "authors": ["K. Ram", "Raj Kumar", "Sun Ta", "R. Munjal"], "summary": "Carbon requirements for grain filling in wheat are mainly from current assimilation by photosynthesis and remobilization of reserves from the stems (Yang et al., 2000). Remobilization of assimilates is an active process that involves translocation of stored reserves from stems and sheaths to grains (Gupta et al., 2015). Stem reserves contribute 20 to 40% weight of the grain in non-stressed condition (Vignjevic et al., 2015) and this can be up to 70% under stressed conditions during grain filling (Rebetzke et al., 2008). Drought and high temperature induced earlier mobilization of non-structural reserve carbohydrates from stem and leaf sheaths, which provided a greater proportion of grain International Journal of Current Microbiology and Applied Sciences ISSN: 2319-7706 Volume 7 Number 04 (2018) Journal homepage: http://www.ijcmas.com", "title": "Stem Reserve Mobilization in Relation to Yield under Different Drought and High Temperature Stress Conditions in Wheat (Triticum aestivum L.) Genotypes", "claims": null}, {"metadata": {"year": 2004}, "authors": ["A.  Blum"], "summary": "Stem reserves from pre-anthesis plant assimilation are being increasingly recognised as an important source of carbon for grain filling when current photosynthesis is inhibited by drought, heat or disease stress during this stage. Genotypic and environmental factors affecting reserve accumulation and utilisation for grain filling are reviewed. The genetic improvement of stem reserve storage and utilisation as a potent mechanism for grain filling under stress is discussed, and practical guidelines for selection work are provided.", "title": "Improving wheat grain filling under stress by stem reserve mobilisation", "claims": null}, {"metadata": {"year": 1997}, "authors": ["Abraham  Blum"], "summary": "Stem reserves from pre-anthesis plant assimilation are being increasingly recognised as an important source of carbon for grain filling when current photosynthesis is inhibited by drought, heat or disease stress during this stage. Genotypic and environmental factors affecting reserve accumulation and utilisation for grain filling are reviewed. The genetic improvement of stem reserve storage and utilisation as a potent mechanism for grain filling under stress is discussed, and practical guidelines for selection work are provided.", "title": "Improving wheat grain filling under stress by stem reserve mobilisation", "claims": null}, {"metadata": {"year": 2011}, "authors": ["A. Gupta", "K. Kaur", "N. Kaur"], "summary": "The effect of water deficit on stem reserve mobilization and sink activity in wheat (Triticum aestivum L.) cultivars, viz., C306 (drought tolerant) and PBW343 (drought sensitive) was studied. Drought was maintained in pot raised plants by withholding irrigation at 95 days after sowing (DAS), i.e. just five days before the initiation of anthesis. Drought induced a significant reduction in mean biomass of all the internodes of sensitive cultivar as compared to those of tolerant one. Mobilized dry matter and mobilization efficiency were observed to be higher in the internodes of tolerant cultivar, both under control and stress conditions, which resulted in enhanced translocation of stem reserves to the grains. Water soluble carbohydrates (WSC), which mainly occur as fructans, were observed to be higher in the internodes of tolerant cultivar than those of sensitive one. When drought was applied, fructans were mobilized more effectively from the internodes of tolerant cultivar. A significantly higher sucrose synthase activity in the grains of tolerant cultivar, under drought conditions, increased the sink strength by unloading the assimilates in the sink, thereby increasing further mobilization of assimilates to the grains. Grains of sensitive cultivar attained maturity much earlier as compared to the tolerant one, both under control and stress conditions. The longer duration of grain maturation in tolerant cultivar supported enhanced mobilization of stem reserves, thus restricting heavy decrease in grain yield, under stress conditions, as compared to the sensitive cultivar. It may, therefore, be concluded that certain characteristics viz., enhanced capability of fructan storage, higher mobilization efficiency, stronger sink activity and longer duration of grain maturation might help the drought tolerant cultivar in coping the stress conditions", "title": "Stem Reserve Mobilization and Sink Activity in Wheat under Drought Conditions", "claims": null}, {"metadata": {"year": 2018}, "authors": ["S. Gare", "R. Wagh", "A. Ingle", "N. Soni"], "summary": "Wheat (Triticum aestivum) is the first important and strategic cereal crop for the majority of world\u2019s populations. In 100 grams, wheat provides 327 calories and is an excellent source of multiple essential nutrients, such as protein, dietary fiber, manganese, phosphorus and niacin. Several B vitamins and other dietary minerals are in significant content. High temperature (>30\u00b0C) at the time of grain filling is one of the major constraints in increasing productivity of wheat in tropical countries like India (Zhao 2007). This survey/review may likewise help in interdisciplinary study regards to influence of temperature stress on stem reserve mobilization when wheat plants suffer from arrested photosynthesis during stress", "title": "Effect of temperature on stem reserve mobilization for grain development in wheat", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Pireivatlou As", "Aliyev Rt"], "summary": "Two experiments were conducted on 11 diverse wheat genotypes, under both well watered, and drought stress field experiments. Drought decreased vegetative organs (above ground dry matter) weight at anthesis and maturity, grain yield per spike, grains per spike and 1000 grain weight by 5.7, 24.5, 21.2, 15.7 and 6.4 %, respectively. Translocation of dry matter from vegetative organs (above ground dry matter) to developing kernels and mobilization efficiency were considerably increased under drought stress condition, by 60.1 and 74 %, respectively. The contribution of pre anthesis assimilate was also highly increased under the drought stress condition. Translocation of dry matter from the peduncle, penultimate, and the lower internodes ranged from 51.2 to 76.9, 106.8 to 182.3 and from 100 to 208.6 mg under well watered and drought stress conditions, respectively. Mobilization of dry matter was higher under the drought stress condition than in well watered for peduncle by 50.2 %, penultimate internode 70.7 and for lower internodes 111.6 % respectively. Drought increased the contribution of pre-anthesis assimilates to grain yield by 81.5 % in peduncle, 108.1 % in penultimate internode and by 153.8 % in lowerinternodes respectively. Drought was also increased the translocation efficiency by 84, 91.3 and 111.6 % for peduncle, penultimate internode and lower internodes, respectively. Vegetative organ (above ground dry matter) at anthesis was correlated with grain weight per spike (r = 0.68*), translocation of dry matter (r = 0.34) under drought stress condition. Translocation of dry matter was correlated with lower internodes, penultimate and peduncle maximum weight under drought stress condition by r = 0.55, 0.56 and 0.34, respectively.", "title": "Stem reserve and its contribution to grain yield of wheat (Triticum aestivum L.) genotypes under drought stress conditions", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Samuel C\u00f3rdova S\u00e1nchez", "Iris Amairani Garc\u00eda Alejandro", "C. Ruiz", "S. Garc\u00eda", "Ra\u00fal Casta\u00f1eda Ceja", "S. Vel\u00e1zquez", "M. Villegas", "L. D. C. L. Espinoza", "Cintya Valerio C\u00e1rdenas", "Rosa Graciela Santos Arguelles"], "summary": "For the replanting of the sugarcane crop, plants produced from buds of different reserve sizes and from different positions on the stem are used, without up to now an optimum reserve and position size has been determined to obtain a quality plant. Therefore, five treatments were evaluated for nutritional reserve: T1 (short reserve), T2 (without reserve), T3 (half reserve), T4 (long basal reserve) and T5 (long reserve superior) and for the position a design was used 2x3 factorial (two cultivars: MEX69-290 and COLPOSCTMEX05-223 and three yolk positions: basal, middle and apical), the treatments were distributed in a completely randomized arrangement. The buds were sown in unicel vessels with a liter capacity and sifted sand was used as substrate. It was evaluated; emergency percentage, root length and plant height. The results indicate that the use of buds with different positions in the stem is possible to produce seedlings of good quality, since more than 50% of sprouting was obtained in both cultivars. The results on nutrient reserves showed that T3 had the longest root length (36.2 cm) and T1 had the highest plant height (61.4 cm), showing that there is no direct relationship between plant height and root length. To obtain a quality plant it is enough to use short buds or half reserve and any position. For the commercial production of plants, it is recommended to plant in substrates rich in nutrients.", "title": "EFFECT OF NUTRITIONAL RESERVES AND POSITION OF THE YEMAS IN THE STEM IN THE QUALITY OF SUGAR CANE PLANTS", "claims": null}, {"metadata": {"year": 1982}, "authors": ["R. W. Daniels", "M. Alcock", "D. Scarisbrick"], "summary": "SUMMARY Pre-anthesis stem reserve contribution to grain yield was assessed in two spring barley cultivars of contrasting height. It was greatest in the taller, but final grain yields were similar. Partitioning of total reserve capacity to various plant parts showed that the leaf and sheath below the peduncle were most important, followed by stem internodes which had increasing reserve capacity up to internode 4. It is suggested that stem reserves are valuable for yields commonly achieved in U.K. commercial practice. High grain yields were associated with large positive increases in stem dry weight after anthesis. This would indicate that the source capacity to boost yield is more than proportional to that required to fill the grain alone.", "title": "A reappraisal of stem reserve contribution to grain yield in spring barley (Hordeum vulgare L.)", "claims": null}], "query": "wheat stem sugar reserve mobilization", "summary_abstract": "The research on wheat stem sugar reserve mobilization highlights the critical role that stem reserves play in grain filling, particularly under stress conditions such as drought and high temperatures. Several studies emphasize that stem reserves, which are primarily accumulated before anthesis, can significantly contribute to grain weight, especially when current photosynthesis is compromised.\n\nRam et al. (2018) note that stem reserves can account for 20-40% of grain weight under non-stressed conditions, and this contribution can rise to as much as 70% during periods of stress. This mobilization of stored carbohydrates from stems and sheaths is an active process that is crucial for grain filling. Blum (2004) further supports this by discussing how stem reserves become increasingly important when photosynthesis is inhibited due to environmental stresses.\n\nGupta et al. (2011) conducted a comparative study on drought-tolerant and drought-sensitive wheat cultivars, finding that the tolerant cultivar exhibited higher mobilization efficiency and greater translocation of stem reserves to grains under drought conditions. This suggests that certain genetic traits can enhance the ability to utilize stem reserves effectively, thereby improving resilience to stress.\n\nPireivatlou and Aliyev (2008) also observed that drought conditions significantly increased the translocation of dry matter from vegetative organs to developing kernels, indicating that pre-anthesis assimilates play a vital role in grain yield under stress. Their findings showed that the mobilization of stem reserves was more pronounced under drought, with substantial increases in translocation efficiency.\n\nOverall, the collective findings from these studies underscore the importance of stem sugar reserve mobilization in wheat, particularly in the context of environmental stress. Enhancing the genetic traits associated with stem reserve accumulation and mobilization could be a promising strategy for improving wheat resilience and yield in challenging conditions.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2004}, "authors": ["M. Brettingham"], "summary": "The Labour government's systematic investment in the health service is delivering concrete results, Nigel Crisp, chief executive of the NHS, said last week in his end of year report.\n\nThe financial expansion, which started in 1998 and has seen the NHS receive several large cash injections, is now resulting in shorter waiting times and falling numbers of premature deaths.\n\nThe reduction in waiting times, which the government has been under particular pressure to deliver, gives the report a focus. The number of inpatients on waiting lists has fallen by 35% since it peaked in 1998, falling from more than a million then to 850 000 today.\n\nThe report shows that about 70 000 outpatients are now waiting \u2026", "title": "NHS waiting list has fallen by 35% since 1998", "claims": null}, {"metadata": {"year": 2022}, "authors": ["A. O'dowd"], "summary": "\u201cOur research highlights hope and opportunities to reduce waiting times in the present day: by addressing shortages of healthcare staff andphysical resources urgently; by working with integrated care systems in the spirit of prevention, collaboration, inclusion, and community based models of care; and in aligning a vision for the health service with a plan that brings staff, patients, and the public along on the journey to get there.\u201d", "title": "Multipronged efforts are needed to reduce NHS waiting times, says review", "claims": null}, {"metadata": {"year": 2010}, "authors": ["A. Harrison", "J. Appleby"], "summary": "Abstract Recent authors have proposed that waiting times for elective treatment should be reduced to the point where the costs of doing so exceed the benefits. This paper considers how this criterion could be put into effect. Taking benefits first it argues that these could be estimated in three different ways \u2013 social cost benefit, clinical and user valuation \u2013 that would not necessarily produce consistent results and hence a choice has to be made between them. It then considers the costs of reducing waits and argues, citing relevant evidence, that these may range widely according to whether or not reductions can be achieved through simple management measures or whether more long-term capacity is required. It concludes therefore that the apparently simple criterion proposed for defining the point where waiting times are optimal is hard to establish. Choice of criterion must be made in the light of the overall values that a given health care system is intended to promote.", "title": "Optimising waiting: a view from the English National Health Service", "claims": null}, {"metadata": {"year": 2009}, "authors": ["A. Harrison", "J. Appleby"], "summary": "In recent years, the English NHS has achieved substantial reductions in waiting times for hospital treatment. This paper considers first whether the data used by the Government provide an accurate description of changes in waiting times and identifies some of the limitations of the measures used. It then attempts to identify how reductions have been achieved. It argues that some features of central government policy have been important - such as the use of targets - others, such as the introduction of new private sector capacity have not. It also shows that changes at local level have been critical to achieving the recorded improvements, but the precise impact of these is hard to identify.", "title": "Reducing Waiting Times for Hospital Treatment: Lessons from the English NHS", "claims": null}, {"metadata": {"year": 1996}, "authors": ["C. Gray"], "summary": "Reforms involving the National Health Service (NHS) have greatly reduced the length of waiting lists in the United Kingdom. The key to the reductions was additional funding from the government, the chief executive of the NHS said during a recent visit to Ottawa. Decreasing the size of the waiting lists created intense stress for NHS personnel, who had to work longer hours, and it also lowered demand for private-sector care.", "title": "NHS reforms reduce length of waiting lists but create widespread unease.", "claims": null}, {"metadata": {"year": 2008}, "authors": ["C. Propper", "M. Sutton", "C. Whitnall", "F. Windmeijer"], "summary": "Abstract Waiting times have been a central concern in the English NHS, where care is provided free at the point of delivery and is rationed by waiting time. Pro-market reforms introduced in the NHS in the 1990s were not accompanied by large drops in waiting times. As a result, the English government in 2000 adopted the use of an aggressive policy of targets coupled with the publication of waiting times data at the hospital level and strong sanctions for poor performing hospital managers. This regime has been dubbed \u0091targets and terror\u0092. We estimate the effect of the English target regime for waiting times for hospital care after 2001 by a comparative analysis with Scotland, a neighbouring country with the same healthcare system that did not adopt the target regime. We estimate difference-in-differences models of the proportion of people on the waiting list who waited over 6, 9 and 12 months. Comparisons between England and Scotland are sensitive to whether published or unpublished data are used but, regardless of the data source, the \u0091targets and terror\u0092 regime in England lowered the proportion of people waiting for elective treatment relative to Scotland.", "title": "Did 'Targets and Terror' Reduce Waiting Times in England for Hospital Care?", "claims": null}, {"metadata": {"year": 1981}, "authors": ["R. Feldman", "D. Ballard"], "summary": "This paper estimates a model of demand for the services of general practitioner physicians (GPs) in the British National Health Service (NHS). A wide range of services is available from the NHS to any person free of charge at the time of use. Money prices have been replaced, except for nominal charges for prescriptions and appliances, b^ nonprice rationing methods such as queuing for hospital admissions and waiting for physicians' office visits. Nonprice rationing has been cited as a major difficulty of the NHS [1, p. 219]. While this problem has not surfaced in the United States to the same degree as in England, it is nevertheless instructive to study the British experience. About 40 percent of U.S. health expenditures in 1980 were paid by public sources [2] and, if national health insurance legislation is enacted here, the public share could rise toward the level observed in the nationalized British system. In addition, some private health insurance plans in the U.S. have features that resemble the NHS, e.g., free care at the time of use. The NHS may thus provide a model for the analysis of these U.S. plans. We examine the effect of office waiting time on the demand for GPs' services. Using a utility-maximizing model of physicians' behavior, we predict that waiting time should be directly related to exogenous demand pressures on the physician's practice. Longer waiting time, in turn, should reduce both the number of visits per person and the number of patients in the physician's practice (which the British call the physician's \"listsize\"). We specify and estimate an empirical version of the model with three equations: waiting time, visits per patient, and listsize. A key finding is that the elasticity of demand for GPs' services with respect to waiting time is between -.48 and -.92, depending on our definition of variables and functional form. This result has important implications for national health insurance for outpatient visits (if such legislation is passed in the U.S.). If insurance were to reduce money prices to zero, demand would become relatively more sensitive to differences in time prices. In turn, as Acton has noted [3], this would permit persons with a lower opportunity cost of time to bid services away from those with a higher opportunity cost of time. National health insurance might, therefore, have important distributional effects, as well as the intended reduction of money price barriers to access. Even without national health insurance, policy-makers in the U.S. might want to study the British National Health Service. Increas-", "title": "The Role of Waiting Time in a Prepaid Health Care System: Evidence from the British National Health Service", "claims": null}, {"metadata": {"year": 1986}, "authors": ["E. Scrivens", "G. Hart"], "summary": "Ministers are anxious to see shorter hospital waiting lists. Instead of buying services from the private sector, health authorities may do better by trading with each other. But, if the full potential of trading is to be realised, the financing and management of the NHS will have to alter.", "title": "How to reduce waiting lists: A case for trading within the NHS", "claims": null}], "query": "Health economic benefits of reducing NHS waiting times", "summary_abstract": "The collective findings from the provided abstracts highlight the significant health economic benefits associated with reducing NHS waiting times. A consistent theme across the literature is that shorter waiting times not only improve patient outcomes but also enhance the overall efficiency of the healthcare system.\n\nBrettingham (2004) notes that substantial investments in the NHS have led to a marked reduction in waiting times, with inpatient waiting lists decreasing by 35% since 1998. This reduction is linked to a decline in premature deaths, suggesting that timely access to care can have profound health benefits. Similarly, Gray (1996) emphasizes that government funding reforms have been pivotal in decreasing waiting lists, which in turn alleviates pressure on NHS staff and reduces the demand for private healthcare services.\n\nO'Dowd (2022) underscores the importance of addressing healthcare staff shortages and resource limitations to further reduce waiting times. This aligns with the notion that a collaborative approach involving integrated care systems can enhance service delivery and patient satisfaction.\n\nHarrison and Appleby (2010) discuss the complexities of determining the optimal waiting time, suggesting that while reducing waits can yield benefits, the costs associated with such reductions can vary significantly. They argue for a careful evaluation of the trade-offs involved in managing waiting times, indicating that the benefits of reduced waiting times must be weighed against the financial implications of achieving them.\n\nMoreover, Propper et al. (2008) provide evidence that the introduction of target-driven policies in England has effectively lowered waiting times compared to Scotland, which did not adopt similar measures. This suggests that strategic policy implementation can lead to significant improvements in patient access to care.\n\nIn summary, the literature collectively supports the notion that reducing NHS waiting times yields substantial health economic benefits, including improved patient outcomes, reduced mortality rates, and enhanced system efficiency. However, achieving these reductions requires careful consideration of resource allocation and policy strategies to ensure that the benefits outweigh the associated costs.", "summary_extract": null}, {"papers": [{"metadata": {"year": 1999}, "authors": ["Raymond Papp"], "summary": "Last Fall, my university began an initiative to offer courses online in what many call a \"distance learning\" environment. Being an early adopter of the Internet and WWW in my traditional on-campus courses, I jumped at the chance to offer a course in an on-line environment. This paper will briefly explore the differences between on-campus and on-line education, discuss my approach for implementing the course (including a comparison/contrast of student perceptions and performance in both on-campus and on-line sections of the same course), and conclude with some implications for educators endeavoring to teach in a distance learning environment.", "title": "\"On-campus\" vs. \"On-line\": Student Perceptions & Performance", "claims": null}, {"metadata": {"year": 2001}, "authors": ["Xiaoyan Xie", "Fuzong Lin", "Zhang Tao"], "summary": "More and more universities and colleges are providing online courses not only for on-campus students but also for off-campus students. Tutors have to consider the differences between on- and off-campus students in order to improve effective instruction. Comparisons are made in this paper between on- and off-campus performances in online learning from four areas: learning time, path of browsing courseware, intercommunication and adaptability towards online learning. The last two areas are emphasized. Multiple approaches were adopted to collect data, which include questionnaires, posted documents, online logs, interviews and observations. This study shows that the rush time of online learning, paths of browsing courseware and favourite intercommunication means of on- and off-campus students are similar. But there are also some differences between these two groups such as competence of self-learning, enthusiasm of interpersonal exchange, dependence on tutors, feeling of learning stress, etc.", "title": "Comparison between on- and off-campus behaviour and adaptability in online learning: A case from China", "claims": null}, {"metadata": {"year": 2005}, "authors": ["G. Ury", "M. McDonald", "G. McDonald", "Brian Dorn"], "summary": "In previous papers, the authors reported results of smaller-scale studies. The purpose of this study is to combine and expand those individual smaller scaled studies, to determine if significant differences exist between student performance in online and traditional classroom environments. The study includes more than 1300 observations spread across seven courses that are part of the computer science and information systems curriculum at Northwest Missouri State University. Student performance was compared by grade point average, ACT composite scores, number of credit hours completed, instructor, and delivery method. The only significant difference found was between student performance and delivery method in three high volume courses that serve multiple majors and minors. Online students in these three courses obtained a significantly lower average grade than onground students. In four other courses that service upper-level computer science majors no significant differences in performance were found. The varied results of the study could be a simple statement of fact. Different courses in different programs might have different performance results. It could be concluded that online students are simply satisfied with a little lower grade in particular courses or that traditional students perform better because of the availability of added resources planned and implemented for online curriculum. It could be concluded that faculty continue to deal with problems in effectively transferring traditional classroom learning to the online environment. It could also be speculated that the difference is any combination of the above conclusions.", "title": "Student Performance Online vs Onground: A Statistical Analysis of IS Courses", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Di  Xu", "Shanna S. Jaggars"], "summary": "Using a dataset containing nearly 500,000 courses taken by over 40,000 community and technical college students in Washington State, this study examines the performance gap between online and face-to-face courses and how the size of that gap differs across student subgroups and academic subject areas. While all types of students in the study suffered decrements in performance in online courses, those with the strongest declines were males, younger students, Black students, and students with lower grade point averages. Online performance gaps were also wider in some academic subject areas than others. After controlling for individual and peer effects, the social sciences and the applied professions (e.g., business, law, and nursing) showed the strongest online performance gaps.", "title": "Performance Gaps Between Online and Face-to-Face Courses: Differences Across Types of Students and Academic Subject Areas", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Di Xu", "S. Jaggars"], "summary": "Using a dataset containing nearly 500,000 courses taken by over 40,000 community and technical college students in Washington State, this study examines the performance gap between online and face-to-face courses and how the size of that gap differs across student subgroups and academic subject areas. While all types of students in the study suffered decrements in performance in online courses, those with the strongest declines were males, younger students, Black students, and students with lower grade point averages. Online performance gaps were also wider in some academic subject areas than others. After controlling for individual and peer effects, the social sciences and the applied professions (e.g., business, law, and nursing) showed the strongest online performance gaps.", "title": "Performance Gaps between Online and Face-to-Face Courses: Differences across Types of Students and Academic Subject Areas", "claims": null}, {"metadata": {"year": 2016}, "authors": ["C. B. Gregory", "J. Lampley"], "summary": "As part of a nationwide effort to increase the postsecondary educational attainment levels of citizens, community colleges have expanded offerings of courses and programs to more effectively meet the needs of students. Online courses offer convenience and flexibility that traditional face-to-face classes do not. These features appeal to students with family and work responsibilities that typically make attending classes on campus difficult. However, many of the students who tend to take courses in this instructional format have characteristics that place them at high-risk for academic failure. Because of the traditional mission of community colleges, they generally serve more students who fit this high-risk profile. Despite the promise and potential of online delivery systems, studies have associated distance with higher student withdrawal rates. In addition, research has indicated that online students tend to earn lower grades than students in comparable face-to-face classes. The existence of contrasting findings in the literature exposes the need for additional empirical research relative to the overall success of students in online courses, as well as on factors associated with success in distance education. This is especially true for community college students. The purpose of this study was to determine if significant differences existed in student success at the community college level in online courses as compared to face-to-face courses. In addition, the researchers investigated the relationship between selected demographic, academic, enrollment, and external environmental factors and student success in online courses. The study involved secondary data analysis of quantitative data relevant to students enrolled in course sections taught by instructors who taught both online and face-to-face sections of the same course within the same semester from fall 2012 through spring 2015. The target population included 4,604 students enrolled at a public 2-year community college located in Tennessee. Results indicated there was a significant difference in success between students taking a course online and students taking a course face-to-face. Also, there was a significant difference in success based on instructional method when the following factors were considered: age group, gender, student academic classification, and Pell Grant eligibility status. There was no significant difference in success based on instructional method when first-generation college student status was considered.", "title": "Community College Student Success in Online Versus Equivalent Face-to-Face Courses", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Di  Xu", "Shanna Smith Jaggars"], "summary": "Using a large administrative dataset from a statewide system including 34 community and technical colleges, the authors employed an instrumental variable technique to estimate the impact of online versus face-to-face course delivery on student course performance. The travel distance between each student's home and college campus served as an instrument for the likelihood of enrolling in an online section of a given course. In addition, college-by-course fixed effects controlled for within- and between-course selection bias. Analyses yield robust negative estimates for online learning in terms of both course persistence and course grade, contradicting the notion that there is no significant difference between online and face-to-face student outcomes\u2014at least within the community college setting. Accordingly, both two-year and four-year colleges may wish to focus on evaluating and improving the quality of online coursework before engaging in further expansions of online learning.", "title": "The impact of online learning on students\u2019 course outcomes: Evidence from a large community and technical college system", "claims": null}, {"metadata": {"year": 1999}, "authors": ["R. Sims", "Allan H. Schuman"], "summary": "The past five years have borne witness to a revolution in education with an acceleration in the use of online technologies to assist or, in many cases, supplant traditional modes of instruction (Bjorner 1993; Velsmid 1997). Peterson's Guide reports that nearly 400 accredited colleges and universities in North America currently employ online instruction of some sort (Velsmid). In addition, Herther (1997) noted that over 150 accredited institutions offer entire bachelor's degree programs to students who rarely, if ever, visit campus. The asynchronous nature of many online programs together with their accessibility from home, office, or hotel room are obvious advantages to students (see Bjorner). Additionally, as the cost of traditional education increases, market pressures are forcing more and more institutions to consider online offerings (see Gubernick and Ebeling 1997) that do not incur the costs of dormitories, athletic programs, etc. The Florida State University system expects online programs to save about 40% of the cost of in-class programs (\"Caught\" 1998). It should be noted, however, that Duke University charges a premium for its online MBA ($82,500 vs. $50,000 for its on-campus equivalent). As more and more online courses and programs proliferate, the questions of quality and comparability of such instruction with traditional methods naturally arise. Gubernick & Ebeling report a study conducted by the University of Phoenix (a private, for-profit institution) that demonstrated standardized achievement test scores of its online graduates were 5% to 10% higher than graduates of competing on-campus programs at three Arizona public universities. While one may legitimately question the degree of comparability of the subject populations, these results are similar to those summarized by Vasarhelyi and Graham (1997) in which investigators at the University of Michigan concluded that computer-based instruction yielded higher average scores than traditional instruction. To date, the most methodologically sound investigation to evaluate the effectiveness of online instruction was conducted by Gerald Schutte at Cal State, Northridge (as cited by McCollum 1997). \"Schutte randomly divided his statistics class into two groups. One attended class as usual, listening to lectures, handing in homework assignments, and taking examinations. The other took an online version of the course, completing assignments on a World Wide Web site, posting questions and comments to an electronic discussion list, and meeting with their professor in an Internet chat room. After an orientation session, students in the virtual class went to Dr. Schutte's classroom only for their midterm and final exams. On both tests, Dr. Schutte found, the wired students outscored their traditional counterparts by an average of 20 percent.\" The present study extends Schutte's paradigm by looking at pre and posttest scores of students enrolled in online and inclass versions of the same class taught by the same instructors over a variety of disciplines. Methodology Students enrolled in five different undergraduate online courses during the Fall semester 1997 participated in a test-retest study designed to measure their learning of the course material. These students were compared with students enrolled in traditional inclass courses taught by the same instructors. The course titles were Organization Behavior, Personal Finance, Managerial Accounting, Sociological Foundations of Education, and Environmental Studies. Student participation was voluntary; names were only used to ensure a matching of the pre / posttest results. Subjects In total, 40 undergraduate students were enrolled in the online courses and 59 undergraduate students were enrolled in the inclass courses during the testing period. Pretests Instructors designed pretests to measure the level of knowledge students had of the course content prior to the start of the course. \u2026", "title": "Learning in an Online Format versus an In-Class Format: An Experimental Study", "claims": null}], "query": "Online university versus on campus and academic performance differences", "summary_abstract": "The research question regarding the differences in academic performance between online university courses and traditional on-campus courses has been explored through various studies, revealing a complex landscape of outcomes.\n\nPapp (1999) initiated the discussion by comparing student perceptions and performance in both online and on-campus sections of the same course, suggesting that there are notable differences in how students engage with the material in these two environments. This foundational work set the stage for further investigations into the performance metrics of students in different learning formats.\n\nXie et al. (2001) expanded on this by examining specific areas of performance, such as learning time and adaptability to online learning. Their findings indicated that while some aspects of online learning were similar for both on-campus and off-campus students, significant differences existed in self-learning competence and feelings of learning stress, which could impact overall academic performance.\n\nUry et al. (2005) conducted a larger-scale study involving over 1300 observations and found that online students in certain high-volume courses performed significantly worse than their on-campus counterparts. However, in upper-level courses, no significant differences were noted, suggesting that the type of course may influence performance outcomes.\n\nXu and Jaggars (2014) provided a comprehensive analysis of nearly 500,000 courses taken by over 40,000 students, revealing that all student subgroups experienced performance decrements in online courses, with the most pronounced declines among younger students and those with lower GPAs. This study highlighted that the performance gap varied across academic disciplines, particularly in social sciences and applied professions.\n\nFurther reinforcing these findings, Gregory and Lampley (2016) noted that community college students, who often face higher risks of academic failure, tend to earn lower grades in online courses compared to face-to-face classes. Their research indicated that demographic factors such as age and gender also played a role in the success rates of online learners.\n\nIn a similar vein, Xu and Jaggars (2013) employed an instrumental variable technique to assess the impact of course delivery methods on student performance, concluding that online learning was associated with lower course persistence and grades, particularly in community college settings.\n\nOverall, the collective evidence suggests that while online education offers flexibility and accessibility, it often comes with challenges that can negatively impact academic performance compared to traditional on-campus learning. Factors such as course type, student demographics, and individual learning styles appear to significantly influence these outcomes, indicating a need for further research and potential improvements in online course delivery methods to enhance student success.", "summary_extract": null}, {"papers": [{"metadata": {"year": 1999}, "authors": ["Hui Chen", "Yan-jun Fang", "Li Yuan", "T. An"], "summary": "An on-line determination of chlorine dioxide (ClO2) in potable water using chlorophenol red (CPR) by gas diffusion flow-injection analysis (FIA) was investigated in the presence of various chlorinated species that can occur under normal water-treatment conditions. A gas diffusion membrane is used to separate the donor (sample) stream from the acceptor (detecting) stream (the donor stream transports the sample stream to the membrane separate device, and the acceptor stream collects all of the penetrated analytes and transports quantitatively to the detector) and makes it possible for this method to eliminate interference from metal ions, as well as other oxychlorinated compounds such as chlorite and chlorate. The system is more selective for chlorine dioxide than chlorine. The linear range of ClO2 concentration is 0\u20130.5 mg\u00b7mL\u22121 with a detection limit of 0.02 \u03bcg\u00b7mL\u22121 (S/N = 3) and a sampling frequency of 50 h\u22121. \u00a9 1999 John Wiley & Sons, Inc. Lab Robotics and Automation 11: 157\u2013161, 1999", "title": "An on\u2010line determination of chlorine dioxide using chlorophenol red by gas diffusion flow\u2010injection analysis", "claims": null}, {"metadata": {"year": 2007}, "authors": ["F. Tran", "D. Rouleau", "D. Couillard"], "summary": "A new model for mixtures (two and more solutes) of aqueous electrolyte solutions was found to be as accurate as other models, or more accurate, in prediction of new experimental results of the ternary systems HClO4\u2013NaClO3\u2013H2O and HClO4\u2013NaCl\u2013H2O. The water activity values are then used to study the mechanism of the chloride\u2013chlorate reaction, generating chlorine dioxide: \n \n \n \n2H+ + ClO\u22123 + Cl\u2212 ClO2 + 1/2Cl2 + H2O \n \n \n \nSpectrophotometric measurements of the production rate of ClO2 have confirmed that the intermediary species in the proposed equilibrium \n \n \n \nH+(mH2O) H+(m-n)H2O + nH2O \n \n \n \nIs actually H(H20)+m-n. The final kinetic expression for the reaction rate of chlorine dioxide generation, used in pulp bleaching, is then derived to explain the high order with respect to acid concentration.", "title": "Study of ClO2 generation reaction via experimental data of aqueous mixed electrolyte solutions: HClO4\u2013NaCLO3\u2013H2O and HCLO4\u2013NaCl\u2013H2O", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Liu Xiao-geng"], "summary": "A new spectrophotometric method for the determination of chlorine dioxide(ClO2) in tap water was developed based on methyl red(MR) oxidation and discoloration.Wavelength scanning showed that MR had maximal absorption at 518 nm.Under the experiment conditions of c(MR)/c(ClO2) = 6.4,pH 4.0-4.5,25 \u2103 and reaction time of 40 min,a linear equation was obtained as \u0394A518 =-1.7845c(ClO2) +1.5228(r = 0.992) in the range of 0.0004-0.80 mg/L,the apparent molar absorptivitye = 1.2 \u00d7 105 L/(mol\u00b7cm),and the detection of limit 0.0036 mg/L.MR discoloration by ClO2 oxidation was a first-order reaction.The dynamic equation at 25 \u2103 was ln[c\u221e/(c\u221e-c)] =-0.000689t + 2.263669 with the activation energy Ea = 87.3 kJ/mol and the half-time t1/2 = 16.8 min.The average recoveries for ClO2 in tap water and ClO2 disinfectant were 98.2%-103.2%(n = 5) with a relative standard deviation of 2.43%-3.83%.This method and the recommended standard N,N-diethyl-p-phenylendiamin(DPD) method showed no significant difference(P = 0.05) according to F-test and t-test.Furthermore,satisfying determination results were obtained from this method.", "title": "Spectrophotometric Determination of Chlorine Dioxide in Tap Water Using Methyl Red", "claims": null}, {"metadata": {"year": 2005}, "authors": ["L. Zhong-bin"], "summary": "The optimum reaction conditions of ClO_2-Fe \u2161 -luminal chemiluminescence CL system were studied by flow injection technique in detail . A new method of CL for the determination of chlorine dioxide is proposed . The detection limit of the method is 4.0 \u03bcg/L ClO_2 and the linear range is 4.0\uff5e680 \u03bcg/L. The method is highly selective , simple , sensitive and has been applied to the determination of ClO_2 in tap water with satisfactory results .", "title": "ClO_2-Fe(II)-Luminal Flow Injection Chemiluminescence System and Its Analytical Application", "claims": null}, {"metadata": {"year": 2001}, "authors": ["Z. Ying", "S. Yao"], "summary": "A method was described for determination of chlorine dioxide in water samples by using porosity polytetrafluoroethylene membrane to separate ClO 2 from ClO -,ClO - 2,ClO - 3 et al and UV absorption spectroscopy. The linear range is 0.24 mg/L\uff5e11.81 mg/L for ClO 2 with detection limit of 5.2 mg/L. The recovery was 96.8\uff5e102 8% with the standard addition method.", "title": "Continuous Determination of Chlorine Dioxide in Water Samples by Membrane Seperation and UV Spectroscopy", "claims": null}, {"metadata": {"year": 2001}, "authors": ["Z. Ying"], "summary": "A method is described for the determination of chlorine dioxide by using porosity polytetrafluoroethylene membrane In this paper, the method is studied according to analytical application in a drinking water plant The conclusion is come to that there is no obvious difference between the method and another method which is commended by Vulan company(USA)if the concentration of ClO 2 in water is more than or equal to 0 08mg/L", "title": "The application of continuous determination of chlorine dioxide by UV-Vis and using membrane to separate gas from water", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Sun Da-hu"], "summary": "Objective To establish differential spectrophotometry method to determine chlorine dioxide(ClO2) in drinking water.Methods The differential spectrophotometry method was conducted to scan in wave length of 574 nm.Results The experiment demonstrated that linear correlation coefficient was high when the ranges were 0.10-0.50 mg/L,0.26-1.31 mg/L,and 0.50-2.70 mg/L,the linearity was good.Conclusion The method has good accuracy and precision,it is suitable for determination of low concentration chlorine dioxide.", "title": "Determination of Chlorine Dioxide in Water by Chlorophenol Red Differential Spectrophotometry", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Zhao Chang-jun"], "summary": "A reverse flow injection spectrophometric method for the determination of chlorine dioxide was developed with leucomethylene blue as the coloration reagent. By this method low concentration chlorine dioxide in water was determined in the presence of Cl_2 and anion species such as ClO~-, ClO~-_2 and ClO~-_3. The linear range of ClO_2 measurement is 0~1.91 mg/L with a detection limit of 0.028 mg/L. The presence of chlorine and hypochlorite ion can be masked by using oxalic acid. Chlorate does not interfere with the measurement. The permissible maximum concentration of ClO~-_2 is 3.0 mg/L.", "title": "New method for the flow injection spectrophotometric deternination of low concentration chlorine dioxide in water using methylene blue", "claims": null}], "query": "ClO/ClO2 data in water analysis", "summary_abstract": "The research on the determination of chlorine dioxide (ClO2) in water highlights various analytical methods that have been developed to accurately measure its concentration, particularly in potable water. Several studies have focused on improving selectivity and sensitivity in the presence of other chlorinated species.\n\nChen et al. (1999) introduced a gas diffusion flow-injection analysis method that effectively separates ClO2 from interfering substances like chlorite and chlorate, achieving a detection limit of 0.02 \u03bcg/mL and a linear range of 0\u20130.5 mg/mL. This method is noted for its high selectivity towards ClO2 compared to chlorine.\n\nTran et al. (2007) explored the kinetics of ClO2 generation in aqueous solutions, providing insights into the reaction mechanisms that can be useful for understanding ClO2 behavior in water treatment processes.\n\nXiao-geng (2011) developed a spectrophotometric method based on the oxidation of methyl red, achieving a detection limit of 0.0036 mg/L and demonstrating high recovery rates in tap water samples. This method showed no significant difference in results compared to the standard N,N-diethyl-p-phenylendiamin (DPD) method.\n\nZhong-bin (2005) proposed a chemiluminescence method for ClO2 determination with a detection limit of 4.0 \u03bcg/L, emphasizing its simplicity and sensitivity for tap water analysis.\n\nYing and Yao (2001) described a method using a polytetrafluoroethylene membrane for separating ClO2 from other chlorinated species, achieving a detection limit of 5.2 mg/L. This method was validated in a drinking water plant setting.\n\nSun (2010) established a differential spectrophotometry method that demonstrated good accuracy and precision for low concentrations of ClO2, with a linear correlation coefficient indicating reliable results.\n\nZhao (2005) developed a reverse flow injection spectrophotometric method that effectively measures low concentrations of ClO2 in the presence of other chlorinated compounds, achieving a detection limit of 0.028 mg/L.\n\nCollectively, these studies illustrate a range of effective methodologies for the analysis of ClO2 in water, each with unique advantages in terms of selectivity, sensitivity, and applicability in real-world water treatment scenarios.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2022}, "authors": ["S. Veretenenko"], "summary": "The stratospheric polar vortex is a large-scale cyclonic circulation that forms in a cold air mass in the polar region and extends from the middle troposphere to the stratosphere. The polar vortex is implicated in a variety of atmospheric processes, such as the formation of ozone holes, the North Atlantic and the Arctic Oscillations, variations in extratropical cyclone tracks, etc. The results presented in this work show that the vortex plays an important part in the mechanism of solar activity influence on lower atmosphere circulation, with variations in the vortex intensity being responsible for temporal variability in the correlation links observed between atmospheric characteristics and solar activity phenomena. In turn, the location of the vortex is favorable for the influence of ionization changes associated with charged particle fluxes (cosmic rays, auroral and radiation belt electrons) that affect the chemical composition and temperature regime of the polar atmosphere as well as its electric properties and cloudiness state. In this work, recent results concerning solar activity effects on the state of the stratospheric polar vortex as well as its role in solar\u2013atmospheric links are discussed.", "title": "Stratospheric Polar Vortex as an Important Link between the Lower Atmosphere Circulation and Solar Activity", "claims": null}, {"metadata": {"year": 1959}, "authors": ["C. E. Palmer"], "summary": "In winter the polar stratospheric air within the earth's shadow forms the core of an intense \u2018cold low\u2019 which extends from about 10 km to at least 50 km and possibly to the base of the ionosphere. Compared with the tropospheric general circulation, this vortex seems to be remarkably stable, particularly in the Southern Hemisphere. \n \nRecent research work in Canada and the United States on the characteristics of the vortex is reviewed. The chief conclusions are that the vortex is more stable in the Southern than in the Northern Hemisphere, that \u2018explosive warmings\u2019 in the lower stratosphere of the Northern Hemisphere follow the breakdown of the vortex at high levels close to the pole, and that the breakdown extends from above downward over a period of several days. It is suggested that the high-level breakdown is correlated with solar activity.", "title": "The stratospheric polar vortex in winter", "claims": null}, {"metadata": {"year": 2019}, "authors": ["S. Lee", "A. Butler"], "summary": "The stratospheric polar vortex is a westerly circulation that forms over the winter pole around 10-50 km above the surface, which is known to influence mid-latitude weather patterns. During 2018-19, the Arctic polar vortex demonstrated an unusually large amount of variability,\r\nincluding a strong and persistent sudden stratospheric warming (SSW) event, a strong vortex event, and a dynamic final stratospheric warming (FSW). In this article we discuss the evolution of the vortex, placing it in the context of wider observed climatology, and comment on its apparent impacts on tropospheric weather patterns \u2013 notably, the lack of a surface climate response to the SSW of similar magnitude to the February-March 2018 \u201cBeast from the East\u201d cold-wave.", "title": "The 2018\u20132019 Arctic stratospheric polar vortex", "claims": null}, {"metadata": {"year": 2000}, "authors": ["G. Manney", "J. L. Sabutis"], "summary": "The 1999\u20132000 Arctic stratospheric vortex was unusually cold, especially in the early winter lower stratosphere, with a larger area near polar stratospheric cloud formation temperatures in Dec and Jan, and much lower temperatures averaged over Nov\u2013Jan, than any previously observed Arctic winter. In Nov and early Dec, there was a double jet in the upper stratosphere, with the anticyclone cutoff in a region of cyclonic material. By late Dec, there was a discontinuous vortex, large in the upper stratosphere, small in the lower stratosphere; evolving to a strong, continuous, relatively upright vortex by mid\u2010Jan. This vortex evolution in 1999\u20132000 is typical of that in other cold early winters. Despite unusually low temperatures, the lower stratospheric vortex developed more slowly than in previous unusually cold early winters, and was weaker than average until late Dec.", "title": "Development of the polar vortex in the 1999\u20132000 Arctic winter stratosphere", "claims": null}, {"metadata": {"year": 2013}, "authors": ["D. Waugh", "L. Polvani"], "summary": "The Stratosphere: Geophysical Mon Copyright 2010 b 10.1029/2009GM The intense cyclonic vortices that form over the winter pole are one of the most prominent features of the stratospheric circulation. The structure and dynamics of these \u201cpolar vortices\u201d play a dominant role in the winter and spring stratospheric circulation and are key to determining distribution of trace gases, in particular ozone, and the couplings between the stratosphere and troposphere. In this chapter, we review the observed structure, dynamical theories, and modeling of these polar vortices. We consider both the zonal mean and three-dimensional potential vorticity perspective and examine the occurrence of extreme events and long-term trends.", "title": "Stratospheric Polar Vortices", "claims": null}, {"metadata": {"year": 2020}, "authors": ["S. Lee"], "summary": "The polar vortex is the dominant feature of the wintertime stratosphere. Sometimes, it is unusually strong, while at others it is very weak or destroyed in an event known as a sudden stratospheric warming (SSW). Both can play an important role in driving the type and predictability of the weather we experience at the surface.", "title": "The stratospheric polar vortex and sudden stratospheric warmings", "claims": null}, {"metadata": {"year": 2018}, "authors": ["V. Harvey", "C. Randall", "L. Goncharenko", "E. Becker", "J. France"], "summary": "The polar vortices play a central role in vertically coupling the atmosphere from the ground to geospace by shaping the background wind field through which atmospheric waves propagate. This work extends the vertical range of previous polar vortex climatologies into the upper mesosphere. The mesospheric polar vortices are defined using the CO gradient method with Microwave Limb Sounder satellite data; the stratospheric polar vortices are defined using a stream function\u2010based algorithm with data from meteorological reanalyses. Strengths and weaknesses of the two vortex definitions are given, as well as recommendations for when, where, and why to use each definition. Midwinter mean vortex geometry in the mesosphere is funnel shaped in the Arctic, with a wide top and narrow bottom. The Antarctic mesospheric vortex tapers with height in early winter and broadens with height in late winter. The seasonal evolution of mesospheric vortex frequency of occurrence, size, and zonal symmetry in both hemispheres is presented. Unexpected behavior above 60 km includes late season vortex broadening in both hemispheres, especially following winters without sudden stratospheric warmings. Following extreme stratospheric disturbances the polar night jet in the mesosphere strengthens and shifts poleward, resulting in a mesospheric vortex that contracts. Overall, the mesospheric polar vortices are more similar between the two hemispheres than their stratospheric counterparts. The vortex climatology presented here serves as an observational benchmark to which the mesospheric polar vortices in high\u2010top climate models can be evaluated.", "title": "On the Upward Extension of the Polar Vortices Into the Mesosphere", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Alvaro de la C\u00e1mara", "Carlos R. Mechoso", "Kayo  Ide", "Richard L. Walterscheid", "Gerald  Schubert"], "summary": "The present paper examines the vortex breakdown and large-scale stirring during the final warming of the Southern Hemisphere stratosphere during the spring of 2005. A unique set of in situ observations collected by 27 superpressure balloons (SPBs) is used. The balloons, which were launched from McMurdo, Antarctica, by the Strat\u00e9ole/VORCORE project, drifted for several weeks on two different isopycnic levels in the lower stratosphere. We describe balloon trajectories and compare them with simulations obtained on the basis of the velocity field from the GEOS-5 and NCEP/NCAR reanalyses performed with and without VORCORE data. To gain insight on the mechanisms responsible for the horizontal transport of air inside and outside the well-isolated vortex we examine the balloon trajectories in the framework of the Lagrangian properties of the stratospheric flow. Coherent structures of the flow are visualized by computing finite-time Lyapunov exponents (FTLE). A combination of isentropic analysis and FTLE distributions reveals that air is stripped away from the vortex\u2019s interior as stable manifolds eventually cross the vortex\u2019s edge. It is shown that two SPBs escaped from the vortex within high potential vorticity tongues that developed in association with wave breaking at locations along the vortex\u2019s edge where forward and backward FTLE maxima approximately intersect. The trajectories of three SPBs flying as a group at the same isopycnic level are examined and their behavior is interpreted in reference to the FTLE field. These results support the concept of stable and unstable manifolds governing transport of air masses across the periphery of the stratospheric polar vortex.", "title": "Polar night vortex breakdown and large-scale stirring in the southern stratosphere", "claims": null}], "query": "polar vortex stretching stratosphere", "summary_abstract": "The research question regarding the stretching of the polar vortex in the stratosphere is addressed through various studies that explore the dynamics, stability, and implications of the polar vortex in both the Arctic and Antarctic regions.\n\nVeretenenko (2022) highlights the polar vortex's role in atmospheric processes, including its interaction with solar activity, which can influence the vortex's intensity and, consequently, the stratospheric conditions. This suggests that variations in the polar vortex can have significant implications for the stratosphere's structure and behavior.\n\nPalmer (1959) provides insights into the stability of the polar vortex, noting that it tends to be more stable in the Southern Hemisphere compared to the Northern Hemisphere. He discusses the phenomenon of \"explosive warmings\" that can occur following the breakdown of the vortex, indicating that such breakdowns can lead to significant changes in stratospheric dynamics.\n\nLee and Butler (2019) examine the variability of the Arctic polar vortex during the winter of 2018-19, noting a strong and persistent sudden stratospheric warming (SSW) event. This variability is crucial as it can affect mid-latitude weather patterns, demonstrating the polar vortex's influence on broader atmospheric conditions.\n\nManney and Sabutis (2000) describe the evolution of the Arctic stratospheric vortex during the winter of 1999-2000, emphasizing its unusually cold temperatures and the dynamics of vortex formation and breakdown. They note that the vortex's behavior can be influenced by external factors, including solar activity.\n\nWaugh and Polvani (2013) discuss the polar vortex's structure and dynamics, emphasizing its importance in the winter and spring stratospheric circulation. They highlight how the polar vortex affects the distribution of trace gases, such as ozone, and the interactions between the stratosphere and troposphere.\n\nLee (2020) reiterates the polar vortex's significance as a dominant feature of the winter stratosphere, noting that its strength can vary dramatically, impacting surface weather predictability.\n\nHarvey et al. (2018) extend the analysis of polar vortices into the upper mesosphere, revealing how these vortices shape atmospheric wave propagation and influence the overall atmospheric structure.\n\nLastly, de la C\u00e1mara et al. (2010) investigate the breakdown of the vortex during the Southern Hemisphere's final warming, using balloon observations to analyze air transport dynamics within and around the vortex. Their findings support the idea that coherent structures in the flow can lead to significant air mass transport across the vortex's boundaries.\n\nCollectively, these studies underscore the polar vortex's critical role in stratospheric dynamics, its interaction with solar activity, and its influence on weather patterns, highlighting the complexity and variability inherent in this atmospheric phenomenon.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2018}, "authors": ["W. Young", "R. Gearry", "Paul Cotter", "K. Fraser", "W. McNabb", "N. Roy"], "summary": "Irritable Bowel Syndrome (IBS) is a functional gastrointestinal (GI) disorder featuring chronic or recurrent abdominal discomfort, usually with changes in GI habit. However, the mechanisms responsible for IBS are poorly understood. Although alterations in the GI microbiome has been implicated in IBS, there is a lack of consensus on what the exact role of the microbiome is, and how it changes, in IBS. To gain a better understanding of the link between the microbiome and IBS, we undertook shotgun metagenomic sequencing of faecal samples from a case\u2010control study. The overall aim of the study was to identify microbial and metabolomic factors that provide mechanistic insights into functional GI disorders and increase the predictability of phenotypes for use in nutrition intervention studies. Faecal samples from 112 individuals with functional GI symptoms (cases) or those that were asymptomatic (controls) were shotgun sequenced using the Illumina NextSeq platform. Taxonomic classifications were determined using Metaxa2 and the SILVA 128 database. Gene functions were assigned with the MG\u2010RAST pipeline. Bacterial genera that discriminated case\u2010controls from IBS groups included Faecalibacterium, Blautia, Roseburia, Bilophila, and Streptococcus, which were relatively more abundant in certain IBS subtypes. For example, Faecalibacterium was relatively more abundant in individuals presenting with constipation associated IBS (IBS\u2010C; P<0.001), while Blautia was more prominent in those with the diarrhea form of IBS (IBS\u2010D; P=0.015). Gene functions that best separated groups included those related to carbohydrate metabolism (higher in IBS compared to case\u2010controls), protein metabolism, and virulence factors. Our results suggest that carbohydrate fermentation by the microbiome may be an important factor in IBS. This work shows that efforts to understand the role of the microbiome in IBS need to include consideration of microbial function rather than just microbial composition.", "title": "Exploring the link between Irritable Bowel Syndrome and the microbiome", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Sampan  Attri", "Ravinder  Nagpal", "Gunjan  Goel"], "summary": "Abstract The present study characterized the colonization and development of gut microbial communities in healthy Indian infants from North-Western Himalayan region in the province Himachal Pradesh. The diversity and transitions of core genera was assessed targeting the 16S rRNA V3-V4 hypervariable region on an Illumina platform. Analysis of more than 17,000 filtered high quality reads indicated that the diversity was lowest in the month 2 followed by gradual increase towards month 4 (1.24 folds increase in Shannon index). The microbial population in month 1 was dominated by Firmicutes and Proteobacteria followed by dominance of Actinobacteria and Firmicutes in the month 4. The analysis of aggregate microbiota at class level indicated relatively higher abundance of Clostridia, Bacteroides and Actinobacteria in month 1, 3 and 4, respectively. The global comparison of dominance of different phyla with the similar subjects indicated that the Indian microbiome is more similar with studies conducted with Swedish infants, although the differences in DNA extraction protocols, geographical location and sequencing platforms as confounding factors cannot be neglected. The findings in this small cohort study could facilitate future studies exploring various aspects of the human gut microbiome in Indian subcontinent.", "title": "High throughput sequence profiling of gut microbiome in Northern Indian infants during the first four months and its global comparison", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Yuxia Liu", "Wenhui Li", "Hongxia Yang", "Xiaoying Zhang", "Wenxiu Wang", "Sitong Jia", "Beibei Xiang", "Yi Wang", "Lin Miao", "Han Zhang", "Lin Wang", "Yujing Wang", "Jixiang Song", "Yingjie Sun", "Lijuan Chai", "Xiaoxuan Tian"], "summary": "Irritable bowel syndrome (IBS) is a chronic gastrointestinal disorder characterized by abdominal pain or discomfort. Previous studies have illustrated that the gut microbiota might play a critical role in IBS, but the conclusions of these studies, based on various methods, were almost impossible to compare, and reproducible microorganism signatures were still in question. To cope with this problem, previously published 16S rRNA gene sequencing data from 439 fecal samples, including 253 IBS samples and 186 control samples, were collected and processed with a uniform bioinformatic pipeline. Although we found no significant differences in community structures between IBS and healthy controls at the amplicon sequence variants (ASV) level, machine learning (ML) approaches enabled us to discriminate IBS from healthy controls at genus level. Linear discriminant analysis effect size (LEfSe) analysis was subsequently used to seek out 97 biomarkers across all studies. Then, we quantified the standardized mean difference (SMDs) for all significant genera identified by LEfSe and ML approaches. Pooled results showed that the SMDs of nine genera had statistical significance, in which the abundance of Lachnoclostridium, Dorea, Erysipelatoclostridium, Prevotella 9, and Clostridium sensu stricto 1 in IBS were higher, while the dominant abundance genera of healthy controls were Ruminococcaceae UCG-005, Holdemanella, Coprococcus 2, and Eubacterium coprostanoligenes group. In summary, based on six published studies, this study identified nine new microbiome biomarkers of IBS, which might be a basis for understanding the key gut microbes associated with IBS, and could be used as potential targets for microbiome-based diagnostics and therapeutics.", "title": "Leveraging 16S rRNA Microbiome Sequencing Data to Identify Bacterial Signatures for Irritable Bowel Syndrome", "claims": null}, {"metadata": {"year": 2019}, "authors": ["R. Pittayanon", "Jennifer T. Lau", "Yuhong Yuan", "G. Leontiadis", "F. Tse", "M. Surette", "P. Moayyedi"], "summary": "BACKGROUND & AIMS\nIrritable bowel syndrome (IBS) is common but difficult to treat. Altering the gut microbiota has been proposed as a strategy for treatment of IBS, but the association between the gut microbiome and IBS symptoms has not been well established. We performed a systematic review to explore evidence for this association.\n\n\nMETHODS\nWe searched databases, including MEDLINE, EMBASE, Cochrane CDSR, and CENTRAL, through April 2, 2018 for case-control studies comparing the fecal or colon microbiomes of adult or pediatric patients with IBS with microbiomes of healthy individuals (controls). The primary outcome was differences in specific gut microbes between patients with IBS and controls.\n\n\nRESULTS\nThe search identified 2631 citations; 24 studies from 22 articles were included. Most studies evaluated adults presenting with various IBS subtypes. Family Enterobacteriaceae (phylum Proteobacteria), family Lactobacillaceae, and genus Bacteroides were increased in patients with IBS compared with controls, whereas uncultured Clostridiales I, genus Faecalibacterium (including Faecalibacterium prausnitzii), and genus Bifidobacterium were decreased in patients with IBS. The diversity of the microbiota was either decreased or not different in IBS patients compared with controls. More than 40% of included studies did not state whether cases and controls were comparable (did not describe sex and/or age characteristics).\n\n\nCONCLUSIONS\nIn a systematic review, we identified specific bacteria associated with microbiomes of patients with IBS vs controls. Studies are needed to determine whether these microbes are a product or cause of IBS.", "title": "Gut Microbiota in Patients With Irritable Bowel Syndrome-A Systematic Review.", "claims": null}, {"metadata": {"year": 2021}, "authors": ["L. Aldars-Garc\u00eda", "M. Chaparro", "J. Gisbert"], "summary": "Inflammatory bowel disease (IBD) is a chronic relapsing\u2013remitting systemic disease of the gastrointestinal tract. It is well established that the gut microbiome has a profound impact on IBD pathogenesis. Our aim was to systematically review the literature on the IBD gut microbiome and its usefulness to provide microbiome-based biomarkers. A systematic search of the online bibliographic database PubMed from inception to August 2020 with screening in accordance with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines was conducted. One-hundred and forty-four papers were eligible for inclusion. There was a wide heterogeneity in microbiome analysis methods or experimental design. The IBD intestinal microbiome was generally characterized by reduced species richness and diversity, and lower temporal stability, while changes in the gut microbiome seemed to play a pivotal role in determining the onset of IBD. Multiple studies have identified certain microbial taxa that are enriched or depleted in IBD, including bacteria, fungi, viruses, and archaea. The two main features in this sense are the decrease in beneficial bacteria and the increase in pathogenic bacteria. Significant differences were also present between remission and relapse IBD status. Shifts in gut microbial community composition and abundance have proven to be valuable as diagnostic biomarkers. The gut microbiome plays a major role in IBD, yet studies need to go from casualty to causality. Longitudinal designs including newly diagnosed treatment-na\u00efve patients are needed to provide insights into the role of microbes in the onset of intestinal inflammation. A better understanding of the human gut microbiome could provide innovative targets for diagnosis, prognosis, treatment and even cure of this relevant disease.", "title": "Systematic Review: The Gut Microbiome and Its Potential Clinical Application in Inflammatory Bowel Disease", "claims": null}, {"metadata": {"year": 2020}, "authors": ["M. Agnello", "L. Carroll", "N. Imam", "R. Pino", "C. Palmer", "Ignacio Varas", "C. Greene", "Maureen Hitschfeld", "Sarah Gupta", "D. Almonacid", "M. Hoaglin"], "summary": "Objective Irritable bowel syndrome (IBS) is a common gastrointestinal disorder that is difficult to diagnose and treat due to its inherent heterogeneity and unclear aetiology. Although there is evidence suggesting the importance of the microbiome in IBS, this association remains poorly defined. In the current study, we aimed to characterise a large cross-sectional cohort of patients with self-reported IBS in terms of microbiome composition, demographics, and risk factors. Design Individuals who had previously submitted a stool sample for 16S microbiome sequencing were sent a comprehensive survey regarding IBS diagnosis, demographics, health history, comorbidities, family history, and symptoms. Log ratio-transformed abundances of microbial taxa were compared between individuals reporting a diagnosis of IBS without any comorbidities and individuals reporting no health conditions. Univariable testing was followed by a multivariable logistic regression model controlling for relevant confounders. Results Out of 6386 respondents, 1692 reported a diagnosis of IBS without comorbidities and 1124 reported no health conditions. We identified 3 phyla, 15 genera, and 19 species as significantly associated with IBS after adjustment for confounding factors. Demographic risk factors include a family history of gut disorders and reported use of antibiotics in the last year. Conclusion The results of this study confirm important IBS risk factors in a large cohort and support a connection for microbiome compositional changes in IBS pathogenesis. The results also suggest clinical relevance in monitoring and investigating the microbiome in patients with IBS. Further, the exploratory models described here provide a foundation for future studies.", "title": "Gut microbiome composition and risk factors in a large cross-sectional IBS cohort", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Nuttachat Wisittipanit", "H. Rangwala", "P. Gillevet"], "summary": "The interaction and inter-play of microbes with human host cells is responsible for several disease conditions and of criticality to human health. In this study we analyze the microbial communities within the human gut and their roles in Inflammatory Bowel Disease (IBD). These microbial communities can be profiled using either Length Heterogeneity PCR (LH-PCR) or small subunit (SSU) rRNA sequences. Classification methods based on support vector machines (SVM) and k-nearest neighbor (KNN) were developed to differentiate between healthy controls and IBD patients at various intestinal locations using those profiles. The results show that there exist significant operational taxonomic units (OTUs) or microbial species that are differentially abundant between IBD and healthy control state at specific intestinal locations. Moreover, the classification performances of the sequence data outperform those of LH-PCR profiles and the lowest taxonomic level (Genus-Species) is more likely to have superior classification performances than the higher taxonomic levels.", "title": "Analysis of Microbiome Data across Inflammatory Bowel Disease Patients", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Valentina Taverniti", "S. Guglielmetti"], "summary": "Irritable bowel syndrome (IBS) is an intestinal functional disorder with the highest prevalence in the industrialized world. The intestinal microbiota (IM) plays a role in the pathogenesis of IBS and is not merely a consequence of this disorder. Previous research efforts have not revealed unequivocal microbiological signatures of IBS, and the experimental results are contradictory. The experimental methodologies adopted to investigate the complex intestinal ecosystem drastically impact the quality and significance of the results. Therefore, to consider the methodological aspects of the research on IM in IBS, we reviewed 29 relevant original research articles identified through a PubMed search using three combinations of keywords: \"irritable bowel syndrome + microflora\", \"irritable bowel syndrome + microbiota\" and \"irritable bowel syndrome + microbiome\". For each study, we reviewed the quality and significance of the scientific evidence obtained with respect to the experimental method adopted. The data obtained from each study were compared with all considered publications to identify potential inconsistencies and explain contradictory results. The analytical revision of the studies referenced in the present review has contributed to the identification of microbial groups whose relative abundance significantly alters IBS, suggesting that these microbial groups could be IM signatures for this syndrome. The identification of microbial biomarkers in the IM can be advantageous for the development of new diagnostic tools and novel therapeutic strategies for the treatment of different subtypes of IBS.", "title": "Methodological issues in the study of intestinal microbiota in irritable bowel syndrome.", "claims": null}], "query": "I need all research papers which used Shotgun sequencing to study gut microbiome of IBS patients in India", "summary_abstract": "The research question regarding the use of shotgun sequencing to study the gut microbiome of IBS patients in India is addressed in the literature, particularly through the work of Young et al. (2018). This study utilized shotgun metagenomic sequencing on fecal samples from individuals with functional gastrointestinal symptoms, including IBS, to explore the microbial and metabolomic factors associated with the disorder. The findings highlighted specific bacterial genera that were significantly associated with different IBS subtypes, such as Faecalibacterium and Blautia, suggesting that the microbiome's functional role, particularly in carbohydrate metabolism, is crucial in understanding IBS (Young et al., 2018).\n\nWhile other studies included in the abstracts provided insights into the gut microbiome and IBS, they did not specifically focus on the Indian population or employ shotgun sequencing. For instance, Liu et al. (2021) conducted a meta-analysis of existing 16S rRNA sequencing data to identify microbiome biomarkers for IBS but did not include Indian patients or shotgun sequencing in their methodology. Similarly, Pittayanon et al. (2019) and Agnello et al. (2020) reviewed various studies on the gut microbiome in IBS but did not specifically address the Indian context or the use of shotgun sequencing.\n\nIn summary, the only relevant study that directly answers the research question is by Young et al. (2018), which employed shotgun sequencing to analyze the gut microbiome of IBS patients in India, revealing significant microbial associations with different IBS subtypes. Other studies, while informative about IBS and the gut microbiome, do not meet the specific criteria of the question.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2023}, "authors": ["Yue-Ming Zhang", "Ru-Meng Wei", "Ming Ni", "Qi-Tao Wu", "Yun Li", "Y. Ge", "X. Kong", "Xue-Yan Li", "Guiping Chen"], "summary": "Growing evidence clearly demonstrates that maternal rodents exposure to sleep deprivation (SD) during late pregnancy impairs learning and memory in their offspring. Epigenetic mechanisms, particularly histone acetylation, are known to be involved in synaptic plasticity, learning, and memory. We hypothesize that the cognitive decline induced by SD during late pregnancy is associated with histone acetylation dysfunction, and this effect could be reversed by an enriched environment (EE).", "title": "An enriched environment improves maternal sleep deprivation\u2010induced cognitive deficits and synaptic plasticity via hippocampal histone acetylation", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Yan  Peng", "Wei  Wang", "Tao  Tan", "Wenting  He", "Zhifang  Dong", "Yu Tian Wang", "Huili  Han"], "summary": "BackgroundSleep deprivation during pregnancy is a serious public health problem as it can affect the health of pregnant women and newborns. However, it is not well studied whether sleep deprivation at different stages of pregnancy has similar effects on emotional and cognitive functions of the offspring, and if so, the potential cellular mechanisms also remain poorly understood.MethodsIn the present study, the pregnant rats were subjected to sleep deprivation for 6\u00a0h per day by gentle handling during the first (gestational days 1\u20137), second (gestational days 8\u201314) and third trimester (gestational days 15\u201321) of pregnancy, respectively. The emotional and cognitive functions as well as hippocampal long-term potentiation (LTP) were tested in the offspring rats (postnatal days 42-56).ResultsThe offspring displayed impaired hippocampal-dependent spatial learning and memory, and increased depressive- and anxiety-like behaviors. Quantification of BrdU-positive cells revealed that adult hippocampal neurogenesis was significantly reduced compared to control. Electrophysiological recording showed that maternal sleep deprivation impaired hippocampal CA1 LTP and reduced basal synaptic transmission, as reflected by a decrease in the frequency and amplitude of miniature excitatory postsynaptic current in the hippocampal CA1 pyramidal neurons.ConclusionsTaken together, these results suggest that maternal sleep deprivation at different stages of pregnancy disrupts the emotional and cognitive functions of the offspring that might be attributable to the suppression of hippocampal LTP and basal synaptic transmission.", "title": "Maternal sleep deprivation at different stages of pregnancy impairs the emotional and cognitive functions, and suppresses hippocampal long-term potentiation in the offspring rats", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Grace Violeta Espinoza Pardo", "J\u00e9ferson Ferraz Goularte", "Ana L\u00facia Hoefel", "Alexandre Luz de Castro", "Luiz Carlos Kucharski", "Alex Sander da Rosa Araujo", "Aldo Bolten Lucion"], "summary": "The present study aimed to analyze the effects of sleep restriction (SR) during pregnancy in rats. The following three groups were studied: home cage (HC pregnant females remained in their home cage), Sham (females were placed in tanks similar to the SR group but with sawdust) and SR (females were submitted to the multiple platform method for 20 h per day from gestational days (GD) 14 to 20). Plasma corticosterone after 6 days of SR was not different among the groups. However, the relative adrenal weight was higher in the SR group compared with the HC group, which suggests possible stress impact. SR during pregnancy reduces the body weight of the female but no changes in liver glycogen, cholesterol and triglycerides, and muscle glycogen were detected. On GD 20, the fetuses of the females submitted to SR exhibited increased brain derived neurotrophic factor (BDNF) in the hippocampus, which indicates that sleep restriction of mothers during the final week of gestation may affect neuronal growth factors in a fetal brain structure, in which active neurogenesis occurs during the deprivation period. However, no changes in the total reactive oxygen species (ROS) in the cortex, hippocampus, or cerebellum of the fetuses were detected. SR females showed no major change in the maternal behavior, and the pups' preference for the mother's odor on postpartum day (PPD) 7 was not altered. On GD 20, the SR females exhibited increased plasma prolactin (PRL) and oxytocin (OT) compared with the HC and Sham groups. The negative outcomes of sleep restriction during delivery could be related, in part, to this hormonal imbalance. Sleep restriction during pregnancy induces different changes compared with the changes described in males and affects both the mother and offspring.", "title": "Effects of sleep restriction during pregnancy on the mother and fetuses in rats", "claims": null}, {"metadata": {"year": 2023}, "authors": ["K. Gulia"], "summary": "Abstract We spend one-third of our lives in sleep, yet the core function of it still remains an enigma due to underlying complex neural processing in this altered state of consciousness. Sleep requirement varies with phase of development. Neonates spent about 85% of their time in sleep, which is polyphasic in nature. Gradually, this pattern takes the shape of a monophasic sleep in adolescents and adults, with changing micro- and macroarchitecture in every phase. Deprivation of sleep in adults impairs learning and memory, and reduces theta coherence among hippocampus and amygdale during sleep. However, sleep loss during pregnancy can affect the ontogenetic development of networks for sleep\u2013wakefulness and the cognitive development of offspring. Even in normal pregnancy, poor sleep quality, reduced rapid eye movement (REM) sleep, and sleep fragmentation are common observation during the last trimester of pregnancy. Delta power, a marker for the homeostatic drive for sleep, in the NREM sleep during the last trimester of pregnancy and postpartum is increased. However, further sleep loss during late pregnancy is a growing concern. Neonates that are born to the total sleep-restricted dams present significant alterations in their emotional development (symptoms of hyperactivity, increased risk-taking behavior during periadolescence) and immature sleep\u2013wakefulness patterns. The REM sleep restriction during late pregnancy elicits depressionlike traits in neonates, which persist until middle age. For a healthy development of brain and body, thorough understanding of the dynamic nature of sleep in relation to age and state (pregnancy) is instrumental in preventing the above-mentioned conditions of prenatal origin. Although sleep is essential for an active brain (for work during day), it remains an underestimated phenomenon. This review highlights the importance of sleep during pregnancy for a healthy brain network programming in offspring.", "title": "Effect of Sleep Restriction during Pregnancy on Fetal Brain Programming and Neurocognitive Development of Offspring: A Review", "claims": null}, {"metadata": {"year": 2016}, "authors": ["E. Hoekzema", "E. Barba-M\u00fcller", "C. Pozzobon", "M. Picado", "F. Lucco", "D. Garc\u00eda\u2010Garc\u00eda", "J. Soliva", "A. Tobe\u00f1a", "M. Desco", "E. Crone", "A. Ballesteros", "Susanna Carmona", "\u00d3. Vilarroya"], "summary": "\uf0b7 Abstract\u2022 \uf0b7 Introduction\u2022 \uf0b7 Results\u2022 \uf0b7 Discussion\u2022 \uf0b7 Methods\u2022 \uf0b7 References\u2022 \uf0b7 Acknowledgments\u2022 \uf0b7 Author information\u2022 \uf0b7 Supplementary information Pregnancy involves radical hormone surges and biological adaptations. However, the effects of pregnancy on the human brain are virtually unknown. Here we show, using a prospective ('pre''post' pregnancy) study involving first-time mothers and fathers and nulliparous control groups, that pregnancy renders substantial changes in brain structure, primarily reductions in gray matter (GM) volume in regions subserving social cognition. The changes were selective for the mothers and highly consistent, correctly classifying all women as having undergone pregnancy or not inbetween sessions. Interestingly, the volume reductions showed a substantial overlap with brain regions responding to the women's babies postpartum. Furthermore, the GM volume changes of pregnancy predicted measures of postpartum maternal attachment, suggestive of an adaptive process serving the transition into motherhood. Another follow-up session showed that the GM reductions endured for at least 2 years post-pregnancy. Our data provide the first evidence that pregnancy confers long-lasting changes in a woman's brain. Woman\u2019s Brain change to prepare for Baby 4 Subject terms: \uf0b7 Brain \uf0b7 Cognitive neuroscience \uf0b7 Neuroscience \uf0b7 Social neuroscience", "title": "Woman\u2019s Brain change to prepare for Baby", "claims": null}, {"metadata": {"year": 2015}, "authors": ["A. Wilkerson"], "summary": "New mothers often complain of impaired cognitive functioning, and it is well documented that women experience a significant increase in sleep disturbance after the birth of a child. Sleep disturbance has been linked to impaired cognitive performance in several populations, including commercial truck drivers, airline pilots, and medical residents, though this relationship has rarely been studied in postpartum women. In the present study 13 pregnant women and a group of 22 non-pregnant controls completed one week of actigraphy followed by a battery of neuropsychological tests and questionnaires in the last month of pregnancy (Time 1) and again at four weeks postpartum (Time 2). Pregnant women experienced significantly more objective and subjective sleep disturbance than the control group at both time points. They also demonstrated more impairment in objective, but not subjective cognitive functioning. Preliminary analyses indicated increased objective sleep fragmentation from Time 1 to Time 2 predicted decreased objective cognitive performance from Time 1 to Time 2, though small sample size limited the power of these findings. Implications for perinatal women and need for future research were discussed.", "title": "Cognitive Performance as a Function of Sleep Disturbance in the Postpartum Period", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Gabriel Natan Pires", "Monica Levy Andersen", "M\u00e1rcia  Giovenardi", "Sergio  Tufik"], "summary": "The modern living standard has imposed upon society a situation of chronic sleep deprivation. This chronic loss of sleep affects women more than it does men. As a result, the postponement of pregnancy has become a common choice due to the priority given to social and domestic activities. For women, pregnancy represents a condition of intense physical and physiological changes that subject the pregnant woman to a number of potentially stressful situations, ultimately interfering with their quality of sleep. Chronic sleep deprivation, along with the changes imposed on women through pregnancy, can lead to several harmful consequences for the pregnant woman and the child, and can potentially undermine the mother-infant relationship. This article discusses circumstances under which sleep deprivation and poor sleep quality during pregnancy could result in damage to the mother-infant relationship, specifically through maternal fatigue, postpartum depression and changes in pregnancy-related hormonal secretions and activity.", "title": "Sleep impairment during pregnancy: possible implications on mother-infant relationship.", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Jen Jen Chang", "Grace W Pien", "Stephen P Duntley", "George A Macones"], "summary": "Sleep duration in the population has been declining. Women occupy an increasingly prominent place in the work force without reducing most of their responsibilities at home. Consequently, sleep needs are often pushed to the bottom of women's daily priority list. Prior research has indicated that sleep deprivation is associated with higher levels of pro-inflammatory serum cytokines. This is important because higher plasma concentrations of pro-inflammatory serum cytokine levels are associated with postpartum depression and adverse birth outcomes such as preterm delivery. However, little research has directly examined how sleep deprivation may affect maternal and fetal outcomes. This review summarizes the existing data on the effect of sleep deprivation during pregnancy on maternal and fetal outcomes. We review supporting evidence for the hypotheses that sleep deprivation during pregnancy increases the risk of preterm delivery and postpartum depression, and that systemic inflammation is the causal mechanism in the association. Prior research on sleep in pregnancy has been limited by varying data collection methods, subjective self-reported sleep measures, small and non-representative samples, cross-sectional designs; descriptive or non-hypothesis driven studies. Future research with longitudinal study designs is needed to allow examination of the effect of sleep deprivation on adverse maternal and fetal outcomes.", "title": "Sleep deprivation during pregnancy and maternal and fetal outcomes: is there a relationship?", "claims": null}], "query": "how pregnancy reshape the mother brain to cope with sleep deprivation", "summary_abstract": "The research question regarding how pregnancy reshapes the mother\u2019s brain to cope with sleep deprivation is addressed through various studies that highlight the cognitive and emotional impacts of sleep deprivation during pregnancy, as well as the neurobiological adaptations that occur in the maternal brain.\n\nZhang et al. (2023) emphasize that sleep deprivation (SD) during late pregnancy can lead to cognitive decline in offspring, potentially linked to epigenetic changes such as histone acetylation dysfunction. This suggests that the mother\u2019s brain may undergo changes that affect not only her cognitive functions but also the developmental outcomes of her children.\n\nPeng et al. (2016) further explore the effects of sleep deprivation at different stages of pregnancy, revealing that it impairs emotional and cognitive functions in offspring, likely due to disruptions in hippocampal long-term potentiation (LTP) and neurogenesis. This indicates that the mother\u2019s experience of sleep deprivation can have lasting effects on her brain's ability to support both her own cognitive functions and those of her developing child.\n\nGulia (2023) discusses how sleep loss during pregnancy can alter the development of sleep-wake networks and cognitive functions in offspring. The study highlights that sleep fragmentation and poor sleep quality are common during pregnancy, particularly in the last trimester, which can lead to significant emotional and cognitive challenges for both mothers and their children.\n\nHoekzema et al. (2016) provide evidence that pregnancy induces substantial changes in the mother\u2019s brain structure, particularly in areas related to social cognition. These changes, characterized by reductions in gray matter volume, are thought to facilitate the transition into motherhood and may help mothers adapt to the demands of caring for a newborn, potentially compensating for the cognitive impairments associated with sleep deprivation.\n\nWilkerson (2015) notes that new mothers often experience significant sleep disturbances, which correlate with cognitive impairments. This study suggests that the cognitive challenges faced by mothers may be exacerbated by sleep fragmentation, indicating a need for further research into how these changes affect maternal functioning.\n\nIn summary, the collective findings from these studies suggest that pregnancy induces significant neurobiological adaptations in the mother\u2019s brain, which may help her cope with the cognitive and emotional challenges posed by sleep deprivation. These adaptations, while potentially beneficial for maternal attachment and caregiving, also highlight the risks associated with sleep loss during this critical period, affecting both maternal and offspring outcomes (Zhang et al., 2023; Peng et al., 2016; Gulia, 2023; Hoekzema et al., 2016; Wilkerson, 2015).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2014}, "authors": ["Daniel C. Moos", "D. Pitton"], "summary": "Cognitive load theory (CLT) can explain the challenges faced by student teachers. This study, guided by the CLT, included 26 pre-service teachers. Participants completed a cognitive load self-report questionnaire and were interviewed at two points during their student teaching. Results revealed that student teachers decreased mental effort related to monitoring their students\u2019 level of attention, meeting needs of diverse learners, and managing internal and external distractions. Qualitative analysis revealed: (1) student teachers became aware of limited cognitive resources; (2) lesson planning imposes cognitive load during student teaching; and (3) cognitive overload limits the ability to make modifications during teaching.", "title": "Student teacher challenges: using the cognitive load theory as an explanatory lens", "claims": null}, {"metadata": {"year": 2010}, "authors": ["\u00d6. Springer"], "summary": "Cognitive load is a theoretical notion with an increasingly central role in the educational research literature. The basic idea of cognitive load theory is that cognitive capacity in working memory is limited, so that if a learning task requires too much capacity, learning will be hampered. The recommended remedy is to design instructional systems that optimize the use of working memory capacity and avoid cognitive overload. Cognitive load theory has advanced educational research considerably and has been used to explain a large set of experimental findings. This article sets out to explore the open questions and the boundaries of cognitive load theory by identifying a number of prob lematic conceptual, methodological and application-related issues. It concludes by pre senting a research agenda for future studies of cognitive load.", "title": "Cognitive load theory, educational research, and instructional design: some food for thought", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Ton de Jong"], "summary": "Cognitive load is a theoretical notion with an increasingly central role in the educational research literature. The basic idea of cognitive load theory is that cognitive capacity in working memory is limited, so that if a learning task requires too much capacity, learning will be hampered. The recommended remedy is to design instructional systems that optimize the use of working memory capacity and avoid cognitive overload. Cognitive load theory has advanced educational research considerably and has been used to explain a large set of experimental findings. This article sets out to explore the open questions and the boundaries of cognitive load theory by identifying a number of problematic conceptual, methodological and application-related issues. It concludes by presenting a research agenda for future studies of cognitive load.", "title": "Cognitive load theory, educational research, and instructional design: some food for thought", "claims": null}, {"metadata": {"year": 2018}, "authors": [], "summary": "Cognitive load theory is supported by a significant number of randomised controlled trials (RCTs). This large body of evidence indicates that instruction is most effective when it is designed according to the limitations of working memory. Cognitive load theory indicates that when teaching students new content and skills, teachers are more effective when they provide explicit guidance accompanied by practice and feedback, not when they require students to discover for themselves many aspects of what they must learn.", "title": "Cognitive load theory: Research that teachers really need to understand", "claims": null}, {"metadata": {"year": 2018}, "authors": [], "summary": "Cognitive load theory is supported by a significant number of randomised controlled trials (RCTs). This large body of evidence indicates that instruction is most effective when it is designed according to the limitations of working memory. Cognitive load theory indicates that when teaching students new content and skills, teachers are more effective when they provide explicit guidance accompanied by practice and feedback, not when they require students to discover for themselves many aspects of what they must learn.", "title": "Cognitive load theory: Research that teachers really need to understand", "claims": null}, {"metadata": {"year": 2021}, "authors": ["M. Kennedy"], "summary": "There are numerous reasons why students with disabilities struggle in school. A key reason is professionals in the field may not pay enough attention to students\u2019 overwhelmed cognitive capacity. Cognitive load theory explains that all humans have limited capacity at any given time to use their auditory, visual, and tactile inputs (independently or collectively) to acquire new information and store it in long-term memory. When available cognition is overwhelmed \u2013 which can be caused by any number of reasons \u2013 learning cannot occur. In this article, we introduce the key aspects of cognitive load theory and give specific examples of how special educators can use this information to shape their instruction to support students\u2019 unique needs.", "title": "Cognitive Load Theory: An Applied Reintroduction for Special and General Educators", "claims": null}, {"metadata": {"year": 2016}, "authors": ["L. Longo"], "summary": "Cognitive Load Theory is an approach that considers the limitations of the information processing system of the human mind. It is a cognitivist theory that has been conceived in the context of instructional design. One of the main open problems in the literature is the lack of reliable models and technologies to assess cognitive load of learners, thus limiting the application of the theory in practice. This project was aimed at tackling this open problem through the use of a previously developed mobile, responsive web-based prototypical technology, to assess the cognitive load of students in a typical third-level classroom. It was also aimed at exploring the impact of such a technology to instructional design and the potential benefits it can bring to lecturers to improve teaching practices and optimally align their instructional materials to learners.", "title": "Informing Instructional Design by Cognitive Load Assessment in the Classroom.", "claims": null}, {"metadata": {"year": 2021}, "authors": ["T. K. Rao"], "summary": "The present education system has a lot of load in itself. As the education market increasing, the stakeholders of it are also looking to make the curriculum tougher and harder. The competitiveness is clearly visible in the content to be learnt rather than psychological satisfaction of the learning. It is often observed that the curriculum developers are keen to focus more on knowledge and skills in comparison to individual capabilities and pace of learning. When it comes to education system, the parents\u2019 expectations and society needs are far away from needs of the learner actually. Now the student has a big role to play to meet all those expectations. The students have to learn mathematics, they should speak English fluently, and they should get all the prizes in sports and so on. In the process, the learner may undergo a tremendous pressure for each activity he or she takes up. Somewhere, the cognitive fluctuations or cognitive conflict sets in mind and leading work very hard on. The learner has to process all the related incoming information in each and every area of subject he or she chooses, ultimately results in load on memory. As the amount of information to be processed heaps up, heavy cognitive load the outcome will be. Many past researches have been reviewed in this direction to come to solid conclusion.", "title": "Educational Underpinnings of Cognitive Load Theory", "claims": null}], "query": "cognitive load theory in prekindergarten education", "summary_abstract": "Cognitive Load Theory (CLT) plays a significant role in prekindergarten education by highlighting the limitations of working memory and the importance of instructional design that accommodates these limitations. The theory posits that when learning tasks exceed cognitive capacity, learning is hindered (\u00d6. Springer, 2010; Ton de Jong, 2010). This is particularly relevant in early education settings, where young learners are still developing their cognitive skills.\n\nResearch indicates that effective instruction for young children should be structured to minimize cognitive overload. For instance, explicit guidance, practice, and feedback are more beneficial than discovery-based learning approaches, especially when introducing new content and skills (2018). This is crucial in prekindergarten, where children are often faced with a variety of new concepts and skills that can overwhelm their cognitive resources.\n\nMoreover, the challenges faced by educators, such as managing diverse learners and distractions, can also be understood through the lens of CLT. Student teachers reported a decrease in mental effort related to monitoring student attention and meeting diverse needs, suggesting that awareness of cognitive load can help educators manage their instructional strategies more effectively (Daniel C. Moos & D. Pitton, 2014).\n\nAdditionally, the needs of students with disabilities highlight the importance of considering cognitive load in instructional design. When cognitive capacity is overwhelmed, learning cannot occur, underscoring the necessity for tailored instructional approaches that account for individual cognitive limitations (M. Kennedy, 2021).\n\nIn summary, the application of Cognitive Load Theory in prekindergarten education emphasizes the need for instructional strategies that align with the cognitive capacities of young learners. By optimizing the use of working memory and reducing cognitive overload, educators can enhance learning outcomes for all students, particularly those with unique needs.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2000}, "authors": ["Ian Stumpf"], "summary": "Medium\u2010sized regional building contractors in the UK are exhibiting poorer performance in the 1980s and the 1990s and are less likely to survive than their larger or smaller counterparts. The market structure of contracting appears to be changing, putting pressure on these intermediate firms. Evidence drawn from the Department of the Environment (DoE) statistical series shows industry composition is changing, in particular the gradual decline over time of the middle market. An analysis of company accounts for a sample of approximately 200 contractors shows that medium\u2010sized firms are also displaying inferior business ratios. Possible explanations are offered, including barriers to entry, such as capitalization, economies of scale (pecuniary and market), along with changes in construction demand.", "title": "Competitive pressures on middle\u2010market contractors in the UK", "claims": null}, {"metadata": {"year": 2008}, "authors": ["T. Brown"], "summary": "This paper explores the dynamics of government-to-government contracting at the local level in order to examine how governments' shared organizational characteristics, notably a governance structure based on political accountability, potentially make them more attractive vendors for services that risk contract failure. Relying on panel data from the 1992 and 1997 International City/County Manager Association's (ICMA) Alternative Service Delivery surveys along with data from the U. S. Census and other sources, this paper identifies service areas in which governments most frequently turn to government vendors. In particular, a comparison of public works and transportation services\u2014a service area with low risks of opportunism leading to contract failure\u2014and health and human services\u2014a service area with high risks of opportunism leading to contract failure\u2014shows that contracting governments are more likely to utilize governments over private firms and nonprofits for high-risk services. This is not the case for low-risk services, suggesting that governments view other governments as trusted contract vendors.", "title": "The Dynamics of Government-to-Government Contracts", "claims": null}, {"metadata": {"year": 2012}, "authors": ["J. Johnston", "Amanda M. Girth"], "summary": "Theory tells us that competition is the chief driver of improved efficiency and effectiveness in government contracting, yet contract provider markets are often noncompetitive. This study offers a detailed, contextualized examination of public administrative responses to thin contract markets. Following an inductive approach with data from semistructured interviews with contract administrators, the authors offer a preliminary typology of the conditions that give rise to thin markets, and the \u201cmarket management\u201d strategies used to create, enhance, and sustain competition in the markets from where governments purchase goods and services. The authors then review the efficacy and implications of these strategies for public services to citizens.", "title": "Government Contracts and \u201cManaging the Market\u201d", "claims": null}, {"metadata": {"year": 1991}, "authors": ["S. Macmanus"], "summary": "responses varied by type ofproduct or service being provided. Contracting out is one of the most popular forms of cost containment utilized by governments at all levels (David, 1988; Morley, 1989; Chi, 1988; Butler, 1985; MacManus, 1990a). Ironically, at the very time when governments are most actively and aggressively seeking more opportunities to rely on the private sector, news media accounts indicate that the business community has become more cautious about selling to government (Goldstein, 1989). It is estimated that less than 2 percent of all U.S. businesses sell to the federal government (Holtz, 1980: xii) in spite of the fact that the U.S. government spent $195 billion on goods, services, and research and development in 1988 alone. Although the proportion of businesses selling to state and local governments is probably somewhat higher, it is evident that businesses are skeptical about selling to government in spite of the size of the public-sector market. Yet for governments to get the most benefit from contracting out, genuine competition for government contracts must exist. If a significant proportion of the business community does not perceive that government procurement is competitive, or that the competition is fair and equitable and produces costs savings, then government procurement experts must devise strategies to broaden the vendor pools.", "title": "Why Businesses are Reluctant to Sell to Governments", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Pamela Healy", "Sang Won Sok", "Alejandro Ramirez"], "summary": "Abstract : In the environment of shrinking budgets, there is a trend toward competitive contracting. Research indicates that the government can achieve significant cost savings from competition among industry. This paper will determine how much cost savings could be achieved. It will also analyze numerous contract-related Government Accountability Office reports and provide a summary of the Better Buying Power initiatives. This paper will also identify circumstances that prohibit full and open competition and patterns where competition is most successful. Finally, this paper will provide recommendations to assist federal executives in maximizing competitive contracting and provide the knowledge needed help achieve mandates for improved efficiency and reduced costs.", "title": "The value of competitive contracting", "claims": null}, {"metadata": {"year": 1990}, "authors": ["R. H. DeHoog"], "summary": "Contracting for public services from public or private suppliers is now a common prescription to improve government efficiency. The competitive bidding model is usually viewed as the ideal contracting process. However, this article explains that two other approaches\u2014the negotiation model and the cooperation model-may be more appropriate under certain conditions. The primary factors that are likely to determine which of the three approaches is most suitable are (a) the characteristics of the external environment (especially the number of service suppliers), (b) the level of organizational resources (e.g., personnel, funds, time, and expertise), and (c) the degree of uncertainty about funding, future events, service technologies, and causal relationships between service outputs and desired outcomes. The main point is that there is no one best way to contract for services; rather, government units should adapt their contracting procedures to both internal external conditions to implement service contracting in an effective manner.", "title": "Competition, Negotiation, or Cooperation", "claims": null}, {"metadata": {"year": 1993}, "authors": ["S. F. Bovet"], "summary": "Adapting to a tight economy, recession and a client switch from retainer to project assignments, midsize firms around the country have developed new management structures. Taking on senior partners, stressing firm strengths, a team approach and revised employee reporting and motivation procedures are among restructuring moves firms with 10 to 30 practitioners have made in recent months. To cope with recessionary pressures of the past few years, top executives at \"midsize\" firms--those with 10 to 30 practitioners--have revamped management structures in a variety of creative ways. Innovations include making partial \"of counsel\" or full partnership arrangements with experienced independent counselors, who often share office staff and overhead expenses. Executives at leading midsize firms are refocusing their businesses to handle more projects. Some clients are rejecting retainer arrangements, while others are demanding much more service on a retainer basis. PRJ talked recently with chief operating officers of four midsize firms. Clients today are demanding better and more targeted service, all participants agreed. Clients want senior executives of a firm working on their accounts, they added. There is also much higher client demand for strategic counseling and risk assessment or issues management advice, these sources said. Firms are also zeroing in on specialties as a means of attracting new business, roundtable participants pointed out. Besides strategic planning and issues or risk management, coalition building, public affairs and marketing communications were listed as sought-after areas of expertise. In one case, a firm is developing a unique database service that can be marketed as a separate profit center in addition to its more traditional public relations services. The firms represented in this article illustrate a variety of structures in terms of national and international affiliations. A. Brown-Olmstead Associates (ABOA), Atlanta, is part of the international Shandwick group of companies. Agnew, Carter, McCarthy, Inc., Boston, is a stockholder in the Pinnacle Worldwide corporation of independent firms. DMB&B Public Relations in the Detroit area (Bloomfield Hills, MI) functions as an office of Manning, Selvage & Lee, which is owned by DMB&B, an advertising agency. Nuffer, Smith, Tucker, Inc., San Diego, is a Worldcom partner and Hill and Knowlton associate. In addition to reshaping managements, midsize firms have extensively revamped their employee motivation and reward systems, according to PRJ's executive panel. While the recession has kept salary increases low for most staff, CEOs look to share rewards via bonus payments, profit-sharing or group rewards, such as special luncheons or spot incentive payments. Changing reward systems Midsize firms today stress a team approach to account work, those at the roundtable agreed. They also stress rewards such as staff training and the ability of younger executives to work with senior practitioners on interesting projects. Job satisfaction is a key factor in retaining good workers, roundtable participants said. Employee empowerment can be fostered by flattening the bureaucracy, one executive said. Another firm has established a system of peer reviews to keep employee performance and satisfaction high. Roundtable talk about reward systems also led to a debate on how to find, keep and motivate qualified employees. Characteristics of a firm's own culture were considered important in recruiting and retaining good performers. Created hourglass structure Less than a year ago, the traditional pyramid structure at A. Brown-Olmstead Associates (ABOA) was scrapped in favor of \"an hourglass shape,\" according to CEO and President Amanda Brown-Olmstead. Brown-Olmstead founded the firm 20 years ago as a traditional pyramid, with a CEO, several senior people reporting to her, and account service and support staff in layers below that. \u2026", "title": "PRJ Roundtable Report: Midsize Firms Take New Shapes to Suit Service, Profit Goals", "claims": null}, {"metadata": {"year": 1995}, "authors": ["Karen D. Sorber", "R. Straight"], "summary": "INTRODUCTION Acquiring supplies and services is one function common to most government organizations.(1) Generally, customers submit their requirements to a designated contracting office, which then follows a set of complex rules and regulations while taking action to execute a contract eventually to satisfy the requirements. This process is often frustrating to customers who have no choice but to accept whatever quality of service is provided by their designated (i.e., monopoly) contracting office. Even the people who are doing the contracting have themselves expressed frustration with the system. According to Kelman (1990:10), 29 percent of those providing he service said that the process takes too long. In spite of these problems, contracting offices in the public sector are routinely being asked to take on more and more responsibilities. Today there is much emphasis on contracting out services that government organizations have traditionally performed. According to Osborne and Gaebler (1992:87), procurement of services is challenging primarily because of the difficulty in writing and monitoring contracts for services. Despite the difficulty, Osborne and Gaebler found that customers have tended to be highly satisfied with contracted services (Ibid., 89). The authors believe that establishing competitive contracting offices (CCOs) that succeed or fail on their own performance will reduce much of the frustration associated with the federal government's contracting process. Adopting the CCO concept can lead to reductions in costs and improvements in performance--key goals for the current administration. OVERVIEW OF CCOs The authors presented the concept of CCOs at the 1989 Acquisition Research Symposium (Straight and Sorber, 1989:287). Under that concept, project officers (customers) would have a free hand in selecting any CCO to prepare and award their contracts. Customers would pay the CCO for its services and the chief of each office would be responsible for its financial \"bottom line.\" The project officer's freedom to select the best CCO, and the resultant financial pressure on CCOs to provide timely and effective support, should naturally lead to a more effective and efficient contracting process. In the 1989 paper, the authors discussed the advantages to the customer when dealing in a competitive environment rather than with a monopoly, given the latter's tendency toward inadequate service, inflexibility, and indifference to customer requirements. An important feature of the CCO concept is the provision of financial awards to managers and employees when the organization exceeds its financial operational goals (i.e., revenues exceed expenses). In conjunction with the pressure to compete for contracting business, those awards should motivate both managers and employees to develop a keener customer focus. IMPLEMENTATION ISSUES In a subsequent paper, presented at the 1991 symposium, the authors examined several implementation issues associated with CCO establishment, many of which focused on customer satisfaction (Straight and Dean, 1991:171). Drawing upon the results of a survey of people in the acquisition community, the authors concluded that the proposed CCO system should result in greater customer satisfaction. However, many respondents were concerned that contracting personnel would face a conflict of interest in that they might be tempted to satisfy the customer at the expense of regulatory and legal compliance. PROFESSIONALS' ETHICS Several thoughts on the morality of contracting personnel are offered. First, and most important, contracting personnel are held to a high standard of ethical behavior. They routinely have to disclose financial interests, certify compliance with various procurement integrity laws and regulations, and receive ethics training. When scandals do occur, senior government officials, inspectors general, and others scrutinizing the procurement process generally find that, with the exception of the bad apples involved in the scandal, the procurement workforce is highly ethical. \u2026", "title": "COMPETING CONTRACTING OFFICES: WORKING BETTER, COSTING LESS", "claims": null}], "query": "What are the competitive dynamics of a mid sized company in government contracting", "summary_abstract": "The competitive dynamics of mid-sized companies in government contracting reveal several challenges and strategies that these firms face in a complex and evolving market. Stumpf (2000) highlights that medium-sized contractors in the UK have been struggling to maintain performance levels compared to their larger and smaller counterparts, primarily due to a changing market structure that favors economies of scale and poses barriers to entry. This trend suggests that mid-sized firms may find it increasingly difficult to compete effectively in government contracting environments.\n\nBrown (2008) discusses the preference of governments to contract with other governmental entities for high-risk services, indicating that mid-sized companies may need to position themselves strategically to compete against these trusted vendors. This reliance on government vendors for certain services can limit opportunities for mid-sized firms, particularly in sectors where they might traditionally excel.\n\nJohnston and Girth (2012) provide insight into the non-competitive nature of many contract provider markets, emphasizing the need for public administrators to implement market management strategies to foster competition. This suggests that mid-sized firms must navigate not only the competitive landscape but also the administrative frameworks that govern contracting processes.\n\nMacmanus (1991) notes a general skepticism among businesses regarding the competitiveness of government procurement, which could further hinder mid-sized firms from entering or succeeding in this market. The perception of a non-competitive environment may deter potential vendors from engaging with government contracts, thereby limiting the pool of competitors.\n\nHealy, Sok, and Ramirez (2014) emphasize the importance of competitive contracting as a means for governments to achieve cost savings. They argue that mid-sized firms could benefit from understanding the patterns of successful competition and the circumstances that allow for full and open competition, which could enhance their chances of securing contracts.\n\nDeHoog (1990) suggests that mid-sized firms should adapt their contracting approaches based on external conditions and organizational resources. This adaptability is crucial for navigating the complexities of government contracting, where the ideal contracting model may vary depending on the specific context.\n\nLastly, Bovet (1993) illustrates how mid-sized firms have restructured their management and operational strategies in response to economic pressures, indicating a proactive approach to enhancing their competitiveness. By focusing on specialties and improving service delivery, these firms can better position themselves in the government contracting arena.\n\nIn summary, mid-sized companies in government contracting face significant competitive dynamics influenced by market structure, government preferences, and perceptions of procurement competitiveness. To thrive, these firms must adapt their strategies, enhance their service offerings, and navigate the complexities of the contracting environment effectively.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2006}, "authors": ["R. Lam"], "summary": "Generalized anxiety disorder (GAD) is a common, chronic and disabling anxiety disorder with considerable comorbidity with depression as well as with other anxiety disorders. Although tricyclic antidepressants and benzodiazepines have been found to be efficacious in patients with GAD, tolerability problems and other risks limit their use in clinical practice. In placebo-controlled, acute (<8\u2009weeks) trials, several medications, including the selective serotonin reuptake inhibitors ([SSRIs] escitalopram, paroxetine, and sertraline) and others (venlafaxine, buspirone, pregabalin), have demonstrated efficacy in patients with GAD. Indeed, current guidelines for the treatment of GAD recommend SSRIs as first-line pharmacological therapy because of their efficacy and tolerability profiles. Although GAD is a chronic condition that is usually present for years, with symptoms typically fluctuating in intensity over time, there have been few randomized, controlled trials of pharmacotherapy beyond the acute phase of treatment. However, data from recent relapse-prevention studies and longer-term maintenance studies with paroxetine, venlafaxine and escitalopram strongly support the value of continued treatment for at least a further 6 months. This article focuses on pharmacological treatment, and reviews recently available data from acute, long-term and relapse-prevention trials in patients with GAD. In addition, issues relating to the natural course of GAD are highlighted as important considerations to guide selection of pharmacotherapy.", "title": "Generalized anxiety disorder: how to treat, and for how long?", "claims": null}, {"metadata": {"year": 2004}, "authors": ["Fr\u00e9d\u00e9ric  Rouillon"], "summary": "Abstract Generalized anxiety disorder (GAD) is a common (lifetime prevalence: 5.1%), recurrent condition, which often heralds other psychiatric disorders, notably depression. As by definition it is a disorder progressing over months, treatment should be designed on a long term basis. And yet, few studies have been conducted beyond the classical 6\u20138 weeks characterizing the acute treatment phase. This is especially true of anxiolytics, but also of antidepressants, with the exception of paroxetine and venlafaxine, which are the only drugs approved in this indication in Western countries. The efficacy of psychotherapy, notably relaxation and cognitive-behavioral therapy, is established in the treatment of GAD, but its preferred indications and possible combination with antidepressants are still to be specified. Long term, not to say very long term studies of GAD, as well as depression, will still be required in the future to improve its management and specify therapeutic modalities (combination treatment, optimal duration, continuous or intermittent therapy, choice of psychotherapeutic techniques or agents, \u2026). Early and adequately prolonged treatment should not only result in more numerous remission periods, but also in decreased frequency of co-morbidities whether depressive, addictive, or of another nature, and should also reduce the social impact of GAD.", "title": "Long term therapy of generalized anxiety disorder", "claims": null}, {"metadata": {"year": 2015}, "authors": ["M. Stein", "J. Sareen"], "summary": "Key Clinical PointsGeneralized Anxiety Disorder Generalized anxiety disorder is characterized by persistent anxiety and uncontrollable worry that occurs consistently for at least 6 months. This disorder is commonly associated with depression, alcohol and substance abuse, physical health problems, or all these factors. In primary care, patients with this disorder often present with physical symptoms such as headaches, muscle tension, gastrointestinal symptoms, back pain, and insomnia. Brief validated screening tools such as the Generalized Anxiety Disorder 7 (GAD-7) scale should be used to assess the severity of symptoms and response to treatment. First-line treatments for generalized anxiety disorder are cognitive behavioral therapy, pharmacotherapy with a selective serotonin-reuptake inhibitor (SSRI) or a serotonin\u2013norepinephrine reuptake inhibitor (SNRI), or cognitive behavioral therapy in conjunction with either an SSRI or an SNRI. Pregabalin and buspirone are suitable second-line or adjunctive medicat...", "title": "CLINICAL PRACTICE. Generalized Anxiety Disorder.", "claims": null}, {"metadata": {"year": 2000}, "authors": ["V. Mah\u00e9", "A. Balogh"], "summary": "&NA; Generalized anxiety disorder (GAD) is one of the most common anxiety disorders and has a poor prognosis, although it is often thought to be a minor complaint. This disorder has a chronic course of 5\u201015 years and longer. Long\u2010term treatment with the commonly used benzodiazepines is controversial because of concerns over tolerance and dependence. We performed a thorough search of the literature for clinical trials of a duration of over 2 months conducted in patients with generalized anxiety disorder in order to identify any successful long\u2010term treatment of this disorder. Only eight long\u2010term reports of studies conducted in well\u2010defined homogeneous groups of patients diagnosed with generalized anxiety disorder were found with the methodology of these studies presenting a number of limiting factors. The results are inconclusive and no reference drug could be identified. In addition, an adequate evaluation of the long\u2010term treatment of GAD has not yet been performed.", "title": "Long\u2010term pharmacological treatment of generalized anxiety disorder", "claims": null}, {"metadata": {"year": 1987}, "authors": ["G. Butler", "A. Cullington", "G. Hibbert", "I. Klimes", "M. Gelder"], "summary": "A preliminary controlled investigation of the effectiveness of Anxiety Management as a treatment for generalised anxiety disorder (GAD) is described. Patients with a primary diagnosis of GAD, in which the current episode had lasted for at least 6 months but not more than 2 years, were included. Anxiety Management, a self-help treatment including procedures for managing somatic and cognitive symptoms, and for dealing with avoidance and low self-confidence, was given either immediately or after a 12-week waiting period. The average length of treatment was 8.7 sessions. Highly significant changes in anxiety, depression, and problem ratings were shown after treatment. These changes were replicated when the waiting list group had also received treatment, and gains were maintained by both groups for 6 months. Similar degrees of improvement and maintenance of change were shown in subgroups with and without minor depressive disorder or recurrent panic attacks.", "title": "Anxiety Management for Persistent Generalised Anxiety", "claims": null}, {"metadata": {"year": 2000}, "authors": ["M F Gliatto"], "summary": "Patients with generalized anxiety disorder experience worry or anxiety and a number of physical and psychologic symptoms. The disorder is frequently difficult to diagnose because of the variety of presentations and the common occurrence of comorbid medical or psychiatric conditions. The lifetime prevalence is approximately 4 to 6 percent in the general population and is more common in women than in men. It is often chronic, and patients with this disorder are more likely to be seen by family physicians than by psychiatrists. Treatment consists of pharmacotherapy and various forms of psychotherapy. The benzodiazepines are used for short-term treatment, but because of the frequently chronic nature of generalized anxiety disorder, they may need to be continued for months to years. Buspirone and antidepressants are also used for the pharmacologic management of patients with generalized anxiety disorder. Patients must receive an appropriate pharmacologic trial with dosage titrated to optimal levels as judged by the control of symptoms and the tolerance of side effects. Psychiatric consultation should be considered for patients who do not respond to an appropriate trial of pharmacotherapy.", "title": "Generalized anxiety disorder.", "claims": null}, {"metadata": {"year": 2000}, "authors": ["M. F. Gliatto"], "summary": "Patients with generalized anxiety disorder experience worry or anxiety and a number of physical and psychologic symptoms. The disorder is frequently difficult to diagnose because of the variety of presentations and the common occurrence of comorbid medical or psychiatric conditions. The lifetime prevalence is approximately 4 to 6 percent in the general population and is more common in women than in men. It is often chronic, and patients with this disorder are more likely to be seen by family physicians than by psychiatrists. Treatment consists of pharmacotherapy and various forms of psychotherapy. The benzodiazepines are used for short-term treatment, but because of the frequently chronic nature of generalized anxiety disorder, they may need to be continued for months to years. Buspirone and antidepressants are also used for the pharmacologic management of patients with generalized anxiety disorder. Patients must receive an appropriate pharmacologic trial with dosage titrated to optimal levels as judged by the control of symptoms and the tolerance of side effects. Psychiatric consultation should be considered for patients who do not respond to an appropriate trial of pharmacotherapy.", "title": "Generalized anxiety disorder.", "claims": null}, {"metadata": {"year": 2003}, "authors": ["C. Allgulander", "B. Bandelow", "E. Hollander", "S. Montgomery", "D. Nutt", "A. Okasha", "M. Pollack", "Dan J Stein", "R. Swinson"], "summary": "ABSTRACT What are the current recommendations for the long-term treatment of generalized anxiety disorder (GAD)? GAD is a common disorder with a lifetime prevalence of 4% to 7% in the general population. GAD is characterized by excessive, uncontrollable worry or anxiety about a number of events or activities that the individual experiences on more days than not over a 6-month period. Onset of GAD symptoms usually occurs during an individual's early twenties; however, high rates of GAD have also been seen in children and adolescents. The clinical course of GAD is often chronic, with 40% of patients reporting illness lasting >5 years. GAD is associated with pronounced functional impairment, resulting in decreased vocational function and reduced quality of life. Patients with GAD tend to be high users of outpatient medical care, which contributes significantly to healtcare costs. Currently, benzodiazepines and buspirone are prescribed frequently to treat GAD. Although both show efficacy in acute treatment trials, few long-term studies have been perform Benzodiazepines are not recommended for long-term treatment of GAD, due to associated development of tolerance, psychomotor impairment, cognitive and memory changes, physical dependence, and a withdrawal reaction on discontinuation. The antidepressant venlafaxine extended-release (XR) has received approval for the treatment of GAD in the United States and many other countries. Venlafaxine XR has demonstrated efficacy over placebo in two randomized treatment trials of 6 months' duration as well as in other acute trials. Paroxetine is the first of the selective serotonin reuptake inhibitors (SSRIs) to receive US approval for the treatment of GAD. Paroxetine demonstrated superiority to placebo in short-term trials, and investigations into the use of other SSRIs are ongoing. This suggests that other SSRIs, and serotonin and noradrenaline reuptake inhibitors, are likely to be effective in the treatment of GAD. Of the psychological therapies, cognitive-behavioral therapy (CBT) shows the greatest benefit in treating GAD patients. Treatment gains after a 12-week course of CBT may be maintained for up to 1 year. Currently, no guidelines exist for the long-term treatment of GAD.", "title": "WCA Recommendations for the Long-Term Treatment of Generalized Anxiety Disorder", "claims": null}], "query": "how long a patient with generalized anxiety disorder should take medication for", "summary_abstract": "The question of how long a patient with generalized anxiety disorder (GAD) should take medication is addressed in several studies, highlighting the chronic nature of the disorder and the need for extended treatment. GAD is characterized by persistent anxiety and worry lasting for at least six months, and it often coexists with other psychiatric conditions, such as depression (Stein & Sareen, 2015). \n\nCurrent guidelines recommend selective serotonin reuptake inhibitors (SSRIs) as first-line pharmacological treatments due to their efficacy and tolerability (Lam, 2006). While acute treatment trials typically last less than eight weeks, evidence suggests that continued treatment is beneficial. For instance, Lam (2006) emphasizes that maintenance therapy with SSRIs like paroxetine and venlafaxine should extend for at least six months to prevent relapse. Rouillon (2004) also notes that GAD is a recurrent condition that necessitates long-term treatment strategies, although there is a lack of extensive studies beyond the initial treatment phase.\n\nMoreover, the chronicity of GAD, which can last from five to fifteen years or longer, indicates that patients may require medication for months to years (Mah\u00e9 & Balogh, 2000). While benzodiazepines are often used for short-term relief, their long-term use is controversial due to risks of tolerance and dependence (Allgulander et al., 2003). In contrast, SSRIs and serotonin-norepinephrine reuptake inhibitors (SNRIs) are more suitable for long-term management.\n\nIn summary, the consensus among the literature is that patients with GAD should consider long-term pharmacotherapy, particularly with SSRIs or SNRIs, for at least six months, and potentially for years, depending on individual circumstances and treatment response (Lam, 2006; Rouillon, 2004; Stein & Sareen, 2015).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2011}, "authors": ["W. Leap"], "summary": "Gay men\u2019s stories about homophobic violence will, at times, disguise reference to the persons who initiate harassment and injury. Instead, these stories emphasize the details of location or allow the victim of narrated violence to confirm the status of narrative hero. Each approach to story-telling has noticeable effects on the connections between sexual sameness and violence as displayed within the narrative. This paper examines these approaches to story-telling and their effects on homophobic formation (Leap, introduction, this volume), and traces the broader implications of these narrative styles within contemporary politics of gay visibility, assimilation and whiteness.", "title": "Homophobia as Moral Geography", "claims": null}, {"metadata": {"year": 1996}, "authors": ["Debbie  Epstein"], "summary": "Since September 1991 I have been engaged in a research project in which we set out to explore the experiences of lesbian and gay students, teachers, and parents in relation to the English system of education.2 In the course of the project, we have interviewed or held group discussions with some 30 lesbians and gay men, as well as carrying out ethnographic work in four schools and in a lesbian and gay youth group.3 While the majority of our respondents have been white, we have also spoken with lesbians and gays of African and of South Asian descent.4 My own contribution to the field work for this project has been that of interviewing many of our lesbian and gay respondents and participant observation in one of the schools. Doing the research, I have been struck forcibly by the various forms of harassment experienced by our female respondents as well as by those men and boys who identified as and/or were perceived as gay or effeminate by their peers and/or teachers. During the same period, and partly because of the research findings, I have found myself reflecting on my own experiences of harassment and those of my students in both the recent and more distant past. This chapter is, in large part, the result of these reflections and of discussions I have had with students, colleagues, and friends about the issues involved.5", "title": "Keeping them in their Place: Hetero/sexist Harassment, Gender and the Enforcement of Heterosexuality", "claims": null}, {"metadata": {"year": 2018}, "authors": ["B. M. Dank", "R. Refinetti"], "summary": "\"Sexuality and Culture\" serves as a compelling forum for the analysis of ethical, cultural, psychological, social, and political issues related to sexual relationships and sexual behavior. These issues include, but are not limited to: sexual consent and sexual responsibility; sexual harassment and freedom of speech and association; sexual privacy; censorship and pornography; impact of film/literature on sexual relationships; and university and governmental regulation of intimate relationships. The central theme of this volume is the politics of sexuality. Theoretical essays, research reports, and book reviews examine the topics of sexual harassment law as a sexual control mechanism, censorship of sexual materials, and criminalization of commercialized sexuality. A special section focuses on the Clinton-Lewinsky affair with contributions by David Steinberg, John Furedy, and Joseph Fulda. Other articles include: \"Trends Towards Increased Sexual Repression in the Final Two Decades of the Twentieth Century\" by Elizabeth Allgeier; \"Naked but Unseen: Sex and labor conflict in San Francisco's Adult Entertainment Theaters\" by Kerwin Kay; \"A test of the Biopolitics Hypothesis\" by Kenneth Westhues; \"Scientific and Fictive Sociology: The Viability of Research\" by Edwina Taborsky and Reena Sommer; and \"Sex Entertainment for Women on the Web\" by Marjorie Kibby. Also included are reviews of books, including \"Faculty-Student Sexual Involvement: Issues and Interventions, \" by Virginia Stamler and Gerald Stone; \"Heterophobia: Sexual Harassment and the Future of Feminism,\" by Daphne Patai; \"Sex among Allies: Military Prostitution in US-Korea Relations, \"by Katharine H. Moon; and \"American Homo\" by Jeffrey Escoffier. \"The Politics of Sexuality\" will be of interest to general readers as well as to scholars (sociologists, psychologists, legal analysts), policymakers, and members of the sex work and sex entertainment communities.", "title": "The Politics of Sexuality", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Marielle Roy"], "summary": "The harassment of sexual minority students in school has recently become a topic of interest to researchers interested in issues of school safety. The Preventing School Harassment (PSH) survey, which specifically addresses harassment based on sexual orientation, is an important part of this line of inquiry. The present analysis is based on a free-response question included in the 2008 version of the PSH survey, which addressed students\u2019 experiences with the inclusion of sexual minority issues in the classroom. Students\u2019 responses were analyzed to identify the major themes in both the content and context of what students learned in school about these issues. Whereas one-quarter of respondents indicated they learned nothing about sexual minority issues in school, the majority of students who learned about these issues reported learning about ideals of acceptance and equality. Respondents reported learning about LGBTQ issues most often outside of the classroom, and most commonly from personal experiences or from members of student clubs. These results provide insight into how students learn about these issues in school, and may inform future research into methods of increasing students\u2019 feelings of safety at school. Exploring the role of LGBTQ issues 4 Introduction \u201cThere\u2019s nothing wrong with being LGBTQ, it's just the way you are. They're people and equal too... Although that also feels obvious.\u201d \u201cPeople are just uninformed about the GLBT community. I strongly believe that ignorance is the basis of unacceptance and that if people knew what we were and that we are normal, they would not be afraid of us, because that is what they are.\u201d \u2014Student respondents to the 2008 Preventing School Harassment survey As the quotes above demonstrate, today\u2019s high school students have strong opinions about discrimination against people who identify as lesbian, gay, bisexual, transgender, or queer/questioning (LGBTQ). However, the experiences of sexual minority students in schools have only very recently become a topic of research, through the use of both widespread surveys and smaller studies. The findings of the research on this issue are disheartening: harassment based on anti-LGBTQ bias is both intense and pervasive, and sexual minority students often feel unsafe in school (Kosciw, Diaz, & Greytak, 2008; O\u2019Shaughnessy, Russell, Heck, Calhoun, & Laub, 2004). One of the most significant findings of these studies is that anti-LGBTQ harassment is qualitatively different from other types of harassment. In particular, antiLGBTQ slurs are damaging both to sexual minority and heterosexual students. For example, male respondents to a survey of middleand high-schoolers have reported that homophobic slurs are significantly more threatening to a student\u2019s reputation than other types of slurs (Thurlow, 2001). Additionally, respondents in a study of 107 New Zealand high schools revealed pervasive anti-LGBTQ sentiment: only 5% of students and 8% of staff believed that LGBTQ students would feel safe at their school. Content analysis of the participants\u2019 responses revealed two main themes: descriptions of harassment Exploring the role of LGBTQ issues 5 (predominantly verbal) and perceived invisibility (for example, \u201cI don\u2019t know any lesbian/gay/bisexual students\u201d) (Nairn & Smith, 2003). The significance of homophobic slurs in sexual harassment is further highlighted in a study performed by the American Association of University Women (AAUW). Equal numbers (approximately 75%) of respondents to a national survey of American high school students reported that they would be \u201cvery upset\u201d if someone \u201cpulled off or down their clothing\u201d or called them gay or lesbian (American Association of University Women [AAUW] Educational Foundation, 2001). The fact that being called gay or lesbian is as upsetting as being the victim of physical sexual harassment is astounding. Additionally, this particular example of harassment (being called gay or lesbian) was the only example rated by both boys and girls to be of equal significance: 74% of boys and 73% of girls reported that they would be \u201cvery upset\u201d by such an incident (AAUW, 2001). Anti-LGBTQ harassment is both powerful and pervasive. The latest version of the National School Climate Survey (NSCS), distributed by the Gay, Lesbian and Straight Education Network (GLSEN), was released in 2007 and reached approximately 6200 sexual minority students across the United States. Of the survey respondents, 86.2% had been verbally harassed and nearly half (44%) physically harassed because of their sexual orientation. Nearly a third of respondents had missed school at least once because they had felt unsafe (Kosciw et al., 2008). In comparison, under 10% of respondents to the California Healthy Kids Survey (CHKS), a survey representative of all California high school students, reported ever missing school because they felt unsafe (California Safe and Healthy Kids Program Office, 2008). Exploring the role of LGBTQ issues 6 Repeated harassment based on sexual orientation affects students\u2019 mental health, performance in school, and future educational and social development. Eight percent of all respondents (both heterosexual and sexual minority students) to the 2001-2002 CHKS had been the victims of harassment based upon their \u201cactual or perceived sexual orientation.\u201d The students harassed on the basis of anti-LGBTQ bias were over twice as likely as non-harassed students to report depressive symptoms (55% compared to 23%), and three times as likely to have seriously considered suicide (45% compared to 14%). In addition, victims of this type of harassment were more likely than non-harassed students to report having engaged in substance abuse, including tobacco and marijuana use, binge drinking, and use of other illegal drugs (O\u2019Shaughnessy et al., 2004). Victims of anti-LGBTQ harassment report lower grades than non-harassed students: 24% of harassed students earned Cs or lower, compared to 17% of students who were not harassed (O\u2019Shaughnessy et al., 2004). Twelve percent of sexual minority students had no plans for future education beyond high school, nearly twice the percentage of the national student body (Kosciw et al., 2008). One might go so far as to say that students who experience these effects as a direct result of harassment at school are being denied an education. These are serious and complicated problems, but multiple studies have identified several key actions that schools can make that produce strong, pervasive improvements. These changes include adopting an anti-harassment policy that explicitly includes sexual minority students; active involvement of teachers in stopping acts of harassment; the presence of a Gay-Straight alliance on campus; and inclusion of LGBTQ-related issues in regular classroom curricula (O\u2019Shaughnessy et al., 2004). Exploring the role of LGBTQ issues 7 The first two actions are important in that they provide a direct and obvious signal to the student body that staff and administration are committed to preventing harassment on their campus. A school\u2019s responsibility to protect all students is included in district, state and national policies. Including sexual minorities in an anti-harassment policy does not increase a school\u2019s liability, but simply emphasizes its pre-existing responsibility to prevent harassment based on sexual orientation. Including sexual minority students in the anti-harassment policy is a first step towards ending harassment in a school: \u201cA comprehensive policy that is publicized and implemented will clarify for all staff that such behavior is unacceptable and must be responded to\u201d (GLSEN, 2002). Once such a policy is implemented, teachers must take an active role in changing their school climate by addressing slurs and other forms of harassment whenever they occur. Inclusive antiharassment policies provide incentive but not instruction, and so teachers should be trained in ways to address teasing, name-calling, and more serious forms of harassment. When teachers are prepared to handle situations of harassment, these situations can be used as \u201cteachable moments\u201d for the students involved. The formation of Gay-Straight Alliances (GSAs) as student-run clubs has also been related with improvements in school environment. The CHKS showed that 75% of students with a GSA on their campus reported feeling safe at school, compared to 61% of students without a GSA at their school. The presence of a GSA was also correlated with decreased anti-LGBTQ harassment and an increase in students\u2019 perceived support from both school staff and other adults (O\u2019Shaughnessy et al., 2004). Respondents to the NSCS reported similar effects: students at schools with GSAs heard fewer homophobic Exploring the role of LGBTQ issues 8 remarks, felt more a part of their school community, and reported acts of harassment to school administrators more often (Kosciw et al., 2008). These positive effects are most likely due to a combination of factors, rather than the simple fact of the club\u2019s formation. Gay-Straight Alliances may be more likely to form in schools that have already established more positive environments; indeed, the formation of GSAs is often a difficult process in schools with pervasive anti-LGBTQ bias. Opposition to GSAs can often be traced to a misunderstanding of their purpose. However, the Gay-Straight Alliance Network provides a description of the true goals of high school GSAs: \u201c[to] create safe environments in schools for students to support each other and learn about homophobia and other oppressions, educate the school community about homophobia, gender identity, and sexual orientation issues, and fight discrimination, harassment, and violence in schools\u201d (GSA Network). The positive effects of GSAs, as described above, are the strongest possible testament to these goals. Finally, schools that present LGBTQ issues as part of their curriculum evidence the greatest increase in the perc", "title": "Exploring the Role of LGBTQ Issues in the Classrooms of California Schools", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Stephen Tomsen"], "summary": "This chapter examines research evidence to argue that violence directed at gay men, lesbians and transsexual/transgendered people are not wholly distinct from other forms of male-perpetrated violence. It insists that harassment and violence directed against sexual groups are highly gendered and everyday phenomena and narrow views of homophobic prejudice should be refined in order to appreciate this. Furthermore, these acts are widespread and collective social phenomena built on masculine understandings of a sexual mainstream and subordinate others.", "title": "Homophobic violence and masculinities in Australia", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Stephen  Tomsen", "Kevin  Markwell"], "summary": "Prejudice and violence directed against gay men, lesbians and other sexual groups have been viewed as ubiquitous and relatively fixed phenomena in contemporary societies. This perspective must be reconciled with the increased depiction of marginal sexualities and commercial \u2018queering\u2019 of mainstream media and popular culture. This paper presents and discusses data from two sources. Firstly, interviews conducted with self-identifying heterosexuals at the annual Sydney Gay and Lesbian Mardi Gras (SGLMG) parade suggest attendance and participation can occur through a widely enjoyed public display and the temporary suspension of sexual prejudice in such specific carnivalesque occasions. Secondly, gay and lesbian responses to an internet-based questionnaire concerning perceptions and experiences of safety and hostility at this and similar other public events, suggest an undercurrent of threat and incivility, especially in the post-event context. These data sources are not directly compared but analysed in a complementary way to throw new light on how different groups view and experience this event. Our findings reflect how sexual prejudice is a shifting and contradictory collective social practice.", "title": "Violence, Cultural Display and the Suspension of Sexual Prejudice", "claims": null}, {"metadata": {"year": 2002}, "authors": ["S. Fineran"], "summary": "This article provides a historical and legal framework for defining peer sexual harassment from three different perspectives: sex discrimination, mental health, and sexual violence. Major court decisions that define sexual harassment in both education and the workplace are highlighted, and arguments regarding sexual harassment between peers of the same sex are profiled. This research also identifies sexism and heterosexism as a major social violence problem in U.S. education and argues that peer sexual harassment is sexual violence with considerable mental health implications for both boys and girls. Recommendations for social work practice regarding peer sexual harassment in schools are discussed.", "title": "Sexual harassment between same-sex peers: intersection of mental health, homophobia, and sexual violence in schools.", "claims": null}, {"metadata": {"year": 1990}, "authors": ["G. Herek"], "summary": "Hate crimes against lesbians and gay men occur within a broader cultural context that is permeated by heterosexism. Heterosexism is defined here as an ideological system that denies, denigrates, and stigmatizes any nonheterosexual form of behavior, identity, relationship, or community. It operates principally by rendering homosexuality invisible and, when this fails, by trivializing, repressing, or stigmatizing it. This article focuses upon the nexus between cultural heterosexism and individual prejudice against lesbians and gay men. Key components of the ideologies of sex and gender from which heterosexism derives are identified: (a) the personal-public dichotomy, (b) the stigmatization of particular forms of sexuality, and (c) the linkage of heterosexuality to gender-role conformity. Supported by these ideological underpinnings, cultural heterosexism fosters individual anti-gay attitudes by providing a ready-made system of values and stereotypical beliefs that justify such prejudice as \u201cnatural.\u201d By imbuing homosexuality with a variety of symbolic meanings, cultural heterosexism enables expressions of individual prejudice to serve various psychological functions. Further, by discouraging lesbians and gay men from coming out to others, heterosexism perpetuates itself. Recent social trends that may affect the ideology of heterosexism are identified, and their potential for reducing anti-gay prejudice is discussed.", "title": "The Context of Anti-Gay Violence", "claims": null}], "query": "Give me references about gay culture and sexual harassment", "summary_abstract": "The research question regarding gay culture and sexual harassment is addressed through various studies that highlight the pervasive nature of harassment faced by sexual minorities, particularly in educational settings. \n\nLeap (2011) discusses how narratives of homophobic violence often obscure the identities of the perpetrators, focusing instead on the experiences of victims. This storytelling approach reflects broader societal issues related to gay visibility and the politics of assimilation, suggesting that the way these stories are told can influence perceptions of violence within gay culture.\n\nEpstein (1996) provides a qualitative exploration of harassment experienced by lesbian and gay students within the educational system. The findings reveal that both male and female respondents face significant harassment, often linked to their sexual orientation or perceived gender nonconformity. This highlights the urgent need for awareness and intervention in educational environments to protect sexual minority students.\n\nRoy (2010) emphasizes the alarming prevalence of anti-LGBTQ harassment in schools, noting that such harassment is qualitatively different from other forms of bullying. The study indicates that a significant percentage of sexual minority students feel unsafe, with many reporting experiences of verbal and physical harassment. The implications of this harassment extend beyond immediate safety concerns, affecting students' mental health and academic performance.\n\nTomsen (2013) argues that violence against sexual minorities is not an isolated phenomenon but rather part of a broader spectrum of male-perpetrated violence. This perspective encourages a deeper understanding of the gendered nature of harassment and violence, suggesting that societal norms around masculinity contribute to the perpetuation of such behaviors.\n\nFineran (2002) provides a legal and historical context for understanding peer sexual harassment, framing it as a significant social issue intertwined with sexism and heterosexism. This framework is crucial for addressing the mental health implications of harassment for both boys and girls, emphasizing the need for comprehensive strategies in educational settings.\n\nHerek (1990) situates hate crimes against sexual minorities within a cultural context of heterosexism, which marginalizes non-heterosexual identities. This ideological framework not only fosters individual prejudice but also perpetuates a cycle of invisibility and stigmatization for LGBTQ individuals.\n\nCollectively, these studies underscore the urgent need for systemic changes in educational environments to combat harassment and promote safety for sexual minority students. They highlight the complex interplay between cultural attitudes, individual experiences, and institutional responses to harassment in the context of gay culture.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2014}, "authors": ["Weidong Huang", "T. Bednarz"], "summary": "How we visualize graph data is important for us to make sense of it. A number of aesthetic criteria have been used in practice to guide the visualization process and judge the quality of graph drawings. These aesthetics are limited since they often conflict with each other. It is generally agreed that in order to make visualizations effective, well-grounded perception and cognitive theories and design principles are needed. Some attempts have been made to develop visualization theories. In this paper, we present a preliminary study which we conducted with a cognitive approach to add to this growing body of research. More specifically, we propose a graph visualization model, which is further conceptualized into a two-stage assessment cycle. Examples of potentially useful methodologies and theories are introduced and their implications for producing user-friendly visualizations are discussed.", "title": "Towards a Cognitive Approach to User-Centered Visualization Design and Evaluation", "claims": null}, {"metadata": {"year": 2007}, "authors": ["Michael  Workman"], "summary": "In knowledge-work, there are increasing amounts of complex information rendered by information technology, which has led to the common term, information overload. Information visualization is one area where empirically tested semantic theory has not yet caught up with that of the underlying information storage and retrieval theory, contributing to information overload. In spite of a vast body of cognitive theory, much of the human factors research on information visualization has overlooked it. Specifically, information displays have facilitated the data gathering (ontological) aspects of human problem-solving and decision-making, but have exacerbated the meaning-making (epistemological) aspects of those activities by presenting information in linear rather than in graphical (holistic) forms. Drawing from extant empirical research, we present a thesis suggesting that cognitive load may be reduced when holistic information is imbued with transformational grammar to help alleviate the information overload problem, along with a methodological approach for investigation.", "title": "Cognitive Load Research and Semantic Apprehension of Graphical Linguistics", "claims": null}, {"metadata": {"year": 2006}, "authors": ["Weidong Huang", "Seok-Hee Hong", "P. Eades"], "summary": "Performance and preference measures are commonly used in the assessment of visualization techniques. This is important and useful in understanding differences in effectiveness between different treatments. However, these measures do not answer how and why the differences are caused. And sometimes, performance measures alone may not be sensitive enough to detect differences. In this paper, we introduce a cognitive approach for visualization effectiveness and efficiency assessment. A model of user performance, mental effort and cognitive load (memory demand) is proposed and further mental effort and visualization efficiency measures are incorporated into our analysis. It is argued that 1) combining cognitive measures with traditional methods provides us new insights and practical guidance in visualization assessment. 2) analyzing human cognitive process not only helps to understand how viewers interact with visualizations, but also helps to predict user performance in initial stage. 3) keeping cognitive load induced by a visualization low allows more memory resources to be available for high level complex cognitive activities. A case study conducted supports our arguments.", "title": "Predicting graph reading performance: a cognitive approach", "claims": null}, {"metadata": {"year": 2011}, "authors": ["M. Hoffmann", "F. Paglieri"], "summary": "External representations play a crucial role in learning. At the same time, cognitive load theory suggests that the possibility of learning depends on limited resources of the working memory and on cognitive load imposed by instructional design and representation tools. Both these observations motivate a critical look at Computer-Supported Argument Visualization (CSAV) tools that are supposed to facilitate learning. This paper uses cognitive load theory to compare the cognitive efficacy of Rationale TM 2 and AGORA.", "title": "Cognitive effects of argument visualization tools", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Muhammed Yousoof Mohd Sapiyan"], "summary": "Computer programming is a complex skill to acquire for novice learners who are in their initial phase of learning programming. There are many factors that results in difficulties in learning programming. This paper addresses to resolve one core difficulty which is cognitive load [2] [3].Cognitive load theory [13] is a famous theory of learning. It states that the schema of the long term memory is not well built in the case of novices and also there is a limitation of working memory\u2019s capacity. This makes it hard for novices to understand the concepts and equip with the skills necessary to become programmers. Some efforts used to overcome the cognitive load are the visualization tools for learning programming [14] .There is no accountability on how effective these visualization systems helped in reducing the load. The mechanism to measure cognitive load is not used in the visualization systems. There are two methods of cognitive load measurement namely physiological and non physiological measures. Physiological measures include EKG,GSR[12],EEG[11],Temperature[11] etc. and non physiological measures includes rating scale[6] and recent research studies have used EEG as a index for cognitive load measurement [7].We felt that using the physiological measures could be accurate as they are the reflections of the body impulses. There is no user\u2019s control over the measurement. We also found out that among the physiological measures EEG could be more effective as the latest efforts of measuring the cognitive uses EEG. This paper addresses the cognitive load measurement while using visualization tools by the novice programmers using EEG as an index of cognitive load.", "title": "COGNITIVE LOAD MEASUREMENT FOR VISUALIZATIONS IN LEARNING COMPUTER PROGRAMMING USING EEG (ELECTROENCELOGRAPHY)", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Weidong  Huang", "Peter  Eades", "Seok-Hee  Hong"], "summary": "Graph visualizations are typically evaluated by comparing their differences in effectiveness, measured by task performance such as response time and accuracy. Such performance-based measures have proved to be useful in their own right. There are some situations, however, where the performance measures alone may not be sensitive enough to detect differences. This limitation can be seen from the fact that the graph viewer may achieve the same level of performance by devoting different amounts of cognitive effort. In addition, it is not often that individual performance measures are consistently in favor of a particular visualization. This makes design and evaluation difficult in choosing one visualization over another. In an attempt to overcome the above-mentioned limitations, we measure the effectiveness of graph visualizations from a cognitive load perspective. Human memory as an information processing system and recent results from cognitive load research are reviewed first. The construct of cognitive load in the context of graph visualization is proposed and discussed. A model of user task performance, mental effort and cognitive load is proposed thereafter to further reveal the interacting relations between these three concepts. A cognitive load measure called mental effort is introduced and this measure is further combined with traditional performance measures into a single multi-dimensional measure called visualization efficiency. The proposed model and measurements are tested in a user study for validity. Implications of the cognitive load considerations in graph visualization are discussed.", "title": "Measuring Effectiveness of Graph Visualizations: A Cognitive Load Perspective", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Ricarda Moses", "S. Humayoun", "Ragaad Altarawneh", "A. Ebert"], "summary": "Understanding cognitive processes during the interaction with visualizations, speci\ufb01cally limitations of working memory, opens a new perspective on evaluations and encourages a more user-centric design approach. In a user study we evaluated two graph data visualization approaches (i.e., the force-directed layout and the Chord layout) using a cognitive load questionnaire to assess the three load types: intrinsic, extraneous, and germane load. Tasks were designed to encourage insight and sensemaking in the participants during the evaluation. This type of evaluation helps to assess the users\u2019 mental processes during sensemaking and graph reading in the underlying layouts. Further, such study \ufb01ndings would help visualization designers in choosing the appropriate layout type for their graph data.", "title": "Evaluating Cognitive Load: Force-directed Layout vs. Chord Layout", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Juan C. Castro-Alonso", "Paul  Ayres", "John  Sweller"], "summary": "There are basically two formats used in instructional visualizations, namely, static pictures and dynamic visualizations (e.g., animations and videos). Both can be engaging and fun for university students in the fields of health and natural sciences. However, engagement by itself is not always conducive to learning. Consequently, teachers, lecturers, and instructional designers need to utilize the cognitive processing advantages of visualizations as well as engagement to achieve full instructional effectiveness. A cognitive processing focus has outlined many ways in which instructional visualization can be optimized. Specifically, cognitive load theory and the cognitive theory of multimedia learning are two research paradigms that provide several methods for directing the design of visualizations by considering how learners process visuospatial information. In this chapter, we describe five methods based on these cognitive theories: (a) the split attention effect and spatial contiguity principle, (b) the modality effect, (c) the redundancy effect and coherence principle, (d) the signaling principle, and (e) the transient information effect. For each of these effects, examples of applications for education in health and natural sciences are provided, where the influence of visuospatial processing is also considered. We end this chapter by discussing instructional implications for science education and providing future directions for research.", "title": "Instructional Visualizations, Cognitive Load Theory, and Visuospatial Processing", "claims": null}], "query": "Cognitive Load Theory toward visualization", "summary_abstract": "The research question regarding Cognitive Load Theory (CLT) in relation to visualization has been addressed by several authors, highlighting the importance of cognitive processes in the effectiveness of visual representations. \n\nHuang and Bednarz (2014) emphasize the necessity of grounding visualization practices in cognitive theories to enhance user-friendly designs. They propose a graph visualization model that incorporates cognitive approaches, suggesting that understanding cognitive load can lead to more effective visualizations. Similarly, Huang et al. (2006) argue that combining cognitive measures with traditional performance metrics can provide deeper insights into visualization effectiveness, asserting that lower cognitive load allows for better engagement in complex cognitive tasks.\n\nWorkman (2007) discusses the issue of information overload in knowledge work, suggesting that cognitive load can be alleviated through holistic information presentation rather than linear formats. This aligns with Huang et al. (2009), who introduce a model that connects user performance, mental effort, and cognitive load, advocating for a multi-dimensional approach to evaluate visualization efficiency.\n\nMoses et al. (2017) further explore cognitive processes during interactions with visualizations, assessing different graph layouts through a cognitive load questionnaire. Their findings indicate that understanding cognitive load types\u2014intrinsic, extraneous, and germane\u2014can inform better design choices for visualizations.\n\nIn the context of educational settings, Castro-Alonso et al. (2019) highlight the need for instructional designers to consider cognitive processing advantages when creating visualizations. They outline methods derived from cognitive load theory that can optimize instructional effectiveness, emphasizing the balance between engagement and cognitive processing.\n\nOverall, the collective findings underscore the critical role of cognitive load in the design and evaluation of visualizations, advocating for approaches that minimize cognitive strain to enhance learning and decision-making processes.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2018}, "authors": ["Mingfong Jan", "Wan-Lin Yang"], "summary": "There is a need to reframe teachers' roles from content area experts to that of learning experience designers because of 21st century teaching challenges. As learning experience designers, teachers help students develop 21st century competencies via guided cognitive and social participation in designed learning activities such as games and gamified activities. In this qualitative case study, we explore teachers' design thinking in designing a lesson plan that involves the use of a card game designed for complex system understanding. Six teachers' thoughts about learning activity design are unpacked via the following activities: playing a card game, crafting a game-based lesson plan, and reporting their design thinking via semi-structural interviews. We discuss similarities in the teachers' views on game-based learning and the structure of their designed lessons, as well as implications of the study. This baseline study helps us map out how teachers think about learning experience design. Such understanding is critical for developing teachers as designers.", "title": "Understanding Teachers' Design Thinking in Designing Game-Based Activities", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Cesar C. Navarrete"], "summary": "In a case study on middle-school student educational game creation course in south central US state, the students' creative thinking process is investigated in order to understand perceptions of the digital design and programming involved in the game creation learning approach. Interviewing 12 students at with three different levels of game design experience, students in grade 6, 7 and 8, with 1, 2, and 3 years of game design experience respectively, findings suggest that students enjoyed the learning approach as satisfying and engaging, yet technologically challenging. The students experienced positive opportunities for engaging the creative thinking process in synthesizing social issue information for constructing their understanding through the creation of interactive, educational digital games. Findings suggest that the creative thinking process in student-centered game creation learning approach may provide learners a rich and enjoyable learning experience with the authentic technology use as well as provide for deep, insightful learning.", "title": "Creative thinking in digital game design and development: A case study", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Qing Li", "Shahista Nathoo", "Elise Vandermeiden", "Collette Lemieux"], "summary": "\"This research intends to explore practicing teachers\u00e2\u20ac\u2122 experiences with game design and development. Specifical-ly, the study is guided by the following research questions: 1. What design considerations were taken into account during the game design process of practicing teach-ers? 2. What pedagogical components (if any) emerge when practicing teachers design and build digital games? Employing a naturalistic and qualitative approach, data are collected from 32 graduate students who partici-pated in the study. The analysis of the games created by these practicing teachers indicates some key themes: (A) curriculum goals were predominantly the games\u00e2\u20ac\u2122 objec-tives; (B) sounds and animations provided visual appeal and feedback; (C) the game could be or needed to be used with supporting materials within the class; (D) the need to create a sense of accomplishment for the gamer; and (F) the emphasis of the gamer being able to project or custom-ize his/her identity within the game. Further discussion is also provided.\"", "title": "Practicing Teachers as Digital Game Creators: A Study of the Design Considerations", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Ana C. R. Martins", "Lia Raquel Moreira Oliveira"], "summary": "The design of educational games is a powerful pedagogical strategy that can only enter schools if teachers are given the necessary training and support. In this paper we present a training action course for teachers interested in learning how to use simple frameworks and tools to be able to either design educational games themselves or facilitate educational game design by their students. We propose a syllabus, a design framework, and a software solution for digital game creation. This approach has been used in Portugal with middle school teachers and students and has been shown to be effective, resulting in the production of functional educational games (digital and non-digital), with preliminary results showing positive outcomes in learning and engagement.", "title": "Students as creators of educational games: learning to use simple frameworks and tools to empower students as educational game designers", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Fr\u00e9d\u00e9rique  Frossard", "Anna  Trifonova", "Mario  Barajas"], "summary": "Abstract Creativity has become a key educational objective. How can game-based learning enhance creative pedagogies? This chapter proposes an approach in which teachers become game designers. It provides a model which analyzes creativity according to three dimensions: process, product, and teaching. We describe practical experiences in which teachers designed and applied their own learning games. Results highlight that game design promotes teaching practices that foster students\u2019 creativity.", "title": "Teachers Designing Learning Games", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Zhen Zhigao"], "summary": "With the development of electronic game industry,more and more people have focused on how to apply the new technology into the field of education,on the purpose of developing educational games with unique educational goals,content or method.Games-to-Teach Project is a partnership between MIT and Microsoft to develop conceptual prototypes for the next generation of interactive educational entertainment.The members of this project involve instructors,researchers and students from different subjects or fields.As main practical results,fifteen conceptual prototypes focus on college-level science,engineering,social science,and humanities education,which will make a significant milestone.The paper introduces the research objectives firstly,and then places importance on the fifteen conceptual prototypes which is the main practical results.During the description,an example is provided to introdue the design progress.Besides,the main conclusions of this proiect is summaried.According to the introduction above,the authors propose five suggestions for us to design and develop educational digital games.The educational digital games have broad prospects for instruction.The integration of games and course content can be achieved through a variety of ways.The design of educational digital games is necessary to be supported with teaching and learning theory.The design of educational digital games is better to be instructional targeted.", "title": "Introduction and Enlightenment on the Games-to-Teach Project", "claims": null}, {"metadata": {"year": 2013}, "authors": ["R. Sandford"], "summary": "Summary form only given. Researchers have offered, in recent years, compelling reasons for considering the potential of digital games to support learning (Klopfer et al., 2009; Pelletier, 2009; Mitchell & Savill-Smith, 2004; Gee, 2008), and in response policymakers and educators around the world have demonstrated a commitment to exploring their practical use in school (Ulicsak, 2010; Koh et al. 2009). There remain, however, many questions about how games can best support learning, particularly in formal education. For teachers, the use of games in a formal curriculum setting can present practical and operational issues, as well as surfacing more fundamental tensions: between generational expectations of games and technology, between home and school identities, and between pedagogies associated with accounts of games as learning tools and those more commonly embraced within the context of formal schooling (Sandford et al., 2011; Felicia, 2009; Ito\u0305 et al., 2009; Sandford et al. 2006). In managing these tensions, teachers are increasingly asked to construct themselves as `designers' (Towndrow, 2005; Foo, et al., 2006; Carlgren, 1999), mobilising their professional knowledge in the creation of new strategies and practices that enable them to negotiate these practical and pedagogical challenges. This paper examines the implications of formal game-based learning for teachers developing their own digital learning games, exploring two guiding overarching research questions. What strategies are employed by teachers to manage intergenerational, technological, operational and pedagogic tensions in the classroom? And to what extent is the notion of being a `designer' visible in their professional practice? The paper suggests that there are multiple ways of `being a designer' for teachers, and that the notion of `designer' may be a more problematic representation of teacher agency and identity than currently visible in the literature.", "title": "Teachers as game designers: Using a game in formal learning in a Singapore primary school", "claims": null}, {"metadata": {"year": 2018}, "authors": ["M. Cezarotto", "A. Battaiola", "Virg\u00ednia Kistmann"], "summary": "Digital educational games have been widely used in educational practices. These games are considered as valuable artefacts that can be used to teach a broad range of subjects to different kinds of learners, thus performing a significant role in the internalisation of contents. A large number of companies, both national and international, develop this kind of media. However, the design of educational games is a complex task, encompassing multiple knowledge and several different stages of development. Companies must excel in order to succeed in this highly competitive market. Studies that focus on the connection between design management and game development are still unsatisfactory. With this in mind, this paper aims to contribute to this debate and to the digital educational game industry, by proposing the use of knowledge from design management as a way of improving strategic planning. From a design management perspective, this paper aims to identify ways to promote innovation during the development process of educational games. In order to do so, this study considers the process of development of games and the communication between the academic field and the industry. This research\u2019s approach is exploratory and qualitative, and this study makes use of a literature review and a case study. This paper offers a set of contributions focusing on innovation for developing digital educational games based on strategic design. Therefore, it provides useful information regarding the relevance of the use of design management by educational game companies. Moreover, this study provides a set of actions to improve strategic planning in these companies through a design management perspective.", "title": "The design of educational games and innovation : a case study based on design management", "claims": null}], "query": "what is design thinking in teachers developing digital educational games?", "summary_abstract": "Design thinking in the context of teachers developing digital educational games involves a multifaceted approach that emphasizes creativity, pedagogical considerations, and the integration of technology into learning experiences. The literature reveals several key themes regarding how teachers engage in this process.\n\nFirstly, teachers are increasingly viewed as learning experience designers rather than mere content experts. Jan and Yang (2018) highlight the necessity for teachers to guide students in developing 21st-century competencies through thoughtfully designed learning activities, including games. Their qualitative study illustrates how teachers' design thinking is shaped by their experiences in creating game-based lesson plans, emphasizing the importance of cognitive and social participation in learning.\n\nMoreover, the design process itself is influenced by various pedagogical components. Li et al. (2012) found that practicing teachers prioritize curriculum goals when designing games, and they incorporate elements such as visual feedback and opportunities for student identity projection within the game. This indicates that effective game design is closely aligned with educational objectives and student engagement.\n\nTraining and support for teachers are also critical for successful game design. Martins and Oliveira (2018) discuss a training course that equips teachers with the necessary frameworks and tools to design educational games, resulting in positive outcomes in both learning and engagement. This underscores the importance of professional development in fostering teachers' design capabilities.\n\nFurthermore, the creative process involved in game design is not limited to teachers alone; it extends to students as well. Navarrete (2013) explores how middle school students engage in game creation, highlighting their enjoyment and the challenges they face. This student-centered approach not only enhances their creative thinking but also allows them to synthesize information on social issues through interactive game development.\n\nLastly, the broader implications of game-based learning reveal tensions that teachers must navigate, such as generational expectations and pedagogical practices. Sandford (2013) discusses how teachers are required to adopt a designer identity to manage these challenges effectively, suggesting that the role of a designer is complex and multifaceted.\n\nIn summary, design thinking in the development of digital educational games by teachers encompasses a shift towards viewing educators as designers, the integration of pedagogical goals, the necessity of training, and the engagement of students in the creative process. This holistic approach is essential for creating effective and meaningful educational experiences through digital games.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2021}, "authors": ["N. Anh"], "summary": "Japan is the world's 3rd largest economy with a nominal GDP of USD 5380 billion. Japan's economy has a rapid industrial development process after its defeat in World War II. Vietnam's economy in 2020 will achieve GDP growth of 2.9%, nominal GDP of 300 billion dollars, per capita income reached $2786, if calculated according to purchasing power parity (PPP), the average income of Vietnamese people is about $ 8,500. In 2020, Vietnam has a trade surplus of 19.1 billion USD (an important contribution to this result is the total two-way turnover of 39.6 billion USD, a trade deficit of 1.1 billion USD from Japan. No small contribution to the economic success of Vietnam as a strategic partner of Japan - the third largest economy in the world. Japan is the first country in the G7 group to recognize Vietnam as a market economy and partner, the largest ODA sponsor for Vietnam, the number 1 investor in Vietnam and the 3rd largest trading partner of Vietnam. This paper also refers the lessons learned for Vietnam.", "title": "Experience in Supporting Industry Development in Japan and Lessons for Viet Nam", "claims": null}, {"metadata": {"year": 2013}, "authors": [], "summary": "Vietnam is an emerging country that over the last twenty years has experienced a high GDP growth rate, although in recent years it has not been saved from the effects of the global economic crisis. Since the introduction of the Doi Moi policy (the Renovation) in 1986, Vietnam aims at transforming the structure of its economy. From a purely centralized and state-controlled economy heavily dependent on agriculture, Vietnam aims to evolve into a socialist-oriented market economy structured towards a more balanced equilibrium between manufacturing and services. In 2010 the composition of GDP by sector showed an economic structure in which agriculture accounted for 20.6% of GDP, while industry and services accounted respectively for 41.1% and 38.3%.", "title": "COMMISSIONS SERVICES' ANNEX ON VIETNAM TO THE POSITION PAPER ON THE TRADE SUSTAINABILITY IMPACT ASSESSMENT OF THE FREE TRADE AGREEMENT BETWEEN THE EU AND ASEAN", "claims": null}, {"metadata": {"year": 2004}, "authors": [], "summary": "Vietnam is a developing country and a member of the Association of Southeast Asian Nations (ASEAN). The country has a surface area of 331,000 square kilometres and is located in the tropics. It has a total population of nearly 80 million, of which 75 percent are farmers living in rural areas. Vietnam\u2019s GDP in 2000 was approximately US$32 billion. Beginning in 1986, it has implemented reform policies to shift from a centralised economy to a market economy with socialist orientation. The Vietnamese economy is structured in terms of GDP in 2000 at current prices, as follows (Vietnam Statistic Yearbook 2000): By economic sector: \u2022 Service: 39.1 percent \u2022 Agriculture, forestry, fishery: 14.3 percent \u2022 Industry, construction: 36.6 percent By ownership: \u2022 State: 39 percent \u2022 Household: 32 percent \u2022 Collective: 8.5 percent \u2022 Private: 3.3 percent \u2022 Mixed: 3.9 percent \u2022 Foreign-invested sector: 13.3 percent", "title": "296-304 Vietnam Final May", "claims": null}, {"metadata": {"year": 2006}, "authors": ["V. Suri", "V. Dinh"], "summary": "The Gross Domestic Product (GDP) growth is anticipated to surpass eight percent for a second year running. Domestic investment has been robust and manufactured exports have experienced solid growth. Foreign Direct Investment (FDI), boosted by imminent World Trade Organization (WTO) accession, has soared. Buoyant revenue growth is expected to help contain the budget deficit below its targeted level. However, the impact of increases in the minimum wage on government expenditures needs to be watched. The trade and external current account balances are projected to be in surplus, and foreign reserves have shown a sizable increase in 2006. The stock market, though still small by regional standards, has grown rapidly to reach nearly eight percent of GDP. Inflation, while lower, is still around seven percent and price pressures remain. Credit growth from state owned banks moderated, partly due to the need to adhere to stricter prudential standards. Reforms in the banking sector assume even greater urgency after WTO accession. Vietnam's accession was ratified by its national assembly in November 2006 and commitments start to take effect in 2007. The increasing liberalization of trade in goods and services offers potential for sustaining rapid growth and poverty reduction. The government will need to continue building support for implementing policy changes entailed by accession, while responding to any genuine hardships that may arise. This report takes a look at these factors contributing to the Vietnam economy as well as exports, imports, GDP growth, inflation, stock market regulations, bonds, and the government budget balance. The report gives a good snap shot of the Vietnamese economy as of year 2006.", "title": "Taking stock : an update on Vietnam's economic developments by the World Bank in Vietnam", "claims": null}, {"metadata": {"year": 1993}, "authors": ["G. Jenkins", "S. Terkper"], "summary": "Vietnam is an Asian country with a population of about 70million and annual per capita income of US $190-200. Despite its current status as one of the poorest countries in the world, it has tremendous potential for growth because of its abundant natural resources and an excellent human resource base. Unlike other centrally planned economies of Asia and Eastern Europe, Vietnam had a relatively small public sector administration, aside from the state enterprise structure, and legacy of capitalism in the south of the country. Nevertheless, the impact of public sector on revenue generation had remained very important.", "title": "VIETNAM\u2019S TAX REFORMS: POLICIES IN TRANSITION ECONOMIES", "claims": null}, {"metadata": {"year": 2004}, "authors": ["D. Viet", "V. Suri"], "summary": "The Gross Domestic Product (GDP) growth in 2004 continues to remain strong and will likely exceed last year's level of 7.2 percent. Non-oil exports have remained robust despite facing external constraints. The current account and budget deficits are both expected to narrow. The main macroeconomic development in the last ten months has been the sharp rise in prices generating considerable debate among policy makers on response strategies. Inflation has, however, begun to decelerate. The high international price of oil, on the one hand has been an important factor in the recent upsurge in inflation, has on the other hand boosted export receipts and government revenue. This report takes a look at these factors contributing to the Vietnam economy as well as exports, imports, GDP growth, inflation, stock market regulations, bonds, and the government budget balance. The report gives a good snap shot of the Vietnamese economy as of year 2004.", "title": "Taking stock : an update on recent economic developments and reforms by the World Bank in Vietnam", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Shozo  Sakata"], "summary": "Since the beginning of the new century, Vietnam has enjoyed rapid economic growth. Its economic performance, which had declined in the late 1990s because of the effects of the Asian economic crisis, began to recover in 2000, with Vietnam continuing to enjoy a high growth rate until 2008. The average GDP growth rate between 2001 and 2005 was 7.5%. In three consecutive years \u2014 from 2005 to 2008 \u2014 the growth rate exceeded 8%. Between 1997 and 2007, the value of GDP increased 3.6 times in nominal terms and twofold in real terms (GSO various years).", "title": "Introduction: The Changing Status of Economic Entities in Vietnam", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Nguyen", "Van", "Luan"], "summary": "Viet Nam's impressive progress toward the achievement of socio-economic development goals up to 2010 and 2020 continues as confirmed by a new set of data surveys. Although the development rate has been rather high over the last 10 years, Viet Nam' GDP scale is below 40 billions USD, still less than the turnover of many multinational companies. Viet Nam's GDP per person is at some 482 USD and GDP per day at over 1.3 USD. That rate is higher than the poverty border as earlier measured but only at 65% the new standard of poverty border. At some 2,200 USD, purchasing power is below the 13th in the world and at 60% the average purchasing power in the region. Therefore, while the economic size is still small, sustainability and high developing speed is the right \u6771\u4eac\u60c5\u5831\u5927\u5b66\u7814\u7a76\u8ad6\u96c6 Vol.8 No.2, pp.11-16(2005) 11", "title": "Economic Development and Challenges for Asian Traditional Economies (Vietnam's Case)", "claims": null}], "query": "The Gross Domestic Product (GDP) in Vietnam was worth 408.80 billion US dollars in 2022, according to official data from the World Bank. The GDP value of Vietnam represents 0.18 percent of the world economy.", "summary_abstract": "The research question regarding Vietnam's GDP in 2022, which was reported to be worth 408.80 billion US dollars, can be contextualized through various studies that highlight the country's economic growth trajectory and structural changes over the years.\n\nN. Anh (2021) notes that Vietnam's nominal GDP was approximately 300 billion dollars in 2020, indicating significant growth leading up to 2022. The paper emphasizes Vietnam's strategic partnerships, particularly with Japan, which have bolstered its economic development. This partnership has been crucial in Vietnam's transition towards a market economy, which began with the Doi Moi reforms in 1986.\n\nThe 2013 study highlights Vietnam's emergence as an economy with a high GDP growth rate over the past two decades, despite facing challenges from global economic fluctuations. The shift from a centrally planned economy to a socialist-oriented market economy has been pivotal in this growth, with the GDP composition evolving to include a more balanced contribution from agriculture, industry, and services.\n\nFurther, the 2006 report by V. Suri and V. Dinh discusses the robust domestic investment and export growth that have characterized Vietnam's economy, particularly in the context of its accession to the World Trade Organization. This accession has facilitated trade liberalization, which is expected to sustain rapid economic growth.\n\nOverall, the collective findings from these papers illustrate that Vietnam's GDP has seen substantial growth, reflecting its transition to a market-oriented economy and the impact of international partnerships. The reported GDP value of 408.80 billion US dollars in 2022 aligns with the trend of increasing economic performance and integration into the global economy, as noted in the various studies.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2018}, "authors": ["Batya Friedman", "David G. Hendry", "A. Borning"], "summary": "Value sensitive design is a theoretically grounded approach to the designof technology that accounts for human values in a principled andsystematic manner throughout the design process. In this article weprovide a survey of 14 value sensitive design methods: 1 direct andindirect stakeholder analysis; 2 value source analysis; 3 co-evolutionof technology and social structure; 4 value scenario; 5 value sketch;6 value-oriented semi- structured interview; 7 scalable informationdimensions; 8 value-oriented coding manual; 9 value-oriented mockup,prototype, or field deployment; 10 ethnographically informed inquiryregarding values and technology; 11 model of informed consentonline; 12 value dams and flows; 13 value sensitive action-reflectionmodel; and 14 Envisioning Cards TM. Each of these methods is honedto the investigation of values in technology, serving such purposesas stakeholder identification and legitimation, value representation andelicitation, and values analysis. While presented individually, the methodsare intended to be integrated in a robust value sensitive designprocess. The survey article begins with a brief summary of value sensitivedesign methodology and theoretical constructs. We next providean overview of the 14 methods. Then, we turn to a broader discussion ofvalue sensitive design practice, focussing on some methodological strategiesand heuristics to support skillful value sensitive design practice.Following the broad discussion of practice, we illustrate one method inaction-value scenarios-providing details on its range of purposes andcontexts. We conclude with reflections on core characteristics of valuesensitive design methodology, and heuristics for innovation.", "title": "A Survey of Value Sensitive Design Methods", "claims": null}, {"metadata": {"year": 2002}, "authors": ["Batya Friedman", "P. Kahn", "A. Borning"], "summary": "Value Sensitive Design is a theoretically grounded approach to the design of technology that accounts for human values in a principled and comprehensive manner throughout the design process. It employs an integrative and iterative tripartite methodology, consisting of conceptual, empirical, and technical investigations. We explicate Value Sensitive Design by drawing on three case studies. The first study concerns information and control of web browser cookies, implicating the value of informed consent. The second study concerns using high-definition plasma displays in an office environment to provide a virtual window to the outside world, implicating the values of physical and psychological well-being and privacy in public spaces. The third study concerns an integrated land use, transportation, and environmental simulation system to support public deliberation and debate on major land use and transportation decisions, implicating the values of fairness (and specifically freedom from bias), accountability, and support for the democratic process, as well as a highly diverse range of values that might be held by different stakeholders, such as environmental sustainability, opportunities for business expansion, or walkable neighborhoods. We conclude with direct and practical suggestions for how to engage in Value Sensitive Design.", "title": "Value Sensitive Design: Theory and Methods", "claims": null}, {"metadata": {"year": 2017}, "authors": ["FriedmanBatya", "G  HendryDavid", "BorningAlan"], "summary": "Value sensitive design is a theoretically grounded approach to the designof technology that accounts for human values in a principled andsystematic manner throughout the design process. In this art...", "title": "A Survey of Value Sensitive Design Methods", "claims": null}, {"metadata": {"year": 2002}, "authors": ["Batya Friedman", "P. Kahn", "A. Borning"], "summary": "Value Sensitive Design is a theoretically grounded approach to the design of technology that accounts for human values in a principled and comprehensive manner throughout the design process. It employs an integrative and iterative tripartite methodology, consisting of conceptual, empirical, and technical investigations. We explicate Value Sensitive Design by drawing on three research and design projects. One project involves cookies and informed consent in web browsers; the second involves projection technology in an office environment; the third involves user interactions and interface for an integrated land use, transportation, and environmental simulation.", "title": "Report 02-1201 1 Value Sensitive Design : Theory and Methods", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Batya  Friedman", "Peter H. Kahn", "Alan  Borning", "Alina  Huldtgren"], "summary": "Value Sensitive Design is a theoretically grounded approach to the design of technology that accounts for human values in a principled and comprehensive manner throughout the design process. It employs an integrative and iterative tripartite methodology, consisting of conceptual, empirical, and technical investigations. We explicate Value Sensitive Design by drawing on three case studies. The first study concerns information and control of web browser cookies, implicating the value of informed consent. The second study concerns using high-definition plasma displays in an office environment to provide a \u201cwindow\u201d to the outside world, implicating the values of physical and psychological well-being and privacy in public spaces. The third study concerns an integrated land use, transportation, and environmental simulation system to support public deliberation and debate on major land use and transportation decisions, implicating the values of fairness, accountability, and support for the democratic process, as well as a highly diverse range of values that might be held by different stakeholders, such as environmental sustainability, opportunities for business expansion, or walkable neighborhoods. We conclude with direct and practical suggestions for how to engage in Value Sensitive Design.", "title": "Value Sensitive Design and Information Systems", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Jianan Guo", "Fan-Shing Chen"], "summary": "The applied technology has made great development in today\u2019s society, which were included information technology, nanotechnology, genetic technology, and so on. However, no matter what kind of technology there are still problems that design value demand of technology contradict with human beings, the safety design of technology should not only posses the technical characteristics of the ideal, but give full consideration to the value of the patients demands. Value sensitive design is a kind of theory of technology design method, it pays attention to the technology user\u2019s values of society and ethics psychological demands, it can provide guidance and advice for method of security design of applied technology as represented by high technical, which has has an important significance for the design and application of cutting-edge technology of our nation.", "title": "The Aanlysis on Value Sensitive Design of Applied Technology", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Batya Friedman", "David G. Hendry", "A. Huldtgren", "C. Jonker", "J. Hoven", "A. V. Wynsberghe"], "summary": "In the 2010\u2019s it is widely recognized by computer and information scientists, social scientists, designers, and philosophers of technology that the design of information systems is not value neutral [5-8,11]. Rather, such systems are value laden in part because societal values are major factors in shaping systems, and at the same time the design of the technology reinforces, restructures or uproots societal value structures. Of the many theories and methods to design for this phenomenon one continues to gain traction for its systematic and overarching consideration of values in the design process: Value Sensitive Design (VSD) [5-7]. The aim of this multidisciplinary workshop is to bring together scholars and practitioners interested in ways values can be made to bear upon design and to help continue to build a community by sharing experiences, insights, and criticism.", "title": "Charting the Next Decade for Value Sensitive Design", "claims": null}, {"metadata": {"year": 2021}, "authors": ["David G. Hendry", "Batya  Friedman", "Stephanie  Ballard"], "summary": "In this article, we first offer a model of design knowledge types (know-about, know-that, know-how) and their interrelationships in value sensitive design. Then we demonstrate that value sensitive design is a formative framework, which provides a shaping influence on practice, enables creative appropriation, and supports theory and method development.", "title": "Value sensitive design as a formative framework", "claims": null}], "query": "what are Value sensitive design methods?", "summary_abstract": "Value Sensitive Design (VSD) is a comprehensive and theoretically grounded approach to technology design that systematically incorporates human values throughout the design process. The methodology is characterized by its integrative and iterative tripartite framework, which includes conceptual, empirical, and technical investigations (Friedman et al., 2002; Friedman et al., 2013). \n\nA key aspect of VSD is its focus on identifying and analyzing values that are relevant to stakeholders, which is achieved through a variety of specific methods. Friedman et al. (2018) outline 14 distinct VSD methods, such as stakeholder analysis, value scenario creation, and ethnographically informed inquiry, each designed to facilitate the investigation of values in technology. These methods serve purposes like stakeholder identification, value representation, and values analysis, and they are intended to be integrated into a cohesive design process.\n\nThe application of VSD is illustrated through case studies that highlight its practical implications. For instance, one study examines the implications of web browser cookie management on informed consent, while another explores the use of high-definition displays in office environments to enhance well-being and privacy (Friedman et al., 2002; Friedman et al., 2013). These examples demonstrate how VSD can address complex value considerations, such as fairness, accountability, and environmental sustainability, in technology design.\n\nMoreover, VSD is recognized for its relevance in contemporary discussions about the ethical implications of technology, as it emphasizes that design is not value-neutral but rather shaped by societal values (Friedman et al., 2015). This perspective is increasingly important in the context of advanced technologies, where ethical and societal considerations must be integrated into the design process (Guo & Chen, 2014).\n\nIn summary, Value Sensitive Design methods provide a structured approach to incorporating human values into technology design, ensuring that ethical considerations are central to the development process. The methods and case studies presented in the literature illustrate the practical application of VSD in addressing diverse stakeholder values and fostering responsible technology development (Friedman et al., 2018; Hendry et al., 2021).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2010}, "authors": ["Jung Lee", "Frank A. Cerreto", "Jihyun Lee"], "summary": "According to Ajzen's Theory of Planned Behavior (TPB), behavioral intention (BI) is predicted by attitude toward the behavior (AB), subjective norm (SN), and perceived behavioral control (PBC). Previous studies using the TPB to explain teachers' intentions to use technology have resulted in inconsistent findings. This inconsistency might be due to overly broad definitions of the target behavior. To investigate this potential weakness, we defined a specific target behavior, using computers only to create and deliver lessons, and then used the TPB to investigate teachers' decisions. An elicitation study was used to identify teachers' salient beliefs and develop a closed-ended questionnaire. Results of the closed-ended questionnaire revealed that AB, SN, and PBC all were significant predictors of teachers' intentions. However, AB had twice the influence of SN and three times that of PBC. This finding suggests that teachers must have positive attitudes about using computers to create and deliver lessons. They are less concerned about what others think of this practice, and far less bothered by internal or external constraints. Results provide specific information that can be used to design effective teacher development programs and remind TPB researchers of the importance of using specific definitions of the target behavior.", "title": "Theory of Planned Behavior and Teachers' Decisions Regarding Use of Educational Technology", "claims": null}, {"metadata": {"year": 2004}, "authors": ["Jeffrey J. Martin", "P. Kulinna"], "summary": "Abstract The purpose of our investigation was to examine determinants of teachers' intentions to teach physically active physical education classes (i.e., spend at least 50% of class time with the students engaged in moderate to vigorous physical activity). Based on the theory of planned behavior, a model was examined hypothesizing that teachers' intentions were determined by subjective norm, attitude, and perceived behavioral control. Grounded in self-efficacy theory, it was hypothesized that program goal importance and hierarchical and barrier self-efficacy would also predict intention. Using a series of hierarchical regression analyses, the theory of planned behavior was supported by accounting for 59% of the variance in intention due to attitude, perceived behavioral control, and subjective norm. Self-efficacy theory based variables received minimal support.", "title": "Self-Efficacy Theory and the Theory of Planned Behavior: Teaching Physically Active Physical Education Classes", "claims": null}, {"metadata": {"year": 2011}, "authors": ["J. Brophy"], "summary": "Recent research on teacher planning, thinking, and,--decision making is reviewed. The -ark on planning reveals that teachers typically do not use the obiectives-based, rational models stressed in text books, but instead concentrate on the activities included in a curriculum as they seem to relate to the needs and interests of the students. This work indicates the need for training teachers to plan more effectively, and suggests alternative models of the planning process which might be more appropriate than the rational model. Research on teachers' perceptions, thoughts, and decisions during the teaching process is just beginning, but it suggests exciting possibilities. Work done to date suggests that most teacher perceptions about students are accurate, most teacher decisions about students are logical and based on appropriate information sources, and, in general, that teachers' behavior when interacting with students may be monitored and controlled more consciously than previous work would suggest. TEACHERS' COGNITIVE ACTIVITIES AND OVERT BEHAVIORSJere E. Brophy 3 Research on teaching .at least in the United Stctes, lavgely ignored the mental_ life of the teacher until the last five years. In Dunkin and Biddle's (1974) rview, far example, classroom variables were classified as presage, process, product, or context variables. Little or nothaing was said about teacher planning, thinking, or decision making, because very little research attention had been directed to these topics. Dunkin and Biddle lumped what little there was into the presage variable category, along with information on teachers' beliefs, values, and attitudes. Furthermore, most studies that included presage variables concentrated on their relationships with product (outcome) variables. Presage-process relationships usually were investigated only as a sideline, if at all Finally; the research was virtually silent on the topic teachers' thoughts while engaged in the act of teaching. Much of the explanation for this neglect of the mental life of the teacher lies in the pervasive influence of behaviorism on American social science research. Behaviorists look upon thoughts as mere epiphenomena accompanying behavior, perhaps of interest to the agent experiencing the thoughts but not to the behavioral scientist interested in establishing functional relationshipu and achieving preThis paper is a slightly revised version of a paper prepared for presentation at-the meetings of the international research project on Basic Components in the Education of Mathematics Teachers (BACOMET), conducted April 21-25, 1980 at the Kommen0e Lage in Rieste, Federal Republic of Germany. -Jere E. Brophy is coordinator of the Classroom Strategy Study and a professor of student teaching and professional development, and of counseling and educational psychology. The author wishes to acknowledge and thank Professor Christopher M. Clark for sharing materials and suggestions about coverage and for providing critical reec:ions to an earlier draft of the paper, and June Tinney for assisting in manuscript preparation, diction and control. This position has softened recently even among serious behaviorists, as Bandura (1977), Wichenbaum (1977) and others have stressed the role of thinking (self-talk, verbal behavior, etc.) in directing behavior. Even so, most contemporary American social scientists, including educational researchers, remain more interested in studying teachers' behavior than in studying what is on their minds. This has generally been true even of investigators who are eclectic in, approach and more interested in investigating educational topics in their own right than merely aq a means for testing psychological theories. American teacher educators and researchers generally share the notion that teaching is learned much more through imitation and modeling than through reading text books or participating in teacher education courses. Teaching, at least in the early stages, tends to be described not as a rational process, but instead as a frenzied attempt to learn survival skills under conditions of fear and feelings of inadequacy (Fuller, 1969; Doyle, 1977). For a time, esteem for teachers' thinking and decision-making skills sunk so low that attempts to construct teacher-proof curricula became a fad. This has passed, more out of the realization that teacher proofing is not really oossible than out of positive respect for the role of teachers in planning and implementing curricula. Complaints-about teachers' presumed negative attitudes, lack of subjectmatter knowledge, unwillingness to adopt new techniques, and tendencies to subvert the efforts f curriculum developers by omitting or changing instructional methods or parts of the curriculum considered crucial are still very common (especially among math and science curriculum people, in my experience). Even those more sympathetic to teachers have done little to encourage respect for, let alone investigation of teachers.' perceptions, thoughts, or decisions. Jackson (196t), for example, stressed that the complexities of the teaching task are so numerous and varied as to make it difficult, if not impossible, for teachers to monitor their awn behavior, let alone remember later what was done or why. Although Jackson does i of say so directly, this emphasis oa classroom coarplexities seems to imply that the teacher role is 1essarily reactive, that it is difficult for teachers to retain and cumulate their experiences so as to become more proactive and systematic in their classroom behavior. writings also have stressed the idea that most teacher behavior is reaction to immediate external stimulation, and until recently my research has concentrated on teachers' behavior rather than on their thinking about what they do or why they do it. This has been the case even though I am coauthor of text book that stresses good decision making as central to teaching success (Good and Brophy, 1980) and a teacher education book that seeks to make teachers more successful by convincing them of the need to become more proactive and by helping them to develop feedback mechanisms to enable them to learn more from a experiences (Good and Brophy, 1978). My strategy for resolving this seeming contradiction between my belief that teaching is too complex a task to allow for much reflection or proactive decision raaking,and my belief that better and more systematic decision making is the key to improvement has been to concentrate on developing knowledge about effective teaching and translating it algorithms -that teachers can learn and incorporate into their planning prior to teaching, and on developing ways to help teachers get feedback about their teaching that they can use for review and reflection after they have finished teaching. These algorithms are not necessarily simple nor simpleminded; branching algorithms'that alLowed for context differen or individual differences in students have been Implemented successfully and have led to increased student achievement (Anderson, Evertson, 6, Brophy, 1979). Even so, it would be fair to say that heretofore concern about teacher planning has been to provide input tohe teacher rather than to study the process naturalistically, and my approach to teachers'", "title": "Occasional Paper No . 39 _ _ S \" COGNITIVE ACTIVITIES AND OVERT BEHAVIORS", "claims": null}, {"metadata": {"year": 2007}, "authors": ["F. Crawley"], "summary": "The purpose of this study was to explore the utility of the Theory of Planned Behavior for predicting the behavioral intentions of teachers enrolled in the Institute in Physical Science, an EESA, Title II program funded by the Texas Nigher Education Coordinating Board. In particular, the study investigated three determinants of teachers' behavioral intentions (BI) set forth in the theory of planned behavior, namely attitude toward the behavior (AB), subjective norm (SN), and perceived behavioral control (PBC). The behavior of interest in this study was the intention of teachers in grades 5/6 or 9/10 who were enrolled in the Institute to use of 50% of the activities and investigations completed in the program with students they would teach during the next school year. Data were collected from fifty elementary and secondary teachers. Simple and hierarchical regression analyses were used to determine the relative contributions of attitude, subjective norm, and perceived behavioral control (and their interactions) to the prediction of behavioral intention. Results of this study indicated that attitude, subjective norm, and perceived behavioral control made significant contributions to the prediction of behavioral intention. Examination of the data blocked on external variables revealed attitude toward the behavior to be the single most important predictor of behavioral intention. Intent to perform the behavior appears to be totally under the control of most teachers, with little need for social support and with ample resources and sufficient opportunities available to perform the behavior.", "title": "A Test of the Theory of Planned Behavior", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Ryan  Dunn", "John  Hattie", "Terry  Bowles"], "summary": "Abstract The purpose of this study was to examine the factors influencing teachers\u2019 intentions to engage in professional learning on the mathematics Common Core State Standards (CCSS). The aim was to identify specific factors teachers encounter that impinge upon or facilitates implementing new instructional practices. Ajzen\u2019s (1991) theory of planned behavior was used to examine the influence of three constructs (attitude toward the behavior, subjective norms, and perceived behavioral control) on teachers\u2019 intentions to implement the instructional expectations associated with the CCSS in mathematics. Data for this quantitative study was collected from 152 teachers in an urban school district in California. The result of the study provides support for the adequacy of the theory of planned behavior for predicting and understanding teachers\u2019 behavioral intention toward professional learning. Intention was predicted significantly by perceived behavioral control, subjective norm and attitude toward the behavior.", "title": "Using the Theory of Planned Behavior to explore teachers\u2019 intentions to engage in ongoing teacher professional learning", "claims": null}, {"metadata": {"year": 1990}, "authors": ["F. Crawley"], "summary": "The purpose of this study was to explore the utility of the theory of planned behavior for predicting the behavioral intentions of teachers enrolled in the Institute in Physical Science, an EESA, Title II program funded by the Texas Higher Education Coordinating Board. In particular, the study investigated three determinants of teachers' behavioral intentions (BI) set forth in the theory of planned behavior, namely, attitude toward the behavior (AB), subjective norm (SN), and perceived behavioral control (PBC). The behavior of interest in this study was the intention of teachers in grades 5/6 or 9/10 who were enrolled in the Institute to use 50% of the activities and investigations completed in the program with students they would teach during the next school year. Data were collected from 50 elementary and secondary teachers. Simple and hierarchical regression analyses were used to determine the relative contributions of attitude, subjective norm, and perceived behavioral control (and their interactions) to the prediction of behavioral intention. Results of this study indicated that attitude, subjective norm, and perceived behavioral control made significant contributions to the prediction of behavioral intention. Examination of the data blocked on external variables revealed attitude toward the behavior to be the single most important predictor of behavioral intention. Intent to perform the behavior appears to be totally under the control of most teachers, with little need for social support and with ample resources and sufficient opportunities available to perform the behavior.", "title": "Intentions of Science Teachers To Use Investigative Teaching Methods: A Test of the Theory of Planned Behavior.", "claims": null}, {"metadata": {"year": 2003}, "authors": ["Youngyool Oh"], "summary": "The present study aims at gaining an understanding of structures underlying Korean teachers\u2019 willingness to change their traditionally-oriented mathematics teaching practice toward reform-oriented mathematics instruction. Elementary school teachers from a metropolitan city of Korea (N = 281) participated in this study. To deal with this problem, this study employees the Theory of Planned Behavior as a research framework and Structural Equation Modeling using AMOS 4.0 as a statistical model was used to analyze the data. The findings indicate that the TPB is a useful model for explaining teacher change in teaching practice, suggesting that beliefs-based programs will be effective for successful teacher education.", "title": "Applying Theory of Planned Behavior Model on Studying Teachers' Change in Mathematics Instruction.", "claims": null}, {"metadata": {"year": 2011}, "authors": ["I. Chen"], "summary": "The main purpose of this study was to investigate and predict the behavioral intention of in-service Taiwanese kindergarten teachers regarding whether they would join a graduate level academic program.\u00a0\u00a0The research framework was based on the Theory of Planned Behavior (TPB) in order to identify the most influential component that affected their intention to join a graduate level academic program.\u00a0\u00a0The study results demonstrate that the three components of the theory of planned behavior accurately predicted the factors affecting the kindergarten teachers concerning this intention.\u00a0\u00a0The most powerful components to affect the teachers were attitude and perceived behavioral control.\u00a0\u00a0Both attitude and perceived behavioral control were correlated and had a significant relationship to intention.", "title": "Using The Theory Of Planned Behavior To Understand In-Service Kindergarten Teachers' Behavior To Enroll In A Graduate Level Academic Program", "claims": null}], "query": "The theory of planned behavior and teacher decisions", "summary_abstract": "The research question regarding the Theory of Planned Behavior (TPB) and its influence on teacher decisions is addressed across several studies, highlighting the significance of the model in predicting teachers' behavioral intentions. \n\nJung Lee et al. (2010) found that in the context of using computers for lesson delivery, teachers' attitudes toward the behavior (AB) were the most influential predictor of their intentions, significantly outweighing the effects of subjective norms (SN) and perceived behavioral control (PBC). This suggests that fostering positive attitudes towards technology use is crucial for effective teacher development programs.\n\nSimilarly, Crawley (2007) and Dunn et al. (2018) reinforced the importance of the TPB in predicting teachers' intentions to implement new instructional practices. Both studies indicated that AB, SN, and PBC significantly contributed to predicting behavioral intentions, with AB emerging as the most critical factor. This aligns with findings from Martin and Kulinna (2004), who also reported that these three constructs accounted for a substantial portion of the variance in teachers' intentions to engage students in physically active classes.\n\nFurthermore, Oh (2003) applied the TPB to understand Korean teachers' willingness to shift from traditional to reform-oriented mathematics instruction, concluding that the model effectively explains changes in teaching practices. Chen (2011) echoed this by demonstrating that for Taiwanese kindergarten teachers, both AB and PBC were the most influential components affecting their intentions to pursue further education.\n\nCollectively, these studies underscore the robustness of the Theory of Planned Behavior in elucidating the factors that shape teachers' decisions, particularly emphasizing the primacy of attitudes in influencing their intentions across various educational contexts.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2014}, "authors": ["Ceyhun  Elgin", "Oguz  Oztunali"], "summary": "In this study, we investigate the relationship between the size of the informal economy and the level of environmental pollution/energy use. To this end, we first use different indicators of environmental pollution along with a measure of energy use intensity in a panel dataset consisting of 152 countries over the period 1999\u20132009 and empirically examine the relationship between pollution and the shadow economy. The estimation results show that there is an inverse-U relationship between the size of the informal economy and environmental pollution, that is, small and large sizes of the informal economy are associated with lower environmental pollution and medium levels of informality are associated with higher levels of environmental pollution. Next, we build a two sector dynamic general equilibrium model to suggest an economic mechanism for this observation. Our model identifies two channels through which informality might affect environmental pollution: The scale effect, whereby a larger (smaller) informal economy size is associated with a lower (higher) level of environmental pollution, and the deregulation effect, whereby a larger (smaller) informal economy is associated with higher (lower) pollution levels. As these two effects work in opposite directions, the changing relative strength of one with respect to the informal sector size creates the inverted-U relationship between pollution indicators and informality.", "title": "Pollution and Informal Economy", "claims": null}, {"metadata": {"year": 2023}, "authors": ["M. Abid", "Habib Sekrafi", "Zouheyr Gheraia", "Hanane Abdelli"], "summary": "This paper examines the impact of informal economy and ecological footprint on environmental regulation for a sample of 25 Sub-Saharan Africa economies (SSA) countries from 1991 to 2017. The results obtained from the techniques of static panel (Pooled ordinary least squares (POLS), random effects (RE), fixed effects (FE), and panel corrected standard error (PCSE)) as well as from dynamic panel (generalized method of moments in differences (D-GMM), and generalized method of moments in system (S-GMM)) were all consistent. They showed that the environmental regulation has a significant positive impact for both informal economy and ecological footprint in the selected economies. Regarding the relation between ecological footprint and per capita gross domestic product (GDP), an inverted U-shaped environmental Kuznets curve is verified. The study concludes that weak environmental regulation in the region reinforces informal economy and environmental pollution. Hence, findings from this study can help policymakers in the region to have a better understanding of the role of environmental regulation in reducing informal economy and environmental degradation.", "title": "Regulating the unobservable: The impact of the environmental regulation on informal economy and pollution", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Jiangmin  Yang", "Yiming  Tan", "Desheng  Xue", "Gengzhi  Huang", "Zuge  Xing"], "summary": "This paper aims to the debate on the nexus between informal economies and the environment by investigating the long-term dynamic impacts of China\u2019s informal economies on pollution and considering regional differences in informal economies\u2019 pollution. This paper uses the Multiple Indicators Multiple Causes (MIMIC) model to estimate the size of informal economies and employs econometric models to examine their relationships to pollution based on provincial-level panel data from 2000 to 2017. The results indicate that informal economies\u2019 effects on environmental pollution are not purely positive or negative. Rather, our model indicates that there is an inverted U-shaped relationship between informal economies and pollution in the long run in China; this means that the level of environmental pollution increases at first and then decreases with the growth of informal economies. Further analysis shows that while this inverted, U-shaped relationship is significant in different regions of China, it is affected by different environmental impact factors. The paper concludes by discussing the policy implications for environmental protection and sustainable development.", "title": "The Environmental Impacts of Informal Economies in China: Inverted U-shaped Relationship and Regional Variances", "claims": null}, {"metadata": {"year": 2020}, "authors": ["A. Gupta", "Manash Ranjan", "M. Gupta"], "summary": "A model of a dual economy has been developed with special emphasis on the environmental problems created by the informal sector activities and its role on the efficiency of labour. The model is used to analyze the effects of the inflow of foreign capital and of the effects of subsidization to the different sectors of the economy.", "title": "Title Rural Urban Migration, Informal Sector and Environmental Problems in a Small Open Less Developed Economy : A Theoretical Analysis Sub Title", "claims": null}, {"metadata": {"year": 1996}, "authors": ["L. Perera", "A. Amin"], "summary": "Abstract Livelihoods of the urban poor, particularly the informal economic activities operating on streets and other public places, are usually seen as undesirable for environmental management by urban authorities which are preoccupied with keeping their cities clean. Hence, informal sector activities are often seen as \u00ab\u00abeye-sores\u00bb\u00bb and are evicted from city centers in the name of \u00ab\u00abpublic cleanliness and orderliness\u00bb\u00bb. However, it is seen that environmental problems associated with the informal sector are mostly manifestations of unresponsive physical planning systems rather than attributes inherent to the sector's respective activities. An environmental impact analysis shows that provision of proper business premises to informal enterprises is an effective measure to curb the environmental problems associated with the sector. From this viewpoint, accommodating the informal sector in the urban built-environment is seen as an effective strategy for urban environmental management.", "title": "Accommodating the Informal Sector: A Strategy for Urban Environmental Management", "claims": null}, {"metadata": {"year": 2018}, "authors": ["I. Chirisa", "Tinashe Bobo"], "summary": "Using case studies from Cairo, Harare, Kigali, and Addis Ababa, this study seeks to disentangle the relationship that exists between the informal sector and the urban environments in Africa. It argues that there are two sides to the coin of the informal sector: the informal sector as a major contributor to urban environmental pollution (land, water, air, and sound), and the sector works as a \u201ccleanser\u201d given its ability to re-use the materials that the formal sector has disgorged. The study defines the inputs, processes, throughputs, and outputs in the sector in keeping with the debates of informal sector contributor to poor environmental management and the informal sector cleanser of the potentially polluted environment. In light of these debates, the authors see the extant imperative of balancing between the two debates in order to inform the urban environmental policy. Overall, with improved technology or appropriate technology coupled with rigorous environmental stewardship campaigns, it is possible to create safer cities where brown, green, and red issues are balanced out.", "title": "Informal Sector Operations and the Environment: Reconnoitering the African Urban Space for Sustainable Urban Stewardship", "claims": null}, {"metadata": {"year": 2014}, "authors": ["William Ruzek"], "summary": "Sustainability typically involves the balancing of three major factors: the economy, the environment, and some notion of equity. Though the economy is already a key aspect, the recognition of the informal economy, seems to be absent from the many possible permutations of these three. This paper will explore the various aspects of the informal economy and how it can make a considerable impact on achieving a more sustainable future. Specifically, this paper focuses on how the informal economy can encourage the sustainable use of goods, while offering an alternative to the regulated market economy. By supporting the informal sectors such as farmers markets, street vendors and non-market activities, a shift away from a car-dominated society and singular economic trajectory can begin. The informal sector can provide, social capital, promote local economies, create jobs and provide the need economic shift toward a sustainable future.", "title": "The Informal Economy as a Catalyst for Sustainability", "claims": null}, {"metadata": {"year": 2000}, "authors": ["Matthew E. Kahn", "A. Pfaff"], "summary": "\"If household and firm activities are not observable by government, then they may not be observable by those affected by environmental degradation either.... [W]e have a regulatory challenge: when information is scarce, private bargaining is unlikely to suffice ... and government will lack a basis for regulatory action.\" Many less developed countries (LDCs) contain sizeable shadow economies. For example, informal economic activity constitutes perhaps 70 percent of the GDP of Nigeria and Egypt, and perhaps as much as 30 percent of the GDP of Chile, Costa Rica, Venezuela, Brazil, Paraguay and Colombia.(1) The magnitude of the shadow economy in these and other countries may have serious environmental consequences. Environmental regulators seeking to provide incentives for environmental protection and conservation, for example, face enormous difficulties in monitoring and enforcing laws in the shadow sector. Groups and individuals operating in this sector recognize that they are not likely to be held accountable for actions that degrade environmental quality. The lack of accountability and incentives to comply with strong environmental standards raises the possibility that parties operating in the shadow sector will engage in activities that threaten the quality of the environment. This includes activities that cause an increase in the number of hazardous waste sites, the degradation of local air and water quality, species loss and total greenhouse gas emissions. While informal sectors exist in all economies, the impact of unobserved economic activity on the environment might be more intense in developing countries for at least four reasons: first, as suggested above, the shadow sectors of LDCs are likely to represent a higher proportion of gross GDP than in developed countries; second, LDCs characteristically have more relatively rural and unpopulated areas, in which the inhabitants lack sufficient incentives (economic or otherwise) to motivate well-defined property rights;(2) next, developing countries are poorer, and thus their governments have fewer resources with which to monitor polluters; and finally, if environmental health is understood as a \"luxury good\"(3) then developing countries may lack the motivation to develop institutions that expose shadow sectors to regulation. Given the challenge shadow economies pose to environmental regulators, are there strategies that a well-intentioned government can pursue to mitigate environmental damage? This paper will investigate three major related questions: 1. How does the existence of shadow sectors affect the design of environmental regulation? 2. How might regulators set environmental policies if they had more information concerning economic activity within shadow sectors? 3. Could economic development (including income growth) and increased international trade help \"lift the shadow\" and thus aid environmental regulators in pursuing accountability? In general, environmental economic policies should provide appropriate incentives for firms and households to reduce environmental degradation. Designed appropriately, such government intervention can guide markets toward outcomes that are socially preferable if, as expected, these actors ignore to some extent the environmental consequences of their actions. Appropriate intervention, however, may require expensive studies to gather useful information regarding actors in the shadow economy. Thus, regulators may choose not to gather this information. Consider the underlying policy problems: firms and households make millions of choices every day which go unobserved by regulators and may have an impact on the environment. These are choices such as: the quantity and type of fuel to use; or, whether to dump hazardous wastes or dispose of them properly If a regulator observed each activity and thereby knew how much damage was caused, they could provide appropriate incentives--facing polluters with the full costs of their activities. \u2026", "title": "Informal Economies, Information and the Environment", "claims": null}], "query": "informal economy and environmental economy", "summary_abstract": "The relationship between the informal economy and environmental outcomes is complex and multifaceted, as evidenced by the findings from various studies. \n\nElgin and Oztunali (2014) highlight an inverse-U relationship between the size of the informal economy and environmental pollution, suggesting that both small and large informal economies are associated with lower pollution levels, while medium-sized informal economies correlate with higher pollution. This duality is explained through two mechanisms: the scale effect, where larger informal economies lead to lower pollution, and the deregulation effect, where larger informal economies result in higher pollution levels. \n\nSimilarly, Abid et al. (2023) found that in Sub-Saharan Africa, weak environmental regulations exacerbate the informal economy and environmental degradation. Their study confirms an inverted U-shaped Environmental Kuznets Curve, indicating that as the informal economy grows, environmental regulation becomes increasingly significant in mitigating ecological footprints.\n\nYang et al. (2021) also support the existence of an inverted U-shaped relationship in China, where the growth of informal economies initially increases pollution but eventually leads to a decrease as the economy matures. This suggests that the impact of informal economies on pollution varies regionally and is influenced by specific environmental factors.\n\nChirisa and Bobo (2018) present a nuanced view, arguing that the informal sector can both contribute to urban environmental pollution and act as a \"cleanser\" by reusing materials discarded by the formal sector. This dual role necessitates a balanced approach in urban environmental policy to harness the positive aspects of the informal economy while mitigating its negative impacts.\n\nPerera and Amin (1996) emphasize the importance of integrating informal economic activities into urban planning, suggesting that providing proper business premises for informal enterprises can alleviate environmental issues associated with their operations.\n\nKahn and Pfaff (2000) discuss the regulatory challenges posed by informal economies, particularly in less developed countries, where the lack of accountability and monitoring can lead to significant environmental degradation. They argue for the need for tailored environmental policies that consider the unique dynamics of the informal sector.\n\nIn summary, the literature indicates that the informal economy has a dual impact on environmental outcomes, with its effects varying based on size, regulatory context, and regional characteristics. Policymakers are encouraged to recognize the informal economy's potential benefits while implementing strategies to mitigate its environmental drawbacks.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2019}, "authors": ["S. Syaifuddin", "Hefniy Hefniy"], "summary": "Digital technology as a whole is revolutionizing the way we do business, doing research, choosing entertainment, and even how to socialize. The emergence of digital technology is actually neutral, positive, and negative which can arise depending on its utilization. Parenting is a process of interaction between parents and children that are influenced by the culture and social institutions in which children are raised. The formation of children's characters requires an effective, applicable and productive methodology so that the expected goals can be achieved well. This research uses a qualitative research approach with a case study model. This research was conducted among lecturers at Nurul Jadid University, Paiton Probolinggo. Sources of data in this study consisted of primary sources and secondary sources. Data analysis techniques in this study include data collection, data reduction, data display and conclusion drawing. The implementation of smart techno parenting in shaping the character of children is to teach technology, impose limits on the use of technology, lead to educational games, learning activities and worship associated with learning to use technology. The implication is that children know about technology, children become disciplined in utilizing technology, make it easier to monitor children's attitudes and behaviour, train intellectual and religious intelligence, and increase children's worship and learning.", "title": "SMART TECHNO PARENTING DALAM MEMBENTUK KARAKTER ANAK", "claims": null}, {"metadata": {"year": 2019}, "authors": ["A. Siibak"], "summary": "Many parents of today are feeling increasingly concerned not only for the well-being and safety of their children, but also for their own abilities to take up the role of a \u201cgood\u201d and \u201cresponsible\u201d parent. Empirical research evidence is used in the chapter to illustrate how the data religion cultivated by tech industry, popular press, marketing discourses and general societal expectations of a \u201cresponsible parent\u201d have created a norm for plugged\u2011in parenting resulting in intimate dataveillance of children, both in online and offline contexts. Various digital parenting tools \u2013 from pregnancy apps and baby monitors to parental controls and tracking devices \u2013 and practices\u00a0\u2013 such as sharenting\u00a0\u2013 are used in the chapter to illustrate how the issues related to the digital rights and privacy of the child are almost entirely discarded against the overprotective and technologically moderated parenting stance leading to both commodification as well as datafication of childhood.", "title": "Digital parenting and the datafied child", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Anil Kumar Bheemaiah"], "summary": "Abstract:A virtual parenting experience, inspired by babyX of soul machines, as part of one\u2019s digital life is explored as transcription from digital DNA. Algorithms are presented, using the open Daz3D framework with the Unity engine. An emphasis is placed on an anime reality and anime assets in creating the digital identity. A study on the role of loving kindness, compassion and natural education, humane education by real parenting, in the bringing up and the emergent digital personalities of virtual children is explored. Keywords: Natural Balance, parenting experience, alternatives to adoption, BabyX, Machine DNAWhat:Given the long waiting times and the legal formalities involved in adoption, to an end to a parenting experience, we present a virtual parenting experience based on a digital human on SaaS augmentation, with an emphasis on non physical reality , but an anime space, for an experience. Such experiences can be as realistic as a few decades long and are possible given the shift towards the cloud computing paradigm. We present a design for machine DNA based aging, parenting and bringing up, emphasising the natural balance, loving kindness and compassion, in alternative schooling, rather than a stick and carrot approach of reinforcement learning.How:We define a non linear time based template system based on the Genesis universal assets for male and female children, with a machine genome for the assets and define an aging model for uniform transcription with heuristics for social, spiritual, emotional and computational intelligence.Why:Given the extinction of many indigenous species, especially in Europe, Asia, Americas and the older Eurasia, is the need for machine evolution, the adoption of virtual parenting experiences and the knowledge of digital human identities. Can the parenting experience form a part of this identity, the need for the constant loving kindness, the warmth and humanity of human parenting. We explore this issue in this paper.", "title": "The Parenting Experience.: A Soul Machines like Demonstration(\u2122)", "claims": null}, {"metadata": {"year": 2020}, "authors": ["M. Pratiwi", "Heni Indrayani", "Swita Amalia"], "summary": "The implementation of parenting uses digital content to become the major source to gather information, in particular, parents these days. Digital parenting appears not only to provide the information that parents need but also to acknowledge about parenting through digital media. Moreover, the information is enlightenment and experienced by other parents so that it helps to use digital media. This paper examines the effect of digital parenting optimization and the fulfilment information need by the parents. This explanatory quantitative study was conducted with 100 parents that joined the parenting community and practiced the parenting pattern with a digital media platform to access the information. Furthermore, this study applied the purposive sampling technique. Data analyzing technique is to test the hypothesis with the simple linear regression model. The result of this study shows that the relation between digital parenting and fulfilment information need by the parents is significantly correlated. Also, the value of the correlation coefficient in the medium range is R =04,16 influence to the low range (0.173). Eventually, to fulfil the information for the parent need is not only gain by digital access.", "title": "Optimasi Pola Pengasuhan Digital dalam Pemenuhan Kebutuhan Informasi", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Jussi  Okkonen", "Sirkku  Kotilainen"], "summary": "The artificial intelligent systems are reshaping human reality. Especially children and youth are altering their mobile communication and media usage fundamentally in their everyday lives today. During children\u2019s use the mobile artificial intelligent applications, the \u201crobot\u201d inside the application is learning from the feedback provided by children and youth through their uses. Based on review on current research and preliminary interviews, this paper addresses the topic from two perspectives. The first perspective is about acceptability of such technologies in general, i.e. to whom or what people are willing to grant right to control content, access and actions in digital sphere. The second perspective is about how users, and their parents too, are affected by the use of pervasive and immersive technology that shapes their media practices. The objective of the pilot study is to gain insightful knowledge of understandings of the artificial intelligence based adaptive media uses among children and youth together with practices of media and information literacies.", "title": "Minors and Artificial Intelligence - Implications to Media Literacy", "claims": null}, {"metadata": {"year": 2020}, "authors": ["S. Livingstone", "Alicia Blum-Ross"], "summary": "In the decades it takes to bring up a child, parents face challenges that are both helped and hindered by the fact that they are living through a period of unprecedented digital innovation. Drawing on extensive research with parents both rich and poor, parenting toddlers to teenagers, this book reveals how digital technologies give parenting struggles a distinctive character, as parents determine how to forge new territory with little precedent, or support. It argues that, in late modernity, parents are both more burdened with responsibilities and yet increasingly charged with respecting and developing the agency of their child\u2014leaving much to be negotiated. The book charts how parents enact authority and values through digital technologies\u2014as \u201cscreen time,\u201d videogames, and social media become ways of both being together and of setting boundaries, with digital technologies introducing valued opportunities and new sources of risk. To light their way, parents comb through the hazy memories of their own childhoods and look toward hard-to-imagine futures. This results in deeply diverse parenting in the present, as parents move between embracing, resisting, or balancing the role of technology in their own and their children\u2019s lives. This book moves beyond the panicky headlines to offer a deeply researched exploration of what it means to parent in a period of significant social and technological change. Drawing on qualitative and quantitative research in the United Kingdom, the book offers conclusions and insights relevant to parents, policymakers, educators, and researchers everywhere.", "title": "Parenting for a Digital Future", "claims": null}, {"metadata": {"year": 2021}, "authors": ["K. Ravindran", "Mohamed Ibrahim", "Hyon Lee", "Umar Qureshi", "K. A. Hafeez"], "summary": "Being parents in a world with increasing responsibilities does not prove to be an easy task. Recent advancements make artificial intelligence and computer vision attractive platforms in the reimagining of parental-aid devices like baby monitors. Drawing on market research of existing devices and opinions of previous baby-monitor users, we employed IoT design principles to develop a cheap and convenient baby monitoring system that uses artificial intelligence to provide aid to parents. The system proved to be effective, as features such as cry and roll detection performed as expected. Further optimizations in the artificial intelligent models can be made to increase system efficiencies, allowing the system to be more performant on specialized hardware.", "title": "Design & Development of an AI-Powered Baby Monitoring System", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Alicia Blum-Ross"], "summary": "Last week, we shared a range of insights from our Parenting for a Digital Future project at the 2015 Family Online Safety Institute Conference, which focused on the many benefits, opportunities, and challenges our online lives can offer. Alicia Blum-Ross reflects on the mixed messages that industry and research often provide to parents, as they are now increasingly \u2018digital natives\u2018 themselves. She finds that monitoring and protection are important aspects of digital parenting, but so are fun and enjoyment, too. Alicia is a researcher at the LSE\u2019s Department of Media and Communications. She is interested in youth media production and is part of the Parenting for a Digital Future research project.", "title": "Parents are now \u2018digital natives\u2019 too \u2013 thoughts from the 2015 family online safety institute conference", "claims": null}], "query": "digital parenting and artificial intelligence", "summary_abstract": "The exploration of digital parenting in the context of artificial intelligence (AI) reveals a complex interplay between technology, parental responsibilities, and children's development. Several studies highlight how digital tools and AI applications are reshaping parenting practices and the experiences of children.\n\nSyaifuddin and Hefniy (2019) emphasize the importance of \"smart techno parenting,\" which involves teaching children about technology while imposing limits on its use. This approach aims to foster discipline and enhance children's intellectual and religious intelligence, suggesting that a balanced integration of technology can positively influence character development.\n\nSiibak (2019) discusses the growing concerns among parents regarding their roles in a digital age, where the tech industry promotes a norm of \"plugged-in parenting.\" This trend often leads to the surveillance of children's online and offline activities, raising significant issues about children's digital rights and privacy. The reliance on digital parenting tools, such as tracking devices and parental controls, can commodify childhood experiences, indicating a need for a more thoughtful approach to digital parenting.\n\nIn a more innovative perspective, Bheemaiah (2019) introduces the concept of virtual parenting experiences through AI-driven digital identities. This exploration raises questions about the emotional and educational aspects of parenting in a digital realm, suggesting that virtual experiences can complement traditional parenting methods.\n\nPratiwi et al. (2020) provide empirical evidence that digital parenting significantly fulfills parents' information needs, highlighting the role of digital media in enhancing parenting practices. Their findings suggest that digital platforms can serve as valuable resources for parents seeking guidance and support.\n\nOkkonen and Kotilainen (2019) focus on the impact of AI on children's media usage, noting that adaptive technologies are reshaping how children interact with digital content. This evolution necessitates a reevaluation of parental roles in guiding children's media consumption and fostering media literacy.\n\nLivingstone and Blum-Ross (2020) argue that the challenges of modern parenting are both exacerbated and alleviated by digital technologies. They illustrate how parents navigate the complexities of setting boundaries and fostering agency in their children, ultimately leading to diverse parenting strategies in a rapidly changing technological landscape.\n\nLastly, Ravindran et al. (2021) explore the development of AI-powered baby monitoring systems, which aim to assist parents in managing their responsibilities more effectively. Their research indicates that such technologies can enhance parental support, making the parenting experience more manageable.\n\nCollectively, these studies underscore the dual nature of digital parenting in the age of AI: while technology offers new tools and resources for parents, it also introduces challenges related to privacy, surveillance, and the commodification of childhood. As parents navigate this landscape, the need for a balanced approach that respects children's rights and fosters healthy development remains paramount.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2022}, "authors": ["Shaun Chang", "Joan R. Williams", "B. Sangeorzan"], "summary": "Category: Ankle; Hindfoot Introduction/Purpose: Painful adult acquired flatfoot is classified into multiple types, with a one category reserved for those that have rigid deformity. However, there is a subset of patients with rigid progressive collapsed foot deformity (PCFD) in which navicular and calcaneus are subluxed from the talus causing sub-fibular impingement and erosion and fixed abduction of the forefoot. This condition is not well described in the literature. The treatment of these patients can be technically difficult given the attenuation and erosion of soft tissue and articular constraints. In this study we sought to identify reduction technique, shared characteristics, treatments, and outcomes in patients with severe hindfoot valgus deformity with subluxation and subfibular impingement who were treated by subtalar reduction and arthrodesis, and talonavicular arthrodesis at a single institution. Methods: A retrospective chart review was done of all patients who underwent flat foot reconstruction by a single surgeon. Patient were included in the study if on preoperative weight-bearing (WB) CT they met the following radiographic criteria: 3 sequential sagittal cuts of the calcaneus without the talus present, 3 sequential sagittal cuts of the talus without the calcaneus present, and the presence of a calcaneofibular articulation on coronal cuts. Patients with neuromuscular disorders or peripheral neuropathy were excluded. This review evaluated demographic data, comorbidities, complications, and the need for reoperation. Plane radiographs and CT findings were compared to identify subluxation on plane images. Patients were treated with a double, triple, or pantalar arthrodesis based on the treating surgeon\u2019s discretion. A cohort of 23 patients has already been identified, but additional data review is currently underway. Results: The initial patient cohort consisted of twenty-three patients who met the inclusion criteria with an average age of 64.5 (range, 48-79) years. The initial cohort consisted of patients who were treated from 2009-2014. Seven of the twenty-three patients (30.4%) required reoperation for various reasons ranging from late infection to nonunion and symptomatic hardware (Table 1). We found that the average BMI for patients with this disorder was 34kg/m2. Four patients have fibular stress fractures on presentation, these patients had an average BMI of 41.6kg/m2. Specific techniques for reduction included use of distractors at subtalar and TN joints. Radiographic findings on plane films are linked to findings on CT scan. Recurrence of deformity occurred with tilt of the talus requiring bracing in 2 patients. Incomplete correction was common, particularly early in the study period. Conclusion: PCFD is a very problem seen by foot and ankle surgeons. Physicians should be aware of the challenges presented in treating the subset of patients who have atraumatic dislocation of the subtalar joint. Diagnosis can be made on plane radiographs, though WB CT is useful for treatment planning. Treatment of these patients requires careful physical and radiographic examination. Reduction and arthrodesis is the treatment of choice. Patients should also be counseled about the possible need for reoperation given the high rate of reoperation in this population.", "title": "Treatment and Outcomes of Atraumatic Subtalar Dislocations in Adult Acquired Flatfoot Deformity", "claims": null}, {"metadata": {"year": 2011}, "authors": ["S  Ochman", "J  Evers", "M J Raschke"], "summary": "Injuries of the midfoot are often missed and therefore underestimated. The diagnosis and primary treatment are crucial for the outcome. An accurately made diagnosis and therapy initially can anticipate the extent of post-traumatic malpositioning, arthrosis and the number of subsequent surgeries. Diagnostics should liberally include computed tomography with multiplanar reconstructions; clinically a compartment syndrome should be treated at an early stage. Surgery includes anatomical reconstruction of the relevant joints and ligamentous structures and restoration of axes and lengths of the columns of the foot. Innovations comprise intraoperative 3-D imaging and computer-assisted surgery for quality control. New methods for ligamentous injuries are transosseous suturing and endobutton techniques, which have not yet become established.", "title": "[Injuries of the midfoot].", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Y. Cheng", "H. Yang", "Z. Sun", "L. Ni", "H. Zhang"], "summary": "A rare midfoot injury pattern of navicular\u2014cuneiform and calcaneal\u2014cuboid fracture\u2014dislocation is presented with two typical cases and a systematic review of the literature. This injury usually occurs as a result of high-energy crushing trauma and most often causes plantarly directed dislocation of the midfoot. Initial diagnosis includes a thorough physical examination and adequate radiological imaging, comprising anteroposterior, oblique and lateral X-radiography and computed tomography with three-dimensional reconstruction. Care should be taken to determine any injury to adjacent midfoot joints, in particular the Lisfranc joint. Intrasurgical protection of soft tissue is essential. Open reduction using two parallel incisions with minifragment plate fixation is recommended in more comminuted injuries. The navicular\u2014cuneiform and calcaneal\u2014cuboid joints play important roles in maintaining the arch of the foot and in weight-bearing during locomotion. Without proper therapy in the immediate post-traumatic phase, the long-term results are generally unsatisfactory.", "title": "A Rare Midfoot Injury Pattern: Navicular\u2014Cuneiform and Calcaneal\u2014Cuboid Fracture\u2014Dislocation", "claims": null}, {"metadata": {"year": 2016}, "authors": ["S. Lau", "M. Bozin", "T. Thillainadesan"], "summary": "Musculoskeletal trauma to the foot is a common presentation to EDs. A Lisfranc fracture dislocation involves injury to the bony and soft tissue structures of the tarsometatarsal joint. While it is most commonly seen post high velocity trauma, it can also present post minor trauma. It is also misdiagnosed in approximately 20% of cases. These Lisfranc injuries typically present to EDs with pain particularly with weight bearing, swelling and post a characteristic mechanism of injury. Diagnosis is via clinical examination and radiological investigation\u2014typically plain radiographs and CTs. Once diagnosed, Lisfranc injuries can be classified as stable or unstable. Stable injuries can be immobilised in EDs and discharged home. Unstable injuries require an orthopaedic referral for consideration of surgical fixation.", "title": "Lisfranc fracture dislocation: a review of a commonly missed injury of the midfoot", "claims": null}, {"metadata": {"year": 2016}, "authors": ["S. Lau", "M. Bozin", "T. Thillainadesan"], "summary": "Musculoskeletal trauma to the foot is a common presentation to EDs. A Lisfranc fracture dislocation involves injury to the bony and soft tissue structures of the tarsometatarsal joint. While it is most commonly seen post high velocity trauma, it can also present post minor trauma. It is also misdiagnosed in approximately 20% of cases. These Lisfranc injuries typically present to EDs with pain particularly with weight bearing, swelling and post a characteristic mechanism of injury. Diagnosis is via clinical examination and radiological investigation\u2014typically plain radiographs and CTs. Once diagnosed, Lisfranc injuries can be classified as stable or unstable. Stable injuries can be immobilised in EDs and discharged home. Unstable injuries require an orthopaedic referral for consideration of surgical fixation.", "title": "Lisfranc fracture dislocation: a review of a commonly missed injury of the midfoot", "claims": null}, {"metadata": {"year": 2011}, "authors": ["T  Mittlmeier", "M  Beck"], "summary": "Injuries of the midfoot comprise fractures, dislocations and/or fracture dislocations of the Chopart and Lisfranc joint lines. Fracture dislocations, in particular, represent prognostically severe lesions which may compromise foot function to a substantial degree. A number of injuries of the midfoot (up to one quarter) are still primarily overlooked. As such, a diagnostic algorithm is essential for the adequate detection of the injury components and planning of the therapeutic approach. Standard x-rays in three planes (dorso-plantar, lateral and lateral oblique views) represent the diagnostic basis which is supplemented by CAT scanning and multiplanar reconstruction, especially in cases where a potential midfoot lesion is suspected. MRI may be helpful in mainly ligamentous injuries only. Latent instability may be revealed by dynamic assessment employing stress fluoroscopy. Chronic instability may be detected by weight-bearing radiographs displaying manifest displacement and malalignment under load. The principal strategy of treatment includes anatomical reconstruction of the joint structures, the geometric proportions of the medial, central and lateral foot columns and the diagnostics and treatment of ligamentous instabilities.", "title": "[Injuries of the midfoot].", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Laksha Dutt", "S. Fisher", "David J. Wilson", "P. Ryan"], "summary": "Introduction: Transcuneiform fracture patterns without dislocations are uncommon. In a review of the literature, numerous case reports describe cuneiform fracture-dislocations involving either the transmetatarsal or midtarsal joint complexes due to direct trauma; however, an isolated transcuneiform fracture pattern without dislocation has not been reported. \nMethods: We present a rare injury of the foot: an isolated transcuneiform fracture without associated dislocation of the tarsometatarsal joint or midtarsal joint complex. \nCase: A 29 year old active duty Army male sustained direct blunt trauma to his foot when a 20 ton trailer fell onto it. His initial presentation prompted protected weight bearing after a clinical exam revealed soft tissue injury and tenderness to palpation about the dorsum of his midfoot. Due to persistent symptoms, he was referred to a musculoskeletal specialist. Radiographs and computed tomography (CT) images demonstrated a minimally displaced comminuted transcuneiform fracture involving the medial and intermediate cuneiforms without evidence of dislocation or subluxation. Treatment consisted of non-weight bearing cast immobilization with crutches for ten weeks before being transitioned into a weight bearing control ankle motion (CAM) boot. Radiographic evidence of healing was documented at 4 weeks by callus formation at the first metatarsal and medial cuneiform. The patient returned to low impact activity by 10 month follow-up without surgical intervention. \nDiscussion and Conclusions: Blunt trauma to the midfoot can result in isolated or complex fracture patterns without the typical fracture-dislocation following high mechanism trauma. Appropriate clinical suspicion and careful physical examination can facilitate timely diagnosis and treatment. Minimally displaced transcuneifom fracture patterns without associated midfoot dislocation injuries can be treated nonoperatively with protected weight bearing, close radiographic and clinical follow up, and gradual advancement to activity with acceptable clinical outcomes.", "title": "Transcuneiform Crush Injury: A Case Report", "claims": null}, {"metadata": {"year": 2010}, "authors": ["James L. Thomas", "E. Blitch", "D. Chaney", "K. Dinucci", "K. Eickmeier", "L. Rubin", "M. Stapp", "J. Vanore"], "summary": "Trauma to the toes, lesser metatarsals, and their respective joints involves various mechanisms and injury types (1, 2). These include a history of both direct and indirect trauma. Patients may exhibit symptoms acutely at the time of trauma or at a later onset. Symptoms include pain, swelling, discoloration, loss of joint motion, and difficulty standing and/or walking. An accurate history of the inciting traumatic event should be elicited.", "title": "Diagnosis and Treatment of Forefoot Disorders . Section 5 . Trauma Clinical Practice Guideline Forefoot Disorders Panel", "claims": null}], "query": "what is the differential diagnosis for traumatic adult new midfoot collapse?", "summary_abstract": "The differential diagnosis for traumatic adult new midfoot collapse encompasses a variety of injuries and conditions that can arise from high-energy trauma or other mechanisms. The literature highlights several key injury patterns and diagnostic approaches.\n\n1. **Lisfranc Injuries**: These injuries involve damage to the tarsometatarsal joint and can manifest as fracture dislocations. They are often misdiagnosed, with a significant percentage (approximately 20%) going unrecognized initially (Lau et al., 2016). Diagnosis typically relies on clinical examination and imaging, including plain radiographs and CT scans. Treatment varies based on the stability of the injury, with unstable cases requiring surgical intervention (Lau et al., 2016).\n\n2. **Navicular-Cuneiform and Calcaneal-Cuboid Fracture-Dislocations**: This specific injury pattern is characterized by high-energy trauma leading to dislocation of the midfoot. Diagnosis involves thorough physical examination and advanced imaging techniques, such as CT with three-dimensional reconstruction. Surgical intervention is often necessary for proper alignment and stabilization (Cheng et al., 2012).\n\n3. **Rigid Progressive Collapsed Foot Deformity (PCFD)**: This condition, although not exclusively traumatic, can result from subtalar joint dislocation and is marked by significant challenges in treatment due to soft tissue erosion and joint instability. Diagnosis can be made through radiographic imaging, and treatment typically involves arthrodesis (Chang et al., 2022).\n\n4. **Fractures and Dislocations of the Chopart and Lisfranc Joint Lines**: These injuries can severely compromise foot function and are often overlooked. A comprehensive diagnostic approach, including standard x-rays and CT scans, is essential for identifying the extent of the injury and planning treatment (Mittlmeier & Beck, 2011).\n\n5. **Transcuneiform Fractures**: Although rare, these injuries can occur without associated dislocations. They may result from direct trauma and can often be managed non-operatively if minimally displaced (Dutt et al., 2015).\n\nIn summary, the differential diagnosis for traumatic adult new midfoot collapse includes a spectrum of injuries such as Lisfranc injuries, navicular-cuneiform and calcaneal-cuboid fracture-dislocations, and PCFD, among others. Accurate diagnosis through clinical assessment and advanced imaging is crucial for effective management and to prevent long-term complications.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2018}, "authors": ["J. Joo", "Bang Sulyeong"], "summary": "This study is a descriptive research to examine the relationship between critical thinking disposition, clinical performance ability, and self-concept of nursing profession according to degree of self-leadership among nursing college students. The data were collected from 165 nursing students in 3rd & 4th grade using self-report questionnaire and analyzed by t-test, ANOVA, Pearson`s correlation coefficient, and Multiple Regression. Self-leadership according to general characteristics showed significant differences according to grade, academic achievement, motivation, and major satisfaction. Self-leadership was found to have a positive correlation with critical thinking disposition, clinical performance ability, and self-concept of nursing professional. Analysis of the effects of general characteristics affecting sief-leadership, critical thinking disposition, nursing proFessional selF-concept and clinical performance on selF-leadership showed that they were explained by 49.9%, critical thinking disposition was the most important factor. This suggests that nursing students' self-leadership can be improved effectively by promoting critical thinking disposition. Also in reality, there are many difficulties such as the problems in the system of the majors, but it is necessary to operate a continuous education program to strengthen the self-leadership for the nursing college students by seeking various methods.", "title": "The Effects of Critical Thinking Disposition, Clinical Performance Ability and Self-Concept of Nursing Profession in Nursing Students on Self Leadership", "claims": null}, {"metadata": {"year": 2015}, "authors": ["\ubc15\ud604\uc8fc"], "summary": "Purpose: The purpose of this study was to determine the relationship among the factors of nursing professionalism, critical thinking disposition and self-leadership in nursing students. Methods: The participants were 715 nursing students in tertiary education. Data was collected from March 10 to 14, 2014 using self-report questionnaires and analyzed through descriptive statistics, t-test, ANOVA and Pearson correlation coefficient with SPSS/win 17.0. Results: The mean scores for nursing professionalism (3.48\u00b1.50), critical thinking disposition (3.14\u00b1.27), self-leadership (3.44\u00b1.40) were above average. There was a positive correlation between nursing professionalism and self-leadership (r=.452, p<.01) and between critical thinking disposition and self-leadership (r=.405, p<.001). Conclusion: The result of this study indicates that developing nursing professionalism and critical thinking disposition in nursing students is significant for improving their self-leadership. Therefore the findings suggest meaningful basic data for nursing education.", "title": "\uc77c \uac04\ud638\ub300\ud559\uc0dd\uc758 \uac04\ud638\uc804\ubb38\uc9c1\uad00, \ube44\ud310\uc801\uc0ac\uace0 \uc131\ud5a5, \uc140\ud504\ub9ac\ub354\uc2ed", "claims": null}, {"metadata": {"year": 2014}, "authors": ["J. Ashley", "Kelly D. Stamp"], "summary": "The purpose of this project was to examine the clinical judgment and reasoning skills of nursing students in high-fidelity simulation. Two levels of students (N = 104), novices and those who are slightly more advanced, participated in individual videotaped simulations. Afterward, interviews were conducted to explore what the student was thinking and feeling during simulation. Five themes emerged from the interviews: thinking like a nurse, assessment, looking for answers, communication, and magical or reflective thinking. There was a clear distinction in the reasoning skills of the novice students compared with students with more clinical experience. Tanner's model of clinical judgment in nursing is used to understand the findings of the study.", "title": "Learning to think like a nurse: the development of clinical judgment in nursing students.", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Su Hui Valynn Wong", "Yanika  Kowitlawakul"], "summary": "AIM\nThe aims of this study were to explore year two nursing students' perceptions toward critical thinking and clinical reasoning and to identify the barriers faced by the students in developing critical thinking and clinical reasoning.\n\n\nBACKGROUND\nCritical thinking and clinical reasoning are core competencies emphasized in nursing practices. Nursing students are required to develop and practice these skills throughout their nursing programs to graduate as competent nurses. However, recent studies still report a lack of critical thinking and clinical reasoning in nursing students and fresh graduates. Hence, it is important to recognize the perceptions of nursing students and the barriers that they face in developing critical thinking and clinical reasoning skills.\n\n\nMETHODS\nAn exploratory descriptive qualitative study design was adopted. Twenty nursing students were recruited from a university in Singapore. Individual face-to-face interviews, using semi-structured questions and an interview guide, were conducted in the academic year 2018/2019. The interviews were audio-recorded and transcribed verbatim. Thematic analysis was used to analyze the data.\n\n\nRESULTS\nSeven themes were emerged, namely: 1) essentials for nursing practices, 2) linking theory to practice, 3) individual thought process, 4) stimulating strategies, 5) classroom environment, 6) clinical environment, and 7) students' attributes. Nursing students perceived critical thinking and clinical reasoning as essential for nursing practices and described these skills as linking theory to practice. Strategies such as simulation, case studies, real clinical experiences, and guidance from clinical instructors/preceptors were found to stimulate critical thinking and clinical reasoning for the students. Barriers to developing critical thinking included classroom environments, such teaching methods and student-to-tutor ratios, ward environments/cultures, and students' attributes/attitudes toward learning.\n\n\nCONCLUSION\nThe findings provided areas for improvement in the current nursing education and practices to better support nursing students in developing critical thinking and clinical reasoning skills.", "title": "Exploring perceptions and barriers in developing critical thinking and clinical reasoning of nursing students: A qualitative study.", "claims": null}, {"metadata": {"year": 2016}, "authors": ["K. Oostra", "B. Astle", "H. Meyerhoff"], "summary": "Theme Two: \u2022 Studies in the literature showed similar findings with respect to not knowing (confusion), prioritizing and hypothesis generation. \u2022 Students expressed frustration with the nature of the assignment. Some studies found working in pairs, utilizing simulation or utilizing virtual patients to be beneficial in promoting clinical reasoning. \u2022 Students described being challenged by their lack of experience. Some studies found that experienced nurses used pattern recognition as a clinical reasoning strategy. This strategy may be less available to student nurses. \u2022 Tanner suggests student nurses are confined to using analytic skills because of their lack of experience2 \u2022 Two studies reported similar findings with respect to students or newly graduated nurses using the nursing process in clinical practice.", "title": "Clinical Reasoning on an Assignment: Perceptions of Third Year Baccalaureate Nursing Students", "claims": null}, {"metadata": {"year": 2011}, "authors": ["G. Marchigiano", "Nina B. Eduljee", "K. Harvey"], "summary": "AIM\nClinical assignments in nursing education provide opportunities for students to develop thinking skills vital to the effective delivery of patient care. The purpose of the present study was to examine students' perceived levels of confidence for using thinking skills when completing two types of clinical assignments.\n\n\nBACKGROUND\nClinical educators and managers are challenged to develop teaching and learning strategies that help students think critically and reflectively and transfer these skills into sound nursing practice.\n\n\nMETHOD\nThis study is based on the theoretical framework of critical thinking within the nursing process framework. Undergraduate nursing students (n=51) completed surveys indicating their confidence in using seven thinking skills for nursing care.\n\n\nRESULTS\nStudents indicated significantly more confidence when implementing the journal format as compared with the care plan format when analysing information, determining relevance, making connections, selecting appropriate information, applying relevant knowledge and evaluating outcomes.\n\n\nCONCLUSIONS\nThe findings of the present study propose a new approach for enhancing students' thinking skills. Journaling is an effective strategy for enhancing students' thinking skills.\n\n\nIMPLICATIONS FOR NURSING MANAGEMENT\nNursing managers are in key organisational positions for supporting and promoting the use of the journal format and building supportive and collaborative learning environments for students to develop thinking skills for managing patient care.", "title": "Developing critical thinking skills from clinical assignments: a pilot study on nursing students' self-reported perceptions.", "claims": null}, {"metadata": {"year": 2002}, "authors": ["R. DeMarco", "L. Hayward", "M. Lynch"], "summary": "In the classroom and in clinical areas, knowing how to learn, reason, reflect, think creatively, generate and evaluate ideas, make decisions, and solve problems have been identified as key elements of critical thinking. However, to be successful in the current health care arena, caregivers cannot be satisfied with possessing the ability to solve problems and simply meet preestablished \"outcomes\" (Alfaro-LeFevre, 1998). It is necessary to improve knowledge and practice applications and to explore the best ways to do things within a work group. This qualitative study evaluated the experiences of senior-level nursing students using case-based instruction in a course titled, Leading and Managing in Nursing. It is a replication and extension (Phase II) of an original case-based instruction study, completed with senior physical therapy students (Phase I). Phase III of this study trajectory is the creation of an interdisciplinary case-based course that addresses either or both clinical collaborative care issues or leadership and management issues for health care profession students. From this Phase II study, six thematic groupings emerged as distinct student experiences in case-based instruction-motivation, real world, learning, knowledge development, emerging from within, and group dynamic issues.", "title": "Nursing students' experiences with and strategic approaches to case-based instruction: a replication and comparison study between two disciplines.", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Janet M Phillips", "Ann M Stalter"], "summary": "There is a critical need for nursing leadership in today\u2019s complex health care settings. Systems thinking can be incorporated in nursing education at all levels while using evidence based principles in education. Teaching tips are provided using a systems awareness model to guide nurse educators in assessment and integration of systems thinking while engage learners in interprofessional education and practice. _________________________________________________________________________________ This is the author's manuscript of the article published in final edited form as: Phillips, J. M., & Stalter, A. M. (2016). Integrating Systems Thinking Into Nursing Education. The Journal of Continuing Education in Nursing, 47(9), 395\u2013397. https://doi.org/10.3928/00220124-20160817-05 2 A new era in nursing education has arrived, and with it nurse educators must apply evidence based teaching to advance learner applications into real world scenarios (American Association of Colleges of Nursing [AACN], 2016). Gone are the days where nurses are socialized through content-laden lectures and high stakes exams to symbolize mastery. Today, engaged learners demonstrate mastery through a variety of competencies cultivating a shared vision of a culture of health (Robert Wood Johnson Foundation [RWJF], 2014). The challenge nurse educators face is how to refine the delivery of material across varying delivery modalities (face-to-face, skills labs, online, hybrid, up-scale or lecture hall classrooms, practice settings) while addressing all essential concepts, measuring competencies, and managing time. An additional challenge nurse educators face is preparing nurses to function and lead in complex health care settings (AACN, 2009, BSN Essential II). Systems thinking is an applied process intended to impact cause and effect through collaborative effort where solutions to complex problems improve components, as well as, the greater whole (Stalter et al., 2016). Systems thinking is accomplished through personal ability and confidence with relationships. Personal effort, reliance on authority, and system interdependencies predict systems thinking that promotes interprofessional practice (Stalter et al., 2016) and positive patient outcomes (Dolansky & Moore, 2013). Helping nurses to understand systems thinking and to acquire the skills needed to lead complex systems is imperative. In this column, a model beneficial to guiding nurse educators in assessment of skill acquisition will be used to integrate systems thinking into nursing education (Phillips, Stalter, Dolansky & Lopez, 2016). The model can be used to plan learning experiences and assessment activities for knowledge and skill acquisition.", "title": "Title : Teaching tips for integrating systems thinking into nursing education", "claims": null}], "query": "The Relationship between Systems Thinking, Self-Leadership, and Clinical Reasoning of the Nursing Students", "summary_abstract": "The relationship between systems thinking, self-leadership, and clinical reasoning among nursing students is a multifaceted area of study, as evidenced by the findings from various research papers. \n\nJoo and Sulyeong (2018) highlight that self-leadership in nursing students is positively correlated with critical thinking disposition and clinical performance ability. Their study suggests that enhancing critical thinking can effectively improve self-leadership, which is crucial for developing clinical reasoning skills. Similarly, Park (2015) found a significant positive correlation between nursing professionalism, critical thinking disposition, and self-leadership, indicating that fostering these attributes is essential for improving self-leadership among nursing students.\n\nWong and Kowitlawakul (2020) further emphasize the importance of critical thinking and clinical reasoning as core competencies in nursing. They identify barriers that nursing students face in developing these skills, such as classroom environments and teaching methods, which can hinder their ability to think critically and reason clinically. This aligns with the findings of Oostra et al. (2016), who noted that students often struggle with clinical reasoning due to a lack of experience and the challenges posed by their educational environment.\n\nMoreover, Phillips and Stalter (2017) advocate for the integration of systems thinking into nursing education, suggesting that it can enhance the ability of nursing students to navigate complex healthcare settings. They argue that systems thinking, when combined with evidence-based principles, can improve both individual and collective competencies in nursing practice.\n\nIn summary, the collective findings from these studies underscore the interconnectedness of systems thinking, self-leadership, and clinical reasoning in nursing education. Enhancing critical thinking and self-leadership is vital for developing effective clinical reasoning skills, while addressing educational barriers and integrating systems thinking can further support nursing students in their professional development.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2019}, "authors": ["A.  Arumugam", "T. S. Anudakshaini", "R.  Shruthi", "K.  Jeyavishnu", "S.  Sundarra Harini", "J. S. Sharad"], "summary": "Polyhydroxyalkanoates are polyesters of R-hydroxyalkonic acids, prominently used as bioplastics on grounds of their complete biodegradable and environment-friendly characteristics. There is an upsurge in need of an alternative low-cost, renewable carbon source for the production of PHA for enhanced economic and to exert a positive impact on the industries. In the present work, cashew apple juice (CAJ) was supplemented as a carbon source for Cupriavidus necator to produce PHA. (NH 4 ) 2 SO 4 , NH 4 Cl, NH 4 NO 3 and CO(NH 2 ) 2 , and NaNO 3 were tested and urea was found to be the best nitrogen source that supports optimal growth of the microorganism. The production process was then optimized using response surface methodology by incorporating the effects of total reducing sugar concentration, urea concentration, and inoculum size. Under optimized condition, the resulting PHA yield was found to be 15.78\u00a0g/L with total reducing sugar concentration of 50\u00a0g/L, inoculum size of 50\u00a0mL/L, and urea concentration of 3\u00a0g/L. FT-IR, NMR, TGA, and DSC analysis revealed the product to be a copolymer of hydroxybutyrate and hydroxyvalerate. Graphical abstract", "title": "Low-cost production of PHA using cashew apple (Anacardium occidentale L.) juice as potential substrate: optimization and characterization", "claims": null}, {"metadata": {"year": 2022}, "authors": ["B. N. Dienye", "O. Agwa", "G. Abu"], "summary": "Polyhydroxyalkanoates (PHA) are renewable, biodegradable biopolymer intracellularly accumulated by wide array of microorganisms as carbon reserve. This study investigates the influence of various cultural conditions on PHA production by a recently isolated local species under submerged fermentation. Six PHA producing strains were identified by 16S rDNA gene sequencing and strain Priestia flexa OWO1 showed satisfactory PHA productivity. The effects of production parameters were investigated and extraction of PHA was carried out using sodium hypochlorite method and maximum amount was detected after 72h. Maximum PHA production was obtained at incubation period of 48h, pH of 7.0 and temperature of 30oC. Amongst the hydrolysate of agro waste used, brewers spent grain (BSG) gave maximum yield of 3.01g/L while beans bran powder gave the highest PHA yields of 3.9 g/L amongst the organic nitrogen sources tested.\u00a0 Analysis of the crude PHA by Fourier Transform Infrared Spectroscopy (FT-IR) showed the presence of methyl, methylene as well as carbonyl functional groups. PHA production was higher after optimizing the production conditions as compared to basal medium therefore the utilization of these cheap renewable resources as alternative substrates for production of PHA make the process cost effective and sustainable.", "title": "Molecular Characterization, Optimization and Production of PHA by Indigenous Bacteria Using Alternative Nutrient Sources as Substrate", "claims": null}, {"metadata": {"year": 2001}, "authors": ["J  Yu"], "summary": "Polyhydroxyalkanoate (PHA) was produced from a starchy wastewater in a two-step process of microbial acidogenesis and acid polymerization. The starchy organic waste was first digested in a thermophilic upflow anaerobic sludge blanket (UASB) reactor to form acetic (60-80%), propionic (10-30%) and butyric (5-40%) acids. The total volatile fatty acids reached 4000 mg l(-1) at a chemical oxygen demand (COD) loading rate of 25-35 g l(-1) day(-1). A carbon balance indicates that up to 43% of the organic carbon in the starchy waste went to the organic acids and the rest to biogas, volatile suspended solids and residual sludge accumulated in the reactor. The acid composition profile was affected by COD loading rate: a medium rate around 9 g l(-1) day(-1) gave a high propionic acid content (29% wt) and a high rate around 26 g l(-1) day(-1) led to a high butyric acid content (34% wt). The acids in the effluent solution after microfiltration were utilized and polymerized into PHA by bacterium Alcaligenes eutrophus in a second reactor. Fifty grams of PHA was produced from 100 g total organic carbon (TOC) utilized, a yield of 28% based on TOC, which is comparable with 55 g PHA per 100 g TOC of pure butyric and propionic acids used. PHA formation from individual acids was further investigated in a semi-batch reactor with three acid feeding rates. With a limited nitrogen source (80-100 mg NH(3) per liter), the active biomass of A. eutrophus, not including the accumulated PHA in cells, was maintained at a constant level (8-9 g l(-1)) while PHA content in the cell mass increased continuously in 45 h; 48% PHA with butyric acid and 53% PHA with propionic acid, respectively. Polyhydroxybutyrate was formed from butyric acid and poly(hydroxybutyrate-hydroxyvalerate) formed from propionic acid with 38% hydroxyvalerate.", "title": "Production of PHA from starchy wastewater via organic acids.", "claims": null}, {"metadata": {"year": 2003}, "authors": ["D. Rhu", "W. H. Lee", "J. Kim", "E. Choi"], "summary": "PHA (polyhydroxyalkanoate) production was attempted with SBRs from food waste. Seed microbes were collected from a sewage treatment plant with a biological nutrient removal process, and acclimated with synthetic substrate prior to the application of the fermented food waste. Laboratory SBRs were used to produce PHA with limited oxygen and nutrients. The maximum content of 51% PHA was obtained with an anaerobic/aerobic cycle with P limitation, and the yield was estimated to be about 0.05 gPHA(produced)/gCOD(applied) or 25 kg PHA/dry ton of food waste, assuming more than 40% of the PHA contents were recoverable. PHB/PHA ratios were 0.74 to 0.77 due to the higher acetate concentrations. Economical analysis seemed to suggest the PHA produced from the food waste could be an alternative material to produce the biodegradable plastic to be used for the collection bags for solid waste.", "title": "Polyhydroxyalkanoate (PHA) production from waste.", "claims": null}, {"metadata": {"year": 2003}, "authors": ["J. Kim", "E. Choi"], "summary": "PHA (polyhydroxyalkanoate) production was attempted with SBRs from food waste. Seed microbes were collected from a sewage treatment plant with a biological nutrient removal process, and acclimated with synthetic substrate prior to the application of the fermented food waste. Laboratory SBRs were used to produce PHA with limited oxygen and nutrients. The maximum content of 51% PHA was obtained with an anaerobic/aerobic cycle with P limitation, and the yield was estimated to be about 0.05 gPHAproduced/gCODapplied or 25 kg PHA/dry ton of food waste, assuming more than 40% of the PHA contents were recoverable. PHB/PHA ratios were 0.74 to 0.77 due to the higher acetate concentrations. Economical analysis seemed to suggest the PHA produced from the food waste could be an alternative material to produce the biodegradable plastic to be used for the collection bags for solid waste.", "title": "WST 48.8 EB6.1 corr", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Jason M. Smith", "M. Brzezinski", "J. Melack", "Robert J. Miller", "D. Reed"], "summary": "Nitrate concentrations routinely fall below levels required to sustain growth of giant kelp (Macrocystis pyrifera) during summer and autumn in the Santa Barbara Channel, yet growth continues. We found urea to be consistently present at concentrations of 0.48\u20131.82 \u03bcM, accounting for greater than 20% of the dissolved fixed nitrogen pool during summer (14% overall). Field experiments indicate direct uptake of urea by giant kelp at a rate of 0.19 \u03bcmol N g dw\u22121 h\u22121, comparable to rates for ammonium (0.18 \u03bcmol N g dw\u22121 h\u22121) but lower than for nitrate (0.39 \u03bcmol N g dw\u22121 h\u22121). Co\u2010occurring phytoplankton took up nitrate, urea, and ammonium, 2\u2010, 15\u2010, and 39\u2010fold faster than giant kelp; however, the nitrogen uptake advantage of phytoplankton varies by substrate and season. Together, our results suggest that urea is readily used by giant kelp and may help to sustain growth throughout the year.", "title": "Urea as a source of nitrogen to giant kelp (Macrocystis pyrifera)", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Imam Prasetyo", "Theresia Evila", "Teguh Ariyanto"], "summary": "Urea is a primary nitrogen source for plant. Conventional urea fertilizer is made from reaction between ammonia and carbon dioxide. Wastewater of urea manufacture usually contains urea ammonia in a high concentration. They can be as high as 650-4000 ppm urea and 100 \u2013 1300 ppm ammonia/m3 wastewater. High concentrated urea and ammonia disposal to environment may lead to eutrophication in aquatic ecosystem which cause adverse impact to aquatic organism. Therefore, treatment to take urea up from urea manufacture wastewater is of interest that give double benefits : reduce urea from wastewater to meet an environmentally safe condition and obtain a low cost urea fertilizer for plant. The purpose of this study is to determine urea adsorption capacity of porous carbon in aqueous solution contains ammonia. The porous carbon as adsorbent was made from coconut shell by pyrolisis. Oxidation treatment of carbon surface was performed using sulfuric acid (50%w) at 90oC temperature for 2 hours. The adsorption was conducted at room temperature with initial urea concentration in the range of 500-8000 ppm using urea-ammonia solution as simulation liquid. Results reveal that urea adsorption capacity increase significantly 41%.in urea-ammonia solution compare to urea solution, that is in the range of 27-444 mg/g carbon.", "title": "PENGARUH AMONIA DALAM LARUTAN TERHADAP KAPASITAS ADSORPSI UREA DENGAN KARBON BERPORI", "claims": null}, {"metadata": {"year": 2001}, "authors": ["M. Asad", "M. Asghar", "M. A. Sheikh"], "summary": "The fermentation conditions such as nitrogen source, corn steep liquor, yeast extract and cane molasses were optimized for aamylase production. Aspergillus niger produced maximum alpha-amylase (21.20 IU/ml/min) in the optimum growth medium of waste bread (1.5%), containing 0.2% (NH4hS04, 1.0% corn steep liquor, 0.10% yeast extract and 1.0% cane molasses (added after 12 hr). (NH4hS04 proved to be the best additional nitrogen source amongst urea, NH4H2P04 and NaNOJ\u2022 Alphaamylase produced under optimum conditions was partially purified and maximally isolated by 50% (NH4hS04 precipitation. Specific activity of the enzyme increased by 7.19 folds after (NH4hS04 precipitation.", "title": "PRODUCTION OF a-AMYLASE BY Aspergillus niger AND ITS PARTIAL PURIFICATION", "claims": null}], "query": "0.54 g/L urea as nitrogen source for PHA production", "summary_abstract": "The research question regarding the use of 0.54 g/L urea as a nitrogen source for polyhydroxyalkanoate (PHA) production has been addressed in several studies, highlighting the effectiveness of urea in promoting microbial growth and PHA synthesis.\n\nArumugam et al. (2019) identified urea as the optimal nitrogen source for the growth of *Cupriavidus necator*, leading to a significant PHA yield of 15.78 g/L under optimized conditions. Their study emphasized the importance of urea in enhancing the economic viability of PHA production by utilizing low-cost substrates like cashew apple juice.\n\nIn a broader context, Dienye et al. (2022) explored various nitrogen sources, including organic options, and found that while beans bran powder yielded the highest PHA production, urea's role as a nitrogen source remains critical in optimizing microbial growth conditions. This suggests that urea can be effectively integrated into diverse fermentation processes for PHA production.\n\nYu (2001) also demonstrated that limited nitrogen sources, including urea, can sustain the growth of *Alcaligenes eutrophus*, which is known for its ability to produce PHA from organic acids derived from starchy waste. This indicates that urea can support PHA production even in complex waste-derived substrates.\n\nOverall, the collective findings from these studies underscore the potential of urea, particularly at concentrations around 0.54 g/L, to serve as an effective nitrogen source for enhancing PHA production across various microbial systems and substrates. The use of urea not only supports microbial growth but also contributes to the economic feasibility of producing biodegradable plastics from renewable resources.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2022}, "authors": ["Clarence Clarence", "Keni Keni"], "summary": "This study was conducted to predict purchase intention based on digital marketing, customer engagement, and customer perceived value. This study used a quantitative descriptive method by distributing online questionnaires via Google form to respondents who know and have purchase intentions. This study also used 181 samples which 160 data can be used and analysed using Partial Least Squares-Structural Equation Modelling (PLS-SEM) with the help of Smart PLS software. The results of data analysis showed that digital marketing, customer engagement, and brand preferences can predict purchase intentions. The results of this study are expected to provide implications for companies to determine the right strategy to create and increase consumer buying intentions.", "title": "The Prediction of Purchase Intention Based on Digital Marketing, Customer Engagement, and Brand Preference", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Ni Hongyao"], "summary": "Online purchase intention is one of the most important research areas in management information systems and marketing science. This study extends theory of planned behavior (TPB) by including six important antecedents to online purchase intention. This study is conducted with a survey of 234 college students who have online shopping experiences. The collected survey data is used to test each hypothesis developed by using SPSS18 and AMOS18 in the research model. The results of data analysis confirm perceived ease of use (PEOU), perceived risk, and trust are essential antecedents in determining online purchase intention through behavioral attitude and perceived behavioral control.", "title": "Purchase Intention in B 2 C E-Commerce : An Empirical Study", "claims": null}, {"metadata": {"year": 2011}, "authors": ["\u738b\u5049\u6210"], "summary": "Purchase intention is an important attitude in consumer behavior research. Increasing purchase intention in consumers is the main goal for a company. This is the reason why many surveys measure customers' attitude toward a product or brand and their willingness to purchase. This is the question that the seller wants to understand from its customers. The willingness of its client to repurchase its products has a strong and direct impact on the performance of the enterprise. This research studies whether customer satisfaction, brand trust, and brand preference toward a certain brand have significant effects on consumers' intention to purchase other product of the brand. In addition, the relationship between these three variables are examined as well. This study will offer several managerial implications on how to enhance customers purchase intention in practice.", "title": "\u5982\u4f55\u8b93\u860b\u679c\u4f7f\u7528\u8005\u8cfc\u8cb7Mac\uff1a\u6eff\u610f\u5ea6\u3001\u54c1\u724c\u504f\u597d\u3001\u54c1\u724c\u4fe1\u4efb\u8207\u8cfc\u8cb7\u610f\u9858\u4e4b\u7814\u7a76", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Faiza Rasheed Anas Zia Sohail Younus"], "summary": "In the worst competitive market the consumer products manufacturing industries pay attention on customer purchase intention for maintain their repute in market and enhanced their goodwill. Because loyal customer are good source for create revenue. This study learns and contributes the factors that affect customer purchase intention. The purpose of this study is to observe the effect of independent variable (customer knowledge ,purchase intention, celebrity endorsement and perceived value ) on dependent variable (purchase intention).The study describe that the relation between dependent variable have significant relationship with purchase intention. This is quantitative study and sample size of this study is 100. And 100 questionnaires were used for collection of data. The results of this study shows that perceived value, customer knowledge, celebrity endorsement have significant relationship with purchase intention.", "title": "Identifying the Factors Affecting Customer Purchase Intention", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Lydia Sintia", "Yolanda Masnita Siagian", "K. Kurniawati"], "summary": "Research aims: The research aims to determine the determinants of purchase intention in social commerce.Design/Methodology/Approach: The population of this study was online shoppers. Questionnaires were distributed in Google Forms through various platforms. The sampling technique employed purposive sampling by drawing samples from 372 respondents. The data had been confirmed for their reliability and validity. The data analysis methods utilized were both quantitative and descriptive. Then, Structural Equation Modeling (SEM) was used for quantitative analysis.Research findings: The research uncovered that trust, attitude, and perceived usefulness significantly affected purchase intention. Then, the trust had a significant effect on attitude. Perceived ease of use also significantly affected subjective norms and perceived usefulness. However, this study highlights that subjective norms and alternative evaluations did not affect purchase intention.Theoretical contribution/Originality: This research enriches the reading of researchers on related subjects. Another contribution is that this research demonstrates that strengthening trust, attitude, and perceived usefulness in social commerce will increase purchase intention.Practitioner/Policy implication: This research will help other researchers, managers, investors, and government consider how social commerce opportunities can drive customer purchase intention. Research limitation/Implication: Since this research only discussed social media popular today, further research can discuss other developing social media. In addition, this study only discussed several variables affecting purchase intention. Hence, subsequent research can explain the role of artificial intelligence in encouraging purchase intention in social commerce.", "title": "The Determinants of Purchase Intention in Social Commerce", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Shihong Bi"], "summary": "One of hot topics in online group-buying research is how to improve consumer's willingness to buy and gather their demands effectively. Based on previous research, this paper discusses factors that affect consumer's purchasing intention, such as demand externality, price satisfactory, and perceived trust, and develops a theoretical model. The results of empirical study proved that price satisfactory is the most important factor affecting consumer's purchasing intention, while demand externality and perceived trust also have partial influence. Finally this research also offers managerial implications and reference for future research.", "title": "Consumer Purchasing Intention in Online Group-buying: An Empirical Study", "claims": null}, {"metadata": {"year": 1960}, "authors": ["J. Namias"], "summary": "An earlier article by the same author in the JOURNAL OF MARKETING studied fulfillment of intentions to buy, or not to buy, as a function of income, liquid assets, personal debt, attitudes about one's own financial situation, optimism about market conditions, residence in cities, towns, or open country, and by size of family. It was found, for example, that the greatest number of buyers of household durables comes from consumers who at first do not plan to buy, but later change their minds and do buy. The present article studies fulfillment of intention to buy as a function of expectation of price changes, age, geographic region, and a combination of occupation, education, and income.", "title": "Intentions to Purchase Related to Consumer Characteristics", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Danu Gilang Pinasthika", "A. Suroso", "S. Wulandari"], "summary": "Received: 20 April 2021 This research is aimed to test an empirical study on UNSOED Press. The study took the title: \"The Analysis of Intention to Buy: An Extended Theory of Planned Behavior\". The purpose of this research is to know and find empirical evidence of the influence of Brand Awareness, Price, Attitude, Subjective Norm, Perceived Behavioral Control of Purchase Intention. The population in this study were students in Universitas Jenderal Soedirman. The samples in this study were 211 person. Sample determination using the Convenience sampling method. The analysis tool used is multiple regression. Based on the results of the research and analysis of data shows that:(1) Brand Awareness has no effect on Purchase Intention. (2) Price has a positive effect on Purchase Intention. (3) Attitude has a positive effect on Purchase Intention. 4 Subjective Norm has no effect on Purchase Intention. 5 Perceived Behavioral control has a positive effect on Purchase Intention. The implication of the conclusion above is, UNSOED Press has to build awareness among students or target market. Promotion from faculty and lecturers must be improved. Lecturers can provide suggestions for using UNSOED Press during classroom learning. For the price, respondent said the price was not too expensive and have to maintance quality. Accepted: 24 May 2021", "title": "THE ANALYSIS OF INTENTION TO BUY: AN EXTENDED THEORY OF PLANNED BEHAVIOR", "claims": null}], "query": "why study purchase intention study", "summary_abstract": "The study of purchase intention is crucial for understanding consumer behavior and enhancing marketing strategies. Several papers highlight the various factors influencing purchase intention, providing insights that can guide businesses in their marketing efforts.\n\nClarence and Keni (2022) emphasize the role of digital marketing, customer engagement, and brand preferences in predicting purchase intentions. Their findings suggest that these elements are vital for companies aiming to enhance consumer buying intentions. Similarly, Ni (2017) extends the theory of planned behavior by identifying perceived ease of use, perceived risk, and trust as significant antecedents of online purchase intention, particularly among college students. This underscores the importance of consumer perceptions in shaping their buying decisions.\n\nIn a broader context, Wang (2011) discusses how customer satisfaction, brand trust, and brand preference significantly impact consumers' willingness to purchase. This research indicates that understanding customer attitudes towards products can directly influence a company's performance. Faiza et al. (2015) further contribute to this discourse by identifying factors such as customer knowledge, celebrity endorsement, and perceived value as significant determinants of purchase intention, highlighting the multifaceted nature of consumer decision-making.\n\nLydia et al. (2023) focus on social commerce, revealing that trust, attitude, and perceived usefulness are critical in driving purchase intention. Their study suggests that enhancing these factors can significantly boost consumer engagement in online shopping environments. Additionally, Shihong (2014) identifies price satisfaction as a primary factor affecting purchasing intention, alongside demand externality and perceived trust, indicating that economic considerations are also pivotal in consumer behavior.\n\nLastly, Danu et al. (2021) explore the extended theory of planned behavior, finding that while brand awareness does not significantly affect purchase intention, price, attitude, and perceived behavioral control do. This suggests that while awareness is important, other factors may play a more decisive role in influencing consumer decisions.\n\nCollectively, these studies illustrate that understanding purchase intention is essential for businesses to tailor their marketing strategies effectively. By focusing on factors such as trust, perceived value, and customer engagement, companies can enhance their appeal to consumers and ultimately drive sales.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2012}, "authors": ["R. Lucas-Carrasco", "P. Allerup", "P. Bech"], "summary": "Aim. The objective of our study has been to evaluate the WHO-5 as a new early screening instrument for apathy in a group of elderly persons. Methods. The WHO-5 was compared to the Geriatric Depression Scale (GDS-15). The GDS contains five items measuring well-being and ten items measuring depression. The internal validity of the WHO-5 (total score being a sufficient statistic) was evaluated with both parametric and nonparametric item response theory models. The external validity of the WHO-5 and the GDS was evaluated by ROC using depression as index of validity. Results. The item response theory analyses confirmed that the total score of the WHO-5 is a sufficient statistic. The ROC analysis shows an adequate sensitivity (61%) and specificity (84%). The GDS15 and its two subscales obtained low sensitivity (25\u201342%), but high specificity (90\u201398%). Conclusion. The WHO-5 was found both internally and externally valid when considering decreased positive well-being to be an early indication of apathy reflecting that the wind has begun to be taken out of the \u201cmotivation sail.\u201d", "title": "The Validity of the WHO-5 as an Early Screening for Apathy in an Elderly Population", "claims": null}, {"metadata": {"year": 2017}, "authors": ["A. Bertens", "Justine E. F. Moonen", "M. Waal", "J. Foster-Dingley", "W. Ruijter", "J. Gussekloo", "R. C. Mast", "A. Craen"], "summary": "The Geriatric Depression Scale (GDS)\u20103A, a three\u2010item subset of the GDS\u201015, is increasingly used as a measure for apathy in research settings to assess factors associating with this neuropsychiatric syndrome. We aimed to assess how accurately the GDS\u20103A discriminates between presence and absence of apathy in two populations of community\u2010dwelling older persons, using the Apathy Scale as reference standard.", "title": "Validity of the three apathy items of the Geriatric Depression Scale (GDS\u20103A) in measuring apathy in older persons", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Kah-Yue  Low", "Kai-Shuen  Pheh", "Chee-Seng  Tan"], "summary": "As depression has become a major disease burden in Malaysia, an easy-to-use tool with good psychometric qualities is urgently needed to facilitate the screening process. The current study aimed to investigate the psychometric qualities of the 5-item World Health Organization Well-being Index (WHO-5) and the optimal cut-off point for depression screening in the Malaysian context. A total of 610 undergraduate students answered an online survey consisting of the English version of the WHO-5 and Patient Health Questionnaire-9 (PHQ-9). Results of confirmatory factor analysis supported the single-factor model. The present study found the WHO-5 had a good reliability and concurrent validity. Moreover, receiver operating characteristic analysis indicated that \u226413 was the optimal threshold with acceptable sensitivity and specificity to detect depression. This study concludes the WHO-5 is a brief, valid, and reliable depression screening tool among young adults in Malaysia.", "title": "Validation of the WHO-5 as a screening tool for depression among young adults in Malaysia", "claims": null}, {"metadata": {"year": 2015}, "authors": ["R. Pracheth"], "summary": "Background: WHO-Five-Well-being Index (WHO-5) is an uncomplicated questionnaire used to appraise subjective well-being. This study was conducted to determine the validity and internal consistency of WHO-5 in the appraisal of depression among elderly and establish the agreement between GDS-15 and WHO-5. Methods: After obtaining clearance from the institutional ethics committee, a cross-sectional study was conducted for 6 months in an urban slum on elderly persons aged \u226560 years, with consent. A pre-designed and semi-structured interview schedule was used to collect the information. WHO-5 was used to assess depression. It was translated to local language and then back translated. To confirm the diagnosis of major depression, Mini International Neuropsychiatric Interview (MINI) was adopted. The relationship between WHO-5 mean scores and the grading of depression as per Geriatric Depression Scale Short Form (GDS-15) was determined. Results: A total of 218 study participants were included, of which 44 (20.18%) were depressed. Mean raw WHO-5 score was 14.80 \u00b1 5.14. WHO-5 demonstrated a good internal consistency (Cronbach\u2019s Alpha=0.85). The concurrent validity while considering MINI was gold standard was good. Those found to be normal when subjected to MINI had a significantly lower score (15.28 \u00b1 5.2) when compared to those diagnosed as depressed (7.46 \u00b1 4.31) [t=8.728, df=218, p<0.0001]. When convergent validity was assessed, a good correlation was found between GDS-15 and WHO-5 (r=0.68, p<0.0001). Conclusions: WHO-5 is a valid and reliable tool to assess depression among elderly. The elderly who present to health centres may be subjected to screening of depression and well-being, using WHO-5.", "title": "The utility of WHO-five-well-being index as a screening tool for depression among elderly", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Reinhard  Heun", "Marzia  Bonsignore", "Katrin  Barkow", "Frank  Jessen"], "summary": "Background Depression has a high prevalence in the elderly population; however it often remains undetected. The WHO 5-item Well-Being Index (WHO-5) is a short screening instrument for the detection of depression in the general population, which has not yet been evaluated. The goals of the present study were: 1) to assess the internal and external validity of WHO-5 and 2) to compare the two recent versions of WHO-5.Study population and methods 367 subjects above 50 years of age were examined with the WHO-5. ICD-10 diagnoses were made using a structured interview (CIDI). The internal validity of the well-being index was evaluated by calculating Loevinger\u2019s and Mokken\u2019s homogeneity coefficients. External validity for detection of depression was evaluated by ROC analysis.Results The scale was sufficiently homogeneous (Loevinger\u2019s coefficient: version 1=0.38, version 2=0.47; Mokken coefficient \u03c4; 0.3 in nearly all items). ROC analysis showed that both versions adequately detected depression. Version 1 additionally detected anxiety disorders, version 2 being more specific for detection of depression.Conclusion The WHO-5 showed a good internal and external validity. The second version is a stronger scale and was more specific for the detection of depression. The WHO-5 is an useful instrument for identifying elderly subjects with depression.", "title": "Validity of the five-item WHO Well-Being Index (WHO-5) in an elderly population", "claims": null}, {"metadata": {"year": 2012}, "authors": ["R. Lucas-Carrasco"], "summary": "The World Health Organization (WHO)\u2010Five Well\u2010being Index (WHO\u20105) is a short 5\u2010item index designed to assess the level of emotional well\u2010being. The positive questions of the WHO\u20105, shifted towards measuring cheerfulness and the level of energy, work in the screening of depression. This paper describes the psychometric properties of the Spanish WHO\u20105 in older persons.", "title": "Reliability and validity of the Spanish version of the World Health Organization\u2010Five Well\u2010Being Index in elderly", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Jennifer A.  Halliday", "Christel  Hendrieckx", "Lucy  Busija", "Jessica L.  Browne", "Giesje  Nefs", "Fran\u00e7ois  Pouwer", "Jane  Speight"], "summary": "AIMS\nScreening for depression is recommended internationally. The World Health Organization's 5-item Well-being Index (WHO-5) is used clinically to screen for depression but its empirical suitability for this purpose is not well documented. We investigated the psychometric properties of the WHO-5 and its suitability for identifying likely depression in Australian adults with diabetes.\n\n\nMETHODS\nThe Diabetes MILES - Australia study dataset provided a sample of N=3249 who completed the WHO-5 (positively-worded 5-item measure of emotional well-being) and the PHQ-9 (9-item measure of depressive symptoms). Analyses were conducted for the full sample, and separately by diabetes type and treatment (type 1, non-insulin-treated type 2, and insulin-treated type 2 diabetes). Construct (convergent and factorial) validity and reliability of the WHO-5 were examined. ROC analyses were used to examine the sensitivity and specificity of the WHO-5 as a depression screening instrument, comparing two commonly used WHO-5 cut-off values (\u22647 and <13) with the PHQ-9.\n\n\nRESULTS\nFor the whole sample, the WHO-5 demonstrated satisfactory internal consistency reliability (\u03b1=0.90) and convergent validity with the PHQ-9 (r=-0.73, p<0.001). Confirmatory factor analysis partially supported factorial validity: \u03a72(5)=834.94, p<0.001; RMSEA=0.23, 90% CI 0.21-0.24; CFI=0.98, TLI=0.96; factor loadings=0.78-0.92. The AUC was 0.87 (95% CI: 0.86-0.89, p<0.001). The sensitivity/specificity of the WHO-5 for detecting likely depression was 0.44/0.96 for the \u22647 cut-off, and 0.79/0.79 for the <13 cut-off, with similar findings by diabetes type and treatment.\n\n\nCONCLUSIONS\nThese findings support use of a WHO-5 cut-point of <13 to identify likely depression in Australian adults with diabetes, regardless of type/treatment.", "title": "Validation of the WHO-5 as a first-step screening instrument for depression in adults with diabetes: Results from Diabetes MILES - Australia.", "claims": null}, {"metadata": {"year": 2019}, "authors": ["S  Eriksen", "G H Bj\u00f8rkl\u00f8f", "A-S  Helvik", "M  Larsen", "K  Engedal"], "summary": "BACKGROUND\nLittle is known about the validity of the Norwegian versions of the Geriatric Depression Scale-5 (GDS-5) and the Hospital Anxiety and Depression Scale-D (HADS-D). The aim of this study was therefor to validate the two assessment tools in a population of home-dwelling persons of 60 years of age and above.\n\n\nMETHODS\nA sample of 194 home-dwelling old adults with and without depressive symptoms were recruited. The participants were examined for depressive symptoms (GDS-5, HADS-D) and cognitive impairment. Sociodemographic information was collected. The participants underwent a blinded diagnostic evaluation for a depressive episode according to the diagnostic criteria of ICD-10.\n\n\nRESULTS\nIn all, 56 (28.9%) participants fulfilled criteria for a depressive episode according to ICD-10. The Receiver Operating Characteristics analyses of HAD-D and GDS-5 using the diagnostic criteria of ICD-10 for depression as gold standard was performed. For GDS-5 the Areal under the Curve was 0.81 and for HAD-D 0.75. The cut-off points of the measures that produced the highest accuracies were \u22652 for GDS-5 with a sensitivity of 73.2% and a specificity of 73.2% and \u22654 for HADS-D with a sensitivity of 70.3% and a specificity of 69.6%.\n\n\nLIMITATIONS\nA larger sample would have given the opportunity for analyzing home dwelling old adults with and without home health care separately. The participants were talked through the self-filling questionnaires. The procedure could have influenced the participants' answers.\n\n\nCONCLUSION\nGDS-5 and HADS-D are useful screening tools for old adults, but only fairly good to identify depression according to criteria of ICD-10.", "title": "The validity of the hospital anxiety and depression scale and the geriatric depression scale-5 in home-dwelling old adults in Norway\u2730.", "claims": null}], "query": "Aim. The objective of our study has been to evaluate the WHO-5 as a new early screening instrument for apathy in a group of elderly persons. Methods. The WHO-5 was compared to the Geriatric Depression Scale (GDS-15). The GDS contains five items measuring well-being and ten items measuring depression. The internal validity of the WHO-5 (total score being a sufficient statistic) was evaluated with both parametric and nonparametric item response theory models. The external validity of the WHO-5 and the GDS was evaluated by ROC using depression as index of validity. Results. The item response theory analyses confirmed that the total score of the WHO-5 is a sufficient statistic. The ROC analysis shows an adequate sensitivity (61%) and specificity (84%). The GDS15 and its two subscales obtained low sensitivity (25\u201342%), but high specificity (90\u201398%). Conclusion. The WHO-5 was found both internally and externally valid when considering decreased positive well-being to be an early indication of apathy reflecting that the wind has begun to be taken out of the \u201cmotivation sail.\u201d", "summary_abstract": "The research question focuses on evaluating the WHO-5 as a screening tool for apathy in elderly individuals, particularly in comparison to the Geriatric Depression Scale (GDS-15). The findings from various studies collectively support the validity and reliability of the WHO-5 in assessing emotional well-being and its potential as an early indicator of apathy.\n\nLucas-Carrasco et al. (2012) conducted a study that confirmed the internal and external validity of the WHO-5, demonstrating that it effectively measures decreased positive well-being, which can signal early signs of apathy. Their analysis revealed a sensitivity of 61% and specificity of 84%, indicating that the WHO-5 is a robust tool for screening purposes. In contrast, the GDS-15 showed lower sensitivity (25-42%) but maintained high specificity (90-98%), suggesting that while the GDS-15 is specific, it may not be as effective in identifying apathy early on.\n\nBertens et al. (2017) highlighted the GDS-3A, a subset of the GDS-15, as a measure for apathy, but did not provide direct comparisons with the WHO-5. However, the emphasis on the GDS-3A indicates a growing interest in shorter, more efficient screening tools for apathy in elderly populations.\n\nPracheth (2015) further validated the WHO-5 in an elderly cohort, finding a strong correlation with the GDS-15 (r=0.68, p<0.0001), reinforcing the WHO-5's reliability as a screening instrument for depression and, by extension, apathy. The study reported good internal consistency (Cronbach\u2019s Alpha=0.85) and concurrent validity when compared to a structured diagnostic interview.\n\nHeun et al. (2009) also supported the WHO-5's effectiveness in detecting depression among older adults, noting its good internal and external validity. This suggests that the WHO-5 is not only suitable for depression screening but may also be applicable in identifying related conditions such as apathy.\n\nIn summary, the WHO-5 has been shown to be a valid and reliable tool for early screening of apathy in elderly individuals, with strong psychometric properties and a favorable comparison to the GDS-15. The collective evidence suggests that the WHO-5 could serve as an effective instrument for identifying decreased well-being and potential apathy in this demographic (Lucas-Carrasco et al., 2012; Pracheth, 2015; Heun et al., 2009).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2008}, "authors": ["U. O. D. Silva", "E. sanders-Bush"], "summary": "In\u2010Vitro studies of RNA editing of the serotonin type 2C receptor (5\u2010HT2CR) have shown a pronounced loss of function in the fully edited isoform (VGV). Our goal is to evaluate functional coupling of the edited 5\u2010HT2CR in\u2010vivo using transgenic mice that solely express the VGV isoform. In order to accomplish this goal, we must be able to establish selective activation of the receptor both in\u2010vitro and in\u2010vivo. This makes ligand specificity of the utmost importance. Since pharmacological studies are not available for mice, we compared the relative affinity of antagonists for 5\u2010HT2A versus 5\u2010HT2C receptors, confirming that M100907 and SB206553 were relatively selective. In order to examine receptor specificity in\u2010vivo, N\u2010ethoxycarbonyl\u20102\u2010ethoxy\u20101,2\u2010dihydroquinoline (EEDQ), an irreversible alkylating agent, was injected i.p. to inactivate sites in frontal cortex. We then tested the ability of pretreatment with antagonists to block EEDQ. While 0.25 mg/kg of the 5\u2010HT2A antagonist M100907 prevented 5\u2010HT2A inactivation, it did not protect 5HT2C sites. In contrast, the 5HT 2C antagonist SB206553 did not protect 5\u2010HT2A receptors. (Supported by NIH grants T32MH065782 and R01MH34007)", "title": "Specificity of 5\u2010HT2A Receptor Ligands in Mice", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Caroline J Goodacre", "Steven M Bromidge", "David  Clapham", "Frank D King", "Peter J Lovell", "Mike  Allen", "Lorraine P Campbell", "Vicky  Holland", "Graham J Riley", "Kathryn R Starr", "Brenda K Trail", "Martyn D Wood"], "summary": "Bisaryl cyclic ureas have been identified as high affinity 5-HT2C receptor antagonists with selectivity over 5-HT2A and 5-HT2B. Compounds such as 8 and 22 have shown oral activity in a centrally mediated pharmacodynamic model of 5-HT2C function in rodents.", "title": "A series of bisaryl imidazolidin-2-ones has shown to be selective and orally active 5-HT2C receptor antagonists.", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Aaron M. Bender", "M. Valentine", "Joshua A. Bauer", "Emily L. Days", "C. Lindsley", "W. D. Merryman"], "summary": "Antagonists of the serotonin receptor 2B (5-HT2B) have shown great promise as therapeutics for the treatment of pulmonary arterial hypertension, valvular heart disease, and related cardiopathies. Herein, we describe a high-throughput screen campaign that led to the identification of highly potent and selective 5-HT2B antagonists. Furthermore, selected compounds were profiled for their predicted ability to cross the blood-brain barrier. Two exemplary compounds, VU0530244 and VU0631019, were predicted to have very limited potential for brain penetration in human subjects, a critical profile for the development of 5-HT2B antagonists devoid of centrally-mediated adverse effects.", "title": "Identification of Potent, Selective, and Peripherally Restricted Serotonin Receptor 2B Antagonists from a High-Throughput Screen.", "claims": null}, {"metadata": {"year": 2002}, "authors": ["L. de Angelis"], "summary": "Several lines of evidence support a role for serotoninergic (5-HT) system abnormalities in the pathogenesis and treatment of several psychiatric disorders. This review summarizes information about the association between the 5-HT2A receptor gene and its relevance to schizophrenia, tardive dyskinesia, major depression, suicidality, anorexia nervosa and obsessive-compulsive disorder. Evidence is presented that implies that selective 5-HT2A antagonists may be considered useful in investigating the role of 5-HT2A receptor function and in the treatment of psychosis, and possibly alcohol and cocaine dependence. Additionally, findings are reviewed on the importance of 5-HT2A receptor antagonism in contributing to the therapeutic effect of several clinically effective and potential atypical antipsychotics as well as several antidepressants. In conclusion, the ability of selective 5-HT2A receptor antagonists to interfere with the heightened state of dopamine activity without altering basal tone, suggests that these drugs possess antipsychotic activity and may provide the basis for new therapies for psychosis and drug dependence, in addition to contributing towards a more complete understanding of 5-HT2A receptor function.", "title": "5-HT2A antagonists in psychiatric disorders.", "claims": null}, {"metadata": {"year": 2002}, "authors": ["L. Deangelis"], "summary": "Several lines of evidence support a role for serotoninergic (5-HT) system abnormalities in the pathogenesis and treatment of several psychiatric disorders. This review summarizes information about the association between the 5-HT2A receptor gene and its relevance to schizophrenia, tardive dyskinesia, major depression, suicidality, anorexia nervosa and obsessive-compulsive disorder. Evidence is presented that implies that selective 5-HT2A antagonists may be considered useful in investigating the role of 5-HT2A receptor function and in the treatment of psychosis, and possibly alcohol and cocaine dependence. Additionally, findings are reviewed on the importance of 5-HT2A receptor antagonism in contributing to the therapeutic effect of several clinically effective and potential atypical antipsychotics as well as several antidepressants. In conclusion, the ability of selective 5-HT2A receptor antagonists to interfere with the heightened state of dopamine activity without altering basal tone, suggests that these drugs possess antipsychotic activity and may provide the basis for new therapies for psychosis and drug dependence, in addition to contributing towards a more complete understanding of 5-HT2A receptor function.", "title": "5-HT2A antagonists in psychiatric disorders.", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Austen B Casey", "Meng  Cui", "Raymond G Booth", "Clinton E Canal"], "summary": "Blockade of the serotonin 5-HT2A G protein-coupled receptor (5-HT2AR) is a fundamental pharmacological characteristic of numerous antipsychotic medications, which are FDA-approved to treat schizophrenia, bipolar disorder, and as adjunctive therapies in major depressive disorder. Meanwhile, activation of the 5-HT2AR by serotonergic psychedelics may be useful in treating neuropsychiatric indications, including major depressive and substance use disorders. Serotonergic psychedelics and other 5-HT2AR agonists, however, often bind other receptors, and standard 5-HT2AR antagonists lack sufficient selectivity to make well-founded mechanistic conclusions about the 5-HT2AR-dependent effects of these compounds and the general neurobiological function of 5-HT2ARs. This review discusses the limitations and strengths of currently available \"selective\" 5-HT2AR antagonists, the molecular determinants of antagonist selectivity at 5-HT2ARs, and the utility of molecular pharmacological and computational methods in guiding the discovery of novel unambiguously selective 5-HT2AR antagonists.", "title": "\"Selective\" serotonin 5-HT2A receptor antagonists.", "claims": null}, {"metadata": {"year": 2004}, "authors": ["G. Poissonnet", "J. Parmentier", "J. Boutin", "S. Goldstein"], "summary": "5-HT(2) receptors mediate a large array of physiological and behavioral functions in humans via three distinct subtypes: 5-HT(2A), 5-HT(2B)and 5-HT(2C). While selective 5-HT(2A)antagonists have been known for some time, knowledge of the precise role played by the 5-HT(2B)receptor was hampered by the existence of solely 5-HT(2B)5-HT(2C) mixed antagonists. However, selective 5-HT(2B)antagonists began recently to emerge in the literature. Indeed, four structural classes belonging to the piperazine, indole, naphthylpyrimidine and tetrahydro-beta-carboline scaffolds were reported. In this paper, we will briefly review the structural and pharmacological features of selective 5-HT(2B) antagonists, including patent literature of the last five years.", "title": "The emergence of selective 5-HT 2B antagonists structures, activities and potential therapeutic applications.", "claims": null}, {"metadata": {"year": 2007}, "authors": ["Dieter  Hamprecht", "Fabrizio  Micheli", "Giovanna  Tedesco", "Daniele  Donati", "Marcella  Petrone", "Silvia  Terreni", "Martyn  Wood"], "summary": "Design, synthesis and properties of a new tricyclic series of selective 5-HT2C receptor antagonists are reported. Conformational analysis of a 2-phenyl-dihydropyrrolone scaffold suggested that ring fusion, locking coplanarity between the rings of this moiety, might be tolerated by the 5-HT2C receptor. An interesting effect of this is the change of the nature of the carbon-carbon double bond of the lactam ring from vinylic to aromatic. The changes were found to result in a favourable profile at both, receptor and in vivo level.", "title": "5-HT2C antagonists based on fused heterotricyclic templates: design, synthesis and biological evaluation.", "claims": null}], "query": "recent experiments using the highly selective 5-HT2A receptor antagonist.", "summary_abstract": "Recent research has focused on the development and evaluation of highly selective 5-HT2A receptor antagonists, which have shown promise in various therapeutic contexts, particularly in psychiatric disorders. Silva and Sanders-Bush (2008) conducted in-vivo studies that confirmed the selective action of the 5-HT2A antagonist M100907, which effectively prevented the inactivation of 5-HT2A receptors without affecting 5-HT2C receptors. This specificity is crucial for understanding the functional roles of these receptors in the brain.\n\nMoreover, De Angelis (2002) highlighted the potential of selective 5-HT2A antagonists in treating psychosis and substance dependence, suggesting that these compounds could modulate dopamine activity without disrupting baseline levels. This characteristic may contribute to their antipsychotic effects, making them valuable in the development of new therapeutic strategies.\n\nIn addition, Casey et al. (2022) discussed the pharmacological significance of 5-HT2A receptor antagonism in the context of antipsychotic medications, which are commonly prescribed for schizophrenia and bipolar disorder. They emphasized the need for more selective antagonists to better understand the neurobiological functions of 5-HT2A receptors and their interactions with other serotonergic compounds.\n\nOverall, the collective findings from these studies underscore the importance of selective 5-HT2A receptor antagonists in both basic research and clinical applications, particularly in the treatment of psychiatric conditions and the exploration of serotonergic mechanisms in the brain.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2012}, "authors": ["L. Makarem", "D. Gillet"], "summary": "In this paper we compare the performance of autonomous vehicles at intersections with respect to the type of information shared. In addition to the position of other vehicles, in each simulated scenario one type of information is available. These types of information include the intended directions at intersection and the inertias of the other vehicles. An existing control method based on navigation functions is modified in order to benefit from the availability of such information. The results show that if autonomous vehicles know each other's inertia they achieve significantly smoother paths, use less fuel and avoid full stops.", "title": "Information sharing among autonomous vehicles crossing an intersection", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Mahmoud Pourmehrab", "L. Elefteriadou", "S. Ranka"], "summary": "The pace at which autonomous vehicle technology is reaching consumers is accelerating. Furthermore, the future driverless cars are intended to be talkative with regard to exchanging information with other vehicles, infrastructure, or even cloud. More recent attention has focused on the provision of frameworks to take advantage of automated and connected vehicle technology. This study proposes a system design to allow safe and efficient traffic of both automated/connected and conventional vehicles approaching an isolated intersection. The benefit mainly comes from the ability of automated/connected vehicles to maintain shorter and more stable headways to the vehicle in ahead compared to conventional vehicles. Simulation experiments are conducted to gain insight into the influence of flow and automated vehicle ratio over the performance of intersection.", "title": "Smart intersection control algorithms for automated vehicles", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Chairit  Wuthishuwong", "Ansgar  Tr\u00e4chtler", "Torsten  Bruns"], "summary": "The development of autonomous vehicle or self-driving car integrates with the wireless communication technology which would be a forward step for road transportation in the near future. The autonomous crossing of an intersection with an autonomous vehicle will play a crucial role in the future of intelligent transportation system (ITS). The fundamental objectives of this work are to manage autonomous vehicles crossing an intersection with no collisions, maintaining that a vehicle drives continuously, and to decrease the waiting time at an intersection. In this paper, a discrete model of the one-way single intersection is designed. The vehicle-to-infrastructure (V2I) communication is implemented to exchange information between a vehicle and an intersection manager which is the roadside infrastructure. The safe trajectory of autonomous vehicles for the autonomous intersection management is determined and presented by using discrete mathematics.", "title": "Safe trajectory planning for autonomous intersection management by using vehicle to infrastructure communication", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Yuying Li", "Qipeng Liu"], "summary": "This paper proposes an intersection management strategy for autonomous vehicles under the vehicle-to-infrastructure circumstance. All vehicles are supposed to be fully autonomous and can communicate with the intersection management unit to check the traffic situation. Priority of passing the intersection is decided by a static conflict matrix which represents the potential conflict between lanes of different directions and a dynamic information list which could capture the real-time occupation of each lane in the intersection. Compared with the existing approaches in the literature, the intersection management unit in our strategy is more like a database rather than a computational center, and therefore, requires less computational resource and more likely satisfies the real-time requirement in heavy traffic situations. Simulations are conducted using SUMO (Simulation of Urban MObility), in which the proposed strategy is compared with both fixed and adaptive traffic light methods. The results indicate that the proposed strategy could significantly reduce the average time delay caused by the intersection and the corresponding variance, which shows the efficiency and fairness of the proposed strategy in intersection management.", "title": "Intersection management for autonomous vehicles with vehicle-to-infrastructure communication", "claims": null}, {"metadata": {"year": 2017}, "authors": ["F. Belkhouche"], "summary": "Safety and optimal traffic flow are important priorities in intelligent transportation systems. This paper presents a collaborative method for collision avoidance between autonomous vehicles at an unsignalized intersection. The collision conditions are expressed in terms of the vehicles' speed ratios. The problem is then formulated and solved as an optimization problem with the safety constraints represented by the speed ratios. The cost function is defined in terms of the deviation from the current speeds. Both pairwise and three vehicle formulations are discussed. The proposed formulation allows to easily scale the solution to higher number of vehicles. The method is highly effective in resolving conflicts and achieving smooth flow. The results are formally proven and illustrated using simulation.", "title": "Control of autonomous vehicles at an unsignalized intersection", "claims": null}, {"metadata": {"year": 2012}, "authors": ["L. Makarem", "D. Gillet"], "summary": "Autonomous vehicles can react faster than humans, they have more accurate perception and do not get distracted, sleepy or intoxicated. Autonomous vehicles could also increase the capacity of our current roads by allowing cars to be driven more closely to each other. Accordingly, driverless cars seem to be imminent. One of the challenges with autonomous vehicles is their performance at intersections. This performance directly depends on the level of cooperation between the vehicles.", "title": "Autonomous vehicles share information crossing an intersection", "claims": null}, {"metadata": {"year": 2019}, "authors": ["J. Khoury", "J. Khoury", "Germain Zouein", "Jean-Paul M. Arnaout"], "summary": "Abstract The majority of research efforts in the field of access control of autonomous vehicles at intersections are geared towards fully connected vehicles. The underlying assumptions for such efforts are active vehicle-to-vehicle (V2V), vehicle-to-infrastructure (V2I), infrastructure-to- vehicle (I2V) communications, and/or presence of a central controller. Though efficiency is proven to be significantly enhanced, the assumptions face inherent security and privacy obstacles and require high infrastructure costs. In previous work, the authors designed and demonstrated a simpler, less costly, and more secure approach to autonomous vehicle management at intersections. The approach allows vehicles to make autonomous decisions at intersections based solely on sensing and/or beacon information with no V2V or V2I communications required. This article extends our model to account for various vehicle classes, all possible turns at the intersection with corresponding safe turning speeds, and various intersection geometries. Compared to a fully actuated signal controller, the proposed and improved model is again proven operationally more efficient, as it reduced the average delay per vehicle by at least 21% and up to 51% for the various simulated scenarios. After 40 million seconds of simulation, the proposed model proved collision free operations.", "title": "A practical decentralized access protocol for autonomous vehicles at isolated under-saturated intersections", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Penglin Dai", "Kai Liu", "Q. Zhuge", "E. Sha", "V. Lee", "S. Son"], "summary": "Recent advances in autonomous vehicles and vehicular communications are envisioned to enable novel approaches to managing and controlling traffic intersections. In particular, with intersection controller units (ICUs), passing vehicles can be instructed to cross the intersection safely without traffic signals. Previous efforts on autonomous intersection control mainly focused on guaranteeing the safe passage of vehicles and improving intersection throughput, without considering the quality of the travel experience from the passengers' perspective. In this paper, we aim to design an enhanced autonomous intersection control mechanism, which not only ensures vehicle safety and enhances traffic efficiency but also cares about the travel experience of passengers. In particular, we design the metric of smoothness to quantitatively capture the quality of experience. In addition, we consider the travel time of individual vehicles when passing the intersection in scheduling to avoid a long delay of some vehicles, which not only helps with improving intersection throughput but also enhances the system's fairness. With the above considerations, we formulate the intersection control model and transform it into a convex optimization problem. On this basis, we propose a new algorithm to achieve an optimal solution with low overhead. Finally, we build the simulation model and implement the algorithm for performance evaluation. Comprehensive simulation results demonstrate the superiority of the proposed algorithm.", "title": "Quality-of-Experience-Oriented Autonomous Intersection Control in Vehicular Networks", "claims": null}], "query": "In the context of autonomous vehicles, the importance of the control zone, where vehicles exchange information about the autonomous intersection, and the case of overlaps of different intersections", "summary_abstract": "The research on autonomous vehicles at intersections highlights the critical role of information exchange within control zones, particularly in managing the complexities of overlapping intersections. Several studies emphasize the importance of vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communication in enhancing the performance and safety of autonomous vehicles navigating these areas.\n\nMakarem and Gillet (2012) illustrate that sharing information about vehicle inertia and intended directions significantly improves the smoothness of paths taken by autonomous vehicles, leading to reduced fuel consumption and fewer stops. This finding underscores the necessity of effective communication among vehicles to optimize intersection performance.\n\nPourmehrab et al. (2017) further explore the integration of automated and connected vehicles at isolated intersections, demonstrating that these vehicles can maintain shorter and more stable headways compared to conventional vehicles. This capability is crucial for efficient traffic flow, particularly in scenarios where different types of vehicles interact.\n\nWuthishuwong et al. (2015) focus on the management of autonomous vehicles at intersections, proposing a model that utilizes V2I communication to facilitate collision-free crossings and minimize waiting times. This approach highlights the potential of infrastructure to play a pivotal role in intersection management.\n\nLi and Liu (2020) introduce an intersection management strategy that relies on a static conflict matrix and real-time lane occupation data, showcasing how a database-like management unit can efficiently handle traffic without heavy computational demands. Their simulations indicate a significant reduction in average time delays at intersections, emphasizing the effectiveness of real-time information sharing.\n\nBelkhouche (2017) presents a collaborative method for collision avoidance at unsignalized intersections, demonstrating that optimization based on speed ratios can enhance safety and traffic flow. This method aligns with the broader theme of utilizing shared information to resolve conflicts among vehicles.\n\nKhoury et al. (2019) challenge the reliance on extensive communication systems by proposing a model that allows autonomous vehicles to make decisions based on local sensing and beacon information. Their findings suggest that such an approach can be both cost-effective and efficient, reducing delays and ensuring collision-free operations.\n\nLastly, Dai et al. (2016) emphasize the need for intersection control mechanisms that not only ensure safety but also enhance the travel experience for passengers. Their proposed algorithm aims to balance safety, efficiency, and passenger comfort, further illustrating the multifaceted considerations in managing autonomous vehicles at intersections.\n\nCollectively, these studies underscore the importance of information exchange in the control zone of autonomous vehicles, particularly in the context of overlapping intersections. The integration of communication technologies and innovative management strategies is essential for optimizing traffic flow and ensuring safety in future intelligent transportation systems.", "summary_extract": null}, {"papers": [{"metadata": {"year": 1980}, "authors": ["C  Muhr", "K  Bergstr\u00f6m", "R  Hugosson", "P O Lundberg"], "summary": "A study of the recurrence rate in 41 patients operated and postoperatively treated with irradiation for a pituitary adenoma was performed 10-15 years after operation. 15 of the 41 patients are now deceased among whom 1 had a known recurrence. 25 out of the now 26 living patients were investigated with a clinical examination, perimetry, computed tomography (CT), roentgen examination of the sella turcica and an endocrine evaluation. The outstanding capability of the CT to visualize pituitary adenoma recurrences was clearly demonstrated as this technique revealed recurrences in 6 patients, only 2 of which had progressive visual field defects and none progressive changes on plain sellar films.", "title": "Pituitary adenomas: computed tomography and clinical evaluation in a follow-up after surgical treatment.", "claims": null}, {"metadata": {"year": 1981}, "authors": ["C. Muhr", "K. Bergstr\u00f6m", "R. Hugosson", "P. Lundberg"], "summary": "A study of the recurrence rate in 41 patients operated and postoperatively treated with irradiation for a pituitary adenoma was performed 10-15 years after operation. 15 of the 41 patients are now deceased among whom 1 had a known recurrence. 25 out of the now 26 living patients were investigated with a clinical examination, perimetry, computed tomography (CT), roentgen examination of the sella turcica and an endocrine evaluation. The outstanding capability of the CT to visualize pituitary adenoma recurrences was clearly demonstrated as this technique revealed recurrences in 6 patients, only 2 of which had progressive visual field defects and none progressive changes on plain sellar films.", "title": "Pituitary adenomas: computed tomography and clinical evaluation in a follow-up after surgical treatment.", "claims": null}, {"metadata": {"year": 1986}, "authors": ["J. Trouillas", "C. Girod", "G. Sassolas", "B. Claustrat"], "summary": "Twenty-six out of 400 surgically removed pituitary tumors were identified as gonadotropic adenomas (frequency 6.25%). Morphologic, immunocytochemical, and hormonal characteristics of the gonadotropic adenoma are described. The following morphologic characteristics may suggest the diagnosis: arrangement in cords of cells showing signs of cellular activity, secretory granules, which vary in electron density, form, and size (mean diameter 150 nm) and which are numerous in the extensions near the capillaries, and a rough endoplasmic reticulum arranged in short cisternae. However, owing to the morphofunctional variations from one adenoma to another, we consider that only immunoreactivity with gonadotropin antisera proves the diagnosis. Because the material used to generate the gonadotropin antisera was purified but not pure, for precise characterization of the immunoreactivity, absorption tests with various antigens have to be performed. FSH-LH adenomas (n = 14), FSH adenomas (n = 7), and alpha-subunit adenomas (n = 5) but no LH adenomas were identified in our series. No evident morphologic difference related to the type of immunoreactivity and to the sex was found. Almost all the tumors were large with visual signs caused by suprasellar extension. A recurrence following adenomectomy was noted with a frequency of 12%. The diagnosis of gonadotropic adenoma was considered preoperatively in six out of 26 patients only, on the basis of increased serum gonadotropin levels. Correlations between hormonal levels and pathologic data were established in 17 patients. Seven tumors were associated with high serum gonadotropin levels (FSH and LH: three patients and FSH alone: four patients). In ten cases, the serum gonadotropin levels were normal but serum alpha-subunit assay had not been performed. For the pathologist systematically testing the adenomas with many antisera, gonadotropic adenomas are not as rare as for the clinician. Immunocytochemical studies with gonadotropin antisera and serum gonadotropic determination must be performed in all tumors, especially in those pituitary adenomas that appear nonfunctional.", "title": "The human gonadotropic adenoma: pathologic diagnosis and hormonal correlations in 26 tumors.", "claims": null}, {"metadata": {"year": 1995}, "authors": ["P. Beauchesne", "J. Trouillas", "F. Barral", "J. Brunon"], "summary": "A 37-year-old man developed multiple intracranial, intraspinal, and general metastases from an invasive nonfunctioning pituitary adenoma after surgery and radiation therapy. This is the first gonadotropic pituitary carcinoma reported in literature.", "title": "Gonadotropic pituitary carcinoma: case report.", "claims": null}, {"metadata": {"year": 1995}, "authors": ["P.  Beauchesne", "Jacqueline  Trouillas", "Fabrice  Barral", "J.  Brunon"], "summary": ": A 37-year-old man developed multiple intracranial, intraspinal, and general metastases from an invasive nonfunctioning pituitary adenoma after surgery and radiation therapy. This is the first gonadotropic pituitary carcinoma reported in literature.", "title": "Gonadotropic pituitary carcinoma: case report.", "claims": null}, {"metadata": {"year": 2002}, "authors": ["Toshiaki  Sano", "Kalman  Kovacs", "Sylvia L  Asa", "Shozo  Yamada", "Naoko  Sanno", "Shunichi  Yokoyama", "Hiroshi  Takami"], "summary": "An adenoma resected from a 49-yr-old woman was clinically nonfunctioning but immunopositive for adrenocorticotropic hormone (ACTH) in a few cells. Electron microscopy demonstrated a vacuolar change in Golgi complexes known as \u201choneycomb Golgi.\u201d Recurrent adenoma resected 11 mo later was positive for ACTH in more cells than in the initial one. Two months later the patient presented with typical signs and symptoms of Cushing disease with elevated blood cortisol and ACTH levels and was treated by gamma knife radiotherapy. Among adenomas that are initially clinically silent but became functionally active with recurrence, corticotroph adenomas seem to be the most frequent. Study of transcription factors will be helpful to clarify whether adenomas with honeycomb Golgi represent exclusively gonadotroph adenomas.", "title": "Pituitary adenoma with \u201cHoneycomb golgi\u201d appearance showing a phenotypic change at recurrence from clinically nonfunctioning to typical cushing disease", "claims": null}, {"metadata": {"year": 2000}, "authors": ["I. McCutcheon", "D. Pieper", "G. Fuller", "R. Benjamin", "K. Friend", "R. Gagel"], "summary": "OBJECTIVE AND IMPORTANCE\nPituitary carcinomas are extremely rare. Cases reported in the medical literature in the 20th century included tumors that produced adrenocorticotropic hormone, prolactin, growth hormone, and/or thyrotropin.\n\n\nCLINICAL PRESENTATION\nHere we present a 22-year-old woman with a pituitary carcinoma that was immunohistochemically positive for luteinizing hormone and follicle-stimulating hormone at both the primary and metastatic sites. The patient exhibited elevated serum levels of alpha-subunit.\n\n\nINTERVENTION\nThe patient had experienced failure of previous treatments, including standard surgery and radiotherapy, and presented to us for radical resection of the tumor, with exenteration of the involved cavernous sinus. She was pretreated with cytotoxic chemotherapy and continued to receive this therapy after surgery.\n\n\nCONCLUSION\nThis is the only documented case of a gonadotropin-staining pituitary carcinoma for which hormone production was proven in both the primary and metastatic tumors. Many benign \"nonsecreting\" pituitary adenomas actually produce subclinical amounts of gonadotropins, and malignant nonfunctional pituitary neoplasms may do the same.", "title": "Pituitary carcinoma containing gonadotropins: treatment by radical excision and cytotoxic chemotherapy: case report.", "claims": null}, {"metadata": {"year": 2013}, "authors": ["H. Tatsuoka", "Shojiro Inano", "Y. Hamamoto", "Yuki Takahashi", "J. Takahashi", "S. Yamada", "T. Sano", "H. Koshiyama"], "summary": "We herein report three cases of gonadotroph adenoma in men (36-72 years of age) presenting with visual impairment and suprasellar masses measuring approximately 20 to 30 mm in diameter. Endocrinological examinations were normal, except for slightly increased follicle stimulating hormone (FSH) levels in two cases. Based on the tentative diagnosis of non-functioning pituitary adenoma, transsphenoidal surgery was performed, which revealed that the tumors consisted of FSH- and LH-positive cells. As gonadotroph adenoma is very common among patients with clinically silent pituitary adenoma, it should be diagnosed using pathological examinations.", "title": "Male gonadotroph adenoma: report of three cases and a review of the literature.", "claims": null}], "query": "Recurrence of gonadotroph pituitary adenomas after primary surgery", "summary_abstract": "The recurrence of gonadotroph pituitary adenomas after primary surgery has been explored in several studies, revealing important insights into their behavior and management. \n\nMuhr et al. (1980, 1981) conducted a long-term follow-up study of 41 patients who underwent surgery and subsequent irradiation for pituitary adenomas. They found that 12% of patients experienced recurrence, with computed tomography (CT) proving to be a valuable tool in detecting these recurrences, often identifying cases that were asymptomatic or not evident through other imaging methods.\n\nTrouillas et al. (1986) reported on a cohort of 26 gonadotropic adenomas, noting a recurrence rate of 12% following adenomectomy. This study highlighted the challenges in preoperative diagnosis, as only a small number of patients were identified as having gonadotropic adenomas based on elevated serum gonadotropin levels. The authors emphasized the need for thorough immunocytochemical testing to improve diagnosis and management.\n\nIn a more recent case report, Tatsuoka et al. (2013) described three cases of gonadotroph adenomas in men, which were initially diagnosed as non-functioning adenomas. Following transsphenoidal surgery, the tumors were confirmed to be gonadotroph adenomas, underscoring the importance of pathological examination in identifying these tumors, particularly when they present with visual impairment.\n\nOverall, the literature indicates that while recurrence of gonadotroph pituitary adenomas is not uncommon, the use of advanced imaging techniques and thorough pathological evaluations can significantly aid in their detection and management post-surgery (Muhr et al., 1980; Trouillas et al., 1986; Tatsuoka et al., 2013).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2020}, "authors": ["D. Erokhin", "E. Rovenskaya"], "summary": "The future of the Arctic region is a subject of heated debates in both scientific and policy circles. The region has an enormous economic potential as a storehouse of mineral resources and as a provider of shorter and more cost-effective transportation between Europe and Asia. The Arctic is therefore an essential strategic element of the domestic and foreign policies of all Arctic states. In addition, there is an increasing economic interest in the region on the part of non-Arctic states. However, at present, the future of the Arctic region development remains highly uncertain. Scenario building is a suitable methodology to imagine alternative plausible futures of such a complex and multi-dimensional process and to elaborate successful and robust development strategies. This paper provides an overview of the scenario frameworks of Arctic futures presented in the literature and analyses key factors that determine these scenarios. Overall, we find a growing interest of the international foresight research community in the Arctic region that is evident from a number of thorough scenario-building exercises published recently. At the same time, we observe two drawbacks. First, the existing studies lack a numerical element, that is, the overwhelming majority of the scenario frameworks that can be found in the literature are fully qualitative. Quantitative estimates would strengthen the scenario narratives and enrich communication, which make them a useful addition to support a qualitative scenario framework. Second, the existing studies use a mixture of both internal and external factors to describe the underlying uncertainty. This limits the number of factors that can be taken into consideration and may be confusing for a potential user of these scenario frameworks due to the lack of a systemic view. Such a confusion can happen, for example, if some of the external factors underpinning a particular scenario suddenly develop in a direction that was not anticipated within the scenario framework. The effect of such a change on the set of scenarios and the validity of the scenarios despite this change will be of interest to the user, and a clear systems perspective would be conducive to address these questions. Separating internal and external factors in a scenario building exercise is particularly useful given that the volatility of the global geopolitical, geoeconomic and environmental dynamics is only increasing. It is our intention to address these two drawbacks in a scenario building exercise within the \u201cEmerging trade routes between Europe and Asia\u201d scenario-building project led by IIASA within the Northern Dimension Institute (NDI) Think Tank Action co-funded by the European Union and coordinated by Aalto University, Finland.", "title": "Regional scenarios of the Arctic futures: A review", "claims": null}, {"metadata": {"year": 2007}, "authors": ["L. Brigham"], "summary": "The Arctic is undergoing an extraordinary transformation early in the twenty-first century\u2014a transformation that will have global impacts. Temperatures in the Arctic are rising at unprecedented rates and are likely to continue increasing throughout the century. Significant environmental changes in the region include retreating sea ice, melting glaciers, thawing permafrost, increasing coastal erosion, and shifting vegetation zones. The Arctic Ocean could even be temporarily ice-free during summer 2040, predicts one recent study. These changes have profound consequences for the indigenous people, for all Arctic species and ecosystems, and for any anticipated economic development. The Arctic is also understood to be a large storehouse of yet-untapped natural resources, a situation that is changing rapidly as exploration and development accelerate in places like the Russian Arctic. The combination of these two major forces\u2014intense climate change and increasing natural-resource development\u2014can transform this onceremote area into a new region of importance to the global economy. To evaluate the potential impacts of such rapid changes, we turn to the scenario-development process, the creation of plausible futures to enhance a dialogue among a multitude of stakeholders and decision makers. The key themes providing the framework for the four Arct ic scenarios posed for 2040 include: \u2022 Global climate change, which results in significant regional warming in each of the four scenarios. \u2022 Transportation systems, especially increases in marine and air access. \u2022 Resource development\u2014for example, oil and gas, minerals, fisheries, freshwater, and forestry. \u2022 Indigenous Arctic peoples\u2014 their economic status and the impacts of change on their well-being. \u2022 Regional environmental degradation and environmental protection schemes. \u2022 The Arctic Council and other cooperative arrangements of the Arctic states and those of the regional and local governments. \u2022 Overall geopolitical issues facing the region, such as the Law of the Sea and boundary disputes.", "title": "Thinking about the Arctic \u2019 s Future : Scenarios for 2040 By", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Jenny Andersson"], "summary": "The Arctic region is the site of a geopolitical scramble for two major future assets: the opening up of the Northeast Passage and access to enormous gas reserves. Meanwhile, many other possible futures define the ongoing struggle to establish claims to the Arctic among a variety of \u2018Arctic nations\u2019, including the rights of indigenous people, the preservation of pristine nature, future tourism, and the reestablishing of historical connections with previous colonizing countries in Scandinavia and Russia. The chapter discusses a wide repertoire of future making, including scenario gaming, forecast technologies, and forms of nation branding used as geopolitical instruments for defining expectations and future interests in the Arctic. At a theoretical level, the chapter examines the mutual constitution of imaginaries and interests and highlights ways in which actors attempt to \u2018close\u2019 the future by establishing the dominance of particular expectations or scenarios that suit their interests.", "title": "Arctic Futures", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Alexandra Middleton", "Anastasia Lazariva", "F. Nilssen", "A. Kalinin", "Anastasia", "Belostotskaya"], "summary": "The Arctic region has increasingly come to be seen in a new light because of the global transformations resulting from the disruptive challenges of climate change and shifting global political, social and economic patterns. The harsh environmental conditions there have long constrained economic activity. The climate crisis, while having a negative impact on the region in some senses, opens up new prospects for development in others. The Arctic has become a geopolitical hot spot where global and regional players seek to increase their influence. On one side, the Arctic possesses vast natural resources and increasingly will be an important global source of bio-resources. The area is also one of geopolitical tension. On the other side, the Arctic represents a \u201ctemperature gauge\u201d for distant pollution and waste in the sea. At the same time, powerful voices from supranational institutions are putting heavy pressure on preserving the Arctic as a kind of \u201cnature protected area\u201d with severe restrictions on economic activity and human impact. This paper draws attention to the tension between the regional interest in maintaining and developing a socially, economically and biologically sustainable area of human settlement and the more detached interest in preserving the Arctic as a nature reserve. The study approaches Arctic development from a social, ecological and environmental point of view, mapping key development drivers and the changing geopolitical context. The research utilizes scenario methodology and qualitative expert interviews combined with comprehensive literature studies. Four scenarios illustrate how the Arctic might look in 2050 and what the implications might be for the sustainable development of the region from the economic, social and environmental perspectives.", "title": "Scenarios for Sustainable Development in the Arctic until 2050", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Riina Haavisto", "K. Pilli-Sihvola", "Atte Harjanne", "A. Perrels"], "summary": "Improved weather and marine services (WMS) can have a role to play in the safe and secure development of the Arctic region through either a demand-pull (enhanced by growth in activity) or a supply-push (enhances growth in activity) process. To analyse the nature of the process and the future use and benefits of WMS, a better understanding of possible future developments in the Eurasian Arctic is needed. This report presents six socio-economic scenarios for the Eurasian Arctic by 2040, and a brief synopsis of the implications of each scenario for WMS. The scenarios focus on the development of shipping, resource extraction and tourism industries. The scenario futures, called Wild West, Silicon Valley, Exploited Colony, Shangri La, Conflict Zone and Antarctic, describe the scale and scope of activities in the Eurasian Arctic by 2040. The scenarios have three dimensions: open \u2013 closed, public private and dirty \u2013 clean, which describe the political, economic, social, technological and environmental aspects of different futures. The scenarios are based on a literature review, pre-survey, expert workshop and restructuring and analysis of this material. The methodology used for scenario construction is described in detail and may be used widely by other scenario developers. Our analysis shows that plenty of potential pressures for major changes in the Eurasian Arctic exist. Environmental changes, political shifts and technological development can all push forward drastic new developments in the region. Then again, it is possible that despite all the hype and interest, the Eurasian Arctic remains backwater areas in the global economy. This emphasizes the need for any decision-maker to be able to respond to very different futures. Therefore, robust decision making, a good eye for weak signals and tipping points, and the ability to prepare for risks and seize opportunities as they emerge is required in the Eurasian Arctic. The development of WMS is important in ensuring the safe and secure development of the Eurasian Arctic, unless the development follows the path of \u201cAntarctica\u201d with tourism and research as main activities in the marine regions.", "title": "Socio-economic scenarios for the Eurasian arctic by 2040", "claims": null}, {"metadata": {"year": 2011}, "authors": ["David Mugridge", "Peter Avis", "Peter Race"], "summary": "Abstract : With the change in northern climate over the past decade, current policy and media discussions have focused on the future of the Arctic. DRDC CORA has taken on a number of research projects, including the development of two Arctic security scenarios, a scenario development methodology, and a Capability Inventory Tool (CIT) to identify and characterize legislation and policy on the Arctic, with a view to developing scenarios for future planning and assisting operational commands in their interactions with other operations centres. This report presents one of two scenarios, focused on a defence-centric response scenario. The scenario makes use of the CIT to present an overview of the current situation and presents a projection out to 2015 to set context for a security environment in which there are significant tensions between Canada and Russia over arctic sovereignty. While all efforts have been taken to present a plausible scenario, the research was not based on any intelligence assessments and is not intended to be considered a probable future. Rather, it presents a context to test a range of capabilities required for Canada to be able to meet its strategy and policy objectives.", "title": "Arctic Planning Scenarios: Scenario #1: Defence Scenario", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Jenny Andersson"], "summary": "The Arctic region is the site of a geopolitical scramble for two major future assets: the opening up of the Northeast Passage and access to enormous gas reserves. Meanwhile, many other possible futures define the ongoing struggle to establish claims to the Arctic among a variety of \u2018Arctic nations\u2019, including the rights of indigenous people, the preservation of pristine nature, future tourism, and the reestablishing of historical connections with previous colonizing countries in Scandinavia and Russia. The chapter discusses a wide repertoire of future making, including scenario gaming, forecast technologies, and forms of nation branding used as geopolitical instruments for defining expectations and future interests in the Arctic. At a theoretical level, the chapter examines the mutual constitution of imaginaries and interests and highlights ways in which actors attempt to \u2018close\u2019 the future by establishing the dominance of particular expectations or scenarios that suit their interests.", "title": "Arctic Futures. Expectations, Interests, Claims, and the Making of Arctic Territory", "claims": null}, {"metadata": {"year": 2021}, "authors": ["A. Petrov", "Marya S. Rozanova Smith", "A. Krivorotov", "E. Klyuchnikova", "V. L. Mikheev", "A. Pelyasov", "N. Zamyatina"], "summary": "Scenarios of future development pathways in the Arctic created by groups of experts and stakeholders are an effective way to identify and illustrate possible alternatives and options for this region based on anticipated environmental and socioeconomic changes. Although scenarios that assess development trajectories for the Arctic are becoming increasingly popular, there is a relative lack of regional perspective in foresight exercises devoted to the Russian Arctic. This article presents and discusses development scenarios for the Russian Arctic until 2050 that were built by a diverse group of academics, local officials, Indigenous leaders, and business representatives at a scenario workshop in Naryan-Mar, Russia. The scenarios focus on Russia\u2019s Arctic zone and incorporate future visioning of economic development, international cooperation with the West and China, shipping, human and social capital, and Indigenous peoples\u2019 livelihoods in the context of climate change. We apply a novel circular-axial technique to synthesize and combine the 12 initially created thematic scenarios into four final cross-cutting integrated scenarios that describe alternative futures for the Russian Arctic by 2050: Harmonious Arctic, Self-Reliant Arctic, Resource-Dependent Arctic, and Forgotten Arctic.\u00a0", "title": "The Russian Arctic by 2050: Developing Integrated Scenarios", "claims": null}], "query": "What do scenarios mean for the future of geopolitics? How are scenarios being used in the Arctic?", "summary_abstract": "The future of geopolitics in the Arctic is increasingly shaped by a combination of climate change, resource development, and geopolitical tensions, with scenario-building emerging as a critical tool for understanding and navigating these complexities. The Arctic is undergoing significant transformations, including rising temperatures and melting ice, which not only affect local ecosystems and indigenous communities but also open up new economic opportunities, such as access to untapped natural resources and shorter shipping routes between Europe and Asia (Brigham, 2007; Erokhin & Rovenskaya, 2020).\n\nScenario methodologies are being employed to explore various plausible futures for the Arctic, allowing stakeholders to engage in dialogue about potential outcomes and strategies. For instance, Erokhin and Rovenskaya (2020) highlight the importance of scenario frameworks in addressing the uncertainties surrounding Arctic development, emphasizing the need for a clear distinction between internal and external factors influencing these scenarios. Similarly, Andersson (2018) discusses how different geopolitical actors utilize scenario gaming and forecasting technologies to assert their interests in the region, particularly concerning the Northeast Passage and gas reserves.\n\nThe scenarios developed in various studies reflect a range of potential futures, from increased resource exploitation to the preservation of the Arctic as a nature reserve. For example, Haavisto et al. (2016) present six socio-economic scenarios for the Eurasian Arctic by 2040, illustrating the diverse possibilities for shipping, resource extraction, and tourism. Meanwhile, Petrov et al. (2021) focus on the Russian Arctic, proposing four integrated scenarios that consider economic development, international cooperation, and the livelihoods of Indigenous peoples in the context of climate change.\n\nOverall, the use of scenarios in Arctic geopolitics serves not only to anticipate and prepare for future developments but also to facilitate discussions among various stakeholders, including governments, businesses, and Indigenous communities. This collaborative approach is essential for navigating the intricate interplay of environmental, economic, and geopolitical factors that will define the Arctic's future (Mugridge et al., 2011; Middleton et al., 2021).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2022}, "authors": ["Farahat Ali", "Noshina Saleem"], "summary": "This study is intended to shed light on broader question of which conflict resolution strategies are most effective in married life, with a special focus on the effectiveness of five conflict resolution styles (Integrative, avoidance, distributive, verbal aggression and physical aggression). Extant literature within this domain has produced mixed findings. A total of 350 married dual-earner academicians were taken as the sample for current study and purposive sampling technique was used. Results showed that there was a significant difference between men and women in terms of the conflict resolution styles, with women being more likely to use an integrative style, while men were more likely to use an avoidance, distributive, and verbal aggression conflict resolution style. There was also a significant difference between men and women in terms of the marital satisfaction, with women being more likely to report the higher levels of satisfaction. Integrative and avoidance styles were found to be positively related to marital satisfaction while distributive and verbal aggression styles were found to be negatively related. Physical aggression was not found to be a predictor of the marital\nsatisfaction.", "title": "RELATIONSHIP BETWEEN CONFLICT RESOLUTION STYLES AND MARITAL SATISFACTION AMONG MARRIED ACADEMICIANS", "claims": null}, {"metadata": {"year": 2000}, "authors": ["A. P. Greeff", "T. de Bruyne"], "summary": "The aim of this study was to investigate whether there is one conflict management style that correlated more significantly with marital satisfaction than any other. In addition, spousal satisfaction with how marital conflict is managed was also examined, as were gender differences. Fifty-seven couples who had been married for at least 10 years took part in the study. Results showed that the collaborative conflict management style has the highest correlation with both marital satisfaction and spousal satisfaction with conflict management in the marriage. In contrast, where one or both of the spouses used the competitive conflict management style, the lowest marital satisfaction was reported. The results were also interpreted in terms of cultural and gender differences.", "title": "Conflict management style and marital satisfaction.", "claims": null}, {"metadata": {"year": 2000}, "authors": ["Abraham P. Greeff, Tanya De Bruyne"], "summary": "The aim of this study was to investigate whether there is one conflict management style that correlated more significantly with marital satisfaction than any other. In addition, spousal satisfaction with how marital conflict is managed was also examined, as were gender differences. Fifty-seven couples who had been married for at least 10 years took part in the study. Results showed that the collaborative conflict management style has the highest correlation with both marital satisfaction and spousal satisfaction with conflict management in the marriage. In contrast, where one or both of the spouses used the competitive conflict management style, the lowest marital satisfaction was reported. The results were also interpreted in terms of cultural and gender differences.", "title": "Conflict Management Style and Marital Satisfaction", "claims": null}, {"metadata": {"year": 2011}, "authors": ["M. Moland"], "summary": "Past research regarding the link between conflict resolution styles and marital satisfaction have been consistent; each partner's level of relationship satisfaction is positively related to the frequency with which both partners use constructive strategies to resolve conflict (such as agreement, compromise, and humor) and negatively related to the frequency with which each partner uses destructive strategies to resolve conflict (such as conflict engagement, withdrawal, and defensiveness) (Kurdek, 1995). The purpose of this research was to examine the links between conflict and satisfaction in romantic relationships that are one or more years in duration. Six self-report indices (the Relationship Assessment Scale - RAS, Conflict Resolution Styles Inventory - CRSI, Ineffective Arguing Inventory - IAI, Assessing Relationship Conflicts Questionnaire - ARCQ, Relationship Styles Questionnaire - RSQ, and Last Argument Questionnaire - LAQ) were administered to 235 participants. Several significant findings were obtained. Fifty six percent of the variance in relationship satisfaction was explained by all the conflict measures (CRSI, IAI, and RSQ). Females scored significantly higher than males on conflict engagement. For couples with more severe perpetual problems, Satisfaction was lower on the Relationship Assessment Scale. Married participants scored significantly higher than participants who were in a dating relationship on both conflict engagement and withdrawal. Post-graduates (those earning a Master's degree and above) scored significantly higher than those who either attended and/or graduated from college on relationship satisfaction and positive problem solving, and lower on ineffective arguing. Participants who reported a medium income level scored significantly higher on conflict withdrawal than those who reported they earned a low income.", "title": "Conflict And Satisfaction In Romantic Relationships", "claims": null}, {"metadata": {"year": 2001}, "authors": ["L. Russell-Chapin", "Theodore J. Chapin", "L. Sattler"], "summary": "The data presented in this article report significant predictors and relationships among high levels of marital distress, lack of couple time together, and problems associated with lack of communication. The results are explored in terms of counseling implications.", "title": "The Relationship of Conflict Resolution Styles and Certain Marital Satisfaction Factors to Marital Distress", "claims": null}, {"metadata": {"year": 1983}, "authors": ["I. Winkler", "W. Doherty"], "summary": "This study examined the conflict-related communication styles of American and Israeli couples and how these communication styles related to marital satisfaction in the two groups. An Israeli sample was recruited from Israeli couples currently living in New York City; the American sample was selected from the friendship circles of the Israeli couples. In spite of the close social ties of the two groups, we predicted that the conflict-related communication styles of the Israelis would be less calm and rational than those of the Americans and that such rational modes of communication would be positively associated with marital satisfaction for the Americans but not for the Israelis. The principle instruments were Straus' Conflict Tactics Scale, Ryder's Imagined Situation Inventory, and Spanier's Dyadic Adjustment Scale. Results largely confirmed the hypotheses. Israeli couples were more apt to be verbally aggressive (but less physically violent) and less apt to behave calmly during marital conflict, but these communication styles did not relate to marital satisfaction as strongly for the Israelis as for the Americans. These findings are discussed as a demonstration of the importance of considering cultural context when treating families.", "title": "Communication styles and marital satisfaction in Israeli and American couples.", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Sarah June Carroll", "E. Jeffrey Hill", "Jeremy B. Yorgason", "Jeffry H. Larson", "Jonathan G. Sandberg"], "summary": "Using a sample of married individuals employed full-time (N\u00a0=\u00a01,117), this study examined the mediating effects of constructive and destructive communication between partners on the relationship between work\u2013family conflict and marital satisfaction. Results from structural equation modeling revealed that work\u2013family conflict was significantly and negatively related to marital satisfaction. This relationship became insignificant when constructive and destructive communication were added to the model, evidence for full mediation. This was confirmed with Sobel\u2019s test for mediation. Results suggest that teaching communcation skills may be a potent point of intervention for couples who experience high levels of work\u2013family conflict.", "title": "Couple Communication as a Mediator Between Work\u2013Family Conflict and Marital Satisfaction", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Eyob Ayenew"], "summary": "Background: Relationship researchers have focused on the frequency of conflict in couples\u2019 relationships and the manner in which couples engage in and try to resolve conflicts. Conflict occurs regularly in most close relationships and dealing with conflict, under some conditions, may facilitate the development and maintenance of intimacy and satisfaction in a relationship. Regardless of this understanding, very little is known about individual\u2019s characteristics of conflict resolution behavior that bring into couples relationship satisfaction. Objective:. The purpose of this study is to examine individual\u2019s characteristics in conflict resolution behavior and its effect on couple relationship satisfaction. Methodology: A total of 306 (159 female and 147 male) participants were included from community sample. Three Kebeles in Yeka sub-city of Addis Ababa were selected randomly. Data were collected on relationship satisfaction and conflict resolution behavior of couples. The constructs were measured by Relationship Assessment Scale (RAS, Hendrick, 1988 and Rahim Organizational Conflict Inventory-II (ROCI-II, Rahim, 1983). Pearson correlation and standard multiple regressions were run to check association and prediction among variables under study respectively. Result: The result confirmed that statistically significant correlation was found between/among most variables/constructs in the study. The standard regression revealed that conflict resolution behavior predicted relationship satisfaction. Particularly integrating, dominating and avoiding conflict type of conflict resolution styles predicted relationship satisfaction individually. Conclusion: It is therefore, possible to conclude that all the conflict resolution style sub scales predicted relationship satisfaction significantly.", "title": "Association of Conflict Resolution Style and Relationship Satisfaction between Couples", "claims": null}], "query": "what is the relationship between conflict communication styles and marital satisfaction? give 20 studies summary", "summary_abstract": "The relationship between conflict communication styles and marital satisfaction is a nuanced area of study, with various research findings highlighting the impact of different styles on couples' satisfaction levels. \n\nFarahat Ali and Noshina Saleem (2022) found that integrative and avoidance conflict resolution styles are positively associated with marital satisfaction, while distributive and verbal aggression styles negatively affect it. Their study indicated that women tend to use integrative styles more than men, who are more likely to resort to avoidance and aggression, correlating with lower marital satisfaction for men.\n\nGreeff and de Bruyne (2000) identified the collaborative conflict management style as having the strongest correlation with marital satisfaction, while competitive styles were linked to lower satisfaction levels. This suggests that the manner in which couples manage conflict can significantly influence their overall satisfaction in marriage.\n\nMoland (2011) reinforced these findings by demonstrating that constructive conflict resolution strategies, such as compromise and humor, are positively related to relationship satisfaction, whereas destructive strategies, including withdrawal and defensiveness, correlate negatively. This study emphasized the importance of both partners engaging in constructive conflict resolution to enhance satisfaction.\n\nWinkler and Doherty (1983) explored cultural differences in conflict communication styles, revealing that American couples exhibited calmer and more rational styles compared to their Israeli counterparts. The study found that while rational communication was positively associated with marital satisfaction for Americans, it did not hold the same significance for Israelis, highlighting the role of cultural context in conflict resolution.\n\nCarroll et al. (2013) examined the mediating effects of communication styles on the relationship between work-family conflict and marital satisfaction. Their findings indicated that constructive communication can mitigate the negative effects of work-family conflict on marital satisfaction, suggesting that enhancing communication skills could be a valuable intervention for couples facing such challenges.\n\nAyenew (2016) further supported the notion that conflict resolution behaviors significantly predict relationship satisfaction. Their study found that integrating, dominating, and avoiding styles all had a notable impact on satisfaction levels, indicating that the choice of conflict resolution style is crucial for maintaining healthy relationships.\n\nCollectively, these studies underscore the importance of constructive conflict communication styles in fostering marital satisfaction. Styles that promote collaboration and integration are associated with higher satisfaction, while aggressive and competitive styles tend to diminish it. Additionally, cultural factors and individual characteristics play a significant role in how conflict is managed and its subsequent impact on relationship satisfaction.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2016}, "authors": ["E. Otranto"], "summary": "Abstract: Very often time series are subject to abrupt changes in the level, which are generally represented by Markov Switching (MS) models, assuming that the level is constant within a certain state (regime). This is not a realistic framework because in the same regime the level could change with minor jumps with respect to a change of state; this is a typical situation in many economic time series such as the Gross Domestic Product (GDP) or the volatility of financial markets. We propose to make the state flexible, introducing a very general model which provides oscillations of the level of the time series within each state of the MS model; these movements are driven by a forcing variable. The new model allows for consideration of extreme jumps in a parsimonious way, without the adoption of a large number of regimes (in our examples the two-state MS models are used). Moreover, this model increases the interpretability and in particular the out-of-sample performance with respect to the most used alternative models. This approach can be applied in several fields, also using unobservable data. We show its advantages in three distinct applications, extending particular MS models, which involve macroeconomic variables, volatilities of financial markets and conditional correlations.", "title": "Adding flexibility to Markov Switching models", "claims": null}, {"metadata": {"year": 2016}, "authors": ["A. Dufays"], "summary": "Generalized auto-regressive conditional heteroskedastic model with changing parameters is specified using the sticky infinite hidden Markov-chain framework. Estimation by Bayesian inference determines the adequate number of regimes as well as the optimal specification (Markov-switching or change-point). The new provided algorithms are studied in terms of mixing properties and computational time. Applications highlight the flexibility of the model and compare it to existing parametric alternatives.", "title": "Infinite-State Markov-Switching for Dynamic Volatility", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Yong Song", "T. Wo'zniak"], "summary": "Markov switching models are a family of models that introduces time variation in the parameters in the form of their state, or regime-specific values. This time variation is governed by a latent discrete-valued stochastic process with limited memory. More specifically, the current value of the state indicator is determined by the value of the state indicator from the previous period only implying the Markov property. A transition matrix characterizes the properties of the Markov process by determining with what probability each of the states can be visited next period conditionally on the state in the current period. This setup decides on the two main advantages of the Markov switching models: the estimation of the probability of state occurrences in each of the sample periods by using filtering and smoothing methods and the estimation of the state-specific parameters. These two features open the possibility for interpretations of the parameters associated with specific regimes combined with the corresponding regime probabilities.\n The most commonly applied models from this family are those that presume a finite number of regimes and the exogeneity of the Markov process, which is defined as its independence from the model\u2019s unpredictable innovations. In many such applications, the desired properties of the Markov switching model have been obtained either by imposing appropriate restrictions on transition probabilities or by introducing the time dependence of these probabilities determined by explanatory variables or functions of the state indicator. One of the extensions of this basic specification includes infinite hidden Markov models that provide great flexibility and excellent forecasting performance by allowing the number of states to go to infinity. Another extension, the endogenous Markov switching model, explicitly relates the state indicator to the model\u2019s innovations, making it more interpretable and offering promising avenues for development.", "title": "Markov Switching", "claims": null}, {"metadata": {"year": 2001}, "authors": ["Zhiyuan Ren", "B. Krogh"], "summary": "This paper presents a switching control strategy for multi-mode Markov decision processes. The system to be controlled is modeled as a finite-state controlled Markov chain with a mode that evolves stochastically. Although the system state is observable, the mode is only partially observable in the sense that we know the system mode only when it is in a given set of observable modes. Given a set of controllers for the system, we consider the problem of determining a switching rule that selects the controller to be applied each time the system mode is observable. The objective is to minimize a long-term average cost from the system while satisfying bounds on the long-term average of other given performance measures. We assume the multi-mode model parameters are unknown a priori, so an adaptive switching rule is required. Algorithms are presented for computing approximations to the optimal switching rule based on estimating the model parameters online. The approach is illustrated for an example of dynamic power management of hard disk drives in computer systems.", "title": "Switching control in multi-mode Markov decision processes", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Massimo Guidolin"], "summary": "I survey applications of Markov switching models to the asset pricing and portfolio choice literatures. In particular, I discuss the potential that Markov switching models have to fit financial time series and at the same time provide powerful tools to test hypotheses formulated in the light of financial theories, and to generate positive economic value, as measured by risk-adjusted performances, in dynamic asset allocation applications. The chapter also reviews the role of Markov switching dynamics in modern asset pricing models in which the no-arbitrage principle is used to characterize the properties of the fundamental pricing measure in the presence of regimes.", "title": "Markov Switching in Portfolio Choice and Asset Pricing Models: A Survey", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Chung-Ming Kuan"], "summary": "We discuss the Markov switching model, one of most popular nonlinear time series models. This model involves switching between multiple structures that characterize different time series behaviors in different regimes, the switching mechanism being controlled by an unobservable variable that follows a Markov chain process. We introduce a simple Markov switching model of conditional mean and describe its generalizations, study estimation methods, discuss how to conduct hypothesis testing, and elaborate on two empirical examples.", "title": "Markov switching model (in Russian)", "claims": null}, {"metadata": {"year": 2015}, "authors": ["M. Perlin"], "summary": "Markov state switching models are a type of specification which allows for the transition of states as an intrinsic property of the econometric model. Such type of statistical representations are well known and utilized in different problems in the field of economics and finance. This paper gives an overview of MS_Regress, a Matlab toolbox specially designed for the estimation, simulation and forecasting of a general markov regime switching model. The package was written in an intuitive manner so that the user have at its reach a large number of different markov switching specifications, without any change in the original code. This document introduces the main functionality of the package with the help of several empirical examples.", "title": "MS_Regress - The MATLAB Package for Markov Regime Switching Models", "claims": null}, {"metadata": {"year": 1997}, "authors": ["B. Eftekhari"], "summary": "Using two specification tests, the returns from 17 emerging markets and the S&P 500 are tested for Markov regime switching. A trading strategy is formed that invests in the market only when the Markov switching model estimate for probability of a positive returns state in the next period is above a certain trigger level. The trading rule outperforms the index in the markets that show significant regime switching from the specification tests.", "title": "Markov Regime Switching Model as a Trading Tool", "claims": null}], "query": "flexible markov switching", "summary_abstract": "The research question regarding flexible Markov switching models is addressed through various studies that highlight the limitations of traditional Markov switching frameworks and propose enhancements for greater flexibility and applicability in economic and financial contexts.\n\nOtranto (2016) critiques the conventional Markov Switching (MS) models for their assumption of constant levels within regimes, arguing that this does not accurately reflect the behavior of many economic time series. He introduces a flexible model that allows for oscillations within each state, driven by a forcing variable, thereby accommodating minor jumps without necessitating a large number of regimes. This model demonstrates improved interpretability and out-of-sample performance compared to standard alternatives.\n\nDufays (2016) contributes to the discussion by specifying a generalized auto-regressive conditional heteroskedastic model that incorporates changing parameters through a sticky infinite hidden Markov-chain framework. This approach enhances flexibility and allows for the estimation of the optimal number of regimes, showcasing the model's adaptability compared to existing parametric alternatives.\n\nSong and Wo'zniak (2020) further elaborate on the flexibility of Markov switching models by discussing extensions such as infinite hidden Markov models and endogenous Markov switching models. These extensions allow for an infinite number of states and relate the state indicator to the model's innovations, respectively, thus enhancing interpretability and forecasting performance.\n\nKuan (2013) provides a comprehensive overview of Markov switching models, emphasizing their nonlinear nature and the ability to switch between different structures based on an unobservable variable following a Markov chain process. This flexibility is crucial for capturing varying time series behaviors across different regimes.\n\nOverall, the collective findings from these studies underscore the evolution of Markov switching models towards greater flexibility, enabling them to better capture the complexities of economic and financial time series. The proposed enhancements not only improve model performance but also broaden the applicability of these models across various fields.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2018}, "authors": ["C. G. Achi", "A. O. Coker", "Mynepalli K. C. Sridhar"], "summary": "Agro-food processing industries are major contributors of wastes in most developing countries. With Nigeria leading in cassava food production, little attention has been paid to provide a sustainable and profit-oriented solution to the problem of solid waste resulting from cassava processing industries.", "title": "Cassava Processing Wastes: Options and Potentials for Resource Recovery in Nigeria", "claims": null}, {"metadata": {"year": 2020}, "authors": ["E. Agbaeze", "Felix Olamide Ohunyeye", "Joseph Obamen", "G. Ibe"], "summary": "This article examined the problems and challenges of managing cassava processing in Nigeria. Based on the findings of our study, agricultural policy framework, level of mechanization, infrastructure, agricultural technical experts, and access to capital are the major factors that affect cassava processing in Nigeria, while the challenges are policy inconsistency, high cost of mechanization and inadequate capital investment in infrastructure, inadequate extension agents, as well as stringent credit facility conditions. This study, therefore, concludes that for cassava to play a major role in Nigeria\u2019s national development government must ensure that there is a full exploration of all opportunities that have the potential to yield strong advantage. This could be achieved through a holistic assessment of the cassava process that is geared toward improving the quality of life and well-being of citizens based on Nigeria\u2019s comparative advantage in cassava production and processing.", "title": "Management of Food Crop for National Development: Problems and Challenges of Cassava Processing in Nigeria", "claims": null}, {"metadata": {"year": 2014}, "authors": ["C. Onyenwoke"], "summary": "Cassava is an important root crop consumed as a staple food, boiled, baked or often fermented into other foods and beverages all over the world. It is a very good vehicle for addressing some health related problems and also serve as security food. Cassava undergoes postharvest physiological deterioration (PPD) once the tubers are separated from the main plant. PPD is one of the main obstacles currently preventing farmers from exporting fresh cassava abroad thereby generating income from foreign exchange. Cassava can be preserved in various ways such as coating with wax and freezing. Recent development in plant breeding has resulted in cassava that is tolerant to PPD. Genetic manipulation was considered most appropriate to solving the PPD challenge by adding new traits to elite genotypes without altering other desired characteristics. Processing cassava affects the nutritional value of cassava roots through modification and losses in nutrients of high value. The processing methods include peeling, boiling, steaming, slicing, grating, soaking or seeping, fermenting, pounding, roasting, pressing, drying, and milling. The products from cassava are: High Quality Cassava Flour (HQCF), cassava chips, garri, starch, ethanol etc.", "title": "Cassava post-harvest processing and storage in Nigeria: A review", "claims": null}, {"metadata": {"year": 2006}, "authors": ["K. Taiwo"], "summary": "Cassava (Manihot esculenta) is an important food crop in the tropics\u2014a major carbohydrate staple consumed in various forms by humans. Furthermore, its usage as a source of ethanol for fuel, energy in animal feed, and starch for industry is increasing. Given that cassava represents a valuable subsistence and cash crop in many countries, its domestic and industrial potential needs to be fully exploited. Processing of cassava reduces the moisture content and converts it into a more durable and stable product. Women traditionally process cassava, although it could be processed at the small, medium, or industrial scale. This article discusses the major Nigerian domestic products as well as the potential industrial products of cassava that are yet to be produced locally and the machines used in their production. Although improved technology is one of the keys to increasing the productivity of micro-enterprises and generating broad-based, sustainable economic growth, the extent to which a given technology is acceptable to a given culture may depend on a number of considerations beyond the mechanical efficiency of the device. These topics are discussed and recommendations are suggested to encourage the establishment of more cassava based industries.", "title": "Utilization Potentials of Cassava in Nigeria: The Domestic and Industrial Products", "claims": null}, {"metadata": {"year": 2013}, "authors": ["A. Raji"], "summary": "The recent transfiguration of cassava from a low profile into an industrial raw material, coupled with the new cassava revolutionary policies of the Federal Government of Nigeria have resulted in a serious surge in the demand for cassava and cassava-based products locally and the world over. However, cassava processors are currently finding it extremely difficult to respond positively to this increase in demand due to the prevalence of the traditional processing methods employed in the processing operations. This has made the review of the current processing technologies imperative in order to address the areas requiring technical improvement and further research efforts towards the evolution of cost effective technologies with improved efficiencies which would enhance the capacity to exploit the cassava market potential the world over. Therefore, this paper reviews the presents status of knowledge as regards cassava processing technology and a critical appraisal of the existing cassava processing technologies available to cassava processors and highlights the research need towards the evolution of better and improved cassava processing equipment.", "title": "Trends at Mechanizing Cassava Postharvest Processing Operations", "claims": null}, {"metadata": {"year": 2001}, "authors": ["Dele  Raheem", "Chrysanthus  Chukwuma"], "summary": "The need for an indigenous croplocally grown by subsistence farmers that willhelp bring direct economic benefits to thosewho need it most is very important. Cassava hasa great potential for this purpose if certainproblems are solved, because cassava has playedvital roles in the diet of many Africancountries especially the grassroot people, yetthe utilization of the crop in diversifiedforms has been very limited. There are few foodproducts from cassava that are processedindustrially with added value that will be ofhigher nutritional value and have longer shelflives. There are a number of traditionallyprocessed staple foods from cassava in Nigeriaand other African countries that needimprovement. Most of these foods can beimproved on if the market for them is assuredby the end users, a market that is currentlylacking. It will be necessary to create morepublic awareness within Nigeria and otherAfrican societies to increase the acceptabilityof such novel products and this requires amultidisciplinary approach to interpret whatscientists and technologists are doing to theold crop in the present age.", "title": "Foods from cassava and their relevance to Nigeria and other African countries", "claims": null}, {"metadata": {"year": 2015}, "authors": ["O. Adejumo", "A. Abass", "V. Okoruwa", "K. Salman"], "summary": "An Assessment of the use of Postharvest loss Prevention Technologies for Cassava in Nigeria. Oyeronke Adejumo*, Adebayo Abass , Victor Okoruwa and Kabir Salman University of Ibadan; International Institute of Tropical Agriculture Abstract Food Security remains a major issue in the world today especially in developing countries. Availability and access to food are negatively affected by many factors, prominent of which is post-harvest loss. Minimizing post-harvest losses is thus a critical agricultural problem. Cassava, an important staple food and income earner in Nigeria and other parts of West Africa, is a perishable crop. Improved harvesting, better postharvest handling and optimized processing technologies are crucial for improving supply of cassava-based foods and income to smallholder processors. This study was carried out to examine the factors influencing the choice of post-harvest technologies used by cassava processors in the study area and assess the impact of improved technology on the processors income. Data were collected from 150 cassava processors in Kwara State, Nigeria using structured questionnaire and analyzed with multinomial logit model. Factors such as years of education, post-harvest technology capacity, processing experience, motives for processing, amongst others were found to influence the choice of post-harvest technologies used in the study area. Furthermore, the impact assessment revealed that cassava processors using improved post-harvest technologies had increased income and output compared to those using traditional technologies. The study concludes that policy should be directed towards investment in improved post-harvest technologies by both private and public sector.", "title": "An Assessment of the use of Postharvest loss Prevention Technologies for Cassava in Nigeria", "claims": null}, {"metadata": {"year": 2020}, "authors": [], "summary": "Cassava (Manihot Spp) has become one of the most popular and widely grown plant amongst arable crops, particularly in Nigeria in the recent time. The high tendency to serve as a relief crop to food insecurity because of its copious consumption in various forms by people and its ability to subsist and give appreciable yields on soils where many other crops fail to perform, has endeared its cultivation by many smallholder farmers (Anaglo, Antwi, Manteaw and Kwapong, 2020). This was further supported by the assertion of Anyeagbunam, Nwaekpe, Adiele-Ezekiel, and Ukeje (2015) that cassava has become a very popular crop in Nigeria and is fast replacing other traditional local staples in the country (Chi, Mformi, Wouapi, 2019) while FAOSTAT (2010) maintained that cassava has moved from minor crop to major crop in Nigeria and has gained industrial recognition and relevance (Saani, Alenkhe, Edosio, Patino and Dixon, 2007). This then presupposed that traditional use or utilization of cassava is changing from primarily human consumption to processing into industrial products as well as for exportation. However, Agwu and Anyaeche (2007) had posited that cassava is one of the most important staple food and cash crop that has played and continues to play remarkable roles in agricultural centre-stage in Nigeria. Currently, cassava has become one of the most important food crops in maintaining national food security (Echebiri and Nwaogwu, 2015). Furthermore, cassava has gained prominence as export crops because of its versatility in terms of supplying industrial starch, tapioca and some other important bye products. This is in line with Chinaka and Udemezue (2015) that cassava is very versatile crop with numerous uses. These numerous uses have made cassava very relevant and significant as economic crop both nationally and internationally. Total national output from cassava, like most other arable crops being produced in Nigeria, come from the aggregated efforts of the smallholder/peasants who are indigent farmers that depend largely on traditional farming system for their Abstract: Total national output from cassava in Nigeria comes from the aggregated efforts of smallholder/peasant farmers who depend largely on traditional farming system for their agricultural production activities. Interestingly, all these smallholder farmers are primarily enmeshed in using traditional methods of cultivation which ultimately result in low yield. This study had assessed the adoption level of improved cassava production technologies in Oke-Ogun area of Oyo State, Nigeria. Multistage sampling technique was used to select 165 cassava farmers. It was revealed that various improved cassava production technologies had been introduced and that the farmers were aware of such recommended improved technologies in the study area. Improved cassava production technologies such as ridging, harrowing, fertilizer application, use of 20cm length stem cutting and spacing of 1m x 0.8m were not well adopted by cassava farmers in the study area. Chi-square results revealed that unaffordable improved production technologies (\uab53 2 = 10.795; P = 0.029); low literacy level (\uab53 2 = 10.030; P = 0.040) were significantly related to adoption level of improved cassava production technologies, all at 0.05 level. It was recommended that extension services should be intensified so that farmers could embrace all the recommended improved cassava technologies in the study area.", "title": "Adoption Level Of Improved Cassava Production Technologies In Oke-Ogun Area Of Oyo State, Nigeria", "claims": null}], "query": "The increased importance of cassava in agricultural and economic development as well as in food security particularly in Nigeria should give its processing and waste handling more attention. Presently, the methods involved in cassava processing are referred to as crude and unstandardized, lacking scientific principles and waiting for possible upgrade (Okunade and Adekalu, 2013). ", "summary_abstract": "The collective findings from the reviewed papers underscore the critical need for improved processing methods and waste management in the cassava industry, particularly in Nigeria, where cassava plays a vital role in food security and economic development. Achi et al. (2018) highlight that the cassava processing sector generates significant waste, yet sustainable solutions remain largely unaddressed. This is echoed by Agbaeze et al. (2020), who identify various challenges in cassava processing, including policy inconsistency and inadequate infrastructure, which hinder the industry's potential to contribute to national development.\n\nOnyenwoke (2014) discusses the importance of cassava as a staple food and its susceptibility to postharvest physiological deterioration (PPD), which complicates export opportunities. The author suggests that advancements in preservation techniques and genetic improvements could mitigate these issues. Taiwo (2006) emphasizes the necessity of exploiting cassava's domestic and industrial potential, advocating for the adoption of improved technologies to enhance productivity and economic growth.\n\nRaji (2013) calls for a review of current processing technologies to address inefficiencies and improve the capacity of processors to meet rising demand. This sentiment is supported by Adejumo et al. (2015), who found that the adoption of improved postharvest technologies significantly boosts income for cassava processors compared to traditional methods.\n\nOverall, the literature indicates a pressing need for modernization in cassava processing techniques and waste management practices to fully harness the crop's potential in Nigeria. This modernization is essential not only for enhancing food security but also for fostering economic growth and sustainability in the agricultural sector.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2012}, "authors": ["J. Cook", "J. Farr"], "summary": "four-issue", "title": "Cartilage Repair in the Knee: Part III", "claims": null}, {"metadata": {"year": 2017}, "authors": ["B  Rath", "J  Eschweiler", "M  Betsch", "G  Gruber"], "summary": "BACKGROUND\nCartilage defects around the knee joint frequently occur in the region of the medial femoral condyle and the retropatellar cartilage surface. The distinction between local cartilage defects and large area degenerative cartilage lesions is very important for both prognosis and surgical therapy. The size and position of the lesion, the underlying pathomechanism and the age of the patient are very important factors which should be considered in the therapy algorithm for optimal cartilage defect restoration. Important cofactors such as stability, long leg axis and muscle balance should be taken into account.\n\n\nAIM\nCurrent procedures for cartilage repair of the knee joint and their results are summarized in this article. In addition, the necessity of precise and comprehensive preoperative clinical and radiological diagnostics is displayed to be able to treat co-pathologies in order to enable a\u00a0successful repair of the cartilage defect.\n\n\nRESULTS AND CONCLUSIONS\nPreoperative planning of cartilage-repair techniques usually includes x\u2011ray images and a magnetic resonance imaging (MRI) examination. If MRI is not available, an arthro-computed tomography could be an alternative. Modern and routinely used procedures for cartilage repair at the knee joint are microfracture, autologous matrix-induced chondrogenesis, autologous chondrocyte transplantation, matrix-induced autologous chondrocyte implantation and osteochondral transplantation. Successful surgical cartilage-repair surgeries require a\u00a0correct and individualized indication, addressing of copathologies and a\u00a0standardized rehabilitation that is adapted to the surgical procedure. Evidence-based criteria for an exact time point for the return to sports according to individually operative cartilage repair techniques currently do not exist.", "title": "[Cartilage repair of the knee joint].", "claims": null}, {"metadata": {"year": 2013}, "authors": ["T. Minas"], "summary": "Although cartilage repair has been around since the time of open Pridie drilling, clinical outcomes for newer techniques such as arthroscopic debridement, microfracture (MFX), osteochondral autograft transfers (OATS), osteochondral allograft transplantation and Autologous Chondrocyte Implantation (ACI) are still finding their place in treating injured knees. Early mechanical symptoms are best managed by a gentle arthroscopic debridement of loose articular flaps. This allows the surgeon to assess the defect size, location in the tibio-femoral or patellofemoral joint, status of the cartilage overall and patients response to the intervention. If the symptom improvement is not satisfactory to the patient, after assessing background factors that will influence the results of a cartilage repair procedure, (alignment of the patellofemoral joint or axial alignment, ligament stability and status of the meniscus), the surgeon can choose the best procedure for that individual based on the expected outcomes of the various cartilage repair techniques while addressing the background factors. As all the techniques have failures and informed discussion with the patient prior to performing the procedure is critical in avoiding disappointment for the patient and the surgeon. The repair technique used should incorporate considerations of the defect size, location, and the patient age, activity level, expectations and ability to comply with the longer rehabilitation needed for biological procedures as compared to prosthetic implants.", "title": "CARTILAGE REPAIR AND KNEE JOINT PRESERVATION", "claims": null}, {"metadata": {"year": 2010}, "authors": ["N. S. Kalson", "P. Gikas", "T. Briggs"], "summary": "Defects in knee articular cartilage (AC) can cause pain and disability and present the clinician with an extremely challenging clinical situation. This article describes the most up\u2010to\u2010date surgical techniques that aim to repair and/or regenerate symptomatic focal defects in AC, which include arthroscopic debridement, microfracture bone marrow stimulation and autologous osteochondral allografting, with an emphasis on autologous chondrocyte implantation. In the future, refinement of tissue\u2010engineering approaches promises to further improve outcome for these patients.", "title": "Current strategies for knee cartilage repair", "claims": null}, {"metadata": {"year": 2004}, "authors": ["C. Vangsness", "P. Kurzweil", "J. Lieberman"], "summary": "Following damage or injury, as well as normal wear and tear, articular cartilage in adults has limited healing potential. Repair of cartilage is dependent on the extent and depth of the injury, its location, and the surviving chondrocytes and extracellular matrix. The ultimate goal of cartilage repair is to restore the normal composition and matrix of the cartilage, but for most patients this is not feasible. In these cases, filling the defect using interventions that provide adequate, albeit not optimal, functionality and pain relief is desirable. Therefore, a more realistic goal in these patients may be restoration of their respective activity levels and the delay or avoidance of prosthetic replacement. Depending on the age, activity level, and degree of cartilage damage, several methods to decrease pain and attempt cartilage repair are available; these include lavage and debridement, abrasion arthroplasty, and tissue and cell transplantation. Future strategies for cartilage repair may involve tissue engineering techniques, possibly coupled with specific growth factors. Randomized clinical trials must be established.", "title": "Restoring articular cartilage in the knee.", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Goetz H. Welsch", "Stephan  Domayer", "Vladimir  Juras", "Tallal C. Mamisch", "Siegfried  Trattnig"], "summary": "Articular cartilage lesions are a common pathology of the knee joint and many patients could benefit from cartilage repair. Such surgical treatment options may offer the possibility for patients with cartilage defects to avoid the development of osteoarthritis or delay its progression. Newly developed cartilage repair techniques, including arthroscopic or open surgical approaches, as well as marrow-stimulation techniques, osteochondral grafting, and chondrocyte implantation/transplantation, require knowledgeable and high-\u00adquality follow-up.", "title": "Imaging of Cartilage Repair", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Javier  Vaquero", "Francisco  Forriol"], "summary": "Articular cartilage has a very limited capacity to repair and as such premature joint degeneration is often the end point of articular injuries. Patients with chondral injury have asymptomatic periods followed by others in which discomfort or pain is bearable. The repair of focal cartilage injuries requires a precise diagnosis, a completed knee evaluation to give the correct indication for surgery proportional to the damage and adapted to each patient. Many of the surgical techniques currently performed involve biotechnology. The future of cartilage repair should be based on an accurate diagnosis using new MRI techniques. Clinical studies would allow us to establish the correct indications and surgical techniques implanting biocompatible and biodegradable matrices with or without stem cells and growth factors. Arthroscopic techniques with the design of new instruments can facilitate repair of patella and tibial plateau lesions.", "title": "Knee chondral injuries: clinical treatment strategies and experimental models.", "claims": null}, {"metadata": {"year": 2007}, "authors": ["Mehul R. Shah", "K. Kaplan", "R. Meislin", "J. Bosco"], "summary": "Articular cartilage defects are common and play a significant role in degenerative joint disease. Cartilage is unable to regenerate, secondary to an inherent lack of vascular supply, thus, various techniques have been described in an attempt to treat and potentially restore these defects. Treatment decisions should be based on appropriate evaluation and classification of the pathology. Only then can the surgeon choose to perform a repair or a restoration of the articular surface. Current literature and techniques for the treatment of articular cartilage defects are reviewed, with an algorithm developed for the management of articular cartilage defects by orthopaedic surgeons.", "title": "Articular cartilage restoration of the knee.", "claims": null}], "query": "Knee cartilage repair", "summary_abstract": "The research on knee cartilage repair highlights a variety of surgical techniques and considerations essential for effective treatment of cartilage defects. A consensus emerges around the importance of precise diagnosis and individualized treatment plans based on factors such as the size and location of the defect, the patient's age, activity level, and the presence of co-pathologies (B Rath et al., 2017; T. Minas, 2013).\n\nCurrent techniques for cartilage repair include microfracture, autologous chondrocyte implantation (ACI), osteochondral autograft transfers (OATS), and osteochondral allograft transplantation (N. S. Kalson et al., 2010; T. Minas, 2013). Each method has its own indications and expected outcomes, and the choice of technique should be informed by a thorough preoperative assessment, including imaging studies like MRI (B Rath et al., 2017; C. Vangsness et al., 2004).\n\nDespite advancements, the inherent limitations of articular cartilage healing remain a challenge, as the tissue has a limited capacity for self-repair due to its avascular nature (C. Vangsness et al., 2004; Javier Vaquero et al., 2012). Therefore, while the ultimate goal is to restore normal cartilage function, many interventions aim to alleviate pain and improve activity levels rather than achieve complete restoration (Goetz H. Welsch et al., 2011; Mehul R. Shah et al., 2007).\n\nFuture directions in cartilage repair may involve innovative tissue engineering approaches, potentially incorporating stem cells and growth factors to enhance healing outcomes (Javier Vaquero et al., 2012). Overall, the literature emphasizes the need for a tailored approach to knee cartilage repair, considering both the technical aspects of the procedures and the individual characteristics of the patient.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2020}, "authors": ["Stefanie I. Becker", "Rheaa T. Manoharan", "Charles L. Folk"], "summary": "Visual attention allows selecting relevant information from cluttered visual scenes and is largely determined by our ability to tune or bias visual attention to goal-relevant objects. Originally, it was believed that this top-down bias operates on the specific feature values of objects (e.g., tuning attention to orange). However, subsequent studies showed that attention is tuned to in a context-dependent manner to the relative feature of a sought-after object (e.g., the reddest or yellowest item), which drives covert attention and eye movements in visual search. However, the evidence for the corresponding relational account is still limited to the orienting of spatial attention. The present study tested whether the relational account can be extended to explain attentional engagement and specifically, the attentional blink (AB) in a rapid serial visual presentation (RSVP) task. In two blocked conditions, observers had to identify an orange target letter that could be either redder or yellower than the other letters in the stream. In line with previous work, a target-matching (orange) distractor presented prior to the target produced a robust AB. Extending on prior work, we found an equally large AB in response to relatively matching distractors that matched only the relative color of the target (i.e., red or yellow; depending on whether the target was redder or yellower). Unrelated distractors mostly failed to produce a significant AB. These results closely match previous findings assessing spatial attention and show that the relational account can be extended to attentional engagement and selection of continuously attended objects in time.", "title": "The attentional blink: A relational accountof attentional engagement", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Charles L Folk", "Edward F Ester", "Kristof  Troemel"], "summary": "Previous research has suggested that the involuntary allocation of spatial attention to salient, irrelevant stimuli (i.e., attentional capture) is prevented when attention is in a focused state (e.g., Yantis & Jonides, 1990). Recent work has suggested that although focused attention may be necessary to prevent attentional capture by irrelevant stimuli, it is not sufficient (e.g., Folk, Leber, & Egeth, 2002). The present experiments provide evidence that attentional engagement, rather than attentional focusing, prevents capture. Observers performed a rapid serial visual presentation task in which they were asked to identify a target letter defined by color. Peripheral distractors that shared the color of the target produced evidence of attentional capture. This effect was completely eliminated, however, when the peripheral distractor was preceded by a central distractor designed to engage attention on the stream. It is concluded that attentional engagement serves to lock out capture by irrelevant, salient stimuli.", "title": "How to keep attention from straying: Get engaged!", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Oliver  Perra", "Merideth  Gattis"], "summary": "We report a longitudinal study investigating developmental changes in the structure of attention engagement during early infancy. Forty-three infants were observed monthly from 2 to 4 months. Attention engagement was assessed from play interactions with parents, using a coding system developed by Bakeman and Adamson (1984). The results indicated a developmental transition in attention engagement at 3 months: after this age infants engaged for longer periods and in a wider variety of states. Most infants displayed person engagement at 2 months, passive joint engagement at 3 months, and object engagement at 4 months. To address whether emerging abilities of attention engagement allow infants to follow the attention of social partners, we compared attention engagement to performance on an experimental measure of attention control (reported by Perra & Gattis, 2010). Analyses revealed a positive relation between passive joint engagement and checking back, suggesting that changes in passive joint engagement reflect the development in attention control.", "title": "Attention engagement in early infancy.", "claims": null}, {"metadata": {"year": 2020}, "authors": ["I\u00f1aki Rodr\u00edguez Longarela"], "summary": "This paper develops simple measures of engagement and attention which are particularly appealing for active learning. They can be computed at a very low cost with Electronic Response Systems which are wildly available and whose use has substantially increased with the COVID-19-driven shift to online teaching. Their use is illustrated in an empirical application which uses data gathered during seven editions of a large-enrollment higher education course. Furthermore, an econometric model to test the impact on these variables of any novel features of the lectures is also developed. The span of students\u2019 attention and engagement is found to be unexpectedly long over a given lecture. However, as the course evolves, attention builds up whereas engagement drops quickly. Our methods are further illustrated by displaying an evaluation of the introduction of a gamification feature in the classroom in the form of a contest. This new element proves effective in raising engagement after some adjustments but it delivers no impact on attention.", "title": "Measuring Engagement and Attention by Means of Electronic Response Systems", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Nicholas Gaspelin", "S. Luck"], "summary": "ABSTRACT Luck et al. (2021) reviewed evidence that observers can learn to suppress attentional capture by salient distractors. Several commentaries were written in response to this review paper, many of which raised important and interesting issues. Here, we respond to these commentaries. Although there has been substantial progress in the attentional capture debate, there are still remaining issues that need to be addressed before the debate is completely resolved. Specifically, we summarize the need for an independent measure of bottom-up salience and better metrics of how attentional control unfolds over time. Ultimately, the field may need a more refined theoretical model of visual attention that distinguishes between attentional priority, attentional orienting, and attentional engagement.", "title": "Progress and remaining issues: A response to the commentaries on Luck et al. (2021)", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Nancy E. Moss", "Lauren  Moss-Racusin"], "summary": "Assessment in this domain examines a youngster\u2019s neurologically-based capacity to screen out distracting stimulation; focus on the most important information, activity, or communication in their immediate environment; and then sustain concentration on this information, activity, or communication for an optimal amount of time. Specific aspects of attentional functioning are also investigated. These include attention to auditory versus visual information, attention to simple segments of information versus complex information that requires more advanced mental manipulations, attention to fundamentally interesting versus more monotonous information, and the ability to maintain attention and concentration even in the face of interference from new information and task demands.", "title": "Attention and", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Panagiotis Mavridis"], "summary": "This report presents the work during my internship concerning the creation of a model of attention for a virtual agent capable of interacting with a human through body movements. Attention is defined as the process of filtering on the intelligent entities senses. Depending on the stimuli the attention is characterized as bottom up (external stimuli) or top down. Different models of attention argue on the time a perception filter acts on attended or unattended information or introduce the notion of attentional capacity and argue that the filtering relates to the semantic meaning of the perceived information. Also, specific behaviour expressed by body movements, gestures and gaze is related to attention that is an intrinsic human mechanism. To create believable virtual agents that resemble human, implementation of attention mechanisms is needed. The existing computational models are grouped into methods that perform either visual search to find points of interest or semantic processing or implement overt behaviour or combine methods that are used for expressive embodied conversational agents. Based on the previous work, we propose some ideas about the conception of our model. We think that a combination of bottom up and top down method along with a vision field filtering for the agent that depend on its head orientation in order to show a more believable behaviour. A module of Attention was created as a part of a multi-module architecture virtual agent architecture. The modules communicate through messages. The Attention module consists of a vision field filter, a motion calculation, a state machine, a strategy pattern. The vision field filters whether the user is visible or not to the agent. Then the motion calculation takes into account the visible parts of the user that move in order to calculate a level of attention for the agent. Then depending on the input from the vision filter and the motion calculation a state is calculated and an action (message is sent) is taken.", "title": "Model of attention for a virtual agent able to interact with a user through body movements", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Jelena Ve\u0161i\u0107"], "summary": "battles over the remaining resources. The attention has been extracted from the domains of knowledge and human relations \u2013 the domains of politics \u2013 and incorporated into the flow of contemporary capitalism.1 It is of no surprise it becomes hunted for and colonized both from inside and out, turning into a new market niche, a sight of pursuit of profit. There is little left from the palette of human behaviour not yet detected, caught, conquered or utilized. As attention stands for time, one thing that financial capitalism does best is turn time into currency. It is also a promising resource; the attention span can shrink, but it can also be expanded, controlled, disciplined and made use of in various ways. Attention is a special form of temporary human existence: the Attentive Behaviour assumes the state of being under the speedy drive of the stress hormone \u2013 to be Alert, Awake, Watchful, Observant, Aware; in order for the others to distinguish from the various forms of mental illnesses, while being Attentive, one also needs to be Courteous, Perceptive, Proactive (or Else).2 Today, attention is perpetually coupled to its opposite, to its inverse other, to its annulment and disappearance \u2013 distraction. So, the contemporary struggle for attention manifests as the never-ending succession of distractions. In order to catch one\u2019s attention, you need to distract it first, that is: in order to be attracted, the attention dedicated elsewhere needs to be distracted. The reality of our post-attention times is the reality of distraction, and anxiety as the consequence of over-exposure to the requests for attention coming from all around.3 The continuum of the self \u2013 \u2018the immersive self\u2019, \u2018the absorbed self\u2019 or \u2018the introspective self\u2019 \u2013 is broken into pieces. The sovereignty of contemplation is cancelled in favour of the existence under the constant interruptions. In previous times, paying close attention to something actually meant reaching the necessary point of focus in order to observe, follow and contemplate, for example, an artwork \u2013 a film, a theatre play, a music piece and so forth.4 Even the examples in online dictionaries that contextualize the meaning of the adjective attentive are most often pairing it with the notion of the audience: \u2018He set down his box and smiled at his attentive audience\u2019; \u2018Never before she had such an attentive audience\u2019. Attention is here described as the thing that connects the audience and performers. Or, in other words, without the medium of attention the message \u2013 the meaning \u2013 is not conveyable. But it is not for certain anymore that the contemporary audience still matches so well with the notion of attentive. Now, it seems to be rather corresponding with the notions of absentminded and distracted. In order to preserve attention as prerequisite for the economy of art experience, institutions are searching for instruments of capturing audience\u2019s commitment without reduction or dispersion. But there are also other proposals, which tend to abandon altogether the logic of the old representationalist order summarized in the notion of attentive audience and \u2018perspective as symbolic form\u2019 (Panofsky 1991 [1927]). The idea that unites such proposals lays in overcoming the hierarchy of a frontal viewing experience \u2013 the screen, the stage, the painting \u2013 which spatially organizes individual or collective contemplation. It is something that experimental art practices dealt with since the 1960s and 1970s and what eventually \u2018big-banged\u2019 with the famous Ridley Scott\u2019s (1983) \u2018Big Brother\u2019 campaign for Macintosh computers.5 The world witnessed the female runner with the hammer who breaks the monumental screen that used to hypnotize the large (attentive) audience; their individualities, now finally released \u2013 \u2018liberated\u2019 \u2013 were reduced to sameness before by their collective submission to the screen. Scattered Democratization Performance in the times of disappearance of attention", "title": "Scattered Democratization", "claims": null}], "query": "attentional engagement", "summary_abstract": "The research question of attentional engagement is addressed through various studies that explore how attention is allocated, maintained, and influenced by different factors. \n\nBecker et al. (2020) extend the relational account of attentional engagement, demonstrating that attention can be tuned not only to specific features of objects but also to their relative characteristics in a temporal context. Their findings indicate that attentional engagement is affected by the presence of distractors that match the relative features of a target, suggesting that attentional engagement operates dynamically over time.\n\nFolk et al. (2009) emphasize the role of attentional engagement in preventing attentional capture by irrelevant stimuli. Their experiments reveal that when attention is actively engaged, it can effectively block out distractions, highlighting the importance of attentional engagement over mere attentional focusing.\n\nPerra and Gattis (2012) investigate developmental changes in attentional engagement in infants, finding that as infants grow, their ability to engage attention evolves significantly. This developmental trajectory suggests that attentional engagement is a critical component of cognitive development, allowing infants to interact more effectively with their environment.\n\nMoss and Moss-Racusin (2021) focus on the assessment of attentional functioning in children, examining how they can screen out distractions and maintain focus on relevant information. Their work underscores the complexity of attentional engagement, which involves various aspects of attention, including the ability to sustain concentration amidst competing stimuli.\n\nIn a broader context, Longarela (2020) discusses the theoretical underpinnings of attentional engagement, suggesting that a refined model of visual attention is necessary to understand how attentional priority, orienting, and engagement interact.\n\nOverall, these studies collectively highlight that attentional engagement is a multifaceted construct influenced by contextual factors, developmental stages, and the interplay between focused attention and distractor management. The findings suggest that enhancing attentional engagement can lead to improved cognitive performance and better management of distractions in various settings.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2010}, "authors": ["Bryan W. Sokol", "Stuart I. Hammond", "Marvin W. Berkowitz"], "summary": "The relationship between character education and developmental psychology has long been marked by tension. Recent scholarly advances within these two disciplines, however, offer a promising new pathway of dialogue and productive exchange. The present chapter is the result of one such exchange. Our efforts represent an attempt to \u201cpsychologize\u201d character in order to clarify its structure and trace its developmental contours. With a clearer picture of the psychological processes contributing to character, we can better identify the best educational practices for promoting its growth. Our discussion focuses on the processes associated with children\u2019s developing self-regulation, autonomy, perspective taking, moral reasoning, empathy, and emotional competence.", "title": "The Developmental Contours of Character", "claims": null}, {"metadata": {"year": 2015}, "authors": ["\u041c. \u0412. \u0427\u0438\u0445\u0430\u0447\u0451\u0432", "\u041c. \u0412. \u0410\u043b\u0435\u0449\u0435\u043d\u043a\u043e"], "summary": "This article discusses different perspectives on a problem of nature of a human character in psychology. The main content of the research is the analysis of domestic and foreign literature on the patterns of nature. Considerable attention is paid to factors of development as a driving force of the process of character formation, and conditions as the circumstances,due to which something depends in this process. The authors come to the conclusion that the character is developed on the basis of existing human internal features of the mental system. Among the factors determining the process of development of character the researchers name social environment with sociopolitical component, family component, school educational component; upbringing process, with the main accent on moral and spiritual traits in character. The outside factors are subject to change themselves and change an individual and his character. The changeability of the character is not always understood as instability feature but is connected with many expected process in its development.", "title": "\u041a \u0432\u043e\u043f\u0440\u043e\u0441\u0443 \u043e \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0435 \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0430 \u0438 \u0435\u0433\u043e \u0440\u0430\u0437\u0432\u0438\u0442\u0438\u044f", "claims": null}, {"metadata": {"year": 2002}, "authors": ["H. Kohut"], "summary": "What fascinates me most about literature is its portrayal of human beings and their relationships. For many years I have been developing a psychological approach in which I try to understand the behavior of realistically drawn characters in the same way that we understand the behavior of real people. These characters are not flesh and blood creatures, of course, but are imagined human beings who have many parallels with people like ourselves. Numerous critics have maintained that it is inappropriate or impossible to explain the behavior of fictional characters in motivational terms, but I argue in Chapter 1 that the rejection of psychological analysis has been a major critical error.", "title": "Imagined Human Beings : A Psychological Approach to Character and Conflict in Literature Preface", "claims": null}, {"metadata": {"year": 2002}, "authors": ["Marisa Bortolussi", "P. Dixon"], "summary": "After plot, the most intuitively important aspect of a story concerns the characters. For example, in some simple stories, characters create the plot: The villain creates a problem that the hero must overcome. In some complex, literary narratives, characterization would seem to be an overriding motivation of the implied author, with the events of the narrative merely serving to provide information about the characters. Not surprisingly, then, character and characterization have been a productive area of scholarship in narratology and literary studies. Important work has also been done in linguistics and discourse processing. Our view is that work in personality and social psychology is also directly relevant to understanding character in narrative, although the connection has not always been made in discourse processing research. In this chapter, we review some of the work in these disciplines. We then discuss some categories of features that are relevant to character in narrative and provide a general framework for how these might be used by readers. Finally, we provide some evidence on the use of characterization features by readers. Theories of Literary Character Theories of literary character can be situated on a continuum ranging from traditional to contemporary and more radical models. (For an excellent coverage of these theories, see Margolin, 1989, 1990b). The central issue in this debate has been the relationship between literary character and real people. Traditional theories treated literary characters uncritically as analogues of real people.", "title": "Psychonarratology: Characters and Characterization", "claims": null}, {"metadata": {"year": 1996}, "authors": ["Peter B. Murray"], "summary": "This chapter employs modern psychology to analyze the way character and cognition are shaped in social roles and the way theatrical roles affect actors. The analysis especially focuses on the causes of our identifying or not identifying with the selves we enact in social roles and the causes of the sense actors have that they \u201cbecome\u201d or do not become the characters they play. To provide a synthesis of these aspects of the psychology of acting and social role-playing, the discussion will draw on the observations of writers in areas ranging from sociological role theory to developmental psychology to the training of actors.", "title": "Character Formation and the Psychology of Role-Playing and Acting", "claims": null}, {"metadata": {"year": 1985}, "authors": ["Stephen M. Johnson"], "summary": "This book integrates object relations theory, ego psychology, and character analytic approaches to provide a new understanding of human behavior and character development. In itself, this integration is a remarkably innovative undertaking, yielding a consistent, understandable and clinically useful view of psychopathology, therapy, and health. Johnson uses an active treatment approach that draws upon all major schools of psychotherapeutic thought, choosing techniques that serve specific purposes and outlining changes in behavioral, affective, and cognitive domains that are necessary for lasting characterological change. Focusing on character pathology resulting from disorders in attachment, the book discusses etiology, characteristic affects, behaviors and cognitions, bodily expressions of character, and therapeutic objectives and techniques.", "title": "Characterological Transformation: The Hard Work Miracle", "claims": null}, {"metadata": {"year": 1992}, "authors": ["D. McLean"], "summary": "This paper explores the concept of character from the point of view of its use both in everyday language and in psychoanalytic literature. It also examines, with clinical illustrations, some of the complex interactions of maturation and experience in the formation of character.", "title": "Maturational and experiential components of character formation.", "claims": null}, {"metadata": {"year": 1987}, "authors": ["Stephen M. Johnson"], "summary": "It sacrifices fulfillment to driven achievement, self-worth to praise from others, and the joy of human connectedness to superficial, manipulative relationships. Johnson's carefully conceived integrative psychoanalytic developmental theory will enable therapists to make sense of the array of symptoms seen in character disorders and character styles so that an eclectic treatment approach may be pursued with the guidance of a unified theory. This is the second book in a series that began with Characterological Transformation.", "title": "Humanizing the narcissistic style", "claims": null}], "query": "Psychology and novel character development", "summary_abstract": "The exploration of psychology in the context of novel character development reveals a rich interplay between psychological theories and literary characterization. Several authors emphasize the importance of understanding characters through a psychological lens, suggesting that the behaviors and motivations of fictional characters can be analyzed similarly to real individuals.\n\nSokol et al. (2010) highlight the significance of developmental psychology in character education, noting that understanding psychological processes such as self-regulation, empathy, and moral reasoning can enhance character development. This perspective suggests that characters in literature can be crafted to reflect these developmental traits, making them more relatable and realistic.\n\nChikhachev and Aleshchenko (2015) delve into the nature of human character, asserting that character formation is influenced by various factors, including social environment and upbringing. This insight can inform writers about the complexities of character development, as it underscores the role of external influences on a character's growth and transformation throughout a narrative.\n\nKohut (2002) argues for the validity of applying psychological analysis to fictional characters, positing that understanding their motivations can enrich literary criticism. This approach encourages authors to create characters with depth, mirroring the complexities of real human behavior.\n\nBortolussi and Dixon (2002) discuss the relationship between character and plot, suggesting that character-driven narratives often prioritize the development of characters over the unfolding of events. This indicates that a well-developed character can serve as the backbone of a story, guiding the plot through their actions and decisions.\n\nMurray (1996) examines the impact of social roles on character and cognition, particularly in the context of acting. This perspective can be applied to novel writing, where characters may embody various roles that reflect their psychological states and societal influences.\n\nJohnson (1985, 1987) integrates various psychological theories to provide a comprehensive understanding of character development, emphasizing the importance of attachment and relational dynamics in shaping character traits. This integration can serve as a valuable framework for authors seeking to create psychologically nuanced characters.\n\nIn summary, the intersection of psychology and character development in literature suggests that a deep understanding of psychological principles can significantly enhance the creation of complex, relatable characters. By considering factors such as developmental processes, social influences, and psychological motivations, writers can craft characters that resonate with readers on a profound level.", "summary_extract": null}, {"papers": [{"metadata": {"year": 1997}, "authors": ["Masahide Sato", "M. Uwaha"], "summary": "We study time evolution of parallel straight steps with repulsive interaction between steps. If step kinetics is asymmetric in the upper and the lower terraces (Schwoebel effect), a vicinal face becomes unstable when undersaturation exceeds a critical value, and an array of large bunches described by the Benney equation appears. In the one-sided model (the extreme limit of the asymmetry) a pairing instability occurs. In this case the instability always ends up with formation of step pairs, and with large undersaturation hierarchical bound states of step pairs are formed. On the contrary many-body bound states appear in the general asymmetric model.", "title": "Kinematical Bound States of Steps Caused by Asymmetry in Step Kinetics.", "claims": null}, {"metadata": {"year": 2004}, "authors": ["H.  Flashner", "A.  Beuter", "A.  Arabyan"], "summary": "The present study extends past work on modeling and control of stepping. The relationship between joint space kinematic data and routine motor control (i.e., open loop) during human stepping is investigated. A model of open loop stepping control using joint kinematics is described. Different functional approximations are employed to simulate experimental joint kinematic data collected on a subject stepping repeatedly over an obstacle. Results indicate that joint kinematics can be characterized by a small number of functions yielding a simple analytical description of open loop motor control. The different basis functions used and their associated coefficients reflected the qualitative behavior of joint trajectories thus allowing flexibility in the formulation of system kinematics. This approach provides a tool to study movement pathologies and movement development by identifying the basis functions governing the kinematics of motion and their associated coefficients. The model presented here is helpful in studying the segmentation of multiarticular movements into their elementary components by analytically modeling the discrete organization of motor behavior.", "title": "Fitting mathematical functions to joint kinematics during stepping: Implications for motor control", "claims": null}, {"metadata": {"year": 2021}, "authors": ["I. Grinevich", "V. Nikishin", "G. Spri\u0146\u0123is"], "summary": "Abstract In this paper, a kinematic analysis of Theo Jansen\u2019s stepping mechanism has been carried out and an algorithm for finding the output link trajectory from the given dimensions of the stepping mechanism elements, implemented by Mathcad program, has been developed. It is possible to output characterising parameters of all intermediate links with any number of intermediate links of a step cycle of the mechanism. The dimensions of the mechanism elements have been selected so that they provide the optimal smooth trajectory of the stepping point, minimising the mechanism oscillations in the vertical plane. A comparison of the trajectories of the foot in this study with the classical trajectory of Theo Jansen and the trajectory from article [7] has been provided. A minimum swing of the oscillation of the centre of mechanism masses in the vertical plane has been selected as an optimality criterion, combined with the maximum smoothness of the trajectory, provided that maximization of the step height is not required.", "title": "Research of Kinematic Stepping Mechanism", "claims": null}, {"metadata": {"year": 2010}, "authors": ["B. V. Basten", "P. Peeters", "A. Egges"], "summary": "Especially in a constrained virtual environment, precise control of foot placement during character locomotion is crucial to avoid collisions and to ensure a natural locomotion. In this paper, we present the step space: a novel technique for generating animations of a character walking over a set of desired foot steps in real\u2010time. We use an efficient greedy nearest\u2010neighbor approach and warp the resulting animation such that it adheres to both spatial and temporal constraints. We use smart aligning and inverse kinematics to ensure spatial constraints and time warping for temporal constraints. We will show that our technique can generate realistic locomotion animations very efficiently even though we impose many constraints on the animation. Copyright \u00a9 2010 John Wiley & Sons, Ltd.", "title": "The step space: example\u2010based footprint\u2010driven motion synthesis", "claims": null}, {"metadata": {"year": 2010}, "authors": ["B. V. Basten", "P. Peeters", "A. Egges"], "summary": "Especially in a constrained virtual environment, precise control of foot placement during character locomotion is crucial to avoid collisions and to ensure a natural locomotion. In this paper, we present the step space: a novel technique for generating animations of a character walking over a set of desired foot steps in real-time. We use an efficient greedy nearest-neighbor approach and warp the resulting animation such that it adheres to both spatial and temporal constraints. We use smart aligning and inverse kinematics to ensure spatial constraints and time warping for temporal constraints. We will show that our technique can generate realistic locomotion animations very efficiently even though we impose many constraints on the animation. Copyright \u00a9 2010 John Wiley & Sons, Ltd. \n \nIn this paper, we present the step space: a novel technique for generating animations of a character walking over a set of desired foot steps in real-time. We use an efficient greedy nearest-neighbor approach and employ smart aligning and warping to ensure that the resulting animation adheres to both spatial and temporal constraints.", "title": "The step space: example-based footprint-driven motion synthesis", "claims": null}, {"metadata": {"year": 2004}, "authors": ["H.  Flashner", "A.  Beuter", "A.  Arabyan"], "summary": "In a previous study (Beuter et al. 1986) the authors modeled a stepping motion using a three-body linkage with four degrees of freedom. Stepping was simulated by using three task parameters (i.e., step height, length, and duration) and sinusoidal joint angular velocity profiles. The results supported the concept of a hierarchical control structure with open-loop control during normal operation. In this study we refine the dynamic model and improve the simulation technique by incorporating the dynamics of the leg after landing, adding a foot segment to the model, and preprogramming the complete step motion using cycloids. The equations of the forces and torques developed on the ground by the foot during the landing phase are derived using the Lagrangian method. Simulation results are compared to experimental data collected on a subject stepping four times over an obstacle using a Selspot motion analysis system. A hierarchical control model that incorporates a learning process is proposed. The model allows an efficient combination of open and closed loop control strategies and involves hardwired movement segments. We also test the hypothesis of cycloidal velocity profiles in the joint programs against experimental data using a novel curve-fitting procedure based on analytical rather than numerical differentiation. The results suggest multiob-jective optimization of the joint's motion. The control and learning model proposed here will help the understanding of the mechanisms responsible for assembling selected movement segments into goaldirected movement sequences in humans.", "title": "Modeling of control and learning in a stepping motion", "claims": null}, {"metadata": {"year": 2011}, "authors": ["F. Mateji\u010dek"], "summary": "Ud\u017ebenik sadr\u017ei temeljne pojmove gibanja cestice i tijela u ravnini i prostoru. Prikazani su osnovni izvodi za vektore brzina i ubrzanja i drugih bitnih kinematickih velicina.", "title": "Kinematics with exercises", "claims": null}, {"metadata": {"year": 2014}, "authors": ["M. Tsai", "Hsueh-Yung Lung"], "summary": "This paper proposes a novel two-phase optimization method to solve the joint variables of a real human model using captured motion data. A dual-mode 3D vision system was used to generate a body geometric model. Twenty-three link segments were separated from the body model. Markers were then affixed on each link for motion capturing by the same vision system. A body kinematic model was then constructed from the geometric model by assigning joint constraints between every two adjacent links. The model contains five kinematic chains with 48 joint freedoms. Joint variables were solved by the first-phase optimization to obtain an appropriate initial posture between two adjacent links. For each kinematic chain, redundant joint variables were solved using the second-phase optimization. By setting proper weightings, the resulting postures of the kinematic model closely match up with those of the motion captured data, while the endpoints can trace the original trajectories as well. Furthermore, the kinematic model offers more reasonable motions than the geometric model that replicates the raw motion data since some excessive movements are corrected by the joint constraints.", "title": "Two-phase optimized inverse kinematics for motion replication of real human models", "claims": null}], "query": "false step kinematics", "summary_abstract": "The research question regarding false step kinematics is addressed through various studies that explore the dynamics and control of stepping mechanisms, both in biological and mechanical contexts.\n\nSato and Uwaha (1997) investigate the time evolution of steps in a vicinal face, highlighting the role of repulsive interactions and the Schwoebel effect, which leads to instabilities in step formation under certain conditions. Their findings suggest that the dynamics of step formation can lead to complex structures such as step pairs and many-body bound states, which are crucial for understanding the kinematics of steps in materials.\n\nIn the realm of human motion, Flashner et al. (2004) present a model of open-loop stepping control that utilizes joint kinematics to simulate stepping over obstacles. Their work emphasizes the importance of joint trajectories and the hierarchical control structure that governs stepping motions, which can be applied to analyze movement pathologies and enhance our understanding of motor control.\n\nGrinevich et al. (2021) focus on the kinematic analysis of a stepping mechanism, developing an algorithm to optimize the trajectory of the output link. Their research aims to minimize oscillations and maximize smoothness in the stepping motion, which is essential for achieving efficient and stable kinematics in mechanical systems.\n\nBasten et al. (2010) contribute to the discussion by introducing a novel technique for generating realistic locomotion animations in virtual environments. Their method emphasizes precise control of foot placement, which is critical for avoiding collisions and ensuring natural movement, thereby linking the concept of kinematic control to practical applications in animation and robotics.\n\nOverall, these studies collectively highlight the intricate relationship between kinematic principles and the control of stepping motions, whether in biological systems or mechanical models. They underscore the significance of understanding both the theoretical and practical aspects of false step kinematics in various contexts.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2005}, "authors": ["Marc  Ramael", "Kurt  Segers", "Corinne  Buysse", "Jan Van den Bossche", "Eric Van Marck"], "summary": "An immunohistochemical study of the epidermal growth factor (EGF) receptor in non-neoplastic pleural mesothelium (35 cases) and in human malignant mesothelioma (36 cases) was made, using a murine monoclonal antibody OM-11-951. All malignant mesotheliomas and non-neoplastic pleural biopsies exhibited a strong cytoplasmic immunoreactivity in mesothelial cells. Nuclear immunoreactivity was detected in mesothelial cells of all specimens of both malignant and non-neoplastic pleura. No statistically significant differences were found between malignant mesothelioma and non-neoplastic pleural mesothelium. There were differences, between the three subtypes of mesothelioma, in the number of cells that exhibited nuclear staining. Statistically significant differences were noted between the epithelial subtype and the mesenchymal subtype (P< 0.005), epithelial subtype versus the mixed cell type (P< 0.005) and between the mesenchymal component of the mixed cell type and the mesenchymal type (P<0.0005). We conclude that there is strong expression of EGF receptor in both malignant mesothelioma and in non-neoplastic pleural mesothelium. Different staining patterns are seen when comparing the different subtypes of mesotheliomas with each other. EGF receptor expression cannot be used to distinguish between malignant and benign mesothelium.", "title": "Immunohistochemical distribution patterns of epidermal growth factor receptor in malignant mesothelioma and non-neoplastic mesothelium", "claims": null}, {"metadata": {"year": 1990}, "authors": ["G. Merlino"], "summary": "The epidermal growth factor (EGF) receptor is a transmembrane, cell-surface glycoprotein that mediates the mitogenic action of a family of ligands, including EGF and transforming growth factor alpha (TGF alpha). Perturbation of this signal transduction pathway by exposure to excess ligand, by overproduction of the normal EGF receptor, or by the presence of specific mutated forms of this receptor can result in dramatic alterations in cellular phenotype, including malignant transformation. Overstimulation of normal cells is avoided by precise control of the synthesis and degradation of EGF receptors. Regulation occurs at multiple levels, including transcriptional control. A number of DNA-binding proteins have now been identified which positively and negatively modulate EGF receptor gene transcription.", "title": "Epidermal growth factor receptor regulation and function.", "claims": null}, {"metadata": {"year": 1990}, "authors": ["J. Haley"], "summary": "The epidermal growth factor receptor, a transmembrane protein tyrosine kinase, plays a crucial role in cellular signalling both in embryonic development and in adult tissues. By phosphorylation of substrate proteins on tyrosine, signals may be transmitted which result in gene expression, ion flux and mitogenesis to name but a few of the pleiotropic effects of receptor activation. The epidermal growth factor receptor is relatively well studied amongst an increasing family of tyrosine kinases and makes a good model system for studying structure-function relationships involved in signal transduction. Here, the structure-function relationships for cellular transformation by the EGF receptor proto-oncogene are reviewed as well as data pertaining to its overexpression in human tumors.", "title": "Regulation of epidermal growth factor receptor expression and activation: a brief review.", "claims": null}, {"metadata": {"year": 1990}, "authors": ["Hale Jd"], "summary": ": The epidermal growth factor receptor, a transmembrane protein tyrosine kinase, plays a crucial role in cellular signalling both in embryonic development and in adult tissues. By phosphorylation of substrate proteins on tyrosine, signals may be transmitted which result in gene expression, ion flux and mitogenesis to name but a few of the pleiotropic effects of receptor activation. The epidermal growth factor receptor is relatively well studied amongst an increasing family of tyrosine kinases and makes a good model system for studying structure-function relationships involved in signal transduction. Here, the structure-function relationships for cellular transformation by the EGF receptor proto-oncogene are reviewed as well as data pertaining to its overexpression in human tumors.", "title": "Regulation of epidermal growth factor receptor expression and activation: a brief review.", "claims": null}, {"metadata": {"year": 1984}, "authors": ["R. St-Arnaud", "J. Chabot", "G. Pelletier", "F. Labrie", "P. Walker"], "summary": "Epidermal growth factor (EGF) is a Mr 6045 polypeptide first characterized for its ability to stimulate mitogenesis in epidermal and epithelial cells. The first step in the action of the growth factor is its binding to specific, high affinity membrane receptors. These receptors have been studied in a number of tissues and cell culture lines. The level of EGF receptors is modulated by many agents. EGF down-regulates its receptor. In addition, the number of EGF receptors is decreased by other growth factors (platelet-derived growth factor; transforming growth factor), by many tumor promoters and by viral transformation. Several hormones also can regulate EGF binding in its target tissues.", "title": "[Epidermal growth factor: structure, location, phosphorylation and regulation of its receptor].", "claims": null}, {"metadata": {"year": 1985}, "authors": ["Joseph  Schlessinger", "Irit  Lax", "Shoshana  Gill", "Richard  Kris", "Towia A. Libermann", "Menashe  Bar-Eli", "Ilana  Harari", "Yosef  Yarden"], "summary": "A useful model system for exploring the molecular mechanisms underlying the proliferation of eucaryotic cells is the mode of action of the cellular mitogen epidermal growth factor (EGF). Epidermal growth factor is a small protein containing 53 amino acid residues (Carpenter and Cohen, 1980). It binds tightly to a specific membrane receptor. The occupation of the receptor molecule leads to activation of the pleiotropic response culminating in DNA synthesis and cell proliferation (Carpenter and Cohen, 1980; Schlessinger 1983).", "title": "The Membrane Receptor for Epidermal Growth Factor", "claims": null}, {"metadata": {"year": 2003}, "authors": ["Robert N Jorissen", "Francesca  Walker", "Normand  Pouliot", "Thomas P J Garrett", "Colin W Ward", "Antony W Burgess"], "summary": "The epidermal growth factor (EGF) receptor (EGFR) is one of four homologous transmembrane proteins that mediate the actions of a family of growth factors including EGF, transforming growth factor-alpha, and the neuregulins. We review the structure and function of the EGFR, from ligand binding to the initiation of intracellular signalling pathways that lead to changes in the biochemical state of the cell. The recent crystal structures of different domains from several members of the EGFR family have challenged our concepts of these processes.", "title": "Epidermal growth factor receptor: mechanisms of activation and signalling.", "claims": null}, {"metadata": {"year": 1989}, "authors": ["G. Todderud", "G. Carpenter"], "summary": "Epidermal growth factor (EGF) is a small polypeptide hormone with mitogenic properties in vivo and in vitro. EGF elicits biologic responses by binding to a cell surface receptor which is a transmembrane glycoprotein containing a cytoplasmic protein tyrosine kinase. EGF responses are mediated by ligand binding and activation of this intrinsic protein kinase. The receptor can be phosphorylated by other protein kinases, and this may regulate receptor function. Stimulation of the receptor tyrosine kinase activity by ligand binding must regulate the activity of an as yet undefined molecule(s) responsible for transmitting a mitogenic signal to the nucleus.", "title": "Epidermal growth factor: the receptor and its function.", "claims": null}], "query": "epidermal growth factor receptors overexpression mesothelium", "summary_abstract": "The research question regarding the overexpression of epidermal growth factor receptors (EGFR) in mesothelium is addressed through various studies that highlight the presence and implications of EGFR in both malignant and non-neoplastic pleural mesothelium.\n\nRamael et al. (2005) conducted an immunohistochemical study that revealed strong cytoplasmic immunoreactivity for EGFR in both malignant mesothelioma and non-neoplastic pleural biopsies. They found no statistically significant differences in EGFR expression between malignant and benign mesothelium, indicating that while EGFR is overexpressed in both contexts, it cannot be used as a distinguishing factor between the two. However, they noted differences in nuclear staining patterns among the various subtypes of mesothelioma, suggesting that while EGFR is present, its expression may vary in a subtype-specific manner.\n\nFurther insights into the role of EGFR in cellular transformation and signaling are provided by Merlino (1990) and Haley (1990), who discuss the receptor's function as a transmembrane protein that mediates mitogenic actions. They emphasize that overexpression or mutations in the EGFR can lead to significant alterations in cellular behavior, including malignant transformation. This suggests that while EGFR is present in both benign and malignant mesothelium, its overexpression may contribute to the malignant phenotype in certain contexts.\n\nOverall, the collective findings indicate that EGFR is indeed overexpressed in mesothelium, both in malignant and non-neoplastic forms, but its expression alone does not serve as a reliable marker for malignancy. The nuances of its expression patterns among different mesothelioma subtypes may provide further avenues for research into its role in mesothelioma pathology (Ramael et al., 2005; Merlino, 1990; Haley, 1990).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2014}, "authors": ["Rafael  Ferreira", "Luciano de Souza Cabral", "Frederico Luiz Gon\u00e7alves de Freitas", "Rafael Dueire Lins", "Gabriel de Fran\u00e7a Pereira e Silva", "Steven J. Simske", "Luciano  Favaro"], "summary": "The massive quantity of data available today in the Internet has reached such a huge volume that it has become humanly unfeasible to efficiently sieve useful information from it. One solution to this problem is offered by using text summarization techniques. Text summarization, the process of automatically creating a shorter version of one or more text documents, is an important way of finding relevant information in large text libraries or in the Internet. This paper presents a multi-document summarization system that concisely extracts the main aspects of a set of documents, trying to avoid the typical problems of this type of summarization: information redundancy and diversity. Such a purpose is achieved through a new sentence clustering algorithm based on a graph model that makes use of statistic similarities and linguistic treatment. The DUC 2002 dataset was used to assess the performance of the proposed system, surpassing DUC competitors by a 50% margin of f-measure, in the best case.", "title": "A multi-document summarization system based on statistics and linguistic treatment", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Aishwarya Krishnakumar", "R. FathimaNaushinA", "L. MrithulaK.", "B. Bharathi"], "summary": "The notion of summarising is as old as the ancient Greek and Roman plays. The information available in various situations, from plays to meetings to non-fiction work, has been and is summarised for as long as we remember. This abstract itself is an example of the usage of the concept of summarization. In this era of technology, everything around us is digitized. People tend to develop ideas that perform activities that only humans were able to do before the innovation of modern technology. Summarizing text documents is one such example. Today, we have developed various NLP and AI models to perform text summarization. While efficient models exist for native English, little attention is given to Indian languages. This paper discusses the work done by SSNCSENLP in ILSUM Indian Language Summarization on the multilingual code-mixed text task of FIRE 2022. In this paper, we present a comparison of the performance of a few existing models. From our best-evaluated model, we were ranked among the top ten on the validation sets for all three Indian languages\u2014English, Gujarati, and Hindi. To summarize the above mentioned languages we have used mT5_m2m_CrossSum, XL-Sum, Bert and the mT5-small models of which mT5_m2m_CrossSum generated precise summaries of the given text.", "title": "Text summarization for Indian languages using pre-trained models", "claims": null}, {"metadata": {"year": 2001}, "authors": ["Dawn J Lawrie"], "summary": "LANGUAGE MODELS FOR HIERARCHICAL SUMMARIZATION (PROPOSAL FOR DISSERTATION)", "title": "LANGUAGE MODELS FOR HIERARCHICAL SUMMARIZATION (PROPOSAL FOR DISSERTATION)", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Shibo Ji", "Bo Yang"], "summary": "With the advent of the era of Big Data, Internet transmission has become more convenient and rapid, and the textual information published and received by each person every day is exploding. Text summarization technology can automatically summarize long texts to obtain more concise summary information. However, constructing a summary model that can generate long texts with fluent semantics is still a complex research problem, which usually suffers from long-distance dependency problems, unregistered words problems, and poor readability problems. In this paper, we proposes a Long-Short Transformer based text Summarization model (LSTS) that captures both local and global information of the input document while using a pointer network to alleviate the problem of unlabeled words and introducing an attention mechanism in the Decoder to alleviate the problem of repeated words. We conducted sufficient experiments in the two large-scale datasets to verify the model\u2019s effectiveness.", "title": "Abstractive Text Summarization Based on Long-Short Transformer", "claims": null}, {"metadata": {"year": 2014}, "authors": ["K. Sarkar"], "summary": "As the amount of on-line information in the languages other than English (such as Chinese, Japanese, German, French, Hindi, etc.) increases, systems that can automatically summarize multilingual documents are becoming increasingly desirable for managing information overload problem on the Web. This chapter presents an overview of automatic text summarization with special emphasis on multilingual text summarization. The various state-of-the-art multilingual summarization approaches have been grouped based on their characteristics and presented in this chapter.", "title": "Multilingual Summarization Approaches", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Haoqi Sun", "Ning Luo", "Li-juan Zhou", "Songwei Wei"], "summary": "With the popularization and development of the Internet, text information has shown an exponential growth trend. This information overload phenomenon affects the ability of users to receive critical information, and people\u2019s demand for quick access to information is increasing. Text summarization technology uses computers to automatically extract the key information of text, which helps to grasp the text content accurately and quickly, so it has a good application prospect. The traditional rule-based method simply counts the word frequency and lacks the consideration of the semantic information of the text, so the results are not accurate enough. To this end, this paper proposes an extractive text summarization algorithm based on multiple feature weighting, which comprehensively considers the global information, surface information, structural information, and semantic information of the text by weighting the sentence position, the total amount of keyword information, keyword distribution, and semantic similarity. This method retains the advantages of the rule-based approach from not requiring data annotation and saving computational resources while improving the text understanding capability of the model. Experimental results show that the model improves the evaluation results of the datasets, improving the quality and accuracy of text summarization. And when the model is applied to the text processing system, the user can quickly obtain the required information, effectively speeding up the process of obtaining and processing information.", "title": "Design and Implementation of Text Processing System Based on Summarization Algorithm", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Pallavi Kohakade", "S. Jadhav"], "summary": "Automatic text summarization is an essential natural language processing application that goals to summarize a given textual content into a shorter model. The fast growth in media information transmission over the Internet demands text summarization using neural network from asynchronous combination of text. This paper represents a framework that utilizes the techniques of NLP technique to examine the elaborative information contained in multi-modal statistics and to enhance the aspects of text summarization. The basic concept is to bridge the semantic gaps among text content. After, the generated summary for important information through multimodal topic modeling. Finally, all the multi-modal factors are considered to generate a textual summary by maximizing the importance, non-redundancy, credibility and scope through the allocated accumulation of submodular features. The experimental result shows that Text Summarization framework outperforms other competitive techniques.", "title": "Text Summarization Framework Using Machine Learning", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Shilpadevi Vasant Bhagwat", "S. Thokal"], "summary": "In recent years, much work has been performed to summarize meeting recordings, sport videos, movies, pictorial storylines and social multimedia. Automatic text summarization is an essential natural language processing (NLP) application that goals to summarize a given textual content into a shorter model. The fast growth in multimedia information transmission over the Internet demands multi-modal summarization (MMS) from asynchronous combination of text, image, audio and video. This paper represents an MMS framework that utilizes the techniques of NLP, speech processing and OCR watermarking technique to examine the elaborative information contained in multimodal statistics and to enhance the aspects of multimedia news summarization. The basic concept is to bridge the semantic gaps among multi-modal content. For audio scheme, convert the audio signals into textual format. For visual scheme, extracts text from images using OCR technique. After, the generated summary for important visual information through content-picture matching or multi-modal topic modeling. Finally, all the multi-modal factors are considered to generate a textual summary by maximizing the importance, non-redundancy, credibility and scope through the allocated accumulation of submodular features. The contribution work is to identify theme of visual scheme. The experimental result shows that Multi-Modal Summarization framework outperforms other competitive techniques.", "title": "A Survey on Automatic Summarization Using Multi-Modal Summarization System for Asynchronous Collections", "claims": null}], "query": "information summarization large language models", "summary_abstract": "The research question regarding information summarization using large language models is addressed through various studies that highlight advancements in text summarization techniques, particularly in the context of handling large volumes of data.\n\nFerreira et al. (2014) present a multi-document summarization system that effectively extracts key aspects from a set of documents while minimizing redundancy and ensuring diversity. Their approach utilizes a novel sentence clustering algorithm based on a graph model, achieving significant performance improvements over existing systems.\n\nKrishnakumar et al. (2022) focus on the application of summarization techniques in Indian languages, showcasing the development of models like mT5_m2m_CrossSum, which excel in generating precise summaries for multilingual code-mixed text. This highlights the adaptability of summarization models to diverse linguistic contexts.\n\nJi and Yang (2023) introduce a Long-Short Transformer based text summarization model (LSTS) that addresses common challenges in summarization, such as long-distance dependencies and readability issues. Their model captures both local and global information, demonstrating effectiveness in generating coherent summaries from extensive texts.\n\nSarkar (2014) emphasizes the growing need for multilingual summarization systems as online information in various languages increases. This underscores the importance of developing summarization techniques that can operate across different languages to manage information overload.\n\nHaoqi et al. (2022) propose an extractive summarization algorithm that incorporates multiple feature weighting, enhancing the model's ability to understand text semantics while maintaining the efficiency of traditional rule-based methods. Their findings indicate improved accuracy in summarization tasks.\n\nKohakade and Jadhav (2020) discuss a framework that integrates NLP techniques with multimodal data to enhance text summarization. Their approach aims to bridge semantic gaps and improve the quality of generated summaries, demonstrating the potential of combining various data types for better summarization outcomes.\n\nBhagwat and Thokal (2019) extend the concept of summarization to multimedia contexts, proposing a multi-modal summarization framework that integrates text, audio, and visual information. Their work illustrates the growing trend of utilizing diverse data sources to create comprehensive summaries.\n\nCollectively, these studies illustrate the evolution and sophistication of text summarization techniques, particularly through the use of large language models and multimodal approaches, addressing the challenges posed by information overload in the digital age.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2017}, "authors": ["Stavros Degiannakis", "G. Filis"], "summary": "Accurate and economically useful oil price forecasts have gained significant importance over the last decade. The majority of the studies use information from the oil market fundamentals to generate oil price forecasts. Nevertheless, the extant literature has convincingly shown that oil prices are nowadays interconnected with the financial and commodities markets. Despite this, there is scarce evidence as to whether information from these markets could improve the forecasting accuracy of oil prices. Even more, there is limited knowledge whether high frequency data, given their rich information, could improve monthly oil prices. In this study we fill this void, employing a Mixed Data-Sampling (MIDAS) method using both oil market fundamentals and high frequency data from 15 financial and commodities assets. Our findings show that either the daily realized volatilities or daily returns of these assets significantly improve oil price forecasts relatively to the no-change forecast, as well as, relatively to the well-established models of the literature. These results hold true even when we consider tranquil and turbulent oil market conditions.", "title": "Forecasting oil prices", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Stavros  Degiannakis", "George  Filis"], "summary": "The paper examines the importance of combining high frequency financial information, along with the oil market fundamentals, in order to gain incremental forecasting accuracy for oil prices. Inspired by French et al. (1987) and Bollerslev et al. (1988), who maintain that future asset returns are also influenced by past volatility, we use daily volatilities and returns from financial and commodity markets to generate real out-of-sample forecasts for the monthly oil futures prices. Our results convincingly show that although the oil market fundamentals are useful for long-run forecasting horizons, the combination of the latter with high-frequency financial data significantly improve oil price forecasts, by reducing the RMSE of the no-change forecast by approximately 68%. Results are even more impressive during the oil price collapse period of 2014\u201315. These findings suggest that we cannot ignore the information extracted from the financial markets when forecasting oil prices. Our results are both statistically and economically significant, as suggested by several robustness tests.", "title": "Forecasting oil prices: High-frequency financial data are indeed useful", "claims": null}, {"metadata": {"year": 2013}, "authors": ["C. Baumeister", "Pierre Gu\u00e9rin", "L. Kilian"], "summary": "The substantial variation in the real price of oil since 2003 has renewed interest in the question of how to forecast monthly and quarterly oil prices. There also has been increased interest in the link between financial markets and oil markets, including the question of whether financial market information helps forecast the real price of oil in physical markets. An obvious advantage of financial data in forecasting oil prices is their availability in real time on a daily or weekly basis. We investigate whether mixed-frequency models may be used to take advantage of these rich data sets. We show that, among a range of alternative high-frequency predictors, especially changes in U.S. crude oil inventories produce substantial and statistically significant real-time improvements in forecast accuracy. The preferred MIDAS model reduces the MSPE by as much as 16 percent compared with the no-change forecast and has statistically significant directional accuracy as high as 82 percent. This MIDAS forecast also is more accurate than a mixed-frequency realtime VAR forecast, but not systematically more accurate than the corresponding forecast based on monthly inventories. We conclude that typically not much is lost by ignoring high-frequency financial data in forecasting the monthly real price of oil. JEL Classification Codes: C53, G14, Q43.", "title": "Baumeister , Christiane ; Gu\u00e9rin , Pierre ; Kilian , Lutz Working Paper Do high-frequency financial data help forecast oil prices ?", "claims": null}, {"metadata": {"year": 2023}, "authors": ["R. Ellwanger", "Stephen Snudden"], "summary": "Given the importance of crude oil in policy models, for making investment decisions, and for purchasing oil-intensive goods, there is wide interest in accurately predicting its prices. An established finding in the literature is that oil futures are not particularly useful for forecasting the spot price of crude oil. We revisit this finding by proposing a different way to construct futures-based forecasts and by extending the sample period, allowing us to also evaluate the effectiveness of multiyear-ahead forecasts. Contrary to the conventional wisdom, we show that futures forecasts significantly improve upon the accuracy of monthly no-change forecasts. This occurs for two reasons. First, we show that futures curves constructed with end-of-month futures prices rather than monthly average futures prices have always had substantive predictive power of average spot prices at short horizons. Incorporating information from end-of-month futures prices improves the meansquared prediction error and the directional accuracy of the no-change forecast for average spot prices by 40 percent at the one-month horizon. The improvements remain statistically significant for forecasts up to 12 months ahead. The forecast gains are remarkably robust and independent of the sample period. Second, the predictive content of crude oil futures prices at longer forecast horizons has improved since the mid-2000s. We show that, whenever the end of the forecast evaluation period is extended beyond 2014, futures-based forecasts are found to be significantly more accurate predictors than no-change forecasts. The result holds for forecasts from one year to five years ahead. It is particularly strong for forecast horizons beyond two years, which were previously difficult to evaluate due to the illiquidity of longer-dated futures contracts. The enhanced effectiveness of the futures-based forecasts coincides with an increase in trading activity in oil futures markets over the same period. The findings hold for both Brent and WTI crude oil, and can also be applied to the U.S. refiners\u2019 acquisition cost of crude oil. Moreover, the futures-based forecasts work well for both average nominal and real prices. The results show that a decade\u2019s worth of additional data, as well as a simple modification to the originally proposed implementation of futures-based forecasts, change the evidence on the usefulness of oil futures as predictors of spot prices. Given their transparency and ease of implementation, futures-based forecasts provide a natural point of reference to evaluate the merit of alternative forecasts of the price of crude oil.", "title": "Futures Prices are Useful Predictors of the Spot Price of Crude Oil", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Stavros Degiannakis", "G. Filis"], "summary": "The paper examines the importance of combining high frequency information, along with the market fundamentals, in order to gain incremental forecasting accuracy for oil prices. Inspired by French et al. (1986) and Bollerslev et al. (1988), who maintain that future asset returns are also influenced by past volatility, we use daily volatilities and returns from financial and commodity markets to generate real out-ofsample forecasts for the monthly oil futures prices. Our results convincingly show that although the oil market fundamentals are useful for long term forecasting horizons, the combination of the latter with asset realized volatilities, as these are constructed using ultra-high frequency data, significantly improve oil price forecasts in short-run horizons. These findings are both statistically and economically significant, as suggested by several robustness tests.", "title": "1 Forecasting oil prices", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Frederik Kunze", "Markus Spiwoks", "K. Bizer", "T. Windels"], "summary": "This paper evaluates survey forecasts for crude oil prices and discusses the implications for decision makers. A novel disaggregated data set incorporating individual forecasts for Brent and Western Texas Intermediate is used. We carry out tests for unbiasedness, sign accuracy, and forecast encompassing, followed by the computation of coefficients for topically oriented trend adjustments and the Theil's U measure. We also control for the forecast horizon finding heterogeneous results. Forecasts are more precise for shorter horizons, but less accurate than the naA\u00afve prediction. For longer horizons, topically oriented trend adjustments become more pronounced, but forecasters tend to outperform the naA\u00afve predictions.", "title": "The usefulness of oil price forecasts\u2014Evidence from survey predictions", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Stavros Degiannakis"], "summary": "Oil price volatility forecasting is of major importance due to the financialisation of the oil market over the last 10 years or so and the fact that the oil market participants (e.g. oil-intensive industries, policy makers, portfolio traders) form decisions based on such forecasts. The current practice focuses on predicting the conditional oil price volatility (using daily, weekly or monthly frequency) or the realized volatility (using intraday day). Even more, we observe that the bulk literature evaluates these forecasts using statistical loss-functions, such as the Mean Squared Error.", "title": "OIL PRICE VOLATILITY FORECASTS : WHAT DO USERS NEED TO KNOW ?", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Mehmet Yarlik", "Stavros Degiannakis", "G. Lloyd"], "summary": "Furthermore, in the literature, high oil price volatility periods have also caused new debate on the potential drivers ( Kilian and Murphy, 2014; Van Robays, I., 2016; Degiannakis and Filis, 2017). Comprehending oil price volatility determinants is also crucial due to the fact that oil price volatility by itself could have a negative impact on economic activity (Jo, 2014; Van Robays, I., 2016). At the same time, the literature has experienced increased emphasis on the link between oil and financial markets. The critical question for this new link is whether financial market information can assist in forecasting oil price volatility. Therefore, this paper focuses not only on forecasting oil price volatility but also its potential determinants in the context of when they are most likely to exert their predictive power. The study of Degiannakis and Filis (2017) is the first study to extract information from other markets to help improve oil price volatility forecasts. However, their study forecasts daily oil price volatility whereas this paper forecasts on a monthly basis. The rationale for this is that policy makers, firms and oil companies are more interested in the longer run forecasts and a wider range of oil benchmarks and, in consequence, this paper uses both Brent and WTI volatility.", "title": "Forecasting Oil Price Volatility: The Role of Mixed-Frequency- Data (MIDAS) Model", "claims": null}], "query": "why is forecasting weekly oil prices useful? who would benefit from that? how are weekly oil prices more useful than monthly oil prices?", "summary_abstract": "Forecasting weekly oil prices is crucial for various stakeholders, including policymakers, investors, and businesses reliant on oil. The literature highlights several key benefits of such forecasts, particularly in terms of accuracy and timeliness compared to monthly forecasts.\n\nDegiannakis and Filis (2017, 2018) emphasize that integrating high-frequency financial data with oil market fundamentals significantly enhances the accuracy of oil price forecasts. Their studies demonstrate that using daily realized volatilities and returns from financial markets can improve forecasting performance, especially during volatile market conditions. This suggests that weekly forecasts, which can incorporate more immediate market information, are more responsive to rapid changes in oil prices than monthly forecasts.\n\nBaumeister et al. (2013) also support the notion that high-frequency data, such as weekly changes in U.S. crude oil inventories, can lead to substantial improvements in forecast accuracy. Their findings indicate that mixed-frequency models utilizing this data can outperform traditional monthly forecasts, thus underscoring the advantages of weekly forecasting.\n\nEllwanger and Snudden (2023) further contribute to this discussion by showing that futures-based forecasts, particularly when constructed using end-of-month prices, can significantly enhance predictive accuracy for short-term oil price movements. This highlights the importance of timely data in making informed decisions, which is particularly relevant for investors and businesses that operate in oil-intensive sectors.\n\nIn summary, forecasting weekly oil prices is beneficial as it allows for more timely and accurate predictions, which are essential for effective decision-making in various sectors. The integration of high-frequency data not only improves forecast accuracy but also provides a competitive edge in rapidly changing market conditions (Degiannakis & Filis, 2017; Baumeister et al., 2013; Ellwanger & Snudden, 2023).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2010}, "authors": ["Fernando Z\u00fa\u00f1iga", "Seppo Kittil\u00e4"], "summary": "Benefactives are constructions used to express that a state of affairs holds to someone\u2019s advantage. The same construction sometimes also serves as a malefactive, whose meanings are generally not a simple mirror image of the benefactive. Benefactive constructions cover a wide range of phenomena: malefactive passives, general and specialized benefactive cases and adpositions, serial verb constructions and converbal constructions (including e.g. verbs of giving and taking), benefactive applicatives, and other morphosyntactic strategies. The present book is the first collection of its kind to be published on this topic. It includes both typological surveys and in-depth descriptive studies, exploring both the morphosyntactic properties and the semantic nuances of phenomena ranging from the familiar English double-object construction and the Japanese adversative passive to comparable phenomena found in lesser-known languages of Africa, Asia, and the Americas. The book will appeal to typologists and linguists interested in linguistic diversity and it will also be a useful reference work for linguists working on language description.", "title": "Benefactives and malefactives: typological perspectives and case studies", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Fernando Z\u00fa\u00f1iga"], "summary": "a. Some natural languages have a potentially open class of monoclausal constructions (AFFACTIVES) that typically consist of an overt predicate \u03c0 portraying a particular state of affairs and another (overt or covert) predicate \u03c0\u2019 of which some entity is a semantic argument (AFFICIARY). It is either explicitly said or conventionally implicated that the condition or state of the latter changes as a result of \u03c0 (AFFACTION). b. Parameter I: affaction type (neutral, benefaction, malefaction) c. Parameter II (CENTRALITY): argumenthood of afficiaries with respect to \u03c0 (agentive [autoaffaction], patientive, and peripheral). d. Parameter IIa (INVOLVEMENT): relation between peripheral afficiary and \u03c0 or its arguments (possessive, engagement-participative, event-participative, deputative, and absolute). e. Parameter III : relation between \u03c0 and \u03c0\u2019 (AND, CAUSE, PURPOSE, ...). e. Parameter IV: overtness and determinacy of \u03c0\u2019.", "title": "Benefactives: form and function from a crosslinguistic perspective", "claims": null}, {"metadata": {"year": 1999}, "authors": ["Gert Webelhuth", "Farrell Ackerman"], "summary": "In this paper we examine the topicalization paradigm for ten different verbal constructions in German. We argue that a uniform explanation for the observed behaviors follows from the interpretation of the relevant expressions as (parts of) lexical representations. To this end we motivate a revision of Functional Uncertainty as proposed in Kaplan and Zaenen 1989 to account for filler/gap relations in long-distance dependencies. We assume with the original formulation of this principle that topicalized elements share values with the (grammatical) function status of an entity an indeterminate distance away. We appeal to the inventory of functions posited within LEXICAL-FUNCTIONAL GRAMMAR (LFG), inclusive of the frequently neglected PREDICATE function, which, we argue, is associated with both simple and complex predicates. In addition we show that topicalization, given this function-based proposal, should not be limited to maximal categories. We argue that the need to posit a PREDICATE function for German topicalization is supported by an independent line of research within LFG concerning the analysis of complex predicates. For this purpose we employ the proposals of T. Mohanan (1990/1994), which argue for the independence of the construct PREDICATE from its categorial realization. We show that this type of proposal extends to provide a uniform account of the German topicalization paradigm. This permits us to explain the similarities and differences in the behaviors of various sorts of predicators as well as certain idiomatic expressions interpreted as complex predicates.", "title": "A Lexical-Functional Analysis of Predicate Topicalization in German", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Jiao Bao-qian"], "summary": "In western academic,the topics has been studied for a long time,but the study was neglected later,while such study revive in the later 20 century.The topical thinking is a kind of problem thinking,a function process of exploring argument precondition with specific thinking procedure.In the postwar western legal theory,the present of topical thinking has change the traditional close system thinking,and thus has important significance,though still exists some defects.", "title": "On Topics and Its Thinking", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Marek Kettner"], "summary": "The study aims to explain the meaning and function of the concept of topicality (Aktualit\u00e4t) in Benjamin\u2019s philosophy of history and to show that it can present a relevant alternative to contemporary historiography. In times when society is improving the accuracy of its grasp of history and accumulating ever more data about it, Benjamin\u2019s philosophy is valuable by concerning itself primarily with the problem of presentation (Darstellung) of the topicality of particular historical phenomena. Thanks to a better understanding of the concept of topicality, the reader will be able to see why Benjamin thought that to respond to what is \u201cactual\u201d in the sense of \u201ctopical\u201d or \u201ctimely\u201d in history is more important than specifying or accumulating data about it. However, in the presentation of a single topical historical object the whole of history is monadologically expressed: Benjamin\u2019s focus on particular, unique phenomena didn\u2019t imply a resignation vis-\u00e0-vis the task of expression of the whole.", "title": "D\u011bjiny napsan\u00e9 na ze\u010f. K Benjaminovu pojmu aktuality", "claims": null}, {"metadata": {"year": 2010}, "authors": ["T. Smith"], "summary": "The first of its kind, this study classifies the semantics of benefactive constructions from over sixty languages according to event structure types, identifying two basic types. The \u201cagentive benefactive\u201d construction, always includes an agent and expresses that the agent intentionally carries out the act for the sake of the affectee. The \u201cevent benefactive\u201d construction, on the other hand, expresses that the affecting event may or may not have an agent, and includes windfall situations. The subtypes of the agentive benefactive are: \u201cunrestricted agentive benefactive\u201d (agent can be anybody), \u201cnon-self-benefactive\u201d (agent is not an affectee), \u201cselfbenefactive\u201d (agent and affectee are identical), and \u201cshared-benefit construction\u201d (which expresses that \u2018X does something for the benefit of X himself and Y\u2019).", "title": "Cross-linguistic categorization of benefactives by event structure", "claims": null}, {"metadata": {"year": 2011}, "authors": ["L. Marten"], "summary": "In (1b), the applied object b\u00e1may\u00f3 \u2018mother\u2019 is licensed by the applicative marker -el-. In this example, the applied object is thematically a beneficiary, but other thematic roles, such as location, instrument or reason, can typically be expressed by applicatives. Furthermore, different readings of benefactive applicatives can be distinguished, including plain benefactives, recipient benefactives and substitutive benefactives (Kittil\u00e4 2005, Van Valin & LaPolla 1997, Z\u00fa\u00f1iga & Kittil\u00e4 2010). However, languages differ as to how different interpretations of applicatives are", "title": "Bemba benefactive constructions in the typology of applicatives", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Ren\u00e9 Lacroix"], "summary": "The paper deals with the expression of benefactives in Laz, an endangered South Caucasian language spoken in North-East Turkey. Laz has two ways of encoding benefactives: an applicative derivation and a postposition ( \u009aeni ). In the preliminary sections, I present the system of cross-referencing affix es, the major alignment types and the system of valency operators. Then I describe the morphosyntax and semantics of the applicative derivation and the postposition used to encode beneficiaries, before some tentative remarks about the factors that may condition the choice between the applicative and the postposition. Both the applicative and the postposition have a range of other functions: the applicative may express the recipient, the locative and the possessor; the postposition may express the cause, the purpose and also has the meaning \u2018about, concerning\u2019.", "title": "Benefactives in Laz", "claims": null}], "query": "topical benefifical", "summary_abstract": "The collection of papers provides a comprehensive exploration of the concept of benefactives, which are constructions that express actions or states that benefit someone. Z\u00fa\u00f1iga and Kittil\u00e4 (2010) highlight the diversity of benefactive constructions across languages, including various morphosyntactic strategies such as malefactive passives and benefactive applicatives. They emphasize that these constructions are not merely oppositional to malefactives but encompass a range of semantic nuances.\n\nIn a more detailed classification, T. Smith (2010) identifies two primary types of benefactive constructions: \"agentive benefactive,\" which involves an intentional agent acting for the benefit of another, and \"event benefactive,\" which may not involve a clear agent and can include situations where benefits are incidental. This distinction is crucial for understanding how different languages encode benefactive meanings.\n\nLacroix (2010) provides a case study of the Laz language, illustrating how benefactives can be expressed through both applicative derivations and postpositions. This highlights the morphosyntactic variability in how languages handle benefactive constructions, as well as the multifunctionality of these grammatical tools.\n\nMarten (2011) further elaborates on benefactive applicatives, noting that they can express various thematic roles beyond mere beneficiaries, such as location and instrument. This indicates that the interpretation of benefactive constructions can vary significantly across languages, depending on the specific grammatical mechanisms employed.\n\nOverall, the papers collectively underscore the complexity and richness of benefactive constructions in linguistic studies, revealing that they are not only pivotal for understanding semantic relationships but also for appreciating the diversity of language structures across different cultures.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2021}, "authors": ["Rita Safitri", "Heri Purwanto", "M. Trihudiyatmanto", "M. E. Kaukab"], "summary": "Job performance is a result of work in quality and quantity achieved by an employee in carrying out his duties by the responsibilities that have been given to him. An employee is said to be successful in carrying out his job if the work obtained is higher than the predetermined standard. The purpose of this study was to determine and analyze the effect of training, promotion, and transfer on the work performance of the employees of the Social and Village Empowerment Service Office of Wonosobo Regency. This study uses a quantitative approach. The population of this study was all employees of the Social and Village Empowerment Office of Wonosobo Regency. Methods of data analysis using multiple linear regression. The conclusion of this study is (1) partially training has a significant effect on job performance. (2) partially, job promotion has a significant effect on work performance. (3) partially, the mutation has a significant effect on work performance. (4) simultaneous training, promotion, and transfer have a significant effect on work performance.", "title": "Studi Empiris Faktor Determinan Prestasi Kerja Pegawai", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Dimitri van der Linden", "Dirk H.M. Pelt", "Curtis S. Dunkel"], "summary": "markdownabstractJob Performance: The term job performance can \neither refer to the objective or subjective outcomes \none achieves in a specific job (e.g., the profit of a \nsales persons, the number of publications of a \nscientist, the number of successful operations of \na surgeon) or to work-related activities (e.g., writing \nan article, conducting specific surgical acts). \nIn the majority of research on this topic, job performance \nas an outcome is used. \n \nPersonnel selection: Personnel selection \nrefers to the process of selecting the best \nemployees for specific jobs. \n \nIntroduction \nOne major application of personality research is in \nthe area of personnel selection. The key question \nin this area is to which extent personality can \npredict how well a candidate will perform on the \njob he or she is applying for. Most scholars in this \narea acknowledge that personality has predictive \nvalidity for job performance. In line with this, \npersonality assessment is part of the selection \nprocedure in many organizations.", "title": "Personality, personnel selection, and job performance", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Mohamad Ridza", "Mohamad Qamarul Hazeq"], "summary": "The success of an organization depends on the people who work within the organization. Job performance can be defined as the accomplishment of work-related tasks or skills by an employee or trainee that may refer to specific skills or to overall performance (Hackman and Oldham, 1974). Therefore, it is also used for factors associated with success or failure in job situations and most refers to whether an employee performs their job well. On the other definition, job performance is defined in terms of rise in the level of output of services with the same or reduced level of input as a result of better work methods and improved technology (Sharma, 2007). It means that, employee's performance is more important than equipment and raw material. There are two factors that include under the job performance. First is the task performance where it is a traditional method on doing something like to transform and maintain the raw material to ensure the organizational operation. The contextual performance is about the interpersonal and behavior that supports social interaction within the organization. The successful of the job performance is based on the five core job dimensions which include the autonomy, feedback, skill variety, task significance and task identity. All of the core job dimension will be determine whether or not the employee will be internally motivated to work effectively.Hence,this study focuses on identifying the relationship between core job dimensions and the \nemployees job performance in Malaysia Building Society Berhad (MBSB) Melaka.", "title": "The relationship between core job dimensions and employees job performance at Malaysia Building Society Berhad (MBSB), Melaka. / Mohamad Qamarul Hazeq Mohamad Ridza", "claims": null}, {"metadata": {"year": 1998}, "authors": ["Tsui Ming-sum"], "summary": "This paper defines job performance as a social construct from different perspectives, such as standards, behaviour, and process. A comprehensive model was constructed to explain the multiple realities of the job performance of professional social workers. The different perspectives of job performance: organizational performance, staff performance and quality of service were discussed. The dynamic relationships between various factors behind these components were also identified. Supervision and professional development for social workers, goals setting and strategy formulation for human service organizations, and programme evaluation for service delivery were proposed as guidelines for improving job performance of professional social workers.", "title": "A Job Performance Model for Professional Social Workers", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Dedy Syahyuni"], "summary": "Job performance is one of the ways used by the company to measure how well an employee in carrying out the work and tasks within the company. A good company will use performance as a benchmark the success of human resources management within the company. With a good performance measurement, the company can assess and ultimately improve the deficiencies in the work of the employees. Employees with good performance is an employee who can reach the expectations of the company. A growing number of employees who have good job performance can give benefit for the company. The company will have the good performance so effectiveness and efficiency can be achieved in the work. Effectiveness and efficiency from company, will make the cost of production decreases and eventually the company's earnings will rise. Therefore the company should give you a good appreciation for employees who have a good level of performance, one of them by providing promotional Office for employees. This paper tried to connect both of these variables, and calculation results obtained where there is a relationship of 0.729 And variable affects performance of 0.532 Key Word: job performance, job promotion, ragunan zoo", "title": "Hubungan Antara Kinerja Karyawan Dengan Promosi Jabatan Pada Unit Taman Marga Satwa Ragunan Dinas Kehutanan Provinsi DKI Jakarta", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Made Rekma Adi Maheswari"], "summary": "The definition of performance is a description of the level of achievement of the implementation of an activity program or policy in realizing organizational goals, vision and mission goals of the organization which are included through the strategic planning of an organization. This study used a population of 100 employees who had worked at PT Pramana Artha Raharja. The saturated sample technique was used in the research so that the final sample size was 100 respondents. Data analysis in this study used SmartPLS version 3.0. \nThe results in this study of the variables job demands and job resources have no significant effect on job satisfaction, then the variables job demands and job resources have no significant effect on employee performance while the variable job satisfaction has no significant effect on employee performance. Then the knowledge sharing variable has no effect on moderating the relationship between job demands and job resources on employee performance", "title": "THE INFLUENCE OF JOB DEMANDS AND JOB RESOURCES ON EMPLOYEE PERFORMANCE AND JOB SATISFACTION AND KNOWLEDGE SHARING AS MODERATING VARIABLES", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Surakhmat", "Ansar", "Kurniawaty"], "summary": "Work performance is the work achieved by a person in carrying out the tasks assigned to him based on skills, experience, sincerity, and time. A comfortable work environment, friendly and mutually supportive co-workers, and caring leaders will provide positive motivation at work. Employees will work for hand in hand with joy. Healthy competition in organizations or agencies. This study uses library research to examine the improvement of employee performance. The study results show that performance is what employees do, so it affects how much they contribute to the agency or organization, including the quality of services provided. Organizations in improving employee performance need the development of appropriate human resources with a supportive work environment. Factors used to enhance employee performance include individual abilities (knowledge, skills), the effort devoted, and organizational support. Employee performance is the result of the thought and energy of an employee on the work done, it can be tangible, seen, and counted in number, but in many cases, the results of thought and energy cannot be trusted and seen, such as ideas and innovations from employees to improve organizational progress.", "title": "Working Environment and Position to Employee Performance Improvement", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Halilu Dahiru Abba", "W. Mugizi"], "summary": "Different scholars have defined performance in various ways. Armstrong1 defines performance as being the accomplishment, execution, carrying out, working out of anything ordered or undertaken leading to outputs/ outcomes (accomplishment) or achieving of results. Bautista et al.,2 indicate that performance is the achievement of specific tasks measured against predetermined or identified standards of accuracy, completeness, cost and speed. Hafeez & Akbar3 define performance as referring to the achievement of specified tasks measured against predetermined or identified standards of accuracy, completeness, cost and speed. Performance has been conceptualised as multi\u2013dimensional construct involving job\u2013specific task proficiency, non\u2013job\u2013specific proficiency (such as organisational citizenship behaviour), written and oral communication proficiency, demonstration of effort, maintenance of personal discipline, facilitation of peer and team performance, supervision/ leadership and management/ administration aspects.1 Overall, the definition and conceptualisation of performance point to the effect that performance is the effective accomplishment of tasks. Effective performance in organisations leads to accomplishing of organisational goals and objectives,4 quality of output, workmanship, adherence to standards, achievement of performance standards, increased effectiveness and a better use of available resources.5 Therefore, performance is important as far as organisational success is concerned.", "title": "Performance of academic staff in polytechnics: an analysis of performance levels in North West geo\u2013political zone of Nigeria", "claims": null}], "query": "job performance definition", "summary_abstract": "The concept of job performance is multifaceted and has been defined in various ways across the literature. Generally, job performance refers to the effectiveness with which an employee executes their work-related tasks and responsibilities. Safitri et al. (2021) describe it as the quality and quantity of work achieved by an employee in fulfilling their duties, emphasizing that success is measured against predetermined standards. This aligns with the definitions provided by Ridza and Haze (2014), who note that job performance encompasses the accomplishment of tasks and skills, and can also be viewed in terms of output efficiency, where improved methods and technology lead to higher performance levels.\n\nVan der Linden et al. (2017) further elaborate on job performance by distinguishing between objective outcomes (like sales figures or publication counts) and the work-related activities that contribute to these outcomes. This dual perspective highlights that job performance is not only about results but also about the processes and behaviors that lead to those results.\n\nMoreover, Tsui (1998) presents job performance as a social construct, influenced by various factors such as organizational standards and individual behaviors. This perspective suggests that job performance is shaped by the context in which it occurs, including the dynamics of teamwork and organizational culture.\n\nIn addition, Dahiru Abba and Mugizi (2018) emphasize that performance is a multi-dimensional construct, involving not just task proficiency but also aspects like organizational citizenship behavior and communication skills. This comprehensive view underscores the importance of various competencies in achieving effective job performance.\n\nOverall, the literature suggests that job performance is a critical measure of employee effectiveness, encompassing both the outcomes of work and the processes that lead to those outcomes. It is influenced by individual capabilities, organizational support, and the work environment, all of which contribute to an employee's ability to meet or exceed performance standards (Dedy Syahyuni, 2018; Surakhmat et al., 2023).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2004}, "authors": ["Isabel Maria Madaleno", "Alberto  Gurovich"], "summary": "Abstract The paper discusses the validity of the \u201curban versus rural\u201d dichotomy in scientific literature, statistics and urban planning practice, especially in empirical studies developed around the issue of vegetable and animal farming inside and around cities. Santiago\u2019s rural\u2013urban interface is presented as a case study. Sample results extracted in early 2003 have shown that farming mixed with other land uses is even now a reality in Southern Santiago, supported in solidarity alliances and a characteristic life philosophy, giving researchers the possibility of exploring a living ecocity laboratory.", "title": "\u201cUrban versus rural\u201d no longer matches reality: an early public agro-residential development in periurban Santiago, Chile", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Cristian Silva"], "summary": "ABSTRACT Urban sprawl has been mainly discussed in regard to its negative impacts. However, there is a variety of rural lands that benefit the suburbanization process in social, environmental, economic and political terms. These lands configure a category of rurality rarely considered as part of the urban phenomenon, and usually seen as pending space for further (sub)urbanization. In this sense, planning regimes and institutional changes tend to transform suburban rurality into marketable residential space, triggering reactions of preservation and change. With regard to the capital city of Chile \u2013 Santiago \u2013 it is argued that the meanings about the \u201curban\u201d, the \u201crural\u201d and the \u201ccity\u201d become disputable in the light of suburban rurality and problematic as direct antonyms or unequivocal synonyms. It is also demonstrated that despite dramatic institutional changes, suburban rurality can survive over time if it finds support in social organizations and alternative planning rationales. The research in which this paper is based was conducted in five years and included the revision of several leftover interstitial spaces in which suburban rurality emerges as one of the most meaningful categories of suburban space. To gain further understanding on suburban rurality, this paper is based on a qualitative approach \u2013 including semi-structured interviews and archival revision of historical documents and policy reports \u2013 and focuses on the southern geographical space of the metropolitan area of Santiago de Chile, where most of social housing developments have been located over the last 60 years.", "title": "The rural lands of urban sprawl: institutional changes and suburban rurality in Santiago de Chile", "claims": null}, {"metadata": {"year": 2020}, "authors": ["V\u00edctor Jim\u00e9nez Barrado", "Javiera Larra\u00edn Suckel", "B\u00e1rbara Trincado Olhab\u00e9", "Francisco Cabrera Cona"], "summary": "Urbanization of the countryside affects rural areas, especially in the immediate surroundings of large cities. Normally, this occurs as an unpromoted process, but in Chile, it is driven by the legal framework. This research focuses on rural residential plots (RRPs) around the capital city, Santiago. The analysis seeks to understand the significance and consequences of RRPs during the last four decades and the role of a favorable legal framework in affecting their development. By examining data and official cartography on rural residential plots, the analysis shows a large phenomenon of rapid RRP development in the Metropolitan Region of Santiago de Chile (MR). The study confirms the existence of an ongoing process that is still partially latent and potentially both uncontrolled and evolving. This work demonstrates the negative effect that land liberalization policies can have by promoting territorial transformations that policymakers cannot subsequently control. The conclusions provide a critical perspective on the counter-urbanization process in the context of fragility and scarce resources.", "title": "Promoted Urbanization of the Countryside: The Case of Santiago\u2019s Periphery, Chile (1980\u20132017)", "claims": null}, {"metadata": {"year": 2019}, "authors": ["J. Mart\u00ednez"], "summary": "The integration of a financial-based form of neoliberal capitalism in Latin America has contributed to the growth of trans-national economics, sector specialization, and the rise of an economic management model based on competitive advantages bringing about profound transformations in local territory. The goal of this study is to establish an exploratory perspective on the spatial situation that currently affects rural areas within the Greater Santiago Metropolitan Region, placing emphasis on urban development and the effects of agricultural industry. This issue was addressed by means of analyzing statistics on both area types, together with qualitative analysis of how new stakeholders cause new spatial organization systems to emerge. The results are presented as a first section on urban development in the study area, and a second section showing the impacts of the fruit-growing industry. A discussion is also offered addressing the difficulty of characterizing rural space, with a number of categories that may be used when approaching such a task.", "title": "Transformaciones recientes del espacio rural tradicional de la Regi\u00f3n Metropolitana de Santiago de Chile. Entre la agroindustria y la urbanizaci\u00f3n 1990-2017", "claims": null}, {"metadata": {"year": 1996}, "authors": ["A. Cer\u00f3n"], "summary": "This study analyzes the effect of agrarian change on regional settling dynamics in Chile during the last 35 years. The transformations of agrarian structure have produced important changes in the spatial configuration of country-city relations, particularly regarding the new features of rural-urban migration in regional contexts. Whereas until the sixties rural-urban migration was associated with an occupational shift from agriculture to urban employment, after the seventies this relation practically disappeared, leaving a virtual disassociation between the territorial mobility of the population and the occupational mobility of the labor force. This disassociation is a central trait of the current regional pattern of country-city relations.", "title": "Cambio agrario y poblamiento regional en chile", "claims": null}, {"metadata": {"year": 1994}, "authors": ["Margarita Riffo Rosas"], "summary": "Human settlements in Latin America are characterized by a strong geographical population concentration around one or two urban centers of great dynamism, while the rural population is dispersed in highly under-occupied territories. In Chile, the present process of agricultural modernization starting in the 80's is gradually changing the historical pattern of rural population settlements, encouraging the formation of small size rural villages which are functional as regards modernization since their inhabitants work in fruit plantations and forest resources.", "title": "Sub-urbanizaci\u00f3n del campo y su expresi\u00f3n en los villorrios agr\u00edcolas de la VII Regi\u00f3n", "claims": null}, {"metadata": {"year": 1996}, "authors": ["A. Canales Ceron"], "summary": "\"This study analyzes the effect of agrarian change on regional settling dynamics in Chile during the last 35 years. The transformations of agrarian structure have produced important changes in the spatial configuration of country-city relations, particularly regarding the new features of rural-urban migration in regional contexts. Whereas until the sixties rural-urban migration was associated with an occupational shift from agriculture to urban employment, after the seventies this relation practically disappeared, leaving a virtual disassociation between the territorial mobility of the population and the occupational mobility of the labor force. This disassociation is a central trait of the current regional pattern of country-city relations.\" (SUMMARY IN ENG)", "title": "[Agrarian change and regional settling in Chile].", "claims": null}, {"metadata": {"year": 2004}, "authors": ["I. Madaleno", "Gladys Armijo"], "summary": "Nearly half mankind lives in cities and increasingly in big metr\u00f3polis. Inclusive social, cultural, economic and environmental polides are necessary in order to make urban centres more sustainable. Thus agriculture practises within the urban tissue and in peripheral settings, especially when chemical inputs are avoided, are a must for life quality improvement as well as poverty alleviation schemes. Research undertaken aimed to establish a comparison between inner and peri-urban agriculture in national capitals belonging to the same bio-climatic domain Lisbon, Portugal and Santiago, Chile. Methodology and techniques combine satellite imagery digital work, census data analysis, and samples extracted through fieldwork. Favourable public polides aiming cultivation inside cities were the main focus, towards greener metropolises.", "title": "Urban agriculture in metr\u00f3polis from Iberian America: Santiago, Chile and Lisboa, Portugal", "claims": null}], "query": "Main findings and conflicts about the urbanization of agricultural land in Chile", "summary_abstract": "The urbanization of agricultural land in Chile, particularly around the capital city of Santiago, reveals a complex interplay of social, economic, and environmental factors. Several studies highlight the ongoing transformation of rural areas into urbanized spaces, often driven by legal frameworks and market forces.\n\nMadaleno and Gurovich (2004) emphasize the blurred lines between urban and rural spaces, particularly in Santiago's rural-urban interface, where farming coexists with other land uses. This coexistence is supported by community alliances and a philosophy of sustainable living, suggesting that urban agriculture can thrive even in densely populated areas.\n\nSilva (2020) introduces the concept of \"suburban rurality,\" arguing that certain rural lands are being redefined and integrated into the urban phenomenon. This transformation is not merely a negative consequence of urban sprawl; rather, it reflects a dynamic process where rural areas can benefit from suburbanization, provided they receive support from social organizations and alternative planning approaches.\n\nJim\u00e9nez Barrado et al. (2020) focus on rural residential plots (RRPs) around Santiago, highlighting how a favorable legal framework has facilitated rapid development in these areas. However, they caution that such liberalization can lead to uncontrolled urbanization, which policymakers may struggle to manage effectively.\n\nMart\u00ednez (2019) discusses the impact of neoliberal capitalism on rural areas, noting that agricultural industries, particularly fruit-growing, have significantly influenced spatial organization and urban development in the Greater Santiago Metropolitan Region. This reflects a broader trend of agricultural modernization that has reshaped rural settlements and their relationship with urban centers.\n\nRiffo Rosas (1994) and Canales Ceron (1996) provide historical context, noting that agricultural modernization since the 1980s has altered traditional settlement patterns, leading to the emergence of small rural villages that support agricultural activities. This shift has resulted in a disassociation between population mobility and occupational changes, complicating the dynamics of rural-urban migration.\n\nCollectively, these studies illustrate that the urbanization of agricultural land in Chile is a multifaceted issue, characterized by both opportunities for sustainable development and challenges related to uncontrolled growth and policy management. The integration of agriculture within urban settings is seen as essential for enhancing life quality and addressing poverty, while also necessitating careful planning and community involvement to ensure sustainable outcomes.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2012}, "authors": ["Morris F. Collen"], "summary": "Primary medical record databases are data repositories constructed for direct health care delivery to process clinical information, to carry out the special functions for which the data have been collected, integrated, and stored by health-care providers for the direct care of their patients. Medical record data are collected in a variety of medical sites and for a variety of purposes, including helping physicians in making decisions for the diagnosis and treatment of patients, helping nurses in their patient care functions, and helping technical personnel in their clinical support services. The great utility of medical databases resides in their capacity for storing huge volumes of data, and for their ability to help users to search, retrieve, and analyze information on individual patients relevant to their clinical needs. Michalski et al. (1982) added that medical databases were also constructed, in addition to keeping track of clinical data, to be used to study and learn more about the phenomena that produced the data.", "title": "Primary Medical Record Databases", "claims": null}, {"metadata": {"year": 2011}, "authors": ["D. D\u0105browska"], "summary": "Primary data is an important source ofinformation for Competitive Intelligence. Traditionally, it has been collected from interviews with stakeholders, talks at conferences and other means of direct interpersonal communication. The role of the Internet in the data collection \u2013 if it was used at all \u2013 was that of a provider of supplementary secondary data. Here, this approach is challenged and, using three examples of Social Media, it is shown that the Internet can and does provide valuable primary information to the Competitive Intelligence professional. Accordingly, a case is made for a shift of focus in the data collection process.", "title": "Internet evolved : from secondary to primary competitive intelligence data", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Quy Van Khuc", "Duc-Trung  Tran"], "summary": "The primary investigation, primary data, questionnaire method, model,", "title": "Primary data", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Q. Khuc", "D. Tran"], "summary": "The primary investigation, primary data, questionnaire method, model,", "title": "Primary data", "claims": null}, {"metadata": {"year": 2017}, "authors": ["St-MauriceJustin", "BurnsCatherine"], "summary": "Primary care data is an important part of the evolving healthcare ecosystem. Generally, users in primary care are expected to provide excellent patient care and record high-quality data. In practic...", "title": "An Exploratory Case Study to Understand Primary Care Users and Their Data Quality Tradeoffs", "claims": null}, {"metadata": {"year": 0}, "authors": [], "summary": "The chapter centres on the nature and relationship between data, information and knowledge. In doing this, distinction will be made between phenomenal information and data information, adopting concepts from information theory (e.g., through the work of Brillouin) and measurement theory (e.g., through the work of Fisher as explored by Frieden). Mostly data is defined in terms of information, and this is particularly found in the Information Systems field. Similarly, information is often defined in terms of knowledge. However, it is possible to define all three of these entities independently, while they maintain an interconnection and relationship. Data are measurements of information that are basically an assignment of codes that taken on their own are meaningless. Meaning develops through knowledge, context and intention. Information is contained in structured phenomena by virtue of the variety that is implicit to the structure. This is called bound information. It can be acquired qualitatively or quantitatively. Data information results from processed data. Knowledge is composed of a pattern of interrelated concepts that provide meaning through understanding. Some of the concepts are archetypal, resulting in better understanding because we are familiar with them. Just because people have concepts, it does not mean that they completely understand them. The production of understanding that comes from knowledge requires critical reasoning that is arrived at through reflective thinking. All data, information and knowledge can be created and analysed, and each can have both a primary and secondary form. Primary data is data that is collected first hand and used for the purpose for which it was collected. Secondary data is often the use of someone else's data, and/or for an intention that is different from the originally collection. Analysis of secondary data is sometimes called secondary analysis. Primary information can be qualitative and acquired through a pattern of knowledge, or it can derive from quantitative measures of primary data and the creation of data information through analysis. Secondary information is data information that occurs through data processing and is based on secondary data. It is filtered through two referent patterns of knowledge to give it meaning, that possessed by the viewer and that embedded in the data repository by its designer. Finally, primary knowledge is that knowledge that has been created through first hand through experience, and is often referred to as tacit. It is acquired as a direct process of the individual, being filtered only through worldview, and \u2026", "title": "Chapter 4 Data, Information, Knowledge and Methodology 4.1 the Nature of Data 4.1.1 Definitions of Data", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Tetsuro Hayashi"], "summary": "A primary source is a piece of information about a historical event or period in which the creator of the source was an actual participant in or a contemporary of a historical moment. The purpose of primary sources is to capture the words, the thoughts and the intentions of the past. Primary sources help you to interpret what happened and why it happened. These materials include letters, speeches, diaries, newspaper articles from the time, oral history interviews, documents, photographs, artifacts, or anything else that provides firsthand accounts about a person or event.", "title": "Primary Sources", "claims": null}, {"metadata": {"year": 2004}, "authors": ["Fan Shao-ming"], "summary": "With the development of the times, primary data has passed the three stages from literature catalogue, MARC and Dublin Core. Based on clarifying the history, analyzing the current situation and studying its emergence and development, we expect to grasp the developing direction of primary data and meet the users' demands.", "title": "Literature Catalogue, MARC and Dublin Core: Emergence and Development of Primary Data", "claims": null}], "query": "What is the definition of primary data?", "summary_abstract": "The concept of primary data is defined and elaborated upon across several papers, highlighting its significance in various fields. Primary data is fundamentally described as information collected firsthand for a specific purpose, distinguishing it from secondary data, which involves the use of data collected by others for different intentions (Author, Year). \n\nCollen (2012) emphasizes that primary data in the medical field is derived from medical record databases, which are constructed to support direct healthcare delivery. These databases are essential for processing clinical information and aiding healthcare providers in making informed decisions regarding patient care. The primary data collected in this context serves multiple functions, including supporting clinical decision-making and enhancing patient care.\n\nD\u0105browska (2011) expands the definition of primary data to include its role in Competitive Intelligence, traditionally gathered through direct interpersonal communication. The paper argues for a modern perspective where the Internet serves as a valuable source of primary data, particularly through social media, thus broadening the scope of how primary data can be collected.\n\nHayashi (2017) provides a historical context by defining primary sources as firsthand accounts from individuals who experienced or were contemporaneous with historical events. This definition aligns with the broader understanding of primary data as original information that captures direct experiences and insights.\n\nOverall, primary data is characterized as original, firsthand information collected for specific purposes, whether in healthcare, competitive analysis, or historical research. This data is crucial for informed decision-making and understanding phenomena in various contexts (Collen, 2012; D\u0105browska, 2011; Hayashi, 2017).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2012}, "authors": ["Sean K Lau"], "summary": "Perivascular epithelioid cell neoplasms, also known as PEComas, are unique mesenchymal tumors exhibiting perivascular epithelioid cell differentiation, characterized by a mixed myogenic and melanocytic phenotype. PEComas arising in visceral organs outside of the kidney, liver, and lung are rare, and often pose problems in diagnosis. Examples of this neoplasm originating in the adrenal gland are limited. The present report details the clinical and pathologic features of an unusual case of a pure epithelioid PEComa (epithelioid angiomyolipoma) of the adrenal gland exhibiting clinically malignant behavior in the form of pulmonary metastases, a feature not previously described in tumors of this site. The diagnosis was supported by immunohistochemical studies demonstrating expression of myoid and melanocytic antigens. The present case serves to emphasize the potential of PEComa for clinically aggressive behavior and the importance of distinguishing this tumor from other epithelioid neoplasms that are more commonly encountered in the adrenal gland.", "title": "Malignant PEComa of the adrenal gland.", "claims": null}, {"metadata": {"year": 2023}, "authors": ["E. Battistella", "L. Pomba", "Marica Mirabella", "M. Gregianin", "A. Scapinello", "M. Volante", "A. Toniato"], "summary": "PEComa has become a widely accepted entity, and increased recognition has led to descriptions of this tumor in a wide variety of anatomic sites, including the adrenal gland. PEComa (perivascular epithelioid cell tumor) is a mesenchymal tumor composed of perivascular cells, and the most frequent sites of PEComas are the uterus and retroperitoneum. The incidence is <1 per 1,000,000 people. We report a case of adrenal metastatic PEComa in a 63-year-old man discovered by a spontaneous hematoma of the rectus abdominis. In our case, PEComa of the adrenal gland was a significant diagnostic dilemma as the morphologic and immunophenotypic features of this neoplasm may easily be confused with those of other more commonly encountered lesions.", "title": "Metastatic Adrenal PEComa: Case Report and Short Review of the Literature", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Leela  Pant", "Dipti  Kalita", "Ratna  Chopra", "Abhijit  Das", "Gaurav  Jain"], "summary": "Histological diagnosis of adrenal tumors is often challenging as diverse groups of tumors, both primaries and metastatic, may be seen in the adrenal gland with overlapping morphological features. Immunohistochemistry (IHC) plays the most important role in their diagnosis. Perivascular epithelioid cell tumor (PEComa), a rarely reported tumor in the adrenal gland, shares many features with another rare tumor sarcomatoid adrenocortical carcinoma (ACC). Extensive immunohistochemical study is required to distinguish this tumor from adrenocortical carcinoma and from other morphologically similar tumors. The unique combination of immunoreactivity for melanocytic markers, such as HMB-45 and Melan A, and myogenic markers, such as smooth muscle actin, is the hallmark of PEComas biological behavior, and prognosis of malignant PEComas is yet to be fully understood. Few cases of malignant PEComa have been reported in the adrenal gland. We report a case of malignant PEComa of the adrenal gland posing diagnostic challenge and compare its morphological and immunohistochemical features with those of sarcomatoid ACC.", "title": "Malignant Perivascular Epithelioid Cell Tumor (PEComa) of the Adrenal Gland: Report of a Rare Case Posing Diagnostic Challenge with the Role of Immunohistochemistry in the Diagnosis", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Douglas Kwazneski Ii", "M. Merrill", "Jessica S. Young", "Harry S Sell"], "summary": "Angiomyolipoma and PEComa are rare tumors descending from perivascular epithelial cells (PECs), with distinctive IHC, morphological, and ultrastructural features. The kidney is the most frequent site of origin, but not the only one; however, adrenal gland angiomyolipomas are extremely rare. We describe two cases being found in the adrenal glands. Given the paucity of literature on the subject, more information on this disease is necessary for diagnosis and treatment. Here, we describe two complete case reports, from presentation to treatment and follow-up, along with imaging and microscopic pathology samples, and provide a comprehensive review as to the history and current literature available regarding these extremely rare tumors.", "title": "Angiomyolipoma and Malignant PEComa: Discussion of Two Rare Adrenal Tumors", "claims": null}, {"metadata": {"year": 2021}, "authors": ["E. Alt\u0131nta\u015f", "A. Batur", "M. Kaynar", "Murat Gul", "M. Akand", "S. Goktas", "Z. \u00c7elik", "O. K\u0131l\u0131\u00e7"], "summary": "Epithelioid cellular morphology can be seen in clinically benign usual (or classic) angiomyolipoma (AML). Perivascular Epithelioid Cell Tumors (PEComa) are rarely seen as a variant of AML and usually benign in nature; however, they may have unpredictable pathological behavior. Here, we present a case of renal PEComa with malignant clinical progression and compare it with the current literature. A 56-year-old patient with a history of recurrent side pain present for about four months applied to our clinic. A hypodense mass was detected on the upper pole of the left kidney by ultrasonography. Computerized tomography showed an 8x4 cm mass originating from the upper pole of the left kidney and the adrenal gland, and was thought to invade the psoas muscle. The patient underwent a left transperitoneal radical nephrectomy. During the operation, vena cava inferior repair was required due to invasion and performed. Histopathologic examination revealed a PEComa. During the third month follow-up visit, a recurrent mass and lymph node enlargement were detected at the operation site. The mass was excised, and histopathology revealed a PEComa again. Considered as a rare variant of AML, PEComa is a tumor with the potential to exhibit malignant behavior. Although only a limited number of cases of renal PEComa have been reported; diagnosis, treatment, and follow-up are important due to their high potential for malignancy.", "title": "Renal Epithelioid Angiomyolipoma with Aggresive Progression: A Case Report and Literature Review", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Yuan Pingchen"], "summary": "Objective:To discuss the clinical pathologic features of primary adrenal perivascular epithelioid cell tumor(PEComa).Method:A 34-year-old female was admitted in November 2014 in the department of urology of general hospital of PLA and was diagnosed as primary adrenal PEComa.The clinical,imaging,and pathologic features and treatment of the patient was analyzed retrospectively and related literature was reviewed.Result:Among present literature,scarce cases of adrenal PEComa were reported.The patient had no special symptoms and signs.The imaging methods could easily find the tumor,but it was hard to differentiate it.The final diagnosis depended on the pathologic features and immunohistochemistry techniques.Surgical resection was the preferred treatment and a long-term follow-up plan was also needed after surgery.Conclusion:Primary adrenal PEComa is a very rare tumor that arises from mesenchymal tissues.Its diagnosis mainly depends on the pathological features.The patient should be followed up for a long period because of the uncertain biologic behavior of primary adrenal PEComa.", "title": "Adrenal perivascular epithelioid cell tumor:a case report and literature review", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Santosh Kumar", "A. Lal", "N. Acharya", "Varun Sharma"], "summary": "Abstract A 54-year-old woman had a mass located in the right suprarenal area. On imaging, this mass appeared to be infiltrating the inferior vena cava (IVC). Exploratory laparotomy was undertaken and excision of the tumour was done with the sleeve of the involved IVC. The mass turned out to be a perivascular epithelioid cell tumour (PEComa) on histopathological examination. This report describes previously reported cases of PEComa in brief and highlights the problems associated with the management of this tumour.", "title": "Perivascular epithelioid cell tumour (PEComa) of the inferior vena cava presenting as an adrenal mass", "claims": null}, {"metadata": {"year": 1989}, "authors": ["K. Yoshimura", "Y. Tomooka", "O. Maeda", "S. Saiki", "M. Kuroda", "T. Miki", "M. Usami", "T. Kotake"], "summary": "A case of exohepatic pedunculated hepatocellular carcinoma that was clinically diagnosed as nonfunctioning adrenal tumor is reported. A 66-year-old man was admitted to our hospital for further examination of unstable angina pectoris. Abdominal echogram and CT scan revealed a large tumor in the right retroperitoneal space. Selective right renal arteriography demonstrated that the tumor was fed by the capsular branch of right renal artery and the right adrenal artery. Adrenal function was normal. Preoperative diagnosis of right nonfunctioning adrenal tumor was made. On operation we found that the tumor was pedunculated from the liver and adhered massively to both right kidney and vena cava. The tumor and right kidney were removed. A histopathological examination demonstrated well differentiated hepatocellular carcinoma (Edmondson's grade II type).", "title": "[A case of exohepatic pedunculated hepatocellular carcinoma suspected of adrenal tumor].", "claims": null}], "query": "adrenal and renal pecoma", "summary_abstract": "The research on adrenal and renal perivascular epithelioid cell tumors (PEComas) reveals that these neoplasms, while rare, can exhibit significant clinical and pathological challenges. PEComas are mesenchymal tumors characterized by a unique differentiation of perivascular epithelioid cells, and they can arise in various anatomical sites, including the adrenal gland.\n\nLau (2012) highlights a case of a pure epithelioid PEComa in the adrenal gland that demonstrated malignant behavior, including pulmonary metastases, which is an unusual presentation for tumors in this location. This case underscores the potential for aggressive clinical behavior in adrenal PEComas, necessitating careful differentiation from other more common epithelioid neoplasms.\n\nBattistella et al. (2023) further emphasize the diagnostic challenges posed by adrenal PEComas, noting that their morphological and immunophenotypic features can easily be confused with other lesions. They report a case of metastatic adrenal PEComa that was initially discovered due to a spontaneous hematoma, illustrating the complexity of diagnosing these tumors.\n\nPant et al. (2015) discuss the importance of immunohistochemistry in distinguishing adrenal PEComas from other tumors, particularly sarcomatoid adrenocortical carcinoma. They note that the unique combination of immunoreactivity for melanocytic and myogenic markers is crucial for accurate diagnosis, as malignant PEComas can exhibit unpredictable behavior.\n\nKwazneski et al. (2016) contribute to the understanding of adrenal PEComas by presenting two cases, emphasizing the rarity of these tumors in the adrenal glands and the need for comprehensive literature to guide diagnosis and treatment.\n\nYuan (2015) and Kumar et al. (2010) also report on individual cases of adrenal PEComas, reinforcing the notion that these tumors are infrequent and often present diagnostic dilemmas. Yuan notes that the final diagnosis relies heavily on pathological features and immunohistochemistry, while Kumar discusses the management challenges associated with these tumors.\n\nIn summary, the literature collectively indicates that adrenal PEComas are rare tumors that can exhibit malignant behavior and pose significant diagnostic challenges. Accurate diagnosis relies on a combination of imaging, histopathological examination, and immunohistochemical analysis, with surgical resection being the preferred treatment approach. The potential for aggressive clinical behavior necessitates long-term follow-up for affected patients (Lau, 2012; Battistella et al., 2023; Pant et al., 2015; Kwazneski et al., 2016; Yuan, 2015; Kumar et al., 2010).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2020}, "authors": ["M. Kyere", "M. Ausloos"], "summary": "The objective of this study is to examine empirically the impact of good corporate governance on financial performance of United Kingdom non-financial listed firms. Agency theory and stewardship theory serve as the bases of a conceptual model. Five corporate governance mechanisms are examined on two financial performance indicators, return on assets (ROA) and Tobin's Q, employing cross-sectional regression methodology. The conclusion drawn from empirical test so performed on 252 firms listed on London Stock Exchange for the year 2014 indicates a positive or a negative relationship, but also sometimes no effect, of corporate governance mechanisms impact on financial performance. The implications are discussed. Thereby, so distinguishing effects due to causes, we present a proof that, when the right corporate governance mechanisms are chosen, the finances of a firm can be improved. The results of this research should have some implication on academia and policy makers thoughts.", "title": "Corporate governance and firms financial performance in the United Kingdom", "claims": null}, {"metadata": {"year": 1975}, "authors": ["J. Wilkinson"], "summary": "Unilever Structure Before discussion of the processes of Research Policy formation in Unilever it is desirable to outline the parts of the very complex Unilever organization r~levan~ to the subject. A good survey is grven 10 a booklet published by the Company/ from which the federal nature of the structure is apparent. The com~anies and units have in practice considerable day-to-day autonomy in their operations, which thus provides a very desirab!e ~exibility. They operate, howeyer, within a framework of major objectrve control from the national or regional ma~ag~ments and fr0t? the industry coordinating groups which have an international role. !he number of direct employees of the twin parent companies, Unilever Limited and Unilever N.V., is relatively small compared with the total pay-roll of over 300,000 people. And the number of fully i~ter.national central departments of any SIze IS also very small, of which Research Division is the one on which we focus. Research in Unilever is international and is operated from London. There are 9 laboratories in Europe, including the U.K., which are run as 6 administrative units. The centre, Research Division in London, maintains operating contacts with two associated Research laboratories in U.S.A., and one in India as well as", "title": "Research Policy Formation", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Ali Farhat"], "summary": "The subject of corporate governance and corporate performance has been widely discussed and examined over the last two decades. A great deal of change has developed within British Boardrooms since the emergence of the Cadbury Committee Report in 1992. UK Corporate Governance reforms over the years have been consistently developed where an increase in the number of non-executive directors on board, their roles and their effectiveness, was evident throughout the development of these reports. For instance, the Cadbury Report set a minimum of three non-executive directors for each company. Also, the independent non-executive director has become the catalyst for better performance since it has been recommended by the 1998 Combined Code that at least one-third of the board is to be independent and increased to half by the 2003 Combined Code. Although it has been evident that the level of compliance by companies has increased, the relationship between firm performance and corporate governance has been mixed and inconclusive in previous research. A large number of empirical works found no clear link between firm performance and corporate governance. There is an argument posited by scholars that better firm performance is achieved in well-governed firms. Therefore, the main question addressed in this thesis is whether a relationship exists between internal corporate governance mechanisms and performance of FTSE All shares non-financial firms listed on London Stock Exchange for a period 2005 to 2010. It specifically looks at the link between firm performance (measured as Tobin\u2019s Q and Return on Assets (ROA) and board characteristics (board size, independent directors, CEO duality and Audit committee), managerial ownership, executive remuneration and financial policies (Debt and Dividend) as governance mechanisms. This study will draw upon the agency theory to test whether the hypothesised relationships exist between firm performance and corporate governance mechanisms in the UK. \n \nOrdinary Least Square (OLS) regression analysis produced mixed results. According to OLS regressions, the results provide some evidence of a relationship between some governance mechanisms and firm performance. In general, based on market measures (Tobin\u2019s\u2019 Q), some governance mechanisms (independent directors, board size (apart from in 2006), role, managerial ownership (apart from in 2008), executive remuneration and debt (apart from in 2005)) positively relate to firm performance, while audit (apart from in 2010) negatively relates to firm performance. However dividend pay-out produced mixed results. Based on accounting measures (ROA), independent directors (apart from in 2010), role (apart from in 2006), managerial ownership and executive remuneration positively relate to firm performance. Board size, audit and debt negatively relate to firm performance. However, dividend pay-out produced mixed results. Further analysis using two-stage least square regressions indicate that any causal effect runs from governance to firm performance rather than in the opposite direction. Overall, the findings of the research are period specific. Variables showing significant explanatory power at the start of the sample period may cease to be significant or change sign at the end of the sample period. \n \nIn the corporate world corporate governance has been a growing issue and it has contributed to becoming a key business discipline in the management of companies. This study contributes to the increasing number of research studies on the link between firm performance and corporate governance. The lack of clarity, mixed and permanent relationships provided, show that the association between performance and different governance is complex and dynamic: optimal governance arrangements may differ from firm to firm in relation to board characteristics.", "title": "Corporate governance and firm performance: the case of UK.", "claims": null}, {"metadata": {"year": 2001}, "authors": ["C. Weir", "D. Laing"], "summary": "A number of Committees have been set up in recent years to investigate the governance of UK quoted companies. The key one was the Cadbury Committee, which recommended a number of governance structures as examples of best practice. These included the separation of the posts of CEO and chairman, a significant representation of non\u2010executive directors, the importance of non\u2010executive director independence and the setting up of board subcommittees. This study finds that there has been widespread adoption of the recommended governance structures. However, there is no clear relationship between governance structures and corporate performance. This raises questions about the most effective type of governance mechanism and whether or not the prescriptive recommendations of Cadbury should be replaced with a more flexible approach.", "title": "Governance structures, director independence and corporate performance in the UK", "claims": null}, {"metadata": {"year": 2013}, "authors": ["G. Elena"], "summary": "The corporate governance is one of the key elements for improving economic efficiency, particularly in the current context of financial global crises. This paper discusses the relationship between corporate governance and firm performance. We intend to present the classic and new comings of scientific literature on the topic. Beside the theoretical approaches we also intend to present the practice poin of view, in order to give a complex image over the topic and suggest new progress on the matter.", "title": "Aspects Regarding the Influence of the Governance Structure on the Company\u2019s Performace", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Dr.M. Yasodha", "P. Soundariya.", "R. Varuna."], "summary": "The research paper \u201cA Study on Financial Performance Analysis of ITC Limited and Hindustan Unilever Limited\u201d as entitled. In the modern oriented economy, finance is one of the basic foundations of all kinds of economic activities. Finance statements are prepared primarily for decision making. This study helps to evaluate and analyze the financial performance and its positions by using ratio analysis. The financial performance can be measured\u00a0 by using financial tools such as profitability and liquidity ratio. The research includes analyzing the profit & loss account and balance sheet for the period of five years.", "title": "Financial Analysis of ITC Limited and Hindustan Unilever Limited", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Alex Adegboye", "S. Ojeka", "Kofo Adegboye", "Emmanuel Ebuzor", "Dayo Samson"], "summary": "This paper extends the prior studies on corporate performance by empirically exploring the impact of overall corporate governance structure on firm performance. To unveil the objective of this study, firstly corporate governance index is built using Principal Component Analysis with 6 (six) identified corporate governance mechanisms from prior studies and then examines its effect on firms\u2019 performance. This study draws a sample of twenty-four (24) financial companies from the listed financial institutions in Nigeria for the period of 2013\u20132017. The formulated hypotheses are tested by employing static panel data estimators that are Fixed effect and Random Effect Regression. The results reveal that while controlling for firms\u2019 characteristics, constructed corporate governance indicator has a significant and negative influence on the firm performance measured by Return on Asset and Return on Equity. This finding supports that larger board, larger board committees and significant executive involvement have a detrimental influence on the performance of firms. The result implies a weak corporate governance structure is detrimental to higher financial performance amidst the weak institutions characterized in Nigeria context. That is, weaker corporate governance exhibits lower financial performance. This study then recommends that the corporate governance structure in Nigeria listed firms should be review with the intention to enhance the firm performance. Furthermore, it encourages the regulatory agencies like Central Bank of Nigeria, National Insurance Commission and Securities and Exchange Commission, to monitor the compliance of the listed firms to good governance endeavour.", "title": "VERSLAS: TEORIJA IR PRAKTIKA / BUSINESS: THEORY AND PRACTICE", "claims": null}, {"metadata": {"year": 2009}, "authors": ["M. Page", "A. Abdullah"], "summary": "In the wake of the recent financial crisis, attention has once again turned to corporate governance, with policy reviews of UK corporate governance being undertaken by the FRC and the Walker Review. One key question may relate to the purpose of corporate governance \u2013 is it about the control of risks, the improvement of performance, or both? If this could be clarified, criteria could be developed to measure the success of corporate governance procedures or codes. This research investigates whether companies with particular corporate governance characteristics outperform other companies and have lower levels of risk. The governance characteristics investigated in the report are: board independence; board size; directors\u2019 ownership of equity; and extent of ownership by large block holders. The effects of these characteristics were measured over two three year periods between 1999 and 2004. Three measures of performance were used: one stock market measure (market to book ratio); and two accounting based measures (return on assets and ratio of sales to total assets). Risk was measured in three ways: total risk; systematic risk; and a measure of sudden share price falls. The findings reveal no clear systematic relationship between governance factors and improved performance, and no strong evidence that governance reduces either total or systematic risk. This project was funded by the Scottish Accountancy Trust for Education and Research (SATER). The Research Committee of The Institute of Chartered Accountants of Scotland (ICAS) as also been happy to support this project. The Committee recognises that the views expressed do not necessarily represent those of ICAS itself, but hopes that the project will add to the knowledge about the interaction between corporate governance factors, company performance and risk.", "title": "Corporate governance and corporate performance: UK FTSE 350 companies", "claims": null}], "query": "Critical evaluation of governance structure of Unilever PLC UK. Based the discussion on the UK Corporate Governance Code (April 2016 and July 2018), Financial Reporting Council (FRC). The discussion should focus on board leadership, board committees, risk management strategy, internal control mechanisms and key performance indicators (financial and financial indicators)", "summary_abstract": "The governance structure of Unilever PLC, particularly in relation to the UK Corporate Governance Code, has been the subject of various studies that collectively highlight the complexities and nuances of corporate governance and its impact on performance. \n\nFirstly, the UK Corporate Governance Code emphasizes the importance of board leadership, which includes the separation of the roles of CEO and chairman, as well as the presence of independent non-executive directors. Farhat (2014) notes that the evolution of corporate governance in the UK has led to an increase in the number of non-executive directors, which is believed to enhance firm performance. However, the relationship between governance structures and corporate performance remains mixed, with some studies indicating that while certain governance mechanisms can positively influence performance, others may not have a clear effect (Weir & Laing, 2001; Farhat, 2014).\n\nIn terms of board committees, the establishment of subcommittees is a key recommendation of the Cadbury Committee, which has been widely adopted across UK companies. However, the effectiveness of these committees in improving performance is still debated, as evidenced by the findings of Page and Abdullah (2009), who found no systematic relationship between governance factors and improved performance or reduced risk.\n\nRisk management strategies and internal control mechanisms are also critical components of corporate governance. Kyere and Ausloos (2020) emphasize that the right governance mechanisms can lead to improved financial performance, suggesting that effective risk management is integral to achieving better outcomes. However, the empirical evidence remains inconclusive, with some studies indicating that governance structures may not significantly mitigate risks (Page & Abdullah, 2009).\n\nFinally, the evaluation of key performance indicators (KPIs) such as Return on Assets (ROA) and Tobin's Q reveals a complex interplay between governance and performance. While some governance mechanisms, such as independent directors and board size, have been associated with positive performance outcomes, the results are often context-specific and can vary over time (Farhat, 2014; Kyere & Ausloos, 2020). \n\nIn summary, while the governance structure of Unilever PLC aligns with the principles outlined in the UK Corporate Governance Code, the effectiveness of these structures in enhancing performance and managing risks is not straightforward. The literature suggests that while certain governance practices may contribute to better financial outcomes, the relationship is influenced by various factors, including the specific context of the firm and the dynamic nature of governance itself (Weir & Laing, 2001; Farhat, 2014; Page & Abdullah, 2009).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2012}, "authors": ["Giovanni Melina", "Stefania Villa"], "summary": "This paper studies how fiscal policy affects loan market conditions. First, it conducts a Structural Vector-Autoregression analysis showing that the bank spread responds negatively to an expansionary government spending shock, while lending increases. Second, it illustrates that these results are mimicked by a Real Business Cycle model where the bank spread is endogenized via the inclusion of a banking sector exploiting lending relationships. Third, it shows that lending relationships represent a friction that generates a financial accelerator effect in the transmission of the fiscal shock.", "title": "Fiscal Policy and Lending Relationships", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Giovanni  Melina", "Stefania  Villa"], "summary": "This paper studies how fiscal policy affects loan market conditions. First, it conducts a Structural Vector-Autoregression analysis showing that the bank spread responds negatively to an expansionary government spending shock, while lending increases. Second, it illustrates that these results are mimicked by a Real Business Cycle model where the bank spread is endogenized via the inclusion of a banking sector exploiting lending relationships. Third, it shows that lending relationships represent a friction that generates a financial accelerator effect in the transmission of the fiscal shock.", "title": "Fiscal Policy and Lending Relationships", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Giovanni  Melina", "Stefania  Villa"], "summary": "This paper studies how fiscal policy affects loan market conditions in the US. First, it conducts a Structural Vector-Autoregression analysis showing that the bank spread responds negatively to an expansionary government spending shock, while lending increases. Second, it illustrates that these results are mimicked by a Dynamic Stochastic General Equilibrium model where the bank spread is endogenized via the inclusion of a banking sector exploiting lending relationships. Third, it shows that lending relationships represent a friction that generates a financial accelerator effect in the transmission of the fiscal shock.", "title": "Fiscal Policy and Lending Relationships", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Giovanni  Melina", "Stefania  Villa"], "summary": "type=\"main\" xml:lang=\"en\"> This paper studies how fiscal policy affects loan market conditions in the United States. First, it conducts a structural vector-autoregression analysis showing that the bank spread responds negatively to an expansionary government spending shock, while lending increases. Second, it illustrates that these results are mimicked by a dynamic stochastic general equilibrium model where the bank spread is endogenized via the inclusion of a banking sector exploiting lending relationships. Third, it shows that lending relationships represent a friction that generates a financial accelerator effect in the transmission of the fiscal shock. (JEL E44, E62)", "title": "Fiscal Policy and Lending Relationships", "claims": null}, {"metadata": {"year": 2012}, "authors": ["R. Kollmann", "W. Roeger", "Jan In't Veld"], "summary": "A key dimension of fiscal policy during the financial crisis was massive government support for the banking system. The macroeconomic effects of that support have, so far, received little attention in the literature. This paper fills this gap, using a quantitative dynamic model with a banking sector. Our results suggest that state aid for banks may have a strong positive effect on real activity. Bank state aid multipliers are in the same range as conventional fiscal spending multipliers. Support for banks has a positive effect on investment, while a rise in government purchases crowds out investment.", "title": "Fiscal Policy in a Financial Crisis: Standard Policy vs. Bank Rescue Measures", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Deng Zhi-guo"], "summary": "The urgent concern problem of the post financial crisis era is to accelerate structural adjustment of economy.With its distribution function,fiscal policy constitutes an effective way for the structural adjustment.It can regulate and control the economic structure directly through taxes,investments and public expenditures,and indirectly through guiding social capital,particularly the bank credit capital.Focusing on the indirect regulation and control of fiscal policy on economic structure,this paper applies the relevant theories of fiscal policy,integrates systematic advantage of Input-output Technique and makes an empirical analysis of the guiding function of fiscal policy on bank credit delivery,aiming at enhancing its function in economic structural adjustment.In addition,this analysis can also aid the commercial banks to make their policies to keep consistent with the state macroeconomic regulation and control policy,and thus make timely and effective adjustments of credit delivery.", "title": "An Empirical Study of the Impact of Fiscal Policy on the Bank Credit Structure", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Jeffrey M Lacker"], "summary": "The financial dimension of the current contraction has brought a historic expansion in government lending to financial market participants, mostly through an expanding array of Federal Reserve (Fed) initiatives. This contrasts with the Fed's typical response to recent recessions that has been limited to adjustments of the target Fed funds rate. Restrictions on credit supply and declines of creditworthiness have both contributed to the contraction in lending, although the latter cause has probably been underestimated relative to the former. Fed and other government lending programs have targeted particular sectors, altering the allocation of credit across markets. Also, targeted credit programs contribute to the moral hazard problem inherent in the provision of government-funded credit or guarantees. An alternative approach to monetary policy where the Fed funds target is essentially zero is purchasing Treasuries, which is likely to have little effect on the relative credit spreads on different financial instruments. However, given that targeted lending has taken place, it is critical that regulatory mechanisms be installed so that government regulation matches the scope of government support. Also, targeted lending by the Fed is in effect fiscal policy. Is this a legitimate role for a central bank, or should such lending be subject to legislative approval, with the Fed's role limited to monetary stability?", "title": "Government Lending and Monetary Policy", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Hartmut Brinkmeyer"], "summary": "Monetary policy transmission.- A new view: Implications of financial innovation for the bank lending channel.- Bank lending against the background of the recent crisis.- Empirical analysis: Determinants of bank lending during normal and crisis periods.", "title": "Drivers of Bank Lending: New Evidence from the Crisis", "claims": null}], "query": "fiscal policy, bank lending ", "summary_abstract": "The research question regarding the interplay between fiscal policy and bank lending is addressed in several studies, primarily focusing on how government actions influence loan market conditions and the broader economic landscape.\n\nMelina and Villa (2012, 2013, 2014) provide a comprehensive analysis demonstrating that expansionary fiscal policy, particularly through increased government spending, leads to a decrease in the bank spread and an increase in lending. Their Structural Vector-Autoregression analysis indicates that these effects are consistent across different models, including a Real Business Cycle model that incorporates banking sector dynamics. They highlight the role of lending relationships as a friction that enhances the financial accelerator effect, thereby amplifying the impact of fiscal shocks on lending behavior.\n\nKollmann, Roeger, and In't Veld (2012) further explore the implications of fiscal policy during the financial crisis, emphasizing the significant positive effects of government support for banks on real economic activity. Their findings suggest that the multipliers associated with bank state aid are comparable to those of traditional fiscal spending, indicating that such support can effectively stimulate investment, despite potential crowding out from increased government purchases.\n\nDeng (2011) discusses the indirect regulatory role of fiscal policy in guiding bank credit delivery, asserting that fiscal measures can directly influence economic structure through taxation and public expenditure. This perspective underscores the importance of aligning commercial bank policies with state macroeconomic objectives to enhance credit delivery effectiveness.\n\nLacker (2009) examines the expansion of government lending during economic contractions, noting that such initiatives can alter credit allocation across markets. He raises critical questions about the legitimacy of targeted lending as a form of fiscal policy and the need for regulatory frameworks to ensure that government support aligns with broader economic stability goals.\n\nCollectively, these studies illustrate a clear connection between fiscal policy and bank lending, highlighting how government actions can shape lending conditions, influence economic activity, and necessitate careful regulatory oversight.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2022}, "authors": ["Soniya Assudani Patel", "Karyn M. Frick", "Paul A. Newhouse", "Robert S. Astur"], "summary": "To examine the role of estradiol in hippocampal-dependent spatial memory in women, 86 female undergraduates were tested in a virtual Morris water task (VMWT), a virtual radial arm maze (VRAM), and a mental rotation task (MRT) within a single daily session. The VMWT and RAM were also administered 24hours later to examine the effects of estradiol on memory consolidation. Women on oral contraceptives (OCs) or those who were naturally cycling and exhibited low estradiol (LE) or high estradiol (HE), as determined by salivary assays, were included. At the start of day two, the HE group showed superior spatial reference memory on the VMWT relative to the LE group, as evidenced by significantly shorter distances navigating to the hidden platform. The LE group also had the poorest probe trial performance at the start of day two compared to both other groups. There were no group differences in performance on the RAM or MRT. These results provide support for estradiol's role in the consolidation of spatial reference memory in women, and emphasize the differential sensitivities of various virtual memory tasks in assessing spatial memory function in women.", "title": "Estradiol effects on spatial memory in women", "claims": null}, {"metadata": {"year": 2007}, "authors": ["Lauren L. Harburger", "Jennifer C. Bennett", "Karyn M. Frick"], "summary": "Interpretation of data illustrating that estrogen, with or without progestin, is detrimental to memory in post-menopausal women is complicated by the fact that little is known about the effects of progestins on memory. The present study examined if estrogen, alone or with progesterone, affects spatial memory consolidation in ovariectomized aged female mice. Mice received eight training trials in a spatial Morris water maze followed immediately by injection of water-soluble 17beta-estradiol (E(2); 0.2 mg/kg) or vehicle. Mice were re-tested 24 h later. All mice learned to find the platform on Day 1. On Day 2, the performance of control, but not E(2) mice, deteriorated, suggesting that E(2) enhanced memory for the platform location. In a second experiment, mice were injected with E(2) and 10 or 20 mg/kg water-soluble progesterone. The 10 mg/kg dose of progesterone did not affect estrogen's ability to enhance spatial memory consolidation, but 20 mg/kg blocked this effect. These data indicate that estrogen can improve spatial memory consolidation in aged females and that this effect can be attenuated by progesterone.", "title": "Effects of estrogen and progesterone on spatial memory consolidation in aged females", "claims": null}, {"metadata": {"year": 2014}, "authors": ["F. Wang", "Yanfeng Song", "Jie Yin", "Zihua Liu", "Xiao-Dan Mo", "De-gui Wang", "Li-Ping Gao", "Yu-Hong Jing"], "summary": "Estrogen influences memory formation and insulin sensitivity. Meanwhile, glucose utilization directly affects learning and memory, which are modulated by insulin signals. Therefore, this study investigated whether or not the effect of estrogen on memory is associated with the regulatory effect of this hormone on glucose metabolism. The relative expression of estrogen receptor \u03b2 (ER\u03b2) and glucose transporter type 4 (GLUT4) in the hippocampus of rats were evaluated by western blot. Insulin level was assessed by ELISA and quantitative RT-PCR, and spatial memory was tested by the Morris water maze. Glucose utilization in the hippocampus was measured by 2-NBDG uptake analysis. Results showed that ovariectomy impaired the spatial memory of rats. These impairments are similar as the female rats treated with the ER\u03b2 antagonist tamoxifen (TAM). Estrogen blockade by ovariectomy or TAM treatment obviously decreased glucose utilization. This phenomenon was accompanied by decreased insulin level and GLUT4 expression in the hippocampus. The female rats were neutralized with hippocampal insulin with insulin antibody, which also impaired memory and local glucose consumption. These results indicated that estrogen blockade impaired the spatial memory of the female rats. The mechanisms by which estrogen blockade impaired memory partially contributed to the decline in hippocampal insulin signals, which diminished glucose consumption.", "title": "Spatial Memory Impairment Is Associated with Hippocampal Insulin Signals in Ovariectomized Rats", "claims": null}, {"metadata": {"year": 1994}, "authors": ["V. Luine"], "summary": "Our studies have shown that both stress-induced, enhanced secretion of corticosterone and adrenalectomy-induced, reduced secretion of corticosterone are associated with impaired spatial memory performance. On the other hand, estradiol administration is associated with enhancements in spatial memory performance. Although these changes in performance are small, they are consistent with structural changes induced by these hormones (or their lack) on specific cells within the hippocampus which form the tri-synaptic loop (a summary of the behavioral and morphological effects is shown in Figs. 1 and 2). Thus, these results suggest that the morphological changes induced by the hormones have an impact on hippocampal function. Important goals of future studies are to seek ways to maximize gonadal hormone-dependent enhancements in memory function and to minimize adrenal steroid-dependent impairments in memory function as well as to understand the mechanisms behind these behavioral and morphological changes.", "title": "Steroid Hormone Influences on Spatial Memory", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Vahideh Zarrin", "H. Hatami", "A. Hemmati"], "summary": "Background & Objective: Multiple sclerosis (MS) is a demyelinating disease of central nervous system, which is approximately two to three times more common in women than men. It was suggested that sexual hormones affect MS disease. Over half of MS patients have cognitive deficits, including learning and memory dysfunction. Estrogen also influences most of the cerebral function including learning and memory. According to the involvement of estrogen on learning and memory process, the objective of this study is to investigate the neuroprotective effect of estrogen during MS disease. Materials & Methods: 28 male rats after cannnulating into the CA1 area of hippocampus, randomly divided into four groups (n=7) including control group, sham group (sesame oil, 1 \u03bcl/rat), MS group (0.01% ethidium bromide, 3 \u03bcl/rat), MS + estradiol group (ethidium bromide + estradiol benzoate, 2 \u03bcg/1\u03bcl). MS model was induced by intra hippocampal injection of ethidium bromide and estradiol benzoate was injected as a pretreatment for five days. The Morris Water Maze test was used for studying spatial memory. Data analysis was performed using one-way ANOVA and Tukey test. Results: Ethidium bromide reduced the spatial memory (P<0.05). Pretreatment of estradiol benzoate (2 \u03bcg/1\u03bcl) significantly improved spatial memory during MS disease (P<0.05). Conclusion: It seems that estradiol benzoate improves the reduced spatial memory during MS disease due to its neuroprotective effects.", "title": "A THE EFFECT OF INTRA HIPPOCAMPAL INJECTION OF ESTRADIOL ON SPATIAL MEMORY IN MALE ADULT RATS DURING MULTIPLE SCLEROSIS DISEASE", "claims": null}, {"metadata": {"year": 2002}, "authors": ["H. Varga", "H. N\u00e9meth", "T. T\u00f3th", "Z. Kis", "T. Farkas", "J. Toldi"], "summary": "In a number of species, males appear to have spatial abilities that are superior to those of females. The favored explanation for this cognitive difference is hormonal: higher testosterone levels in males than in females. An alternative explanation focuses on the role of varying levels of estrogens in females during the estrus cycle; females perform as well as males on days of low estrogen, but more poorly on days of high estrogen. Other investigators have reported that estrogens improve both types of memory processes, which depend on the striatal (nonspatial navigation) and hippocampal (spatial) memory systems. Additionally, estrogens have been found to protect the working memory. These contradictory results initiated the present study, in which ovariectomized female rats were trained to escape in a Morris water maze. The daily trials were preceded by estradiol application in low doses (Experiment I) or in higher doses (Experiment II). In Experiment I, no differences at all were found between the latencies of the treated and control groups to reach a submerged platform in a Morris water maze. In Experiment II, however, the animals treated with the higher dose of estradiol showed a small deficit in the acquisition of the Morris water maze task. This study indicates that estradiol at around the physiological level has no effect on spatial learning and memory functions. + Dedicated to Professor Otto Feher on the occasion of his 75 th birthday.", "title": "Weak if any effect of estrogen on spatial memory in rats", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Praisy Ev", "M. Sudha", "R. Sambathkumar"], "summary": "Aim: To review the estrogen responses in correlation with women memory. \nData sources: A search of PubMed was conducted using the search terms estrogen, memory and mood. Clinical and observational studies were selected from the past year articles. Theories on mechanisms of action of estrogen signaling, and it\u2019s effect on various disease were discussed. \nResults: On an observational study by using various tests, the findings suggest that estrogen may facilitate the automatic activation of verbal representations which is similar to our study but it has shown a negative influence on spatial memory. \nConclusion: From this study, it concludes that estrogen plays a major role in the enhancement of verbal memory, on other side it has a negative influence on spatial memory. In females, sexually dimorphic cognitive skills have enhanced because of the influence of estrogen such as verbal articulation and fine motor skills.", "title": "Estrogen Response in Correlation with Women Memory: A Review", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Mark P. Holden", "Elizabeth  Hampson"], "summary": "Recent work has suggested that sex differences may exist in the strategies or types of cues that are utilized by men and women to remember discrete spatial locations or routes through a visual environment. The current study investigated the effects of circulating estradiol levels in women on the relative weighting of categorical versus fine-grained 'metric' information in a test of short-term memory for spatial locations, either presented within a simple geometric surround (a circular enclosure) or within more visually complex landscape scenes. Patterns of displacement error in the point location estimates made by men and women were analyzed. Results confirmed a sex difference in the weighting of metric versus categorical cues. Relative to men, women's estimates of locations were more strongly biased toward the center of the surrounding category (i.e., toward the category 'prototype'). Furthermore, objective measures of estradiol via saliva collected at the time of memory testing showed that among naturally-cycling women estradiol concentrations correlated in a positive, graded, fashion with the degree of emphasis that women placed on categorical information when estimating point locations. No associations were found for progesterone. These findings are consistent with a wider body of research showing that biological sex and reproductive hormone levels, including 17\u03b2-estradiol, can subtly influence performance on certain spatial tasks. This is the first study to show that circulating estradiol levels may influence the relative emphasis placed on categorical versus metric cues when remembering simple point locations.", "title": "Endogenous variation in estradiol in women affects the weighting of metric and categorical information in spatial location memory", "claims": null}], "query": "What are the effects of estrogen on spatial memory?", "summary_abstract": "The collective findings from the reviewed studies indicate that estrogen, particularly estradiol, plays a significant role in influencing spatial memory, with varying effects depending on the context and specific conditions of the subjects involved.\n\nPatel et al. (2022) demonstrated that higher levels of estradiol in women were associated with improved performance in spatial memory tasks, specifically in a virtual Morris water task, suggesting that estradiol enhances memory consolidation. This aligns with findings from Harburger et al. (2007), which showed that estradiol administration improved spatial memory consolidation in ovariectomized aged female mice, although the presence of progesterone could attenuate this effect.\n\nWang et al. (2014) further elucidated the mechanisms behind estrogen's influence on memory by linking it to glucose metabolism in the hippocampus. Their study found that estrogen blockade impaired spatial memory and reduced glucose utilization, indicating that estrogen's effects on memory may be mediated through its regulatory role on insulin signaling and glucose consumption.\n\nLuine (1994) also supported the notion that estradiol administration enhances spatial memory performance, suggesting that hormonal changes can lead to structural modifications in the hippocampus that facilitate memory function. In contrast, Varga et al. (2002) reported that while low doses of estradiol did not affect spatial learning, higher doses resulted in a small deficit, indicating a complex relationship between estrogen levels and memory performance.\n\nInterestingly, Ev et al. (2020) highlighted a potential negative influence of estrogen on spatial memory, suggesting that while it may enhance verbal memory, it could detract from spatial memory capabilities. This notion is echoed by Holden and Hampson (2021), who found that circulating estradiol levels influenced the strategies women used in spatial tasks, emphasizing categorical over metric information, which may reflect a nuanced impact of estrogen on spatial memory processing.\n\nOverall, the evidence suggests that estrogen, particularly estradiol, generally enhances spatial memory, but its effects can be influenced by dosage, the presence of other hormones like progesterone, and the specific cognitive strategies employed by individuals. This complexity underscores the need for further research to fully understand the mechanisms at play and how they may vary across different populations and contexts.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2019}, "authors": ["Chelsea Barabas"], "summary": "Data-driven decision-making regimes, often branded as \u201cartificial intelligence,\u201d are rapidly proliferating across the US criminal justice system as a means of predicting and managing the risk of crime and addressing accusations of discriminatory practices. These data regimes have come under increased scrutiny, as critics point out the myriad ways that they can reproduce or even amplify pre-existing biases in the criminal justice system. This essay examines contemporary debates regarding the use of \u201cartificial intelligence\u201d as a vehicle for criminal justice reform, by closely examining two general approaches to, what has been widely branded as, \u201calgorithmic fairness\u201d in criminal law: 1) the development of formal fairness criteria and accuracy measures that illustrate the trade-offs of different algorithmic interventions and 2) the development of \u201cbest practices\u201d and managerialist standards for maintaining a baseline of accuracy, transparency and validity in these systems. The essay argues that attempts to render AI-branded tools more accurate by addressing narrow notions of \u201cbias,\u201d miss the deeper methodological and epistemological issues regarding the fairness of these tools. The key question is whether predictive tools reflect and reinforce punitive practices that drive disparate outcomes, and how data regimes interact with the penal ideology to naturalize these practices. The article concludes by calling for an abolitionist understanding of the role and function of the carceral state, in order to fundamentally reformulate the questions we ask, the way we characterize existing data, and how we identify and fill gaps in existing data regimes of the carceral state.", "title": "Beyond Bias: Re-Imagining the Terms of \u2018Ethical AI\u2019 in Criminal Law", "claims": null}, {"metadata": {"year": 2021}, "authors": ["W. Gravett"], "summary": "Developments in artificial intelligence and machine learning have caused governments to start outsourcing authority in performing public functions to machines. Indeed, algorithmic decision-making is becoming ubiquitous, from assigning credit scores to people, to identifying the best candidates for an employment position, to ranking applicants for admission to university. Apart from the broader social, ethical and legal considerations, controversies have arisen regarding the inaccuracy of AI systems and their bias against vulnerable populations. The growing use of automated risk-assessment software in criminal sentencing is a cause for both optimism and scepticism. While these tools could potentially increase sentencing accuracy and reduce the risk of human error and bias by providing evidence-based reasons in place of \u2018ad-hoc\u2019 decisions by human beings beset with cognitive and implicit biases, they also have the potential to reinforce and exacerbate existing biases, and to undermine certain of the basic constitutional guarantees embedded in the justice system. A 2016 decision in the United States, S v Loomis, exemplifies the threat that the unchecked and unrestrained outsourcing of public power to AI systems might undermine human rights and the rule of law.", "title": "Sentenced by an algorithm \u2014 Bias and lack of accuracy in risk-assessment software in the United States criminal justice system", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Nurus Sakinatul Fikriah Mohd Shith Putera", "Hartini Saripan", "Mimi Sintia A. Bajury", "Syazni Nadzirah Ya\u2019cob"], "summary": "Artificial Intelligence (AI) promises to heighten human decision-making, including in court. AI sentencing would be better at detecting, organizing, and calibrating all of the variables correlated to sentencing, such as prior criminal records, educational background, substance abuse history, and employment history, resulting in consistencies that traditional sentencing may not be able to provide. AI pervades the Malaysian judiciary system when AI criminal sentencing was launched for the first time to augment the process of meting out sentences in Sabah courts. Despite its promising benefits, AI sentencing may infringe the fundamental principle of due process, presents unacceptable risks of error and implicit bias, and reliance on AI to predict recidivism which forms significant components of the rule of law. The rule of law guarantees that all entities are subject to and accountable to a clear and known law. It enables the judicial branch of the government to be independent and to resolve dispute in a fair manner while upholding the presumption of innocence and preventing the exercise of arbitrary powers. The present research, therefore, examines the use of AI in supporting court processes and human judges, discovering its technical characteristics, practical constraints, and legal theoretical consequences for decision-making processes. Employing jurisprudential analysis as the method of research, this research explores an adjudicatory paradigm that prefers standardisation over discretion, leading in the waning of the notion of rule of law pertinent to the justice system. The metamorphosis to AI adjudication will undoubtedly promote the growth of digitalized dispute resolution by providing efficiency and at least a semblance of impartiality, but it is also poised to birth concerns by making the legal system data-driven, alienating, and disillusioning.\nKeywords: Artificial Intelligence, Artificial Intelligence in Courts, Artificial Intelligence in Criminal Justice\u00a0\neISSN: 2398-4287\u00a9 2022. The Authors. Published for AMER ABRA cE-Bs by e-International Publishing House, Ltd., UK. This is an open access article under the CC BYNC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/). Peer\u2013review under responsibility of AMER (Association of Malaysian Environment-Behaviour Researchers), ABRA (Association of Behavioural Researchers on Asians/Africans/Arabians) and cE-Bs (Centre for Environment-Behaviour Studies), Faculty of Architecture, Planning & Surveying, Universiti Teknologi MARA, Malaysia.\nDOI: https://doi.org/10.21834/ebpj.v7iSI7%20(Special%20Issue).3813", "title": "Artificial Intelligence-Powered Criminal Sentencing in Malaysia: A conflict with the rule of law", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Rik Peeters", "M. Schuilenburg"], "summary": "The use of algorithms to predict behaviour is becoming the gold standard in criminal justice in various countries. This article critically analyses the algorithm-driven risk assessment tools used in predictive policing and predictive justice. First, we propose to see algorithms as essentially bureaucratic instruments. They are the digital offspring of the classic bureaucratic procedure, creating classification through standardised and impersonal decision-making. Second, we argue that the application of algorithms in criminal justice expands the bureaucratic field to areas previously understood as bulwarks of professional judgement. Third, we analyse the shift in purpose of algorithmic decision-making: instead of determining a citizen's status of beneficiary or obligate, we now see algorithmic anticipation of behaviour. This shifts the logic of decision-making over investigations, probations, and sentencing from individual judgement to bureaucratic classification based on the algorithms that are designed into risk assessments tools. This article is both a bureaucratic critique of algorithm-driven risk assessment tools in criminal justice and a call to rethink bureaucracy and bureaucratisation beyond the boundaries of public administration.", "title": "Machine justice: Governing security through the bureaucracy of algorithms", "claims": null}, {"metadata": {"year": 2021}, "authors": ["E. V. Markovicheva"], "summary": "Relevance. The achievements of the digital revolution are actively penetrating all spheres of public life. Law en-forcement is also transforming, and modern technologies are increasingly being used in legal proceedings. The criminal process also feels this informational influence. But digitalization of the criminal process can go in different directions and is associated with certain risks. The greatest concern of scientists and practitioners is the use of intelligent systems when passing court decisions, since in this direction digitalization can also have a negative impact on ensuring the rights of participants in legal proceedings and other persons. Purpose. Disclosure of the possibilities and risks of using intelligent systems for supporting judicial decisions in criminal proceedings. Objective: to reveal modern approaches to digitalization of the criminal procedure; to identify the possibilities of using intelligent systems when the court decides on criminal cases; determine the risks accompanying the use of in-telligent systems for supporting judicial decision-making in criminal proceedings, and ways to minimize them. Methodology. In the process of working on the study, comparative-legal, formal-legal methods and general scientific methods of cognition (analysis, synthesis, analogy) were used. Results. Proposals are formulated aimed at adjusting doctrinal and legislative approaches to ensuring the rights of participants in criminal proceedings, as a prerequisite for further digitalization of the procedural activity of the court in a criminal case. Conclusion. Currently, there are no organizational and legal conditions for the more active use of intelligent systems in the framework of criminal proceedings. It requires a systematic discussion of the possibilities for further digitalization of procedural activities, a change in approaches to the training of legal personnel, the identification of risks associated with ensuring the rights of participants in the process and the corresponding systemic adjustment of legislation.", "title": "Intelligent Decision Support in Criminal Court", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Vincent Chiao"], "summary": "Abstract Over the last few years, legal scholars, policy-makers, activists and others have generated a vast and rapidly expanding literature concerning the ethical ramifications of using artificial intelligence, machine learning, big data and predictive software in criminal justice contexts. These concerns can be clustered under the headings of fairness, accountability and transparency. First, can we trust technology to be fair, especially given that the data on which the technology is based are biased in various ways? Second, whom can we blame if the technology goes wrong, as it inevitably will on occasion? Finally, does it matter if we do not know how an algorithm works or, relatedly, cannot understand how it reached its decision? I argue that, while these are serious concerns, they are not irresolvable. More importantly, the very same concerns of fairness, accountability and transparency apply, with even greater urgency, to existing modes of decision-making in criminal justice. The question, hence, is comparative: can algorithmic modes of decision-making improve upon the status quo in criminal justice? There is unlikely to be a categorical answer to this question, although there are some reasons for cautious optimism.", "title": "Fairness, accountability and transparency: notes on algorithmic decision-making in criminal justice", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Ric Simmons"], "summary": "Predictive algorithms are rapidly spreading throughout the criminal justice system. They are used to more efficiently allocate police resources, identify potentially dangerous individuals, and advise judges at bail hearings and sentencing determinations. These algorithms have the potential to increase the accuracy, efficiency, and fairness of the criminal justice system, and they have been criticized on the grounds that they may reinforce pre-existing biases against minorities. But one aspect of these tools that has not yet been discussed in the literature is whether they will be accepted as legitimate. For centuries, these critical decisions that affect people\u2019s safety and liberty have been made by human beings; now, for the first time in human history, we are delegating large aspects of these decisions to machines. This article addresses whether people will be willing to accept this change, and if not, how we can adapt the algorithms in order to make them more acceptable. \nIn determining whether predictive algorithms are likely to be accepted by criminal defendants, the article draws on the field of procedural justice, which sets out numerous factors that determine whether a participant in a judicial proceeding believes that the process is fair. The article finds that predictive algorithms do not fare particularly well on these factors: they may not be seen as trustworthy or neutral, and they do not give defendants a significant opportunity to participate in the process. The article suggests that criminal defendants would be more likely to view predictive algorithms as legitimate if the algorithms were made more transparent, and if they were designed to ensure that they did not use data that was tainted by past discriminatory practices. \nThe article then examines whether the general population is likely to perceive predictive algorithms as legitimate. It examines various psychological barriers that people have with regard to accepting predictive algorithms. The article presents an original empirical study of six hundred individuals who were presented with a hypothetical case in which a judge uses a predictive algorithm to assist in a bail hearing. The study indicates that individuals are likely to accept predictive algorithms, as long as certain criteria are met.", "title": "Big Data, Machine Judges, and the Legitimacy of the Criminal Justice System", "claims": null}, {"metadata": {"year": 2019}, "authors": [], "summary": "Introduction It is in this context, I am trying to write about the potentialities of applications of A.I. driven modules to improve the deliverables of criminal justice system. Many countries are thinking and using Artificial Intelligence based robotics to clear the backlogs of pendency of cases in their courts. The judgements are based on certain premises of laws , past judgments and issues raised in petitions etc , therefore use of A.I. is much more possible while preparation of petitions , pronouncement of judgements after analysis of counters and rejoinders through A.I. based applications and then only a small part is left out for the human intervention and that can be done quickly. This will save lot of time and energy along with enhanced outputs. Even A.I. can be used as a consultant type arrangement also to analyze and decide the cases when judges cannot read or go through voluminous texts and papers to decide the cases. Even judgement writing with regard to discussion of evidences, merits and arguments etc. can be better analyzed , narrated and written by A.I. based applications. The creation of audio files of arguments etc can further give enough support to judges to expedite the delivery of judgments. Thus arguments etc. can be better interpreted and made precise where things are not clear or need further elaborations. I think that the idea of justice can be made more achievable quickly and effectively if we introduce A.I. based application ecosystem at various levels in the criminal justice system. The digital case diaries and Artificial Intelligence based investigation flow charts with video and audio combinations in the case files, A.I. based examination of evidences and conclusions, supported by legal provisions etc. can improve the quality of investigations and prosecutions. Thus the entire criminal justice ecosystem can be designed on a single platform where A.I based applications will start the process from crime registration to investigation to scrutiny and charge sheet filings to hearings and writing of judgements, to appeals and finally acquittals or convictions to jail to parol etc. They all can be based on data integrated platforms, totally driven by the A.I. enabled eco system. Artificial Intelligence and the Criminal Justice System", "title": "Artificial Intelligence and the Criminal Justice System \u2013", "claims": null}], "query": "The rise of AI-based decision-making tools in the criminal justice system: implications for judicial integrity", "summary_abstract": "The rise of AI-based decision-making tools in the criminal justice system has sparked significant debate regarding their implications for judicial integrity. Several studies highlight both the potential benefits and the inherent risks associated with these technologies.\n\nBarabas (2019) critically examines the use of AI in criminal justice, arguing that while these tools aim to address biases and improve decision-making, they often reproduce existing inequalities. The author emphasizes that merely focusing on algorithmic fairness overlooks deeper issues related to the punitive nature of the justice system, suggesting that a fundamental reformulation of how we approach data and justice is necessary.\n\nGravett (2021) echoes these concerns, noting that while AI can enhance accuracy and reduce human error, it also risks reinforcing biases against marginalized populations. The case of *S v Loomis* illustrates the potential dangers of relying on AI without adequate oversight, raising questions about the erosion of constitutional guarantees.\n\nIn a similar vein, Mohd Shith Putera et al. (2022) discuss the introduction of AI sentencing in Malaysia, highlighting its potential to standardize decisions but also warning of risks to due process and the rule of law. They argue that the reliance on AI could lead to a loss of judicial discretion and an alienating experience for those involved in the legal process.\n\nPeeters and Schuilenburg (2018) provide a bureaucratic critique of algorithm-driven risk assessment tools, suggesting that these technologies shift decision-making from individual judgment to impersonal classification, which may undermine the integrity of judicial processes.\n\nMarkovicheva (2021) further explores the risks associated with AI in judicial decision-making, emphasizing the need for legal frameworks that protect the rights of participants in criminal proceedings. The study calls for a systematic discussion on the digitalization of the criminal process to ensure that rights are upheld.\n\nChiao (2019) raises critical ethical questions about fairness, accountability, and transparency in AI applications within criminal justice. He argues that while these concerns are valid, they also apply to traditional decision-making processes, prompting a comparative analysis of whether AI can indeed improve upon existing practices.\n\nSimmons (2018) addresses the legitimacy of predictive algorithms in the eyes of the public and defendants, suggesting that transparency and the avoidance of biased data are crucial for acceptance. His empirical study indicates that public perception can be favorable if certain criteria are met, highlighting the importance of trust in these technologies.\n\nCollectively, these papers underscore a complex landscape where AI tools in the criminal justice system hold promise for enhancing efficiency and consistency but also pose significant risks to fairness and judicial integrity. The consensus suggests a cautious approach, advocating for robust oversight, transparency, and a critical examination of the underlying biases that these technologies may perpetuate.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2023}, "authors": ["Mouza Alzaabi", "Mahra Almeheiri", "Shuaa Alqubaisi", "A. Shuhaiber"], "summary": "Students and teachers are connected in an orderly way to attain learning outcomes, which is the foundation of attendance systems. Academic institutions, however, encounter poor strategies for these parties' participation. In order to address the low attendance and participation among students and faculty in a university community, a university-wide \u2018AI-Teacher\u2019 system will be built in one of the educational institutions in the Gulf region. The system was prepared for by planning, gathering, modelling, and analyzing the requirements, designing the interfaces and databases, and then coding and testing. The AI-Teacher assistant system is approachable and simple to use. A warning is sent to parents after a student misses a predetermined number of classes, along with an automatic or manual attendance record, alerts, and notifications. Also, the application system, \u2018AI-Teacher\u2019 which works with iOS and Android devices, can improve connections between \"students and instructor\". The study concluded with some recommendations for further research and development.", "title": "'AI-Teacher' Assistant System: A Smart Attendance and Participation tracking system for students", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Xinyan Zhang", "Jing Geng", "Yuqi Chen", "Shengze Hu", "Tao Huang"], "summary": "Intelligent teaching assistant system provides help for teachers in teaching through information technology to reduce the burden of teachers. Most of today\u2019s intelligent teaching assistant systems are still providing teachers with simple functions to assist them in completing some simple tasks in daily teaching. However, the effective integration of intelligent algorithms and classroom instruction has been somewhat neglected. To solve the problem, we design I-assistant, an intelligent teaching assistant system, which combines intelligent algorithms and classrooms to assist teachers to conduct precise teaching. I-assistant consists of four modules, class cognitive level module, class subject ability module, classroom state module, and subject design and evaluation module. In the first three modules, we use artificial intelligence algorithms to analyze student homework, tests as well as video data to present student learning situation visually. In the last module, the system helps teachers with instructional design and promotes effective and accurate teaching in classroom.", "title": "I-assistant: An Intelligent Teaching Assistant System for Classroom Teaching", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Jihyun Kim", "Kelly Merrill", "Kun Xu", "Deanna D. Sellnow"], "summary": "ABSTRACT An increase in demand for online education has led to the creation of a new technology, machine teachers, or artificial intelligence (AI) teaching assistants. In fact, AI teaching assistants have already been implemented in a small number of courses in the United States. However, little is known about how students will perceive AI teaching assistants. Thus, the present study investigated students\u2019 perceptions about AI teaching assistants in higher education by use of an online survey. Primary findings indicate that perceived usefulness of an AI teaching assistant and perceived ease of communication with an AI teaching assistant are key to understanding an eventual adoption of AI teaching assistant-based education. These findings provide support for AI teaching assistant adoption. Based on the present study\u2019s findings, more research is needed to better understand the nuances associated with the learning experience one may have from an AI teaching assistant.", "title": "My Teacher Is a Machine: Understanding Students\u2019 Perceptions of AI Teaching Assistants in Online Education", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Alpona Shirin"], "summary": "The increasing attention to artificial intelligence technologies in daily life and the need to consider it as a priority topic for students in the twenty-first century clearly leads to artificial intelligence (AI) integration in higher education. Therefore, university teachers must be properly prepared to use AI in their teaching for successful integration. In this study, the researcher aimed to survey to investigate Bangladeshi university teachers' attitudes toward AI as a teaching tool. The survey results showed that teachers have minimal understanding of Artificial Intelligence and its assistance in the classroom. However, they considered it as an educational possibility. The findings indicated that teachers require assistance to be effective and competent in their teaching practices; the findings suggested that AI has the potential to contribute as an assistant.", "title": "Artificial Intelligence Technology on Teaching-Learning: Exploring Bangladeshi Teachers\u2019 Perceptions", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Bin Zhang", "Haibin Xia", "Hun-ok Lim"], "summary": "Nowadays, people are facing the issues of declining birthrates and an aging society. Teaching resources are far from enough and the teachers usually have great amount of works, especially in countryside or small towns. Aim to reduce the work of teachers, we developed a teaching assisting system to automatically track and recognize the motions and behaviors of students based on artificial intelligence (AI) technology. The humans are detected accurately by combing the detection results of OpenPose and human area projection method. Each person is identified by fusing his/her personal information, including the features of color, face and moving history, and tracked by using an extended particle filter method based on Markov Chain Monte Carlo (MCMC). By observing the students' behaviors without getting tired with the sensor system instead of the teachers, the performances and growing processes of all the students can be analyzed, referring the excellent knowledge and experiences of professional teachers. This information can be provided to the teacher which can help the teachers to adjust guidance to the students. Moreover, the information can also be provided to education robot so that the robot can also held a class or have interactions with the students.", "title": "Development of an AI based teaching assisting system", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Jun Hyung Kim", "Minki Kim", "D. Kwak", "Sol Lee"], "summary": "This study investigates whether artificial intelligence (AI) can transform the teacher\u2019s role by delivering personalized learning to each individual student. Conducting a randomized controlled trial in collaboration with an education company, we evaluate how providing teachers with AI assistance impacts students\u2019 academic outcomes. We find that providing AI-generated reports to teachers significantly improves students\u2019 study effort and performance, although those effects vary by teacher and class characteristics. Consistent with the organizational literature on technology and worker productivity, our findings indicate that technology overload could undermine teachers\u2019 effective use of AI coaching despite its highly accurate diagnostic ability and ready availability. Although some teachers did not utilize the AI coaching program, we found positive effects among the students of both compliers and non-compliers. We, therefore, decompose our results to identify direct and spillover effects on students\u2019 academic performance and show that failing to account for spillover effects across teachers (externality among peers) and within teachers (learning-by-doing across students) may understate the effects of AI coaching. Finally, we provide practical guidelines for implementing technology in educational settings.", "title": "Assisting Teachers with Artificial Intelligence: Investigating the Role of Teachers Using a Randomized Field Experiment", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Ismail  Celik", "Muhterem  Dindar", "Hanni  Muukkonen", "Sanna  J\u00e4rvel\u00e4"], "summary": "This study provides an overview of research on teachers\u2019 use of artificial intelligence (AI) applications and machine learning methods to analyze teachers\u2019 data. Our analysis showed that AI offers teachers several opportunities for improved planning (e.g., by defining students\u2019 needs and familiarizing teachers with such needs), implementation (e.g., through immediate feedback and teacher intervention), and assessment (e.g., through automated essay scoring) of their teaching. We also found that teachers have various roles in the development of AI technology. These roles include acting as models for training AI algorithms and participating in AI development by checking the accuracy of AI automated assessment systems. Our findings further underlined several challenges in AI implementation in teaching practice, which provide guidelines for developing the field.", "title": "The Promises and Challenges of Artificial Intelligence for Teachers: a Systematic Review of Research", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Tri Duc Tran", "B. Bouchon-Meunier", "C. Marsala", "Georges-Marie Putois"], "summary": "The research and development of an educational agent are commonly focused on the pedagogical aspect; the main objective is to automate the teaching activity, to replace the teacher in virtual learning environment. Our goal is different: it is to create a non pedagogical intelligent assistant that follows students during their learning to prevent dropout. This assistant can be compared to a coach, a motivator, or a mentor that motivates, encourages, and helps students to overcome their difficulties.", "title": "An Intelligent Assistant to Support Students and to Prevent them from Dropout", "claims": null}], "query": "AI as a teacher assistant", "summary_abstract": "The collective research on AI as a teacher assistant reveals a growing recognition of its potential to enhance educational experiences for both students and teachers. Various studies highlight the multifaceted roles that AI can play in the classroom, from improving attendance and participation to providing personalized learning experiences.\n\nAlzaabi et al. (2023) describe the development of an \"AI-Teacher\" system aimed at improving student attendance and engagement by sending alerts to parents and facilitating communication between students and instructors. This system exemplifies how AI can foster connections within the educational community.\n\nZhang et al. (2023) introduce the \"I-assistant,\" which integrates intelligent algorithms to analyze student performance and classroom dynamics. This system not only assists teachers in instructional design but also visualizes student learning situations, thereby promoting more effective teaching strategies.\n\nKim et al. (2020) explore students' perceptions of AI teaching assistants, finding that perceived usefulness and ease of communication are crucial for their acceptance. This suggests that for AI to be effectively integrated into education, it must be user-friendly and clearly beneficial to students.\n\nShirin (2022) emphasizes the need for teacher training in AI technologies, noting that while educators see the potential of AI, many lack the necessary understanding to implement it effectively. This highlights a critical area for development in the integration of AI in education.\n\nZhang et al. (2019) present a system that tracks student behavior using AI, allowing teachers to receive insights into student performance without direct observation. This capability can help tailor educational approaches to individual student needs.\n\nKim et al. (2019) provide evidence that AI can enhance personalized learning by delivering tailored reports to teachers, which in turn positively impacts student performance. However, they also caution against technology overload, which can hinder effective use.\n\nCelik et al. (2022) summarize the various roles teachers can play in AI development, from providing data for training algorithms to participating in the assessment process. They also identify challenges in AI implementation, suggesting that careful planning is necessary for successful integration.\n\nLastly, Tran et al. (2009) propose a non-pedagogical AI assistant that acts as a mentor to prevent student dropout, illustrating another dimension of AI's supportive role in education.\n\nIn summary, the research indicates that AI has significant potential as a teaching assistant, capable of enhancing communication, personalizing learning, and providing valuable insights into student performance. However, successful implementation requires addressing teachers' training needs and ensuring that AI tools are user-friendly and beneficial to both educators and students.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2014}, "authors": ["L. Kiminami", "Shinichi Furuzawa", "\u6728\u5357 \u8389\u8389", "\u53e4\u6fa4 \u614e\u4e00"], "summary": "Hunger eradication is an essential issue for sustainable development while sustainable consumption and production system are essential to eradicate hunger and protect ecosystem. This requires polices that create incentives for producers and consumers to adopt sustainable practices and behaviour (FAO, 2012, p.89; Rio +20). World consumption of sea food products per capita steadily increased by 3.2% of the average annual growth rate in 1961 to 2009, and amounts to 18.4 kg in 2009(FAO, 2012). And the consumption of seafood is not distributed evenly, and considerably regional differences occur (Swartz et al., 2010). In Japan, quantity of consumption for fishery products is declining with the centralization of consumption to some specific fish stocks. The intake of seafood decreased with a peak of around 1997, and the intake of meat exceeded it in 2006, and it will be in the tendency which the difference expands. Moreover, decreasing of the intake of aquatic products and increased of the intake of meat products arise in all the age groups. On the other hand, in the adjacent seas in Japan, reduction of the good fishing resources by over catching causes a price slump of fishery products, and there is a growing anxiety about worsening fishery operators\u2019 business performance. In order to realize a sustainable food system of aquatic products from economical, environmental and social aspects, the behavior of the economic entities including producers, distributers and consumers that contributes to satisfying both of the protection of fishery resources and its appropriate use should be taken into account in the fishery policies and programs. In this paper, the consumer\u2019s consciousness and behavior toward fishery products and the factors that influence them will be analyzed in order to draw policy implication contributing to the construction of a sustainable sea food system which covers the problems of fisheries such as the flow of aquatic products and the environment surrounding it.", "title": "Analyses on the Factors Affecting Consumer's Consciousness Related to Aquatic Resources and Products", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Maite  Carrass\u00f3n", "Anna  Soler-Membrives", "Mar\u00eda  Constenla", "C.  Escobar", "R.  Flos", "Jose Maria Gil", "V.  Luz\u00f3n", "Francesc  Piferrer", "L.  Reig"], "summary": "Abstract Aquaculture products are commonplace in markets around the world. However, despite efforts to minimize the negative perceptions towards aquaculture, several misbeliefs or myths still persist, and thus globally consumers tend to value wild fish more highly than farmed fish . The lack of information has been shown to be one of the most important causes of this preference, driving buying decisions to be more emotional than rational. The aim of this study was to determine whether scientific-supported information contrasting one myth could contribute to a better perception of farmed products. To that end, consensus on a series of aquaculture-related issues among different scientists, external experts, and aquaculture societies was used to build up the scientific information. This information was provided to 300 Spanish consumers using two different communication tools (150 consumers each tool): an interactive web documentary and a written and printed document, to detect possible differences in the change of consumers' perception. Consumers were asked for their degree of agreement on a set of 14 statements before and after providing the scientific information. A variable collecting the assessment of each of the statements was calculated as the Overall-perception. Possible significant differences between the scores before and after providing the information and for the \u2018overall perception\u2019 were analysed separately for each communication tool as well as for the combined sample. Possible relationship between the consumers' perception with the sociodemographic factors, the consumers' knowledge and the fish consumption habits were also assessed. Results show that consumer's perception of aquaculture before the query were moderate (5.6 average in a 0 to 10 scale) but that it increased slightly but significantly and regardless of the communication tool used. Among sociodemographic factors, age and gender were the ones that most influenced consumer's perceptions, being older people those who exhibited a generally more positive opinion towards aquaculture. The effects of consumption habits and knowledge about aquaculture were also the two most explicative factors for change in perception. Importantly, the opinion of consumers with less knowledge about seafood products in general and production methods or consuming only wild fish products, improved after being exposed to the information. These results demonstrate the utility of science- and fact-based communication campaigns to improve the societal perception of aquaculture practices and products, regardless of the tool used to transmit this information.", "title": "Information impact on consumers' perceptions towards aquaculture: Dismantling the myth about feeds for farmed fish", "claims": null}, {"metadata": {"year": 2020}, "authors": ["S. Leng"], "summary": "High demand of seafood products combined with over-harvesting of wild catch aquatic products have elevated aquaculture as the alternative way to strike a balance between seafood product supply and demand. However, uncontrolled aquaculture activities are found to affect the environment negatively as well as the health of consumers. Better awareness and understanding can regulate aquaculture practices and bring about environmental health through consumers\u2019 pressure on the aquaculture production system. This study aimed to identify consumers\u2019 perception on aquaculture industry, to study consumers\u2019 awareness on sustainability of aquaculture practice and also to find ways to increase consumers\u2019 awareness and acceptance of sustainable aquaculture practice and its products. A mixed research method was used in this study combining quantitative questionnaire survey of respondents who bought aquaculture products at wet markets in Penang, and qualitative in-depth focus group discussions (FGDs) with a number of selected respondents. Research findings show that most of the respondents are aware that aquaculture might give lots of negative effects to human health and the environment but, they still chose to buy aquaculture products due to cheaper price than wild catch seafood products. Hence, from the findings, it is clear that even though people have awareness, they have little choice but to accept the quality of aquaculture products. However, the results also show that enhancing consumer awareness and understanding empowers consumers to put pressures on producers paving the way towards sustainable aquaculture and greater acceptance of aquaculture practices and products. 2357-1330 \u00a9 2020 Published by European Publisher.", "title": "Consumer Awareness And Acceptance Of Aquaculture Practices And Products In Penang, Malaysia", "claims": null}, {"metadata": {"year": 2022}, "authors": ["M. Z. Hoque", "N. Sultana", "A. Haque", "Md Tamim Mahamud Foisal"], "summary": "The global decline in ocean fisheries stocks, including those of wild fish, has given impetus to the rapid growth of seafood produced by aquaculture, as well as of farmed fish (FF). However, continuous asymmetric information leads to consumer ambiguity and confusion regarding their knowledge of farmed fish. The personal, psychological, and socioeconomic factors affect individual\u2019s behaviour; therefore, this study considers ambiguity tolerance (AT) and confusion avoidance (CA) as personal factors and the relevant socioeconomic factors and examines if they affect consumers\u2019 perceived FF knowledge. A total sample of 1041 households from the major two urban areas of Dhaka and Chittagong, Bangladesh, were randomly selected and interviewed using a structured questionnaire. The data were analysed employing exploratory factor analysis, descriptive statistics, structural equation modelling, and the multinomial logistic regression model. The findings reveal that both AT and CA affect FF knowledge significantly and positively. Furthermore, AT reduces CA, while CA plays a critical mediating role in explaining the effect of AT on FF knowledge. The socioeconomic variables such as a high rate of fish consumption, the habit of buying fish personally, and the tendency to buy fish from wet markets also significantly influence consumers\u2019 perceived FF knowledge. The findings provide a significant managerial implication for improving a way to develop substantial factors to increase FF knowledge and benefit consumers and society.", "title": "Personal and Socioeconomic Factors Affecting Perceived Knowledge of Farmed Fish", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Rungsaran  Wongprawmas", "Giovanni  Sogari", "Francesco  Gai", "Giuliana  Parisi", "Davide  Menozzi", "Cristina  Mora"], "summary": "Abstract Aquaculture has increased significantly in recent decades and, if managed sustainably, it could contribute to food security and safety. However, several studies have found the evidence that European consumers prefer wild fish relative to farmed fish. Hence, it becomes crucial to provide accurate information on aquaculture products. This study aims to examine the influence of consumers' perceptions and knowledge on their intention to purchase farmed and wild fish. A sample of 804 Italian consumers participated in a survey which included an information treatment. Participants perceived that wild fish tastes better and is more nutritious than farmed fish; further, they considered farmed fish to be cheaper, safer, and more ethical than wild fish. A logit regression was conducted whose results show that several factors, such as fish preference, attitude, and subjective knowledge, affect consumers' intention to purchase. In addition, participants in the treatment group were found to have higher intentions to purchase farmed fish than those in the control group. Therefore, information strategy is crucial to promote the consumption of farmed fish.", "title": "How information influences consumers' perception and purchasing intention for farmed and wild fish", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Zuzanna  Pieniak", "Filiep  Vanhonacker", "Wim  Verbeke"], "summary": "This paper explores consumers\u2019 knowledge about fish and aquaculture and assesses the use and importance of different information cues about fish. Cross-sectional data were collected in 2008 through a consumer survey (n=3213) in the Czech Republic, Germany, Greece, Italy, Portugal, Romania, Sweden and the UK. Consumers\u2019 knowledge about fish generally, and about aquaculture in particular, was relatively low and differed significantly between countries. Consumers from all countries reported an indication of quality and/or food safety as an information cue when buying fish. The information sources most frequently used by Europeans were labelling and sellers in retail or supermarkets. The Internet was identified by consumers in all of the countries as one of the most important sources of information about sea and freshwater fish products. Policy makers and food marketers are encouraged to develop a simple and easily recognisable mark (relating to quality, food safety and nutrition) to assist consumer decision-making. Information campaigns focusing on issues such as the nutritional benefits of eating fish are also recommended.", "title": "Consumer knowledge and use of information about fish and aquaculture", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Desmelati"], "summary": "This study was carried to observe some of factors influencing theselection of aquacultured freshwater fishes (patin and tilapia) by consumers.Among factors observed were the knowledge, attitude and behavior ofrespondents toward the aquacultured fishes. Two hundred and fifty respondentsaged between 15-55 old were interviewed. They were randomly selected fromareas such as at popular shopping complexes, bus stations and UniversityKebangsaan Malaysia campus around Kajang and Bandar Baru Bangi. Range ofscore for \u201cknowledge\u201d was 0-25, attitude 0-75 and behaviour 0-40. Resultsshowed that the average score for knowledge was11.5\u00b13.4, attitude 58.5\u00b14.3 andbehavior 23.5\u00b13.4 of respondent the intake of aquacultured freshwater fish.Pearson correlation\u2019s showed relation between knowledge and level of education(r = 0.76) and behavior (r = 0.85) of respondent toward the selection and intake ofaquacultured freshwater fish in their diet.", "title": "TINGKAT KESUKAAN KONSUMEN TERHADAP IKAN BUDI DAYA AIR TAWAR", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Lauren Errickson", "Douglas R Zemeckis"], "summary": "\n \n \n Americans consistently fail to meet dietary guidelines for seafood intake. Efforts are needed to increase consumption, especially of sustainable seafood that can be supplied by domestic aquaculture. However, consumer receptivity to aquaculture products is mixed. The objective of this study was to elicit industry perspectives regarding influences on consumer purchases of aquaculture products.\n \n \n \n Key informant interviews (n\u00a0=\u00a012) were conducted in late 2020 with U.S. salmon, shrimp, and oyster producers, marketers, and industry interest groups. Participants were recruited via snowball sampling. Virtual interviews were conducted by a trained moderator and assistant moderator/notetaker using a semi-structured interview guide. Qualitative data analysis included a thematic review of interview recordings and notes, with key concepts coded according to a priori themes derived from the literature.\n \n \n \n Interviews yielded important insights into consumer receptivity to aquaculture products. Participants believe that outdated misperceptions of aquaculture persist, noting that despite advances in domestic aquaculture production practices to comply with U.S. standards, some consumers perceive aquaculture as environmentally detrimental and unsustainable. Further, participants believe negative attitudes toward genetically modified organisms, corn and soy-based feeds, antibiotics, and chemicals are misplaced, yet contribute to hesitancy among some consumers. Industry opinions on what is important to consumers reflect strong valuation of seafood quality, freshness, local harvest, and sustainability. Participants suggest product labeling efforts be developed accordingly, and that innovative marketing strategies be undertaken, such as aquaculture product promotion through \u201cknow your farmer\u201d campaigns, chef education initiatives, and home delivery programs.\n \n \n \n For domestic aquaculture products to have a meaningful impact on U.S. seafood intake, positive consumer receptivity is key. Industry perspectives will inform marketing and educational efforts toward addressing consumer hesitancy to purchase aquaculture products by resolving misguided concerns, with important implications for consumer health and sustainability of the domestic seafood supply.\n \n \n \n United States Department of Agriculture.\n", "title": "Industry Insights on Consumer Receptivity to Aquaculture Products in the Retail Marketplace: Considerations for Increasing Seafood Intake", "claims": null}], "query": "write an essay on how general knowledge and awareness correlate with perceived behavioural control in relation to consuming food products especially unfamiliar low trophic level aquaculture products.", "summary_abstract": "The relationship between general knowledge and awareness and perceived behavioral control regarding the consumption of unfamiliar low trophic level aquaculture products is a multifaceted issue that has garnered attention in recent research. The findings from various studies indicate that consumer perceptions, knowledge, and awareness significantly influence their willingness to purchase aquaculture products, particularly in the context of sustainability and health.\n\nKiminami et al. (2014) highlight the importance of consumer behavior in the context of sustainable seafood consumption. They argue that understanding consumer consciousness and behavior towards fishery products is crucial for developing effective policies that promote sustainable practices. This suggests that increased awareness and knowledge about aquaculture can enhance perceived behavioral control, leading to more sustainable consumption choices.\n\nCarrass\u00f3n et al. (2021) further emphasize the role of information in shaping consumer perceptions. Their study found that providing scientifically supported information about aquaculture can significantly improve consumer attitudes towards farmed products. Notably, consumers with less prior knowledge about seafood exhibited a marked improvement in their perceptions after being exposed to factual information. This indicates that enhancing general knowledge about aquaculture can empower consumers, thereby increasing their perceived behavioral control over their purchasing decisions.\n\nLeng (2020) also supports this notion, revealing that while consumers are aware of the potential negative impacts of aquaculture on health and the environment, they often choose these products due to their affordability. This highlights a gap between awareness and action, suggesting that while knowledge exists, it may not always translate into behavior unless consumers feel they have control over their choices.\n\nHoque et al. (2022) delve into the psychological aspects of consumer behavior, identifying factors such as ambiguity tolerance and confusion avoidance that affect perceived knowledge of farmed fish. Their findings indicate that enhancing consumer knowledge can mitigate confusion and lead to more informed purchasing decisions, thereby increasing perceived behavioral control.\n\nWongprawmas et al. (2022) and Pieniak et al. (2013) both point to the necessity of accurate information dissemination to counteract the preference for wild fish over farmed fish. They argue that consumers often hold misconceptions about aquaculture, which can be addressed through targeted information campaigns. Such campaigns can enhance consumer knowledge and awareness, ultimately influencing their purchasing behavior.\n\nIn summary, the collective findings from these studies underscore a clear correlation between general knowledge and awareness and perceived behavioral control in the context of consuming unfamiliar low trophic level aquaculture products. Increased awareness and understanding of aquaculture practices can empower consumers, leading to more informed and sustainable choices. This highlights the critical role of education and information dissemination in shaping consumer behavior and promoting sustainable aquaculture practices.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2017}, "authors": ["S. Lennon"], "summary": "The design and implementation of questioning, specifically in regards towards a higher level of thinking, is a common practice in many secondary social science classrooms (Bickmore & Parker, 2012). Questioning can help the teacher develop critical thinking concepts, scaffold discussions, and prod students towards an elevated level of cognition (Yang, Newby & Bill, 2005). It can also aid in guiding group discourse and help students in developing a rational understanding of a problem or concept (Byun, Lee, & Cerreto, 2014; Godfrey & Grayman, 2014). Yet many educators may feel limited or not prepared in their conception and ability towards this practice. The confusion is merited, in some respects, as questioning is a skill not easily mastered or understood. To conceptualize questioning it may be best to define the word by its basic meanings. The American Heritage Dictionary (1991) defines a question as \u201can expression of inquiry that invites or calls for a reply\u201d (p.1015). It subsequently defines the word, in the case as a noun, with ten definitions clustered in seven primary groups. As a verb, it can be delineated as either transitive or intransitive with the dictionary hav-", "title": "Questioning for Controversial and Critical Thinking Dialogues in the Social Studies Classroom.", "claims": null}, {"metadata": {"year": 2003}, "authors": ["M. Schiltz"], "summary": "With few exceptions, the survey questionnaire reflects the state of researchers\u2019 knowledge and the questions about society that can be imagined at the time of its design. In practice this scientific instrument is built on consensus and validates itself by continued use while being successively refined by integrating changes that reflect the evolution of social norms and societal problems. Its evolution is slow, its innovations are halting; although inappropriate to the study\u2019s original intent, some questions become so usual that they are retained for comparison to the others precedent reliable sources. So, questionnaires are often a curious mixture of well-founded questions, obsolete questions, well-tried questions and exploratory questions, this making does not invalidate the entire procedure; when the questionnaire is itself questioned, it is for other reasons such as the standardization of concepts and techniques, or because it prematurely establishes links between behaviours and social classifications.", "title": "Stockholm Conference", "claims": null}, {"metadata": {"year": 1986}, "authors": ["R. Trahair"], "summary": "or post-stratifying a sample resulting from a telephone survey. Ironically, for a book which looks so much like a cook-book, much of its interest for sociologists will probably lie in its theoretical discussions. One of the major splits within sociology, both in the USA and in Australasia, has been between \u2019quantitative\u2019 approaches (particularly survey methods) on the one hand, and so-called \u2019qualitative\u2019 approaches (such as ethnomethodology or participant observation) on the other. As a result, some ofthose attracted to survey techniques tend to act as if the relationship between \u2019reality\u2019 and \u2019knowledge\u2019 was relatively unproblematic, and that good survey questions could be designed by logical, deductive processes, while sitting at a table. Cannell brings the", "title": "Book Reviews : ASKING QUESTIONS : A PRACTICAL GUIDE TO QUESTIONNAIRE DESIGN. Seymour Sudman and Norman M. Bradburn San Francisco, Jossey-Bass, 1983.397 pp. No price (cloth)", "claims": null}, {"metadata": {"year": 1998}, "authors": ["Mark Smith"], "summary": "How do social scientists study the social world? Is social scientific practice in transformation? Can social science learn from its own past? Social Science in Question provides an accessible and authoritative guide to both traditional debates and new approaches to knowledge construction in the social sciences. The book takes the reader on an intellectual journey starting with the story of modern science and the impact that this has had on social scientific practice, and going on to outline and critically review the major approaches to social scientific inquiry, ranging from positivism to postmodernism. This innovative text poses questions about the recent 'cultural turn' in the social sciences. The central dilemmas of the philosophy of social sciences are readdressed in the light of contemporary debates on: perception and imagination; detachment and involvement; simplicity and complexity; representation and reality; and subjectivity and objectivity.", "title": "Social science in question", "claims": null}, {"metadata": {"year": 0}, "authors": [], "summary": "Do We Make o[ the Distinction Between the Natural andthe Social Sciences?\" KarinKnorr 1. Ever since the rise of the social sciences, social science methodology has been a much disputed issue among social scientists and philosophers of the social sciences 1. Much like the history of social theory, the history of social science methodology is a history of controversy, and this tradition is quite naturally renewed with every appearance ofa new conception of social life. Most recently, methodological discussions have centred arOlllld discrediting \"positivistic\" procedures by showing their essential inadequacy in dealing with the social world. In these discussions, established methodological procedures, such as survey research or laboratory experimentation, were linked to a model of scientific method identified with the natural sciences, and new social methodologies continuously emphasized their rejection of this model. In fact, new rules of social science method have been developed, displayed and defended in a constant dispute of the standard set by this model, and they have made the departure from this standard the declared goal of an indigenous social methodology. Perhaps not surprisingly, the standard itself has fOlllld little consideration in the respective dispute. While the \"positivistic\" conception is vigorously rejected as a model for social science methodologies, it is more or less taken at face value when it is referred to the natural and teclmological sciences. Philosophical investigations which for some time now have directly questioned this model as describing correctly the natural sciences appear to be either ignored or declared irrelevant for the discussion. When they are introduced into the picture, they serve as some sort of background noise against which the original conception of the natural sciences is painstakingly reiterated 2. This paper does not aim at a global rejection of the distinction between the natural and the social sciences, let alone at a rejection of", "title": "Social and Scientific Method or ' What Do We Make o [ the Distinction Between the Natural andthe", "claims": null}, {"metadata": {"year": 2003}, "authors": ["N. C. Schaeffer", "S. Presser"], "summary": "\u25a0 Abstract Survey methodologists have drawn on and contributed to research by cognitive psychologists, conversation analysts, and others to lay a foundation for the science of asking questions. Our discussion of this work is structured around the decisions that must be made for two common types of inquiries: questions about events or behaviors and questions that ask for evaluations or attitudes. The issues we review for behaviors include definitions, reference periods, response dimensions, and response categories. The issues we review for attitudes include bipolar versus unipolar scales, number of categories, category labels, don\u2019t know filters, and acquiescence. We also review procedures for question testing and evaluation.", "title": "The Science of Asking Questions", "claims": null}, {"metadata": {"year": 2012}, "authors": ["E. Ezedike"], "summary": "This paper undertakes a philosophical examination of the nature of scientific sociology which has had most profound influence on the methodology of social sciences. As an ancillary discipline, the role of philosophy in relation to natural sciences is to clear up conceptual ambiguities and lay down standards of intelligibility, scientific status and validity. In this paper, I argue that these standards which are universalizable should equally apply to the social sciences. I posit that if social scientific explanations are to count as \u201cscientific\u201d at all, they must conform to the standards already established in the natural sciences.", "title": "SCIENTIFIC METHODOLOGY IN THE SOCIAL SCIENCES: A CRITIQUE OF THE COMTEAN AND DURKHEIMIAN POSITIVISM", "claims": null}, {"metadata": {"year": 2015}, "authors": ["C. Ilie"], "summary": "The importance of questions in all kinds of private and public communication can hardly be overestimated. Having long been a central investigation topic in classical rhetoric and philosophy, and later in grammar and linguistics, the study of questions and questioning has gradually developed into a multidisciplinary field of inquiry, integrating pragmatics, rhetoric, discourse analysis, sociolinguistics, and anthropology (and this is not an exhaustive list). Starting with some of the most significant definitions and classifications of questions within several theoretical approaches or schools of thought based on formal, functional, sequential, and/or contextual dimensions of use, this article provides an overview of the most influential research on the roles, functions, and goals of questions and answers in specific social contexts and institutional environments.", "title": "Questions and Questioning", "claims": null}], "query": "are questionaire the common method in social science?", "summary_abstract": "The use of questionnaires is indeed a prevalent method in social science research, as highlighted by several authors. Schiltz (2003) notes that the survey questionnaire serves as a fundamental tool for researchers, reflecting the prevailing knowledge and societal questions at the time of its creation. This instrument evolves through continued use and refinement, integrating changes that mirror shifts in social norms and issues. Despite its limitations, such as the retention of outdated questions for comparative purposes, the questionnaire remains a vital component of social research.\n\nMoreover, Trahair (1986) discusses the dichotomy between quantitative and qualitative approaches in sociology, emphasizing that survey methods, including questionnaires, are a significant aspect of quantitative research. This suggests that questionnaires are not only common but also integral to the quantitative study of social phenomena.\n\nSchaeffer and Presser (2003) further elaborate on the complexities involved in crafting effective survey questions, drawing from cognitive psychology and conversation analysis. Their work underscores the importance of methodological rigor in the design of questionnaires, which is essential for obtaining valid and reliable data in social science research.\n\nIn summary, questionnaires are a common and essential method in social science, utilized for their ability to gather data on societal behaviors and attitudes. They are continuously refined to adapt to evolving social contexts, making them a cornerstone of quantitative research methodologies in the field (Schiltz, 2003; Trahair, 1986; Schaeffer & Presser, 2003).", "summary_extract": null}, {"papers": [{"metadata": {"year": 1996}, "authors": ["Karen  Grimmer"], "summary": "Summary The excursion of two anatomical landmarks on the cervical spine was measured as never-injured subjects moved from a standard sitting position to their habitual cervical resting posture in sitting. The angular excursion traced by these two anatomical points was considered simultaneously in order to describe individual resting postures. The data for neither excursion angle approximated the normal distribution. Subjects with extreme cervical excursion were thus identified by dividing the data into five groups, and designating the first and last groups as extremes of the population range. Four groups of subjects with extreme angles of excursion occurring at both anatomical points were identified and photographed, and the differences in their postures discussed. Subjects\u2019 reports of neck pain which had occurred in the preceding month were employed in order to test the hypothesis that more subjects with extreme cervical resting posture reported neck pain than did subjects whose cervical posture more closely approximated the population average. No association was evident, and the hypothesis was thus rejected. Suggestions are made for further investigation of both the hypothesis and the existing data.", "title": "The Relationship Between Cervical Resting Posture and Neck Pain", "claims": null}, {"metadata": {"year": 2019}, "authors": ["David  Ezra", "Ella  Been", "Deborah  Alperovitch-Najenson", "Leonid  Kalichman"], "summary": "The cervical spine is the most mobile component of the spinal column. Its curvature (lordosis) supports the head above the shoulders, maintaining head position, gaze, and visual field. However, there is considerable variability in the cervical spine curvature of different individuals, generating several unresolved questions: What is a normal cervical curvature? How is cervical posture associated with cervical spine pain and pathology? How is erect posture and a modern lifestyle associated with cervical pain and pathology?", "title": "Cervical Posture, Pain, and Pathology: Developmental, Evolutionary and Occupational Perspective", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Su-Hun Lee", "D. Son", "Jun-Seok Lee", "S. Sung", "Sang Weon Lee", "G. Song"], "summary": "Objective The \u201cdisc degeneration precedes facet joint osteoarthritis\u201d hypothesis and multidimensional analysis were actively discussed in lumbar spine. However, in cervical spine degeneration, the multifactorial analyzes of disc degeneration (DD), Modic changes (Mcs), facet degeneration, and endplate degeneration (ED) is still limited. In this cross-sectional study, we aimed to analyze the prevalence and interrelationship of cervical DD parameters. Methods We retrospectively recruited 62 patients aged between 60 and 70 years. The disc height, segmental angle, ossified posterior longitudinal ligament (OPLL), ED, facet joint degeneration (FD), uncovertebral joint degeneration (UD), DD, spinal stenosis (SS), Mc, and cord signal change (CS) were evaluated using a previously well-known grading system. Results The prevalence of cervical degenerative parameters were DD (grade 1, 1.2%; grade 2, 13.3%; grade 3, 54.8%; grade 4, 19.0%; grade 5, 11.7%), OPLL (26.2%), SS (grade 0, 7.7%; grade 1, 42.3%; grade 2, 26.2%; grade 3, 23.8%), UD (39.1%), ED (normal, 69.0%; focal defect, 9.7%; corner defect, 11.7%; erosion, 6.9%; sclerosis, 2.8%), and FD (normal, 48.8%; narrowing, 27.0%; hypertrophied, 24.2%). The interrelationship of degenerative parameters showed close relation between UD, SS, DD, OPLL, Mc. ED, and CS has partial relation with degenerative finding. FD only has relation with UD, and Mc. Conclusion Our results may indicate that FD is a degeneration that occurs independently, rather than as a result of other degenerative factors.", "title": "Relationship Between Endplate Defects, Modic Change, Facet Joint Degeneration, and Disc Degeneration of Cervical Spine", "claims": null}, {"metadata": {"year": 2022}, "authors": ["A. Bele"], "summary": "Structures around the cervical spine are a common source of neck pain and have numerous aetiologies. One of the aetiologies considered here is the sustained abnormal posture; this type of presentation can be seen in the population which works tirelessly in the same position for hours like the patient in our case who works as a technician. Patient\u2019s main concerns were pain in right shoulder radiating till the elbow, the severity of the pain kept the patient awake for nights. Clinical findings include severely restricted extension of neck and reduced lateral flexion and rotation of the affected side; Spurling\u2019s test positive for the affected side and distraction test positive. MRI revealed cervical intervertebral disc protrusion at multiple levels (C3-C4, C4-C5, C5-C6, and C6-C7). This case report concluded that poor cervical posture responsible for overloading on cervical spine leads to disc protrusion in people who work for long hours in abnormal posture.", "title": "36-YEARS-OLD MALE WITH CERVICAL DISC PROTRUSION AND POSTURAL DEVIATIONS", "claims": null}, {"metadata": {"year": 2018}, "authors": ["P. K. RoseBist", "A. Peethambaran", "G. Peethambar"], "summary": "The term cervical spondylosis describes chronic degenerative lesions of multiple or single cervical intervertebral discs and the consequent formation of osteophytes on related vertebral bodies. It is a leading cause of musculoskeletal disability in human beings. The sequence of disc degeneration leads to the clinical syndrome of cervical pain, cervical radiculopathy and cervical myelopathy. The disease is multifactorial. Advancement of age, occupational heavy loading, trauma, whole body vibration etc are possible risk factors. Smoking and genetic factors also contributes to the structural changes in spine. Most common degenerative change is seen in C5-C6 followed by C6-C7 and C4-C5. The pain usually responds to activity modification, neck immobilisation, isometric exercises and medication. Cervical radiculopathy often responds favourably to conservative management but if there is persistence of pain or progressive neurological deficits, then surgery should be attempted through anterior or posterior cervical spine. Cervical spondylotic myelopathy is however a serious disabling condition.", "title": "Cervical spondylosis: analysis of clinical and radiological correlation", "claims": null}, {"metadata": {"year": 2022}, "authors": ["V. Markova", "Silviya Filkova"], "summary": "Deviation from a healthy posture suggests the presence of neuromuscular imbalance and may be associated with certain musculoskeletal complaints, such as neck pain. We measured the craniovertebral angle (CVA) of the spine in 100 healthy adults who worked at a computer for more than two hours per day for more than two years. We found a weak, positive, statistically significant linear relationship between the frequency of reported pain and the degree of pain (Pearson's r=0.246, p=0.014) and a moderate relationship with the measured CVA values for sitting and standing (r=0.403, p=0.001). A higher frequency (at 72%) and severity (60%) of pain symptom was calculated in study participants who had a CVA measured below 50\u00b0 from a standing position (p>0.05). Effective management of cervical pain is beneficial to clinicians and healthcare professionals involved in spinal rehabilitation. The aim of the present study was to investigate the influence of sagittal alignment of the cervical spine on the frequency and severity of pain in this region.", "title": "Severity and Frequency of Pain Symptoms in the Cervical Spine", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Jeffrey D. Petersohn"], "summary": "This chapter reviews the anatomic features producing extremity pain, discusses the clinical presentation of cervical disc and spondylotic disease, and explores the differential diagnosis of upper extremity pain. Clinically relevant findings are emphasized in the history and physical examination. Electrodiagnostic and imaging studies necessary to establish a correct diagnosis are highlighted. Common upper extremity nerve entrapment syndromes are discussed. Following a discussion of the anatomic basis for pain, interventional and surgical methods for treatment are briefly compared.", "title": "Cervical Disc Disease and Extremity Pain", "claims": null}, {"metadata": {"year": 1997}, "authors": ["Koichiro  Hayashi", "Yasuyoshi  Wadano", "Ikuo  Aita"], "summary": "One hundred and twenty-eight cervical discs obtained from 30 autopsied subjects were examined morphologically. The cervical degenerative process was divided into five stages: (i) The initial stage\u2014characterized by the disappearance and calcification of cells of the cartilagenous plate; (ii) the early stage\u2014characterized by vessel invasion and formation of a calcification front; (iii) the progressive stage\u2014characterized by progressive degenerative changes in all disc components; (iv) the late stage\u2014characterized by destruction of the cartilagenous plates; and (v) the terminal stage\u2014c characterized by eburnation of the bony end-plate. The cervical spine has three main differences from other regions of the spine: a large range of motion, the capacity for frequent and rapid movements, and the presence of Luschka's joint. Each of these three features is related to cervical disc degeneration.", "title": "Disc degeneration in the cervical spine", "claims": null}], "query": "what is the relation between cervical disc pathology and cervical posture?", "summary_abstract": "The relationship between cervical disc pathology and cervical posture is a complex interplay that has been explored in various studies. Grimmer (1996) investigated the resting postures of individuals and found no significant association between extreme cervical resting postures and reported neck pain, suggesting that posture alone may not be a definitive predictor of cervical disc issues. However, the study highlighted the variability in cervical postures among individuals, which could have implications for understanding cervical health.\n\nEzra et al. (2019) emphasized the importance of cervical curvature and its variability among individuals, raising questions about how these factors relate to cervical spine pain and pathology. This suggests that while posture may not directly correlate with pain, the overall alignment and curvature of the cervical spine could play a role in the development of cervical disc pathology.\n\nLee et al. (2020) conducted a study on cervical disc degeneration and its interrelationships with other degenerative parameters, indicating that factors such as disc height and facet joint degeneration are interconnected. This multifactorial approach suggests that while posture may influence cervical health, it is one of many factors contributing to disc degeneration.\n\nBele (2022) provided a case report linking poor cervical posture to disc protrusion, particularly in individuals who maintain sustained abnormal postures for extended periods. This highlights a potential causal relationship where poor posture may lead to increased stress on cervical discs, resulting in pathology.\n\nMarkova and Filkova (2022) found a significant relationship between craniovertebral angle (CVA) and the frequency and severity of neck pain among individuals who spend long hours at computers. Their findings suggest that deviations from healthy cervical posture can be associated with musculoskeletal complaints, including cervical disc issues.\n\nIn summary, while there is no definitive consensus on the direct relationship between cervical disc pathology and cervical posture, several studies indicate that poor posture may contribute to cervical disc degeneration and associated pain. Factors such as individual variability in cervical curvature, sustained abnormal postures, and the interplay of various degenerative parameters all play a role in this complex relationship (Grimmer, 1996; Ezra et al., 2019; Lee et al., 2020; Bele, 2022; Markova & Filkova, 2022).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2011}, "authors": ["Kevin M Antshel", "Teresa M Hargrave", "Mihai  Simonescu", "Prashant  Kaul", "Kaitlin  Hendricks", "Stephen V Faraone"], "summary": "Attention deficit hyperactivity disorder (ADHD) is a neurocognitive behavioral developmental disorder most commonly seen in childhood and adolescence, which often extends to the adult years. Relative to a decade ago, there has been extensive research into understanding the factors underlying ADHD, leading to far more treatment options available for both adolescents and adults with this disorder. Novel stimulant formulations have made it possible to tailor treatment to the duration of efficacy required by patients, and to help mitigate the potential for abuse, misuse and diversion. Several new non-stimulant options have also emerged in the past few years. Among these, cognitive behavioral interventions have proven popular in the treatment of adult ADHD, especially within the adult population who cannot or will not use medications, along with the many medication-treated patients who continue to show residual disability.", "title": "Advances in understanding and treating ADHD", "claims": null}, {"metadata": {"year": 2005}, "authors": ["N. Pityaratstian"], "summary": "Several alternatives to psychostimulants have been developed and expanded the variability of the treatment of ADHD. Clonidine is a good option for managing core behavioral symptoms, especially hyperactivity and impulsivity. Bupropion and venlafaxine seem potentially promising. Significant new options include norepinephrine reuptake inhibitors, such as atomoxetine, and possibly selective dopamine agonists. Central anticholinesterases, such as donepezil, may improve core ADHD symptoms.", "title": "Advances in alternative pharmacotherapy of ADHD.", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Narmada  Mannem", "Tejas  Mehta", "Elanagan  Nagarajan", "Naveen K. Yarasi", "Pradeep C. Bollu"], "summary": "Attention deficit hyperactivity disorder (ADHD) is a chronic neurobehavioral disorder that is characterized by inattention, hyperactivity, and impulsivity along with a troubled functioning in family, social, occupational, and academic settings. A brief discussion of the management of ADHD including pharmacotherapy, behavioral interventions, multidisciplinary approach, and new advances in treatment is described. New developments in the management of ADHD including atypical drugs, neurostimulation, and druggable genomes. The outline of the article includes dynamic advances in the treatment of ADHD with an emphasis on a multidisciplinary approach.", "title": "ADHD and Its Therapeutics", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Noorul Hasan", "M. Mujahid", "Badruddeen", "N. Ahmad", "S. Parveen", "Seikh Zohrameena", "Z. Khan"], "summary": "Attention-deficit/hyperactivity disorder (ADHD) is a neurobehavioral condition primarily affecting children but regularly persisting into adolescence and adulthood. The symptoms must present in multiple settings ie home, school, work, be inappropriate for developmental level and interfere with the individual\u2019s level of functioning, social development, learning processes, and quality of life. There are three presentations of ADHD i.e. inattentive, hyperactive and combined. There is a substantial pharmacopoeia available for safe and effective treatment of ADHD. CNS stimulants like methylphenidate, amphetamine are recommended as first-line medication therapy for children.", "title": "Advances & Considerations in Pharmacotherapy of Attention-deficit/hyperactivitydisorder (ADHD)", "claims": null}, {"metadata": {"year": 2009}, "authors": ["D. Greydanus", "A. Nazeer", "D. Patel"], "summary": "Attention deficit hyperactivity disorder (ADHD) is a neurobehavioral developmental disorder found in 3% to 8% of children and adolescents. An important part of ADHD management is psychopharmacology, which includes stimulants, norepinephrine reuptake inhibitors, alpha-2 agonists, and antidepressants. Medications with the best evidence-based support for ADHD management are the stimulants methylphenidate and amphetamine. A number of newer, long-acting stimulants are now available and a number of new medications are considered that are under current research.", "title": "Psychopharmacology of ADHD in pediatrics: current advances and issues", "claims": null}, {"metadata": {"year": 2010}, "authors": ["M. R\u00f6sler", "M. Casas", "E. Konofal", "J. Buitelaar"], "summary": "Abstract Objective. To examine available literature regarding attention deficit hyperactivity disorder (ADHD) in adults. Methods. An electronic literature search of peer-reviewed English language articles using MEDLINE (without time limits) was undertaken. Results. Symptoms of ADHD in adults exert a substantial negative impact on daily life, including work, social life and relationships. Co-morbidities are common, further impairing quality of life. Diagnosis of adult ADHD can be difficult, as current criteria require evidence of symptom onset before the age of 7 years and impact on activities typically undertaken by children. Drug therapy is the first-line treatment for adult ADHD, particularly stimulant medication. However, methylphenidate (MPH) immediate-release tablets require three or more times daily dosing, which can impact on compliance, while demonstrating a loss of symptomatic benefit later in the day. Extended-release preparations of MPH, mixed amphetamine salts and dexamphetamine can provide symptom control for 6\u201312 h and the non-stimulant atomoxetine has demonstrated benefit in reducing ADHD symptoms. These therapies are generally well tolerated, but may be associated with adverse effects on the cardiovascular system, which need to be further assessed in controlled clinical trials. Psychological therapy may be beneficial in adults who continue to experience clinically significant symptoms while receiving pharmacotherapy. Conclusion. Further research in all areas of adult ADHD is urgently needed.", "title": "Attention deficit hyperactivity disorder in adults", "claims": null}, {"metadata": {"year": 2002}, "authors": ["B. Chung", "A. R. Suzuki", "J. McGough"], "summary": "Attention-deficit/hyperactivity disorder (ADHD) is one of the longest recognised and most common neuropsychiatric disorders of childhood. Recent research indicates that ADHD is most often a lifelong condition associated with significant impairment in multiple domains of functioning. ADHD is a clinical diagnosis made on the basis of history and clinical examination. Current molecular, neuroimaging and neuropsychological studies have greatly elucidated our understanding of the basic science of ADHD. The underlying pathophysiology of ADHD has been theorised to be dysregulation of inhibitory noradrenergic frontocortical activity on dopaminergic striatal structures. Pharmacotherapy is recognised as the most effective component of ADHD treatment, although some role exists for proper educational placement, parent management training and social skills development. Methylphenidate and amphetamine are the current standards in ADHD medication treatment. Other medication classes such as tricyclic antidepressants and certain antihypertensives are also used in off-label therapy. Anticipated improvements in new ADHD medications include the development of extended release delivery systems, improved tolerability, alternative mechanisms of action and enhanced efficacy in treatment refractory cases.", "title": "New drugs for treatment of attention-deficit/hyperactivity disorder", "claims": null}, {"metadata": {"year": 2011}, "authors": ["S. McCarthy", "L. Wilton", "M. Murray", "P. Hodgkins", "P. Asherson", "I. Wong"], "summary": "EUNETHYDIS 2012 entitled: Sharpening the cutting edge of ADHD science and clinical practice", "title": "Persistence of pharmacological treatment into adulthood for ADHD patients who started treatment in childhood or adolescence in UK primary care", "claims": null}], "query": "recent advances and therapeutic approache to Adult ADHD", "summary_abstract": "Recent advances in the understanding and treatment of Adult Attention Deficit Hyperactivity Disorder (ADHD) have led to a broader array of therapeutic options. Research indicates that ADHD, often persisting from childhood into adulthood, significantly impacts various aspects of life, including work and social relationships (R\u00f6sler et al., 2010). \n\nPharmacotherapy remains a cornerstone of treatment, with stimulant medications such as methylphenidate and amphetamines recognized as first-line options due to their efficacy (Greydanus et al., 2009; Chung et al., 2002). However, the development of extended-release formulations has improved adherence by providing longer-lasting symptom control, addressing the issue of compliance associated with multiple daily doses of immediate-release medications (R\u00f6sler et al., 2010). Non-stimulant alternatives, such as atomoxetine and norepinephrine reuptake inhibitors, have also gained traction, particularly for patients who may not tolerate stimulants or prefer to avoid them (Pityaratstian, 2005; Antshel et al., 2011).\n\nIn addition to pharmacological treatments, behavioral interventions and a multidisciplinary approach are increasingly emphasized. Cognitive behavioral therapy has shown promise, especially for adults who experience residual symptoms despite medication (Antshel et al., 2011; Mannem et al., 2021). Furthermore, emerging therapies, including neurostimulation techniques and the exploration of druggable genomes, suggest a future direction for more personalized treatment strategies (Mannem et al., 2021).\n\nOverall, the landscape of adult ADHD treatment is evolving, with a focus on both pharmacological and non-pharmacological strategies to enhance patient outcomes and quality of life. Continued research is essential to refine these approaches and address the complexities of this chronic condition (R\u00f6sler et al., 2010; Antshel et al., 2011).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2009}, "authors": ["Jin-yan  Liu", "Xiu-xiang  Xiu", "Pei  Cai"], "summary": "Thiobacillus ferrooxidans, occurring extensively in mine districts, have important effects on the oxidation of metal sulphide and the formation of jarosite. In the coal bio-desulphurization, jarosite mediated by microorganisms decreases the bacterial absorbability of nutrients, occupies the available sites of coal surfaces, ultimately results in residual sulfur, which cannot be removed from coal. The thiobacillus ferrooxidans cultivation conditions with minimal jarosite in 9K medium by varying the initial pH, the initial Fe2+ concentration and the applied potential were studied and determined. Experimental results show that the optimal combination giving the minimal jarosite precipitates (3.73 g/L) and the good growing activity of bacteria is the initial pH of 1.6\u20131.7 with the Fe2+ concentration of 9\u201310 g/L and the applied potential of \u22120.5 V for 7 hours. The results will be of significant importance for the further research on the bacterial cultivation and coal desulphurization.", "title": "Study of formation of jarosite mediated by thiobacillus ferrooxidans in 9K medium", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Kaile Zhao", "G. Gu", "G. Qiu", "Xiao-hui Wang"], "summary": "Precipitation of jarosite is a very important phenomenon that is observed in the bioleaching of pyrrhotite by Acidthiobacillus ferrooxidans (A. ferrooxidans). Jarosite is a major secondary mineral formed in acid supergene environment by oxidation of metal sulphide. The formation of jarosite could decrease leached percentage. The Eh-pH diagram of FeS1.12-H2O showed that the thermodynamic parameters of the jarosite were exists steadily on Eh=360 ~800, pH=2.8~5, and the results of pH condition test is consonant with the conclusions of thermodynamic analysis. By means of XRD and SEM, it could indicate that full propagation of A. ferrooxidans in the solution is beneficial to the formation of jarosite and jarosite mediated by bacterial has a better crystalline form than that synthesized by chemical method. This study indicates that pH value and ferrous/ferric iron concentration are key factors affecting the formation of jarosite. Leached percentage is higher when frequency was set more than 2.0. It is crucial to minimize jarosite formation in order to increase factory's efficiency.", "title": "Study on the jarosite mediated by bioleaching of pyrrhotite using Acidthiobacillus ferrooxidans", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Behzad  Nazari", "E.  Jorjani", "H.  Hani", "Z.  Manafi", "A.  Riahi"], "summary": "Abstract The precipitation of jarosite adversely affects the bio-leaching of copper sulfides in the Sarcheshmeh heap bio-leaching process. The variables of the initial concentration of ferrous iron in the growth medium, pH, and temperature were examined in the laboratory to determine how they affect the precipitation of jarosite in the presence of Acidithiobacillus ferrooxidans bacteria. It was found that the maximum ferric precipitate occurred at a ferrous sulfate concentration of 50 g/L, a temperature of 32 \u00b0C, and an initial pH value of 2.2. The effects of the precipitation of ferric iron on the quantities of ions that are important for A. ferrooxidans bacteria in aqueous phase, i.e., ferric, sulfate, potassium, phosphate, and magnesium ions, also were assessed. The results showed relatively similar patterns for the ferric and potassium ions, and then reason might have been the co-precipitation of these ions as constituent elements of jarosite mineral. At pH values greater than 1.6, the solubility of phosphate ions decreased dramatically due to the co-precipitation of phosphate ions with the jarosite precipitate and due to the significant growth rate of A. ferrooxidans bacteria in this pH range. Due to the dissolution of a gangue constituent in the ore, the magnesium levels increased in the first few days of the bio-leaching process; thereafter, it decreased slightly.", "title": "Formation of jarosite and its effect on important ions for Acidithiobacillus ferrooxidans bacteria", "claims": null}, {"metadata": {"year": 2021}, "authors": ["W. Wenjing", "G. Kun", "Yeong-Hoon Han", "Guo Chuling", "Lu Guining", "Dang Zhi"], "summary": "Jarosite is a potential scavenger for the containments such as chromium ( Cr) in acid mine drainage ( AMD) environments. Microbial decomposition plays an important role in controlling the jarosite stability and Cr behavior. In this study, we investigated the formation of secondary mineral and the fate of Cr during reductive dissolution of Cr\u2043bearing jarosite by Shewanella oneidensis MR\u20431 (S. oneidensis MR\u20431) using a batch experiment under anaerobic conditions. Results indicated that S. oneidensis MR\u20431 can drive the reductive dissolution of jarosite. The incorporation of Cr increased the stability of jarosite, retarding the transformation of jarosite mineral. After 50 days inoculation, the jarosite with no Cr incorporation transformed to vivianite and goethite. However, vivianite was the only secondary mineral for the Cr\u2043bearing jarosite. During the reduction of Cr\u2043bearing jarosite by S. oneidensis MR\u20431, little Cr released from the Cr\u2043bearing jarosite to the solution initially, and then re\u2043adsorbed to the secondary mineral. The Cr in the fraction of crystalline iron oxide\u2043bound phase changed to amorphous iron oxide\u2043bound phase in biotic treatments, the likelihood of Cr release increased. XPS analysis indicated that the Cr(VI) in solid surface was obviously reduced Cr(III). The toxicity of Cr to surrounding environment decreased. The results of this study are of significant to further understand the geochemistry of Fe, S and Cr in AMD environments.", "title": "The microbial transformation of Cr \u2043 bearing jarosite by Shewanella oneidensis MR \u2043 1 and the associated Cr behavior", "claims": null}, {"metadata": {"year": 2008}, "authors": ["C. Weisener", "M. Babechuk", "B. Fryer", "C. Maunder"], "summary": "Iron sulfate minerals such as jarosite-group compounds (e.g., M Fe3(SO 4 ) 2 (OH) 6 ) can be of considerable environmental importance because of their ability to scavenge trace elements and thus contribute to some degree of metal cycling. Jarosite forms in low temperature hydrothermal, acidic, sulfate-rich environments often yielding a range of elemental substituted forms: plumbojarosite (M = Pb), argentojarosite (M = Ag), jarosite (M = K), natrojarosite (M = Na), hydroniumjarosite (M = H3O), and ammoniojarosite (M = NH4). Anthropogenic sources of jarosite are common in mine waste environments, most often associated with the waste products resulting from base metal recovery. Few studies have investigated the effect that dissimilatory metal reducing bacteria may have in the presence of these compounds following the onset of reducing conditions. Jarosite reactivity may differ systematically as a function of its chemical properties. For example, the incorporation of Ag in the mineral lattice may have inhibitory affects on the growth of microbial strains. In this study the reductive dissolution of argentojarosite (M = Ag) in the presence of Shewanella putrefaciens CN32 (10 9 cells ml 1 , pH 7.1) was examined. Using the silver (argento)jarosite, Ag Fe 3 (SO 4 ) 2 (OH) 6 , as a sole terminal electron acceptor we observed the reduction of structural Fe(III) and Ag(I) by CN32 through the release of Fe(II) ions to solution. Aqueous silver concentrations were below instrumental detection. Environmental SEM (ESEM) and TEM micrographs of the microbial clusters revealed the progressive heterogeneous nucleation of Ag(0) nanoparticles within cellular structures and also on adjacent mineral grains. The results of this study are the first presented for the anaerobic dissolution of silver jarosite. This has implications for understanding the processes leading to the mobility or retention of silver in mine waste and industrial landfill environments. It also provides insight into the microbial mechanisms of silver resistance and nanoparticle formation with potential applications for bioleaching and/or biotechnology.", "title": "Microbial Dissolution of Silver Jarosite: Examining Its Trace Metal Behaviour in Reduced Environments", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Ye Ya-ping"], "summary": "In accordance with the case that the bioleaching rate and the separation efficiency of copper ions are seriously influenced by the ferric precipitate because of the excessive concentration of iron ions during the copper bioleaching process in Zijin mine,the kinetics on formation of jarosite both in presence and absence of the leaching bacteria is systematically studied,and the variations of the jarosite quantity,pH,Eh and so on in the jarosite formation process are investigated. The regulation of the ferric precipitate in various conditions is also summarized. The results show that in presence of bacteria,the jarosite forming process is accelerated by bacteria. The production is ammonium jarosite in presence of bacteria,and that Fe(OH)3 colloid is forming in aseptic condition. pH value is one of important influence factor during jarosite forming while there are bacteria.", "title": "Comparative Study on Kinetics of Jarosite Formation in Presence of Bacteria and Aseptic Condition", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Ouyang  Bingjie", "Lu  Xiancai", "Liu  Huan", "Li  Juan", "Zhu  Tingting", "Zhu  Xiangyu", "Lu  Jianjun", "Wang  Rucheng"], "summary": "Abstract Jarosite is a common mineral in a variety of environments formed by the oxidation of iron sulfide normally accompanying with the generation of acid mine drainage (AMD) in mining areas or acid rock drainages (ARD) in many localities. Decomposition of jarosite by dissimilatory iron reducing bacteria (DIRB) influences the mobility of many heavy metals generally accommodated in natural jarosite. This study examined the anaerobic reduction of synthesized jarosite by Shewanella oneidensis strain MR-1, a typical facultative bacteria. The release of ferrous and ferric ion, as well as sulfate and potassium, in the inoculated experimental group lasting 80 days is much higher than that in abiotic control groups. The detection of bicarbonate and acetate in experimental solution further confirms the mechanism of microbial reduction of jarosite, in which lactate acts as the electron donor. The produced ferrous iron stimulates the subsequent secondary mineralization, leading to precipitation and transformation of various iron-containing minerals. Green rust and goethite are the intermediate minerals of the microbial reduction process under anoxic conditions, and the end products include magnetite and siderite. In aerobic environments, goethite, magnetite and siderite were also detected, but the contents were relatively lower. While in abiotic experiments, only goethite has been detected as a product. Thus, the microbial reduction and subsequent mineral transformation can remarkably influence the geochemical cycling of iron and sulfur in supergene environments, as well as the mobility of heavy metals commonly accommodated in jarosite.", "title": "Reduction of jarosite by Shewanella oneidensis MR-1 and secondary mineralization", "claims": null}, {"metadata": {"year": 2006}, "authors": ["K. Sasaki", "T. Sakimoto", "Mai Endo", "H. Konno"], "summary": "Morphological characterization of jarosite groups formed from Fe(III) biologically oxidized with different numbers of Acidithiobacillus ferrooxidans was conducted using FE-SEM. The higher population of A. ferrooxidans resulted in more distinct jarosite mineral shape, and stronger Raman intensities for potassium jarosite, ammoniojarosite and argentojarosite. The morphology of the jarosites might be dependent on iron-oxidizing activity of A. ferrooxidans. The technique was applied to identify jarosite compounds formed during microbially mediated dissolution of arsenopyrite by A. ferrooxidans. It is difficult to identify this jarosite compound by X-ray diffraction and Raman spectroscopy because amounts are typically low and the crystallization is poor in minerals formed by microbially mediated oxidation. However, FE-SEM image provided helpful information for identification of jarosite compounds. The results suggest that morphology would provide useful information for identification and history of jarosite minerals as geochemical samples.", "title": "FE-SEM Study of Microbially Formed Jarosites by Acidithiobacillus ferrooxidans", "claims": null}], "query": "Does microbial activity affect the surface properties of jarosite precipitate", "summary_abstract": "The collective findings from the reviewed papers indicate that microbial activity significantly influences the surface properties and formation of jarosite precipitates. \n\nLiu et al. (2009) highlight that the presence of *Thiobacillus ferrooxidans* affects the oxidation of metal sulfides and the formation of jarosite, which in turn impacts nutrient absorption by bacteria during coal bio-desulphurization. They found that specific cultivation conditions could minimize jarosite formation, suggesting that microbial activity can modulate the characteristics of jarosite precipitates.\n\nZhao et al. (2017) further elaborate on this by demonstrating that the bioleaching process involving *Acidithiobacillus ferrooxidans* leads to the formation of jarosite, which is thermodynamically stable under certain pH and iron concentration conditions. Their findings indicate that microbial activity not only promotes jarosite formation but also enhances its crystalline quality compared to chemically synthesized jarosite.\n\nNazari et al. (2014) examined how *Acidithiobacillus ferrooxidans* affects jarosite precipitation during copper bio-leaching, revealing that microbial presence alters the solubility of various ions, which are crucial for bacterial growth. This suggests that microbial activity can influence the chemical environment surrounding jarosite, thereby affecting its surface properties.\n\nWenjing et al. (2021) explored the role of *Shewanella oneidensis* in the reductive dissolution of Cr-bearing jarosite, showing that microbial processes can stabilize jarosite and affect its transformation into other minerals. This indicates that microbial activity can alter the stability and reactivity of jarosite precipitates.\n\nOuyang et al. (2014) also found that dissimilatory iron-reducing bacteria significantly enhance the release of ions from jarosite, leading to the formation of various iron-containing minerals. This microbial reduction process highlights the dynamic interplay between microbial activity and jarosite properties.\n\nIn summary, the research indicates that microbial activity not only facilitates the formation of jarosite but also influences its surface properties, stability, and mineral transformation. The presence of specific bacteria can enhance the crystallinity and alter the chemical environment, which in turn affects the behavior and characteristics of jarosite precipitates (Liu et al., 2009; Zhao et al., 2017; Nazari et al., 2014; Wenjing et al., 2021; Ouyang et al., 2014).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2003}, "authors": ["J. Lambsdorff"], "summary": "An increase in corruption by one point on a scale from 10 (highly clean) to 0 (highly corrupt) lowers productivity by 4 per cent of GDP and decreases net annual capital inflows by 0.5 per cent of GDP. Central to the impact of corruption on productivity is its adverse influence on bureaucratic quality. Crucial for corruption to deter net annual capital inflows is its association with a lacking tradition of law and order. Anticorruption reform should focus on public sector reform if priority is given to increasing productivity. Legal reform should be addressed if countries want to attract foreign capital.", "title": "How corruption affects economic development", "claims": null}, {"metadata": {"year": 1999}, "authors": ["S. Rose-Ackerman"], "summary": "High levels of corruption limit investment and growth and lead to ineffective government. Developing countries and those making a transition from socialism are particularly at risk, but corruption is a worldwide phenomenon. Corruption creates inefficiencies and inequities, but reforms are possible to reduce the material benefits from payoffs. Corruption is not just an economic problem, however; it is also intertwined with politics. Reform may require changes in both constitutional structure and the underlying relationship of the market and the state. No single \"blueprint\" is possible, but the primary goal should be to reduce the gains from paying and receiving bribes, not simply to remove \"bad apples.\"", "title": "Corruption and Government: Causes, Consequences, and Reform", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Quang Vinh Dang"], "summary": "Corruption has long been considered a national illness in Vietnam but progress in fighting corruption has been modest. In recent years, the Communist Party of Vietnam and the Government of Vietnam have strengthened their efforts to prevent and fight corruption. Despite strong anti-corruption measures being implemented at the national level, provincial authorities have shied away from tackling corruption. One of the reasons for this could be that it is not clear to provincial authorities if and how corruption is affecting local development economically and socially. This article demonstrates that corruption has a negative impact on private sector investment, employment and per capita income at the provincial level in Vietnam. However, corruption is found to have no significant impact on how income is distributed across provinces. The findings demonstrate that more effective anti-corruption measures are necessary to promote the private sector and improve household income. The study provides provincial leaders with empirical evidence and incentives for fighting corruption.", "title": "The impact of corruption on provincial development performance in Vietnam", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Nguyen Ngoc Anh", "N. Minh", "B. Tran-Nam"], "summary": "Despite the Government\u201fs recognition of the serious threat of uncontrolled corruption to the legitimacy and long-term survival of the current political system, Vietnam is still struggling to translate its policies and comparatively strong legislative framework into practice. There exists little reliable, quantitative evidence of the harmful impact of corruption on economic growth in Vietnam. Using the most updated and available data and a model incorporating transmission channels, this article attempts to estimate the direct and indirect effects of corruption on GDP growth rate. In general, the findings confirm the negative association between corruption and economic growth. Investment appears to be the most important transmission channel and the effect of corruption on investment is non-linear so that indirect effects of corruption on growth (via investments) depends on the value of each country\u201fs corruption level. In case of Vietnam, a one unit increase in the corruption perception index (CPI) 1 leads to a 2.15% increase in the proportion of gross domestic investment over GDP, which in turns increases growth rate by 0.372%. Counting both direct and indirect effects, a one-unit increase in the corruption perception index (CPI) will increase the growth rate by 0.509%, indicating that the investment channel accounts for 62.92% of total effects. ============================== This article is an edited version of a paper presented at the 2014 Vietnam Economist Annual Meeting (VEAM), Ho Chi Minh City, 24\uf02d25 June 2014 and the Costs of Corruption in Vietnam Research Symposium, Hanoi, 25\uf02d26 June 2014. The authors would like to thank the organizers and participants for their helpful comments. Financial support from the Department for International Development (DFID) Vietnam is also gratefully acknowledged. The usual caveat applies. ============================== Nguyen Ngoc Anh Development and Policies Research Center (DEPOCEN), Hanoi, Vietnam email: ngocanh@depocen.org 1 The higher the CPI index value, the less corrupt the country is perceived to be.", "title": "Corruption and economic growth (with Vietnam case-study)", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Lai Cao Mai Phuonga"], "summary": "Article history: Received: June 13 2020 Received in revised format: June 24 2020 Accepted: June 26 2020 Available online: June 26 2020 This paper investigates the effects of corruption and long-term investment of businesses in Vietnam using the General Least Square (GLS) estimation method for businesses in 63 provinces in Vietnam from 2016-2018. The results show that corruption was an important factor affecting the long-term investment decisions of Vietnamese enterprises. The ability to predict corruption of businesses can explain the phenomenon of part of the cash flow of businesses flowing out of production and business. Informal costs related to low-level administrative procedures act as \u201cgrease\u201d to help businesses reduce time costs, but when the total amount of unofficial expenses exceeds 10% of revenue of businesses, they become a burden for businesses and restrain them from making long-term investments. Corrupted public officials' behavior has led businesses to misallocate resources and prevent them from making long-term investments. The result shows that the East Asia paradox holds only for the case of informal costs related to administrative procedures in Vietnam. \u00a9 2020 by the authors; licensee Growing Science, Canada.", "title": "Corruption and long-term investment of businesses in Vietnam", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Omer  Gokcekus", "Justin  Myzie"], "summary": "Corruption undermines economic development and damages social stability.1 The current literature on corruption comprises of empirical cross-country analyses and surveys demonstrating perceived corruption\u2019s macrolevel harmful impact on economic, political, and social outcomes in a country.2 Corruption impedes income growth of the poor (Gupta and Alonse-Terme 2002; You and Khagram 2005), and corruption reduces productivity within a country (Lambsdoorf 2003b). Outside investors\u2019 perception of corruption may also discourage FDI and other capital inflows (Mauro 1997; Lambsdoorf 2003a; Wei 2000).3 The course of privatization in highly corrupt countries tends to be less efficient than in countries where corruption is controlled; the inefficiency fosters an environment conducive for monopolies (Bjorvatn and Soreide 2005). Researchers have also identified macrolevel characteristics of countries with corruption. Corruption is prevalent in countries that have a large public sector (Mauro 2002);4 have poor governance (Rose-Ackerman 2004); have a low level and a low quality of openness (Gokcekus and Knoerich 2006); and tend to lack political and civil rights (Harms and Ursprung, 2002). Countries with an unstable legal administration and unstable markets support the growth of corruption (Lambert-Mogiliansky 2002).", "title": "Why and How to Move from Capturing Perception of to Quantifying Corruption", "claims": null}, {"metadata": {"year": 2010}, "authors": ["T. Nguyen", "Mathijs A. Van Dijk"], "summary": "We provide a firm-level analysis of the relation between corruption and growth for private firms and state-owned enterprises (SOEs) in Vietnam. We obtain three different measures of the perceived corruption severity from a 2005 survey among 741 private firms and 133 SOEs. We find that corruption hampers the growth of Vietnam\u2019s private sector, but is not detrimental for growth in the state sector. We document significant differences in the corruption severity across 24 provinces in Vietnam that can be explained by the quality of provincial public governance (such as the costs of new business entry, land access, and private sector development policies). Our results suggest that corruption may harm economic growth because it favors the state sector at the expense of the private sector and that improving the quality of local public governance can help to mitigate corruption and stimulate economic growth.", "title": "Corruption, Growth, and Governance: Private vs. State-Owned Firms in Vietnam", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Daisuke Maruichi", "M. Ab\u00e9"], "summary": "This paper examines the effect of corruption on the business environment in Vietnam. Our survey of firms conducted in Vietnam suggests that corruption is perceived as the most impactful business obstacle for their operation. It was also found that corruption has a significant negative association with the overall satisfaction of the business environment in Vietnam, which supports the hypothesis that corruption has a \u201csand the wheel\u201d effect on firms' business activities. Given these results, it is urgent that the Vietnamese public authorities accelerate efforts in mitigating this issue. Although this paper sheds light on the importance of corruption, it would be useful to conduct follow\u2010up studies examining corruption and its impact in more detail. Such studies could be conducted in segments that most severely suffer from corruption according to our survey, that is, medium\u2010sized enterprises in the hotel/restaurant and construction sectors, Hanoi based, and Vietnamese owned firms.", "title": "Corruption and the Business Environment in Vietnam: Implications from An Empirical Study", "claims": null}], "query": "Corruption and lack of economic reforms threaten Vietnam's economy. Corruption in Vietnam's government and economy leads to inefficient allocation of resources and deters foreign investment (Mai 2013).", "summary_abstract": "The issue of corruption in Vietnam is a significant barrier to economic growth and foreign investment, as highlighted by various studies. Lambsdorff (2003) notes that even a slight increase in corruption can lead to a notable decrease in productivity and capital inflows, emphasizing the detrimental effects of corruption on bureaucratic quality and the legal environment necessary for attracting foreign investment. Similarly, Rose-Ackerman (1999) argues that high corruption levels create inefficiencies that hinder investment and growth, particularly in developing nations transitioning from socialism, like Vietnam.\n\nQuang Vinh Dang (2016) provides empirical evidence that corruption negatively impacts private sector investment, employment, and per capita income at the provincial level in Vietnam. This suggests that despite national anti-corruption efforts, local authorities may not fully grasp the economic implications of corruption, which further complicates the issue. Nguyen Ngoc Anh et al. (2017) reinforce this by demonstrating a clear negative correlation between corruption and GDP growth, with investment being the primary channel through which corruption affects economic performance.\n\nMoreover, Lai Cao Mai Phuong (2020) highlights that corruption significantly influences long-term investment decisions among Vietnamese businesses, indicating that informal costs associated with corruption can deter investment when they exceed a certain threshold. This aligns with findings from Gokcekus and Myzie (2008), who assert that corruption undermines economic development and discourages foreign direct investment (FDI).\n\nLastly, Nguyen and Van Dijk (2010) reveal that while corruption hampers the growth of the private sector, it does not have the same detrimental effect on state-owned enterprises, suggesting a skewed economic landscape that favors the state sector. This disparity underscores the need for improved public governance to mitigate corruption and stimulate economic growth.\n\nIn summary, the collective findings from these studies illustrate that corruption in Vietnam not only leads to inefficient resource allocation but also deters foreign investment, thereby threatening the overall economic stability and growth of the country. Effective anti-corruption measures and reforms in public governance are essential to address these challenges and promote a healthier economic environment.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2020}, "authors": ["Jin Dai", "Chong Chen"], "summary": "The text classification is an important research orientation in the fields of information retrieval and data mining, which has extensive applications in the practical work and scientific research and its research on the algorithm is always a hot topic. At present, the study on the long text like academic texts mainly focuses on abstract extraction. Whereas, due to the complicated content of the text format, there is very little classification research according to the text structure. In this study, the academic texts are classified based on Bidirectional Encoder Representations from Transformers and Bidirectional Gated Recurrent Unit (BERT-BiGRU) model.", "title": "Text classification system of academic papers based on hybrid Bert-BiGRU model", "claims": null}, {"metadata": {"year": 2012}, "authors": ["M. Timonen"], "summary": "Categorization of very short documents has become an important research topic in the field of text mining. Twitter status updates and market research data form an interesting corpus of documents that are in most cases less than 20 words long. Short documents have one major characteristic that differentiate them from traditional longer documents: each word occurs usually only once per document. This is called the TF=1 challenge. In this paper we conduct a comprehensive performance comparison of the current feature weighting and categorization approaches using corpora of very short documents. In addition, we propose a novel feature weighting approach called Fragment Length Weighted Category Distribution that takes the challenges of short documents into consideration. The proposed approach is based on previous work on Bi-Normal Separation and on short document categorization using a Naive Bayes classifier. We compare the performance of the proposed approach against several traditional approaches including Chi-Squared, Mutual Information, Term Frequency-Inverse Document Frequency and Residual Inverse Document Frequency. We also compare the performance of a Support Vector Machine classifier against other classification approaches such as k-Nearest Neighbors and Naive Bayes classifiers.", "title": "Categorization of Very Short Documents", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Ye Wang", "Zhi Zhou", "Shan Jin", "Debin Liu", "Mi Lu"], "summary": "Short text is considerably different from traditional long text documents due to its shortness and conciseness, which somehow hinders the applications of conventional machine learning and data mining algorithms in short text classification. According to traditional artificial intelligence methods, we divide short text classification into three steps, namely preprocessing, feature selection and classifier comparison. In this paper, we have illustrated step-by-step how we approach our goals. Specifically, in feature selection, we compared the performance and robustness of the four methods of one-hot encoding, tf-idf weighting, word2vec and paragraph2vec, and in the classification part, we deliberately chose and compared Naive Bayes, Logistic Regression, Support Vector Machine, K-nearest Neighbor and Decision Tree as our classifiers. Then, we compared and analysed the classifiers horizontally with each other and vertically with feature selections. Regarding the datasets, we crawled more than 400,000 short text files from Shanghai and Shenzhen Stock Exchanges and manually labeled them into two classes, the big and the small. There are eight labels in the big class, and 59 labels in the small class.", "title": "Comparisons and Selections of Features and Classifiers for Short Text Classification", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Pradnya Saval"], "summary": "The increase in the development of internet and social networks brings a lot of user created texts on the internet such as product review, ratings, comments, etc. To classify the semantics of such texts is of great research and practical value. The categorization of these texts is difficult because of the less contextual information they provide. The existing research on text categorization basically includes feature extraction and sentiment analysis. With the development of deep learning various deep learning models have achieved remarkable results. The author explains and compares the various deep learning models to give a basic guidance for deep neural network selection.", "title": "Text Categorization with Dynamic Input Length Using Deep Learning \u2013 A Review", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Bing  Liu", "Yong  Zhou", "Wei  Sun"], "summary": "Text categorization, or text classification, is one of key tasks for representing the semantic information of documents. Traditional deep leaning models for text categorization are generally time-consuming on large scale datasets due to slow convergence rate or heavily rely on the pre-trained word vectors. Motivated by fully convolutional networks in the field of image processing, we introduce fully convolutional layers to substantially reduce the number of parameters in the text classification model. A character-level model for short text classification, integrating convolutional neural network, bidirectional gated recurrent unit, highway network with the fully connected layers, is proposed to capture both the global and the local textual semantics at the fast convergence speed. Furthermore, In addition, error minimization extreme learning machine is incorporated into the proposed model to improve the classification accuracy further. Extensive experiments show that our approach achieves the state-of-the-art performance compared with the existing methods on the large scale text datasets.", "title": "Character-level text classification via convolutional neural network and gated recurrent unit", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Qingyuan Bie"], "summary": "Text classification, also known as text categorization, is a classical task in natural language processing. It aims to assign one or more predefined classes or categories to text documents. Text classification has a wide variety of application scenarios. Traditional binary or multiclass text classification problems have been intensively studied in machine learning research. However, when machine learning techniques applied to short texts, most of the standard text classification approaches have the problems such as data sparsity and insufficient text length. Moreover, due to the lack of contextual information, short texts are highly ambiguous. As a result, simple text classification approaches based on words only, can not represent the critical features of short texts properly. In this work, a novel neural network based approach of short text classification is proposed. We enrich text representation by utilizing words together with entities represented by the content of the given document. Tagme is first utilized to extract named entities from documents. We then utilize Wikipedia2Vec model to get entity vectors. Our model is trained not only on top of pre-trained word vectors, but also on top of pre-trained entity vectors. We compare the performance of the model using pure words and the model using words as well as entities. The impact of different kinds of word vectors on the classification accuracy is also studied. INHALTSVERZEICHNIS v", "title": "Words are not enough ! Short text classification using words as well as entities Masterarbeit von Qingyuan Bie Studiengang : Informatik Matrikelnummer", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Bing  Liu", "Yong  Zhou", "Wei  Sun"], "summary": "Text categorization, or text classification, is one of key tasks for representing the semantic information of documents. Traditional deep leaning models for text categorization are generally time-consuming with large-sized datasets due to slow convergence rate. In this paper, we propose a character-level model for short text classification with a combination of convolutional neural network (CNN), gated recurrent unit (GRU) and highway network (HN), which can capture both the global and the local textual semantics while having a tractable computational complexity. In addition, error minimization extreme learning machine (EM-ELM) is incorporated into the proposed model to improve the classification accuracy further. Extensive experiments show that our approach achieves the state-of-the-art performance when the hybrid model based on EM-ELM is trained using large-sized datasets.", "title": "Character-Level Hybrid Convolutional and Recurrent Neural Network for Fast Text Categorization", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Gabriela  Ram\u00edrez-de-la-Rosa", "Manuel  Montes-y-G\u00f3mez", "Thamar  Solorio", "Luis Villase\u00f1or Pineda"], "summary": "During the last decades the Web has become the greatest repository of digital information. In order to organize all this information, several text categorization methods have been developed, achieving accurate results in most cases and in very different domains. Due to the recent usage of Internet as communication media, short texts such as news, tweets, blogs, and product reviews are more common every day. In this context, there are two main challenges; on the one hand, the length of these documents is short, and therefore, the word frequencies are not informative enough, making text categorization even more difficult than usual. On the other hand, topics are changing constantly at a fast rate, causing the lack of adequate amounts of training data. In order to deal with these two problems we consider a text classification method that is supported on the idea that similar documents may belong to the same category. Mainly, we propose a neighborhood consensus classification method that classifies documents by considering their own information as well as information about the category assigned to other similar documents from the same target collection. In particular, the short texts we used in our evaluation are news titles with an average of 8 words. Experimental results are encouraging; they indicate that leveraging information from similar documents helped to improve classification accuracy and that the proposed method is especially useful when labeled training resources are limited.", "title": "A document is known by the company it keeps: neighborhood consensus for short text categorization", "claims": null}], "query": "long text classification", "summary_abstract": "The research question of long text classification has been addressed in various studies, particularly focusing on the challenges and methodologies specific to classifying lengthy documents. Dai and Chen (2020) highlight the significance of classifying academic texts, proposing a model that combines Bidirectional Encoder Representations from Transformers with a Bidirectional Gated Recurrent Unit (BERT-BiGRU) to effectively handle the complexities inherent in long text formats. Their work emphasizes the need for tailored approaches that consider the structural intricacies of academic writing.\n\nIn contrast, other studies have primarily concentrated on short text classification, which presents its own unique challenges, such as data sparsity and the TF=1 challenge (Timonen, 2012; Wang et al., 2017). These studies illustrate that while short texts are often easier to classify due to their brevity, they lack the contextual richness that longer texts provide, complicating the classification process. For instance, Wang et al. (2017) discuss various feature selection methods and classifiers, underscoring the importance of robust feature extraction techniques in achieving effective classification outcomes.\n\nMoreover, Liu et al. (2020) and Liu et al. (2018) propose innovative models that integrate convolutional neural networks and gated recurrent units to enhance classification accuracy and speed, although their focus remains on short texts. This indicates a broader trend in the literature where methodologies developed for short texts may not directly translate to long text classification due to the latter's complexity.\n\nOverall, while there is a growing body of research on long text classification, particularly in academic contexts, the methodologies and challenges differ significantly from those encountered in short text classification. The studies collectively suggest that specialized models and approaches are necessary to address the unique characteristics of long texts effectively (Dai & Chen, 2020).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2004}, "authors": ["Narinder Dhadwal"], "summary": "A handful of good introductions to ideas in quantum computing\u2014a new, multidisciplinary research area crossing quantum mechanics, theoretical computer science, and mathematics\u2014have appeared in the past few years. This introduction stands out, in being friendly and brief. It provides one of the first overviews of, and introductions to, this nonstandard form of computation from the mathematical and computer science viewpoint. The field of quantum computing promises to solve some complex problems through a massive use of parallelism\u2019s power, achieved via the properties of quantum physics\u2014in particular, the superposition of bits, known as quantum bits, or qubits. This kind of quantum computing model could give scientists enough power to make longer-range predictions about the weather, for example, or could enable them to decode and break any traditional code.", "title": "Book Reviews: Quantum Computing (Natural Computing Series), 2nd ed.", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Noson S. Yanofsky"], "summary": "Quantum Computing is a fascinating new field at the intersection of computer science, mathematics and physics. This field studies how to harness some of the strange aspects of quantum physics for use in computer science. Many of the texts to this field require knowledge of a large corpus of advanced mathematics or physics. We try to remedy this situation by presenting the basic ideas of quantum computing understandable to anyone who has had a course in pre-calculus or discrete structures. (A good course in linear algebra would help, but, the reader is reminded of many definitions in the footnotes.)", "title": "An Introduction to Quantum Computing", "claims": null}, {"metadata": {"year": 1999}, "authors": ["Tony (Anthony) John Grenville Hey"], "summary": "The basic ideas of quantum computation are introduced by a brief discussion of Bennett (1973, 1982) and Fredkin's (1982, 1997) ideas of reversible computation. After some remarks about Deutsch's (1985) pioneering work on quantum complexity and Shor's (1996) factorisation algorithm, quantum logic gates, qubits and registers are discussed. The role of quantum entanglement is stressed and Grover's (1997) quantum search algorithm described in detail. The paper ends with a review of the current experimental status of quantum computers.", "title": "Quantum computing: an introduction", "claims": null}, {"metadata": {"year": 2007}, "authors": [], "summary": "Quantum computing is one of the hottest topics of the last decade. The current emergent desire for a practical quantum computer is driving much of the current research in the fields of mathematics, physics and engineering. As a result, there has been an explosion in the number of books published on the subject. None of these, however, have quite matched up to this excellent text. As the title suggests, this book is an introduction to the field. An acquaintance with the principles of quantum mechanics is taken as read and a good grounding in the mathematics of linear algebra is expected. The first three chapters build up a framework of mathematical language and tools. There is also a liberal sprinkling of quantum physics. Chapter 4 launches into the quantum model of computation. It is here that quantum gates are first discussed in detail. Chapter 5 deals with the superdense coding and quantum teleportation protocols. The next three chapters cover quantum algorithms. The usual suspects such as Deutsch\u2019s algorithm, Simon\u2019s algorithm, Shor\u2019s algorithm and Grover\u2019s quantum search algorithm are all to be found lurking here. The final two chapters discuss quantum computational complexity theory and quantum error correction. The book concludes with a number of useful appendices and an extensive bibliography. The book is written in a clear concise style. The authors manage to condense all of the salient points into a minimum of space while still maintaining readability. It would be an ideal place to start for students embarking on postgraduate study or research in quantum computing. It would be equally useful as a source text for instructors and course developers. The authors should be complemented on this work. I highly recommend it.", "title": "Book reviews", "claims": null}, {"metadata": {"year": 2011}, "authors": ["E. Rieffel", "W. Polak"], "summary": "The combination of two of the twentieth centurys most influential and revolutionary scientific theories, information theory and quantum mechanics, gave rise to a radically new view of computing and information. Quantum information processing explores the implications of using quantum mechanics instead of classical mechanics to model information and its processing. Quantum computing is not about changing the physical substrate on which computation is done from classical to quantum but about changing the notion of computation itself, at the most basic level. The fundamental unit of computation is no longer the bit but the quantum bit or qubit. This comprehensive introduction to the field offers a thorough exposition of quantum computing and the underlying concepts of quantum physics, explaining all the relevant mathematics and offering numerous examples. With its careful development of concepts and thorough explanations, the book makes quantum computing accessible to students and professionals in mathematics, computer science, and engineering. A reader with no prior knowledge of quantum physics (but with sufficient knowledge of linear algebra) will be able to gain a fluent understanding by working through the book. The text covers the basic building blocks of quantum information processing, quantum bits and quantum gates, showing their relationship to the key quantum concepts of quantum measurement, quantum state transformation, and entanglement between quantum subsystems; it treats quantum algorithms, discussing notions of complexity and describing a number of simple algorithms as well as the most significant algorithms to date; and it explores entanglement and robust quantum computation, investigating such topics as quantifying entanglement, decoherence, quantum error correction, and fault tolerance.", "title": "Quantum Computing: A Gentle Introduction", "claims": null}, {"metadata": {"year": 2006}, "authors": ["M. Bellac", "G. Jaeger"], "summary": "Quantum information and computation is a rapidly expanding and cross-disciplinary subject. This book, first published in 2006, gives a self-contained introduction to the field for physicists, mathematicians and computer scientists who want to know more about this exciting subject. After a step-by-step introduction to the quantum bit (qubit) and its main properties, the author presents the necessary background in quantum mechanics. The core of the subject, quantum computation, is illustrated by a detailed treatment of three quantum algorithms: Deutsch, Grover and Shor. The final chapters are devoted to the physical implementation of quantum computers, including the most recent aspects, such as superconducting qubits and quantum dots, and to a short account of quantum information. Written at a level suitable for undergraduates in physical sciences, no previous knowledge of quantum mechanics is assumed, and only elementary notions of physics are required. The book includes many short exercises, with solutions available to instructors through solutions@cambridge.org.", "title": "A Short Introduction to Quantum Information and Quantum Computation", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Robert H. Ewald"], "summary": "This paper reviews the two major approaches to quantum computing and discusses early quantum applications.", "title": "An Introduction to Quantum Computing and Its Application", "claims": null}, {"metadata": {"year": 2022}, "authors": ["R. Hundt"], "summary": "This introduction to quantum computing from a classical programmer's perspective is meant for students and practitioners alike. Over 25 fundamental algorithms are explained with full mathematical derivations and classical code for simulation, using an open-source code base developed from the ground up in Python and C++. After presenting the basics of quantum computing, the author focuses on algorithms and the infrastructure to simulate them efficiently, beginning with quantum teleportation, superdense coding, and Deutsch-Jozsa. Coverage of advanced algorithms includes the quantum supremacy experiment, quantum Fourier transform, phase estimation, Shor's algorithm, Grover's algorithm with derivatives, quantum random walks, and the Solovay\u2013Kitaev algorithm for gate approximation. Quantum simulation is explored with the variational quantum eigensolver, quantum approximate optimization, and the Max-Cut and Subset-Sum algorithms. The book also discusses issues around programmer productivity, quantum noise, error correction, and challenges for quantum programming languages, compilers, and tools, with a final section on compiler techniques for transpilation.", "title": "Quantum Computing for Programmers", "claims": null}], "query": "Find a good introduction to quantum computing", "summary_abstract": "The exploration of quantum computing has garnered significant attention across various disciplines, and several papers provide valuable introductions to this complex field. Dhadwal (2004) emphasizes the interdisciplinary nature of quantum computing, highlighting its foundations in quantum mechanics, theoretical computer science, and mathematics. He notes that quantum computing leverages the unique properties of quantum physics, particularly the concept of qubits, to potentially solve complex problems more efficiently than classical computing.\n\nYanofsky (2011) aims to make the fundamental concepts of quantum computing accessible to a broader audience, requiring only a basic understanding of pre-calculus and discrete structures. This approach is echoed by Rieffel and Polak (2011), who present a comprehensive introduction that demystifies quantum information processing, focusing on the transition from classical bits to qubits and the implications of this shift for computation.\n\nHey (1999) provides a historical context by discussing key developments in quantum computation, including reversible computation and quantum algorithms like Shor's and Grover's. This historical perspective is complemented by Bellac and Jaeger (2006), who offer a self-contained introduction suitable for undergraduates, detailing essential quantum algorithms and the physical implementation of quantum computers.\n\nFor those with a programming background, Hundt (2022) presents an introduction that bridges classical programming with quantum computing, explaining over 25 fundamental algorithms and their simulations. This practical approach is beneficial for students and practitioners looking to understand the application of quantum algorithms in real-world scenarios.\n\nOverall, these papers collectively serve as excellent starting points for anyone interested in quantum computing, providing a mix of theoretical foundations, historical context, and practical applications.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2006}, "authors": ["B. Currie-Alder", "L. Thompson", "R. Bustamante"], "summary": "attention on water management issues focuses on crises of governance and the factors that endanger the water rights of marginalized people. We define effective water governance as the processes that encourage people to actively participate in designing, planning, managing and implementing water management activities while fostering communities ability to innovate and adapt to changing circumstances. In other words, water governance is as much about the art of social change as it is about the science of hydrology. To illustrate, this paper compares the experiences of two initiatives supported by the International Development Research Centre (IDRC): the Water Demand Management Initiative for the Middle East/North Africa (WaDImena) and the Social Water Vision in Latin America. Responding to the opportunities that exist for new research to inform policymaking, each project has conceptualized water governance in a manner relevant to that region. The two initiatives have regionally specific levels of focus and research priorities; but they are both using a water governance framework to achieve their objectives. Effective water governance must encourage participation in the processes for deciding how water is used; promote innovation and learning among stakeholders, and foster adaptation to changes in water availability. An effective water governance framework includes three elements: policies that enable participatory water management, capacity to engage in the policy process, and the ability to negotiate among stakeholders. Further research on water governance contributes to the emerging field of adaptive environmental governance and our understanding of how social change occurs. Ultimately, such knowledge empowers poor women and men to obtain and use water they need to survive, grow food and sustain livelihoods.", "title": "Insights on Water Governance: Research in the Middle East/North Africa and Latin America", "claims": null}, {"metadata": {"year": 2009}, "authors": ["A. Barreira"], "summary": "It has been recognized that the current water crisis is a crisis of water governance (Global Water Partnership 2000). Resolving water governance problems will lead to the achievement of sustainable water resources management and development. Water governance refers to the range of political, social, economic, and administrative systems that are in place to regulate the development and management of water resources and provision of water services at different levels of society (Global Water Partnership 2002). Governance issues have important implications for the management of water resources at all administrative levels \u2014 global, regional, national and local \u2014 and good governance is a prerequisite for the successful implementation of Integrated Water Resources Management (IWRM), according to the Global Water Partnership (GWP) (2002). Water laws provide the framework for water governance systems and are the pillar for achieving effective governance in a given country. The main principles for effective water governance are: openness and transparency, inclusion and communication, coherence and integration, equity and ethics. At the European Union (EU) level, the Water Framework Directive1 (WFD) provides the basic elements to contribute to effective water governance in European Union member states2. The main objective of this article is to present the water governance system of the European Union, based primarily on the European Community Water Framework Directive. Firstly, it reviews the European Union context, in particular European Community (EC) environmental law and policy. It follows with a brief analysis of the instruments shaping the European Community water law architecture. In particular, it will emphasize the Water Framework Directive that, since its entry into force on December 22, 2000, represents the primary water policy legislation in the European Union. Finally, some conclusions will be provided.", "title": "Water Governance at the European Union", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Mihaela Boran"], "summary": "Water governance refers to the political, social, economic and administrative systems in place that influence water\u2019s use and management. Essentially, who gets what water, when and how, and who has the right to water and related services, and their benefits. It determines the equity and efficiency in water resource and services allocation and distribution, and balances water use between socio-economic activities and ecosystems. Governing water includes the formulation, establishment and implementation of water policies, legislation and institutions, and clarification of the roles and responsibilities of government, civil society and the private sector in relation water resources and services. The outcomes depend on how the stakeholders act in relation to the rules and roles that have been taken or assigned to them. The water sector is a part of broader social, political and economic developments and is thus also affected by decisions by actors outside of the water sector. New forms of governance focusing on process-oriented societal co-steering through, for example, formal and informal networks, partnerships and dialogue, have emerged within the water sector. The governance transformation is intrinsically linked to the increasing focus on the complexity of water management and the multifunctional character of water and the search for alternative forms of organisation.", "title": "Water Governance and Its Effectiveness", "claims": null}, {"metadata": {"year": 2008}, "authors": ["E. Burleson"], "summary": "Reasonable and equitable water resource decision-making is at the core of good governance around the world. Sustained water collaboration is an antidote to foreign relations disintegration. Lack of water quality and quantity policies can lead to water insecurity for everyone, yet bureaucratic obstacles such as inertia and corruption must be averted in altering water governance schemes. There are multiple ways to lower transaction costs and strive for optimal water use. Several ingredients of good water governance include: (1) broad participation through the entire decision-making process; (2) transparent flow of information; (3) equitable opportunities to increase well-being; (4) accountability from governments, the private sector, and civil society; (5) coherency of water resource measures; (6) responsiveness to changing water conditions and societal factors; (7) integrative approach to water basin management; and (8) ethical principles that resonate with varying societies based upon inclusive dialogues. Reasonable and equitable use of transboundary water resources can help sustain local, regional, and international peace and security.", "title": "Water is Security", "claims": null}, {"metadata": {"year": 2007}, "authors": ["Alex  Simalabwi"], "summary": "At the Second World Water Forum in 2000, the problems relating to water around the world were described as a consequence of the lack of good governance in water. The Global Water Partnership defines water governance as the range of political, social, economic and administrative systems that are in place to develop and manage water resources, and delivery of water services, at different levels of society.", "title": "National Perspectives on Water Governance: Lessons from the IWRM Planning Process in Malawi and Zambia", "claims": null}, {"metadata": {"year": 2011}, "authors": ["R. Izquierdo"], "summary": "Good management of water resources goes beyond a mere political or socio-economic exercise. It is a responsibility on a global scale, which has to begin with appropriate actions at a local level. The addition of small local projects under unified planning, regulation and management criteria will determine an inflection point in sustainable management of water at a planetary level. It is like a chamber orchestra where every instrument plays its role in a certain location, but always under the unified coordination of a conductor who sets the \u201ctempo\u201d, the intensity, the rhythm and the idiosyncrasy of each individual action.", "title": "Water Governance in Aragon", "claims": null}, {"metadata": {"year": 2010}, "authors": ["N. Grigg"], "summary": "No one can dispute that water resources must be managed responsibly because they are essential to life and limited in their capacity to meet all human and environmental needs. To achieve responsible management requires good policy, capable organizations and workforces, adequate resources, and regulation. This set of controls is referred to as water governance, which is required for all water management sectors with the goal of water stewardship by empowering and regulating sec- tors such as drinking water and sanitation. The paper will explain water governance and offer a framework for it that is based on the premise that water stewardship requires an ef- fective combination of management and governance. It will give examples, link the suc- cess of governance to sustainability in water resources management, and outline the major issues faced in different scenarios around the world. The paper draws from the first vol- ume of a new book series being launched by IWA Publishing with the author as the series editor.", "title": "Water governance: toward a model to address global water needs", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Shengqing Xu", "Jun Ou"], "summary": "Water resources are of great importance for the sustainable development of the Arid and Semi-arid Areas of Northwest China. The theory of good water governance could provide inspirations for properly dealing with the water challenges of these areas. The integrated water resources management, the environmental water flow protection and the public participation are three major requirements of the good water governance. There are three major challenges for promoting the good water governance. The first is related to the clarification of the responsibilities of the water governance. The second is about balancing the diversified water needs. The third is about the optimal approach for improving the public participation. Three policy choices are proposed for promoting the good water governance. The integration of the water governance system, which emphasizes the four principles and the role of the basin management, could be a significant way of clarifying the responsibilities. Improving the water use efficiency could help the water supply for the ecological environment. The capacity building should be enhanced for facilitating the public participation.", "title": "Good Water Governance for the Sustainable Development of the Arid and Semi-arid Areas of Northwest China", "claims": null}], "query": "what is good water governance?", "summary_abstract": "Good water governance is increasingly recognized as a critical factor in addressing global water challenges. It encompasses a range of political, social, economic, and administrative systems that regulate the development and management of water resources and services (Barreira, 2009; Izquierdo, 2011). Effective water governance is characterized by several key principles, including broad participation in decision-making, transparency, accountability, and equity (Burleson, 2008; Boran, 2023). \n\nCurrie-Alder et al. (2006) emphasize that effective governance should not only facilitate participation but also promote innovation and adaptability among stakeholders. This participatory approach is essential for empowering marginalized communities and ensuring that their water rights are protected. The authors argue that good governance is as much about fostering social change as it is about managing hydrological systems.\n\nMoreover, the Global Water Partnership (GWP) highlights that good governance is a prerequisite for successful Integrated Water Resources Management (IWRM), which aims to balance water use among various socio-economic activities and ecosystems (Barreira, 2009). This involves establishing clear roles and responsibilities among government, civil society, and the private sector, as well as ensuring that water policies are coherent and integrated (Boran, 2023).\n\nChallenges to good water governance include bureaucratic inertia, corruption, and the need for effective public participation (Burleson, 2008; Xu & Ou, 2018). Addressing these challenges requires a commitment to ethical principles and inclusive dialogue, which can help build trust and cooperation among stakeholders (Burleson, 2008). \n\nIn summary, good water governance is defined by its ability to engage diverse stakeholders, promote transparency and accountability, and adapt to changing conditions, all while ensuring equitable access to water resources. This multifaceted approach is essential for achieving sustainable water management and addressing the pressing water crises faced globally (Boran, 2023; Currie-Alder et al., 2006).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2022}, "authors": ["A. Gupta"], "summary": "- Introduction of DevOps into the software development life cycle represents a cultural shift in the IT culture, amalgamating development and operations to improve delivery speed in a rapid and maintainable manner. At the same time, security threats and breaches are expected to grow as more enterprises move to new agile frameworks for rapid product delivery. Meanwhile, DevSecOps is a mindset change that revolutionizes software development by embedding security at each step of the software cycle, leading to resilient software. This paper discusses a framework organization can use to embed DevSecOps swiftly and efficiently into the general IT culture.", "title": "An Integrated Framework for DevSecOps Adoption", "claims": null}, {"metadata": {"year": 2021}, "authors": ["S.  Kavitha", "J. V. Anchitaalagammai", "S.  Nirmala", "S.  Murali"], "summary": "The chapter summarizes the concepts and challenges of DevOps in IoT, DevSecOps in IoT, integrating security into IoT, machine learning and AI in IoT of software engineering practices. DevOps is a software engineering culture and practice that aims at unifying software development (Dev) and software operation (Ops). The main characteristic of DevOps is the automation and monitoring at all steps of software construction, from integration, testing, releasing to deployment and infrastructure management. DevSecOps is a practice of integrating security into every aspect of an application lifecycle from design to development.", "title": "Current Trends in Integrating the Internet of Things Into Software Engineering Practices", "claims": null}, {"metadata": {"year": 2021}, "authors": ["S. Kavitha", "J. V. Anchitaalagammai", "S. Nirmala", "S. Murali"], "summary": "The chapter summarizes the concepts and challenges of DevOps in IoT, DevSecOps in IoT, integrating security into IoT, machine learning and AI in IoT of software engineering practices. DevOps is a software engineering culture and practice that aims at unifying software development (Dev) and software operation (Ops). The main characteristic of DevOps is the automation and monitoring at all steps of software construction, from integration, testing, releasing to deployment and infrastructure management. DevSecOps is a practice of integrating security into every aspect of an application lifecycle from design to development.", "title": "Current Trends in Integrating the Internet of Things Into Software Engineering Practices", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Muhamad Efendi", "T. Raharjo", "Agus Suhanto"], "summary": "Application development has progressed along with the rapid digital transformation. The stakeholders will focus more on cutting operational costs with optimal application quality and security. The DevSecOps approach provides solutions for reducing costs in the software life cycle, increasing software product quality and security. Public Company Logistic Agency (PCLA) is like other companies that must be adaptive to new technology. PCLA has an IT Division in charge of providing technology to support business. Application development by the IT Division has several problems, including application projects that mostly exceed the time, significant changes and additions during UAT, and applications that have much vulnerability. A transformation to the DevSecOps approach is needed to address these problems. In this paper, a Systematic Literature Review (SLR) was used to select journals that matched the study topic to obtain the character and transformation phases of DevSecOps in various case studies. A mixed-method approach aims to collect and analyze data companies in the software development lifecycle. For academicians, this study provides a new understanding of application development in a state-owned enterprise based on a sequential approach and their suitable solutions from the DevSecOps approach. For practitioners, the findings provide potential lessons learned and guide a state-owned enterprise to transform to the DevSecOps approach.", "title": "DevSecOps Approach in Software Development Case Study: Public Company Logistic Agency", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Tao Chen", "Haiyan Suo"], "summary": "DevSecOps is the abbreviation of \u201cdevelopment, security and operation and maintenance\u201d[1]. It automatically integrates security at each stage of the software development life cycle, including initial design, integration, testing, deployment and software delivery. DevSecOps can seamlessly integrate system security into the DevOps process, which makes application and infrastructure security the common responsibility of development, security and it operations teams. This study designed an integrated operation and maintenance platform of DevSecOps, which was of great significance to improve the efficiency of enterprise research and development and realize the automation of security protection.", "title": "Design and Practice of Security Architecture via DevSecOps Technology", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Kriti Mittal", "Maryada Sharma", "Manvi Gupta", "Kavita Sheoran"], "summary": "DevSecOps is a combination of development, security, and operations that ensure security integrated with DevOps. There have been many new advances in IT that make it easier to include the DevOps methodology into application design, but these innovations come with a downside. As a result, many compliance monitoring tools have not kept pace with the new developments. Threat problems manifest after app release, where the public as well as the client is affected and the company\u2019s reputation takes a hit. Continuous Integration and continuous deployment do not embrace security checks which impose threat to application. So, there is a need to convert CI/CD (Continuous Integration/Continuous Deployment) to CI/CD/CS (Continuous Integration/Continuous Deployment/Continuous Security).This can be implemented by integrating the AppSec pipeline to DevOps pipeline. Introducing security after Integration Testing in DevOps pipeline provides the facility to detect any vulnerability existing in the product.", "title": "DevSecOps: A Boon to the IT Industry", "claims": null}, {"metadata": {"year": 2013}, "authors": [], "summary": "8 For more information Market shifts are fundamentally changing the way businesses are approaching software. In a new study from the IBM Institute for Business Value (IBV), more than 400 business executives identified five top software trends that will impact the competitiveness of their enterprises in the next five years: 1) proliferation of mobile devices, 2) explosion of unstructured data, 3) need to collaborate across value chains, 4) cloud platforms, and 5) intelligent/connected devices. Other trends include:", "title": "DevOps : The IBM approach Continuous delivery of software-driven innovation", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Vishnu S. Pendyala"], "summary": "Software engineering operations in large organizations are primarily comprised of integrating code from multiple branches, building, testing the build, and releasing it. Agile and related methodologies accelerated the software development activities. Realizing the importance of the development and operations teams working closely with each other, the set of practices that automated the engineering processes of software development evolved into DevOps, signifying the close collaboration of both development and operations teams. With the advent of cloud computing and the opening up of firewalls, the security aspects of software started moving into the applications leading to DevSecOps. This chapter traces the journey of the software engineering operations over the last two to three decades, highlighting the tools and techniques used in the process.", "title": "Evolution of Integration, Build, Test, and Release Engineering Into DevOps and to DevSecOps", "claims": null}], "query": "Emerging trends in Enterprise DevSecOps", "summary_abstract": "The emerging trends in Enterprise DevSecOps reflect a significant shift towards integrating security throughout the software development lifecycle, driven by the need for rapid delivery and heightened security awareness. Gupta (2022) emphasizes that DevSecOps represents a cultural transformation within IT, embedding security at every stage of software development to create resilient applications. This integration is crucial as enterprises increasingly adopt agile frameworks, which can expose them to greater security threats.\n\nKavitha et al. (2021) further elaborate on the challenges and concepts of DevSecOps, particularly in the context of the Internet of Things (IoT). They highlight the importance of automating security measures across all phases of application development, from design to deployment, ensuring that security is not an afterthought but a fundamental component of the process.\n\nThe need for cost-effective solutions in application development is also addressed by Efendi et al. (2021), who argue that adopting a DevSecOps approach can enhance software quality and security while reducing lifecycle costs. Their systematic literature review provides insights into the transformation phases necessary for implementing DevSecOps in various organizational contexts.\n\nChen and Suo (2022) contribute to this discourse by presenting a model for integrating security into the DevOps process, which they argue is essential for improving research and development efficiency. They propose that making security a shared responsibility among development, security, and operations teams is vital for effective risk management.\n\nMoreover, Mittal et al. (2021) point out the necessity of evolving Continuous Integration/Continuous Deployment (CI/CD) practices to include Continuous Security (CS). They advocate for integrating security checks earlier in the development pipeline to mitigate vulnerabilities before application release.\n\nOverall, the literature indicates a clear trend towards a more holistic approach to software development that prioritizes security alongside speed and efficiency. This shift is not only a response to the increasing complexity of software environments but also a proactive measure to safeguard organizational integrity in a rapidly evolving digital landscape.", "summary_extract": null}, {"papers": [{"metadata": {"year": 1999}, "authors": ["P. Bailey"], "summary": "The lower middle class has long had a bad press, for in common with other subaltern groups it has been more represented from without than within. Thus Victorian writers faced with the disquieting irruption of a new breed of petty bourgeois shop and office workers devised a parodic discourse of littleness, whose feminized tropes rendered the clerk as socially insignificant as the sequestered Victorian woman. George Grossmith's comic classic, Diary of a Nobody, pilloried the new social type in Mr. Pooter, whose smaller-than-life adventures stood for all that was ineffectual, pretentious, and banal in his class. Social commentators held the lower middle class responsible for the degeneration of civilization itself, stifled by their suburban respectability and addiction to mass culture. In Howard's End, E. M. Forster drew the clerk, Leonard Bast, with some sympathy but made him the book's major casualty, while belittling a class whose education was learned \u201cfrom the outside of books.\u201d In the interwar years the Marxist poet Christopher Caudwell likened the petty bourgeois world to \u201ca terrible stagnant marsh, all mud and bitterness, without even the saving grace of tragedy.\u201d George Orwell's fictional antihero from the same period, the insurance salesman George Bowling, characterizes the men of his class as \u201cTories, yes-men and bumsuckers.\u201d It is still hard to hide a certain relish in repeating such charges, for putting the boot in on the lower middle class has long been the intellectual's blood sport, an exorcism, so we are told, of the guilty secret so many of us share as closet petit bourgeois denying our own class origins.", "title": "White Collars, Gray Lives? The Lower Middle Class Revisited", "claims": null}, {"metadata": {"year": 2000}, "authors": ["Rita Felski"], "summary": "In contemporary literary and cultural studies, little attention has been paid to the lower middle class, described by one scholar as \u201cthe social class with the lowest reputation in the entire history of class theory.\u201d This article discusses the representation of the lower middle class in literature and scholarly writing. George Orwell's novels of the 1930s and Hanif Kureishi's The Buddha of Suburbia offer some illuminating perspectives on the British lower middle class, though Orwell's novels also reveal a conspicuous disdain for their subject. This disdain is echoed in much of the scholarly writing on the lower middle class. Decried for its reactionary attitudes by Marxists, the \u201cpetite bourgeoisie\u201d also poses problems for a contemporary cultural politics based on the idealization of transgression and on the romance of marginality. Rather than embody an outmoded or anachronistic class formation, however, the lower middle class may offer an important key to the contemporary meaning of class.", "title": "Nothing to Declare: Identity, Shame, and the Lower Middle Class", "claims": null}, {"metadata": {"year": 2002}, "authors": ["Jacqueline R. Devries"], "summary": "WINTER 2002 In Young\u2019s argument, women were unable to free themselves from stereotypes of femininity; similarly, men (like Bennett and Wells) were unable to free themselves from stereotypes of the lower middle class. However, May Sinclair in The Divine Fire (1904) \u201creturn[s] the favor\u201d male writers had done women with her \u201cpositive portrayals of lowermiddle-class men\u201d (156). Sinclair\u2019s central character, Savage Keith Rickman, is both \u201cthe lower-middle-class clerk and the smouldering genius\u201d (164). Self-taught, fluent in classical Greek, he is a poet and \u201ctrue hero\u201d because with \u201cthe transforming influence of a socially superior woman\u201d (191), he, by the novel\u2019s end, \u201cconfront[s] and overmaster[s] the middle-class gentleman on his own turf\u201d (158). Despite social barriers, the upper-class woman and the lower middle class man recognize their love and their mutual exclusion to emerge \u201cas classless and iconoclastic modern individuals\u201d (188). Young concludes: the novel \u201chas opened in the decorous world of the nineteenth-century novel [. . .] and has ended in the passionate turmoil of early twentieth-century modernism\u201d (186). Young\u2019s developmental story tracking representations of the lower middle class may speak less convincingly about the history of the novel than she suggests. For Young, modernism delivers the novel from Victorian class conventions to represent \u201cinner worth\u201d apart from \u201cclass stereotypes.\u201d This, however, may seem an oddly generous version of the decided elitism of modernism. From a quite different sense of literary and cultural history, John Carey (among many others), with a firm grounding in turn of the century education, and conditions of work, leisure, and income, observes in The Intellectuals and the Masses (1992) that modernism was adept at appropriating the lower middle class in fiction, while \u201cexclud[ing] the masses from culture\u201d by \u201cmaking it too difficult for them to understand\u201d (16). In Young\u2019s version, neither Bennett nor Wells adequately represents the complexities of lower middle class subjectivity; but we are to understand that T. S. Eliot, Ezra Pound, James Joyce, Virginia Woolf, or Wyndham Lewis following (or not) Sinclair\u2019s lead do? Further, readers may not share Young\u2019s view of Sinclair\u2019s romanticized Oxbridge ideals as the standard of worth. Accepting this standard, however, may account for the regrettable under-reading here of both Wells and Bennett. In her focus on the lower middle class, Young has selected a worthy subject\u2014 one that is increasingly interesting a number of scholars. In this, they will find her bibliography, especially of relevant periodical articles throughout the century, useful. Julie English Early University of Alabama in Huntsville", "title": "The March of the Women: A Revisionist Analysis of the Campaign for Women's Suffrage, 1866-1914 (review)", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Jane Marcus"], "summary": "When Virginia Woolf used the word \u201cmiddlebrow,\u201d she was describing the editors and readers of vogue, where she was allowed to write what she wanted, and poking fun at her Bloomsbury friends, who would never let their names appear in such a place.1 Of course, by publishing Woolf, vogue immediately ascended out of the middlebrow. \u201cHighbrow\u201d journals like the times literary Supplement refused to publish what Woolf wrote about Henry James, and she dismissed such snobbery for the sake of freedom. The word \u201cmiddlebrow\u201d has a far different valence in the U.S. today, implying comfort and consensus. Laurel Thatcher Ulrich\u2019s book jumps from the academic highbrow world where she earned the Pulitzer Prize and the Bancroft Prize for a midwife\u2019s tale: the life of martha Ballard, Based on her diary, 1785-1812 to the middlebrow world of a kind of populist feminism.2 A chaired professor at Harvard and a Phi Beta Kappa Scholar, Ulrich has spent her career writing as a well-behaved woman about well-behaved women whose voices had been lost to history. Now she has written a book for middlebrow readers that might be called \u201cFeminism Without Tears.\u201d I imagine her heroines\u2014Christine de Pisan, Elizabeth Cady Stanton, and Virginia Woolf\u2014trying to wriggle out of Ulrich\u2019s firm grasp, refusing to stay in the box. \u201cBut, you may say,\u201d\u2014the famous opening words of a room of one\u2019s own, words that invite the reader to object or argue with the speaker\u2014are words that come to my lips throughout this book.3 Certainly the feminism invoked in the potted biographies of her three feminist saints, all from secondary sources, cannot compare intellectually to her prize-winning original research in a midwife\u2019s tale, says my scholarly self. Surely, contradicts my feminist conscience, the broad cultural effect of such an appeal to the unconverted is as important as documenting the lives of obscure white women. What we have here is a positive narrative of (certain) women\u2019s achievements embedded in a feminist history so soft that even Lynne Cheney might be delighted with it. The complicated, equivocal tone of the title\u2014 Well-Behaved Women Seldom make history\u2014with its mild nonthreatening manner and the unsettling use of the word \u201cseldom\u201d is a clue to the", "title": "Middlebrow Feminism", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Simon Grennan", "R. Sabin", "Julian Waite"], "summary": "If it is true that \u2018reading is a transaction, a relation between the cultural vocabulary of the text, and the cultural vocabulary of the reader\u2019, to paraphrase Catherine Belsey (2002: 31), then any discussion of the nineteenth century has to take into account that both vocabularies shift over time. The following few paragraphs are not intended as explanations of terms or of problems with historicisation, but function to illuminate areas where misunderstandings may arise, specifically to do with Duval\u2019s main preoccupations (as we have identified them, such as class, gender and comedy) and her employment (as an actor and artist). There is an added layer of complexity due to the fact that the book is interdisciplinary (incorporating periodical studies, Victorian studies, theatre studies, Comics Studies, drawing studies, comedy studies, media studies, cultural studies, and women\u2019s studies) because not all these disciplines and fields agree on definitions. First is the issue of class. This book has refrained from defining \u2018working class\u2019 and \u2018middle class\u2019.1 The categories are contentious and constructed differently at different times (Munt 2000). For example, in the Duval epoch (late 1860s to mid-1880s) most \u2018working-class\u2019 people were rural and labour was agricultural: the great migration to the cities was still ongoing. Further, the \u2018lower-middle class\u2019 was coming into being as a distinct group, entailing more than just creating a space away from the crowding, smog and disease of the inner city, and initiating a lifestyle. Further, we can say that wealth (capital) is not a reliable guide to class stratification. For example, membership of the middle and working classes was not entirely based on earnings: as historian Liza Picard points out \u2018a skilled London coach-maker could earn up to five guineas [...] a week \u2013 considerably more than most middle class clerks\u2019 (Picard 2009). However, education and tenure (a bank clerk\u2019s job was for life) defined the middle classes, and social mobility was usually seen as both possible and desirable.2 This meant that poverty conversely was often seen as a matter of personal fault, with the poor as feckless and therefore responsible for their own plight, which in turn justified the workhouse", "title": "Questions of terminology and historicisation", "claims": null}, {"metadata": {"year": 1988}, "authors": ["Penny A. Russell"], "summary": "Although I have been working on the 'upper class' of Victoria for the past three years, I have for a long time felt very apologetic about using that particular expression. I have usually qualified it by asserting that I am only describing certain people as 'upper-class' until I have ascertained more exactly where they fit into class structure and, indeed, class theory. Throughout the period of my research into ladies of the upper class of Melbourne between 1860 and 1880, 1 have been hopefully looking out for a word which would describe them without locking me willy nilly into the language of some class theory whose implications I would prefer to avoid. Ruling-class women, bourgeois women, elite women, high-status women - all were labels which I dodged for this reason, as well as because they were either inaccurate or incomplete as descriptions of the group I have been focusing on. Other looser descriptive terms - such as the 'conservative' classes or the. 'wealthy' classes - were also inadequate, for they imply a homogeneity of experience in the group which my research has shown to be quite inaccurate. There were numbers of undeniably 'upper-class' women without a penny to theft names, and there were many families which, despite a well-established social position, espoused liberal rather than conservative political views.", "title": "The relationship between family life and class formation in nineteenth century Melbourne", "claims": null}, {"metadata": {"year": 1999}, "authors": ["Arlene  Young"], "summary": "The gentleman, as Daniel Defoe recognized, is a cultural construction, the \u2018exalted Creature of our own forming\u2019. Fictional characters are no less cultural constructions, shaped as much by the values and assumptions of the society for which they are created as by the author who creates them. The cultural values of intensely class-conscious nineteenth-century Britain accordingly dictated that fictional characters must have a class identity, and indeed, a character\u2019s class is arguably a more fundamental personal attribute than gender in the novels of this period. Consider, for example, the androgyny of many omniscient narrators, especially George Eliot\u2019s, for while narrators, strictly speaking, are not characters, they are nevertheless constructed fictional identities, albeit often incomplete ones. The narrator of Middlemarch, while dis-embodied, has so distinct a personality that one critic has been prompted to attribute the pervasive wisdom and discernment of the narrative voice to middle-aged sagacity rather than omniscience. More fundamental, but perhaps less obvious, is that narrator\u2019s middle-class sensibility", "title": "The Literary Evolution of the Lower Middle Class: The Natural History of the Gent to Little Dorrit", "claims": null}, {"metadata": {"year": 2013}, "authors": ["S. Lyall"], "summary": "Sara Lyall, \u2018\u2018Multiplying the Old Divisions of Class in Britain\u2019\u2019, New York Times, 3 April 2013 (summarized): Class in Britain used to be a relatively simple matter, or at least it used to be treated that way. It came in three flavors \u2013 upper, middle and working \u2013 and people supposedly knew by some mysterious native sixth sense exactly where they stood. As the very tall John Cleese declared to the less-tall Ronnie Corbett in the famous 1966 satirical television sketch meant to illustrate class attitudes in Britain \u2013 or, possibly, attitudes toward class attitudes \u2013 \u2018\u2018I look down on him, because I am upper class.\u2019\u2019 It is not as easy as all that, obviously. The 2010 election was enlivened at one point by a perfectly serious discussion of whether David Cameron, now the prime minister, counted as upper upper-middle class, or lower upper-middle class. But on Wednesday, along came the BBC, muddying the waters with a whole new set of definitions. Having commissioned what it called \u2018\u2018The Great British Class Survey\u2019\u2019, an online questionnaire filled out by more than 161,000 people, the BBC concluded that in today\u2019s complicated world, there are now seven different social classes. (\u2018\u2018As if three weren\u2019t annoying enough,\u2019\u2019 a woman named Laura Phelps said on Twitter.) These range from the \u2018\u2018elite\u2019\u2019 at the top, distinguished by money, connections and rarefied cultural interests, to the \u2018\u2018precariat\u2019\u2019 at the bottom, characterized by lack of money, lack of connections and unrarefied cultural interests. That might sound kind of familiar, but Fiona Devine, a sociologist who helped devise the study, said, \u2018\u2018It\u2019s what\u2019s in the middle which is really interesting and exciting.\u2019\u2019 The middle categories, as the study defines them, include the \u2018\u2018technical middle class,\u2019\u2019 a group that has a lot of money but few superior social connections or cultural activity; the \u2018\u2018emergent service workers,\u2019\u2019 a young, urban group that has little money but a high amount of social and cultural capital; and the \u2018\u2018new affluent workers,\u2019\u2019 who score high on social and cultural activity, but have only a middling amount of money.", "title": "The UK Seven Social Classes", "claims": null}], "query": "\"virginia woolf\" \"lower middle class\"", "summary_abstract": "The exploration of Virginia Woolf's relationship with the lower middle class reveals a complex interplay of class representation in literature, particularly in the context of early 20th-century British society. Woolf's work, alongside that of her contemporaries, often reflects a nuanced understanding of class dynamics, particularly the lower middle class, which has historically been marginalized in literary discourse.\n\nP. Bailey (1999) discusses the negative portrayal of the lower middle class in Victorian literature, where characters like Mr. Pooter in George Grossmith's *Diary of a Nobody* epitomize the ineffectuality and pretentiousness attributed to this social group. This disdain is echoed in the works of George Orwell, who, while providing insight into the lower middle class, also expresses a notable contempt for their values and lifestyles (Felski, 2000). Such representations contribute to a broader cultural narrative that often vilifies the lower middle class, framing them as stagnant and reactionary.\n\nHowever, the narrative shifts with the work of May Sinclair, who, as noted by Jacqueline R. Devries (2002), offers a more positive portrayal of lower middle-class men, suggesting that characters like Savage Keith Rickman can transcend class stereotypes through personal merit and intellectual achievement. This perspective aligns with Woolf's own literary ambitions, as she sought to challenge the rigid class structures of her time, advocating for a recognition of individual worth beyond social standing.\n\nWoolf's engagement with the concept of \"middlebrow\" culture, as discussed by Jane Marcus (2008), further complicates her relationship with class. By writing for a publication like *Vogue*, she navigated the tensions between highbrow and middlebrow literary spaces, reflecting her awareness of class distinctions and her desire to reach a broader audience. This engagement suggests that Woolf was not only aware of her own class position but also sought to critique and redefine the boundaries of literary culture.\n\nIn summary, the collective insights from these papers illustrate that Virginia Woolf's work engages deeply with the complexities of class, particularly the lower middle class. While often subjected to ridicule and disdain in contemporary literature, figures from this class also emerge as capable of significant personal and intellectual achievements, challenging the stereotypes that have historically defined them. Woolf's contributions to this discourse highlight her role in reshaping the narrative around class and its representation in literature.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2019}, "authors": ["Natalie Elizabeth Anderson", "Julia  Slark", "Kate  Faasse", "Merryn  Gott"], "summary": "BACKGROUND\nAround the world, many paramedics are authorised to withhold or terminate resuscitation. Research indicates this can be a challenging part of their role. Little is known about graduating paramedic student confidence, concerns and learning in this area.\n\n\nMETHODS\nAn online cross-sectional survey of students nearing completion of a paramedic degree in New Zealand, including piloting of a newly-developed confidence scale.\n\n\nRESULTS\nSeventy-two participants reported varying exposure to termination of resuscitation and patient death. Participants felt most confident providing technical procedurally-based care and least confident with non-technical skills. Participants' greatest concerns included making 'the right call', supporting grieving family, controlling emotions and encountering conflict. Clinical exposure with supportive mentoring, clinical simulation, peer reflection and resolved personal experience with death, were considered most useful for professional development.\n\n\nCONCLUSIONS\nExposure to termination of resuscitation and management of the scene of a patient death is variable amongst graduating paramedics. Novice paramedics may benefit from opportunities to observe and rehearse non-technical skills including delivering death notification and communicating with bystanders and bereaved family. The Confidence with Resuscitation Decision-Making and Patient Death Scale has favourable psychometric properties and utility as an outcome measure for future research in this area.", "title": "Paramedic student confidence, concerns, learning and experience with resuscitation decision-making and patient death: A pilot survey.", "claims": null}, {"metadata": {"year": 2017}, "authors": ["L. Holmes", "Russell Jones", "R. Brightwell", "L. Cohen"], "summary": "Introduction This study explores the preparedness of undergraduate student paramedics for the mental health challenges of the paramedic profession from the perspective of course coordinators and their students. Methods Two surveys were developed and administered to course coordinators and students of the 16 undergraduate degree paramedicine courses across Australia and New Zealand. Sixteen course coordinators and 302 students responded. Results Results illustrate there was widespread recognition for the need to include preparation for the mental health challenges of the profession within undergraduate courses. Furthermore, most course coordinators and students had a preference for this topic to be taught using multiple teaching modes with particular preference for teaching the topic via discussion and activity based education. Teaching the topic as a standalone unit was supported by more than a third of course coordinators (43%) and a third of students (32%). Conclusion Six themes were identified as positive by anticipants: caring for people, high acuity work, diversity of work and patients, making a difference to patients and their families, using clinical skills and knowledge and engaging with the community. Students were most confident about communicating with patients and using clinical skills and knowledge. Students were least confident about clinical decision making and the most commonly cited fear was making a clinical mistake. A significant proportion of students (16%) feared for their personal mental wellbeing and 14% reported they were least confident about personal mental health within the profession.", "title": "Student Paramedic Anticipation, Confidence and Fears: Do Undergraduate Courses Prepare Student Paramedics for the Mental Health Challenges of the Profession?", "claims": null}, {"metadata": {"year": 2017}, "authors": ["James Thompson", "D. Houston", "Kathryn Dansie"], "summary": "Introduction The ability to self-assess is essential to the practitioner who often works independently, and reflective practice is entrenched within the paramedic process of care. In order to develop these practices a paramedic student must be able to self-identify mistakes and learn from their errors. However, student assessment has traditionally focused heavily on outcomes, with errors being penalised. Justification for traditional approaches to assessment of paramedic students acknowledges the potentially catastrophic consequences associated with mistakes being repeated in the real world out-of-hospital setting. Responding to the challenge of balancing the reflective practice skills set with \u2018real world\u2019 implications of a paramedic's mistakes, an assessment process was re-designed. The \u2018Student-Tutor Consensus Assessment\u2019 (STCA) was created to rebalance assessment weighting from being exclusively outcomes focussed, and encourage students to apply similar critical lens to events as the paramedics who are assessing them. Parallel tutor and student self-assessments are applied to simulated scenarios, with scores awarded to criteria where consensus has been reached between student and tutor judgements. Methods Final year undergraduate Bachelor of Paramedic Science students enrolled in a capstone topic were invited to complete a paper-based questionnaire at the end of their studies. Questions sought student perceptions about the STCA features and effectiveness. Results There was a 95% response rate (n=90). Responses to the six different questions showed a range of 85.6\u201395.6% broad agreement regarding the value, effectiveness and suitability of the method. Conclusion The pilot STCA approach proved highly successful, with student endorsement for the continued and expanded application of this teaching approach.", "title": "Teaching Students to Think like a Paramedic: Improving Professional Judgement through Assessment Conversations", "claims": null}, {"metadata": {"year": 1994}, "authors": ["M. Kane", "K. Parahoo"], "summary": "As part of a nursing degree course, one of the authors (M.K.) gathered data on the knowledge and use of lifting techniques among a group of undergraduate student nurses. A 17-item questionnaire was administered to one cohort of 19 third-year students. The results show that while they were aware of lifts recommended by the RCN and by other researchers, they reported that they frequently used lifts not recommended by the above sources during their medical placement. This behaviour seemed to mirror the practices of ward staff. Possible explanations for the knowledge-practice gap are offered.", "title": "Knowledge and use of lifting techniques among a group of undergraduate student nurses.", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Linda Ross", "M. Boyle", "B. Williams", "Chris Fielder", "Rebecca Veenstra"], "summary": "Introduction Interpersonal communication skills are essential to the healthcare practitioner aiding in high quality, effective and safe clinical practice. Effective communication exerts a positive influence on the patient's physical and emotional status resulting in better patient outcomes and satisfaction. By identifying strengths and weaknesses, self-assessment of interpersonal communication skills can be used as an intervention tool to inform future curriculum renewal. The objective of this study was to identify paramedic students\u2019 perceptions of their interpersonal communication competence. Methods Second year paramedic students from Monash University (Victoria) were invited to participate in a survey that asked them to record perceptions of their interpersonal communication skills using the Interpersonal Communication Competence Scale (ICCS). The ICCS is a 30-item unipolar questionnaire using a Likert scale ranging from 1 (almost never) to 5 (almost always). Mean and standard deviations (SD) were used to report results. Results Fifty-six second year paramedic students participated in the study. Participants were predominantly aged less than 25 years (85.7%) and male n=36 (64.3%). Students reported \u2018often\u2019 or \u2018almost always\u2019 for the items: \u2018I put myself in others\u2019 shoes\u2019, n=46 (82%), mean=3.98 (SD 0.59); and, \u2018I let others know that I understand what they say\u2019, n=45 (80%), mean=3.96 (SD 0.66). Students reported \u2018sometimes\u2019, \u2018often\u2019 or \u2018almost always\u2019, for the items: \u2018I have trouble convincing others to do what I want them to do\u2019, n=55 (98%), mean=3.5 (SD 0.63); and, \u2018My mind wanders during conversations\u2019, n=41 (73%), mean=3.05 (SD 0.88). Conclusion Preliminary results suggest that student paramedics self-report their interpersonal communication skills highly apart from areas related to assertiveness and listening skills. These results could be indicative of student age, personality or experience level and warrant further research with larger sample sizes.", "title": "Perceptions of Student Paramedic Interpersonal Communication Competence: A Cross-Sectional Study", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Michael Bowen", "Brett Williams"], "summary": "Department of Community Emergency Health and Paramedic Practice, Monash University, Melbourne, Australia Introduction: Professionalism is an essential behavior for paramedic students to demonstrate. In the United States, paramedic accreditation standards require educators to evaluate and document summative affective evaluation on each paramedic student before graduation. The 2009Emergency Medical Services Education Standards identified the affective behaviors as one of the three learning domains and published a grading tool to help educators recognize professional behaviors. However, little attentionwas given to the validity or reliability of this tool. Therefore, the aim of this studywas to evaluate the psychometric properties of the 5-point Paramedic Affective Domain Tool. Methods: This was a retrospective study with educators that completed evaluations on paramedic students from May 2013 to January 2017. A total of 707 cases met inclusion criteria and 131 unique evaluators from 27 different paramedic programs. A Rasch Partial Credit Model was used to analyze the data. Results: Almost 97% of the paramedic students received passing scores and 28.1% (n=199) received perfect scores. Only 3.5% (n=25) failed the evaluation. Scores ranged from 11 to 55 (M = 46, SD = 9.02) and \u03b1 = 0.97. Evidence suggests that the tool is not valid and the clustering of scores suggests minimal information can be gleaned from the results. Conclusion: Serious consideration should be made in the continued use of this tool and future research should focus on developing a new tool that is both valid and reliable.", "title": "AMEP_A_225818 91..98", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Y. Freund", "F. Duchateau", "E. Baker", "H. Goulet", "S. Carreira", "M. Schmidt", "B. Riou", "J. Rouby", "A. Duguet"], "summary": "Aim Before implementing new workshops and teaching in our faculty for performing basic life support (BLS), we aimed to determine the level of self-confidence of medical students with regard to the management of cardiac arrest (CA). Methods We conducted a preinterventional study. A questionnaire was sent to third-year to sixth-year medical students. We recorded sex, year of training, and personal witnessing of CA. We asked them about their theoretical knowledge on 10 main items of BLS and their self-perception of qualification to conduct a CA situation. We tested the respective influence of sex, year of training, and personal witnessing of CA. Results In total, 592 (37%) students completed the questionnaire, 42% of them were men. Less than a third of the students (30%) thought of themselves as being sufficiently qualified to conduct BLS. After the third year, the level of study did not influence their theoretical knowledge or their self-perception of qualification. Male sex and the number of CAs witnessed were the only factors positively associated with better self-confidence regarding qualification. Conclusion Self-perception of qualification in BLS is poor in our faculty. In our study, personal witnessing of CA greatly influenced confidence, whereas level of study did not.", "title": "Self-perception of knowledge and confidence in performing basic life support among medical students", "claims": null}, {"metadata": {"year": 2013}, "authors": ["B. Williams", "M. Boyle", "T. Earl"], "summary": "Abstract Introduction Paramedics rely on establishing a health provider-patient relationship with patients that promotes two-way communication, patient satisfaction, and facilitates appropriate patient assessment and treatment. Paramedics also must have an ability to empathize with patients and their family members in order to develop a successful health provider-patient relationship. The objective of this study was to assess paramedics\u2019 empathy and attitudes toward patients with specific conditions. Methods This was a cross-sectional study using a convenience sample of first-, second-, and third-year, Australian undergraduate paramedic students. Student empathy levels were assessed using two standardized self-reporting instruments: the Jefferson Scale of Physician Empathy (JSPE) Health Professional (HP) version and the Medical Condition Regard Scale (MCRS). Results A total of 94 paramedic students participated in the study. The JSPE demonstrated that male paramedic students had higher mean empathy scores than did female paramedic students (113.25 and 107.5, respectively; P = .042). The JSPE empathy level scores were lowest among first-year paramedic students (mean = 107.53); age was not found to be a significant variable on empathy scores. The Medical Condition Regard Scale revealed lowest scores in compassion towards substance abuse (mean = 46.42). Conclusions The results of this study provide the discipline of paramedic health care with useful data, and provide students, academics, and other educators with important information regarding the improvement of the health provider-patient relationship and paramedic education curriculum development. WilliamsB, BoyleM, EarlT. Measurement of empathy levels in undergraduate paramedic students. Prehosp Disaster Med. 2013;28(2):1-5.", "title": "Measurement of Empathy Levels in Undergraduate Paramedic Students", "claims": null}], "query": "How confidence do student paramedics feel when performing a lifting assessment?", "summary_abstract": "The research question regarding the confidence levels of student paramedics when performing a lifting assessment is addressed indirectly through various studies that explore related aspects of paramedic training and self-perception of skills.\n\nAnderson et al. (2019) highlight that graduating paramedic students exhibit varying levels of confidence, particularly feeling more assured in technical skills compared to non-technical skills. This suggests that while they may be competent in procedural tasks, their confidence in assessments that require critical thinking and interpersonal communication, such as lifting assessments, may be lower.\n\nHolmes et al. (2017) further emphasize that students often feel least confident about clinical decision-making, which could extend to their ability to assess lifting techniques effectively. The fear of making clinical mistakes was a common concern among students, indicating a potential lack of confidence in their practical skills, including lifting assessments.\n\nMoreover, Kane and Parahoo (1994) found that nursing students, who share similar training environments with paramedics, reported a gap between their knowledge of recommended lifting techniques and their actual practices during placements. This gap may reflect a broader issue in paramedic education regarding the application of learned skills in real-world scenarios.\n\nIn summary, while the specific confidence levels of student paramedics in lifting assessments are not directly measured, the findings from these studies suggest that there is a general trend of lower confidence in non-technical skills and clinical decision-making, which likely includes lifting assessments. The need for improved training and practical experience in these areas is evident, as highlighted by the concerns raised by students about their preparedness and the application of their skills in practice (Anderson et al., 2019; Holmes et al., 2017; Kane & Parahoo, 1994).", "summary_extract": null}, {"papers": [{"metadata": {"year": 1977}, "authors": ["L. Hoffman", "D. Amsterdam", "S. Brooks", "L. Schneck"], "summary": "Abstract\u2014 A study was undertaken of the glycosphingolipids in cell cultures derived from cerebellum of Tay\u2010Sachs disease fetal brain in order to determine the suitability of such cell strains as a model for Tay\u2010Sachs disease. The glycosphingolipids in the Tay\u2010Sachs disease cultured cerebellar cells were compared with those found in normal cultured cerebellar cells, normal and Tay\u2010Sachs cultured lung cells, and normal and Tay\u2010Sachs fetal brain. The glycolipids were separated by TLC, then analyzed by GLC of the trimethylsilyi derivatives of the methylglycosides of the sugar moieties. In the cultured cerebellar lines, the predominant gangliosides were GM2, GM3, and GD3. There was a 4\u2010fold increase of GM2 in the Tay\u2010Sachs as compared with the normal line. Only GM3 and GD3 gangliosides were found in the Tay\u2010Sachs and the normal fetal lung cell cultures. The major neutral glycosphingolipids in all of the cultured cells which were analyzed were glucosylceramide, lactosylceramide, digalactosyl\u2010glucosylceramide, and globoside. When the Tay\u2010Sachs cerebellar cells were labelled with [1\u201014C]gluco\u2010samine, some radioactivity was observed in the trihexosylceramide band, indicating the presence of a small amount of a galactosamine\u2010containing trihexosylceramide which may be asialo\u2010GM2 (GA2). The trihexosylceramide in Tay\u2010Sachs fetal brain was identified as GA2 by GLC. Both Tay\u2010Sachs and normal fetal brain gangliosides were more complex than those found in the cultured cells. Long chain fatty acids (C24:0 and C24;1) predominated in all of the glycosphingolipids of the Tay\u2010Sachs and the normal cultured cerebellar cells. In contrast, the glycosphingolipids of Tay\u2010Sachs and normal fetal brain contained mainly the shorter chain fatty acids (C16:0, C18:0, and C18:1). The cerebrosides in both the Tay\u2010Sachs and normal fetal brains were mainly glucosylceramide with only small amounts of the galactosylceramide which predominates in infant brain. Cultured cells from the fetal Tay\u2010Sachs disease", "title": "GLYCOSPHINGOLIPIDS IN FETAL TAY\u2010SACHS DISEASE BRAIN AND LUNG CULTURES", "claims": null}, {"metadata": {"year": 1963}, "authors": ["E. Berman"], "summary": "A NUMBER of glycolipids (in addition to cerebrosides) containing sphingosine, fatty acids and sugars (glucose, galactose and/or hexosamine) have been isolated from spleen and erythrocyte stroma of various species (KLENK and DEBUCH, 1959). Although lipids of this composition have not been identified with certainty in brain tissue, there is nevertheless some indication that they might be present in small amounts (WEISS, 1956; KLENK et al., 1957; SVENNERHOLM and RAAL, 1961). Two such glycolipids have now been isolated from brain tissue of patients with Infantile Amaurotic Familial Idiocy (Tay-Sach's disease). The main biochemical abnormality which characterizes this disease is an accumulation of gangliosides (KLENK, 1939); the two new compounds which have now been identified in brain tissue of Tay-Sachs' disease belong to the same class of lipids. The isolation and chemical composition of these two glycolipids are described in the present communication. Some of this work has appeared in preliminary reports (GATT and BERMAN, 1961 ; BERMAN and GATT, 1962).", "title": "STUDIES ON BRAIN LIPIDS IN TAY\u2010SACHS\u2019DISEASE\u2014I ISOLATION OF TWO SIALIC ACID\u2010FREE GLYCOLIPIDS *", "claims": null}, {"metadata": {"year": 1969}, "authors": ["K.  Sandhoff"], "summary": "Tay-Sachs disease is characterized by an accumulation of two types of glycosphingolipids, on the one hand ganglioside GM? (GalNAc-&1,4-(NeuNAc-&2,3) -Gal+1,4-Glc_13-1,1{2-N-acyl) sphingosine), and on the other its asialo residue (GalNAc-/3-1,4-Gal_P1,4GlcQ1,1(2-N-acyl)sphingosine) in nerve tissue [ l-31 . The disease is assumed to involve a lack of the enzyme which catabolizes the stored substances, similar to that present in other sphingolipidoses [4,5]. In a special case of Tay-Sachs disease with visceral storage of kidney globoside (GalNAc-P_1,3-Gal-/3-1,4-Gal-&l,4 GlcQ-1 ,l (2-N-acyl)sphingosine) a general lack of /3-Nacetylhexosaminidase activity was found [6] (fig. Id). The stored glycosphingolipids (ganglioside GM, and its asialo residue in nerve tissue, and kidney globoside in the visceral organs) in this case had in common a terminal /3-glycosidic-bound N-acetylgalactosamine, being partially hydrolized by PN-acetylhexosaminidase-preparations [6,7]. The same substances also accumulate in the various tissues of conventional Tay-Sachs cases, although the storage level of the asialo residue of ganglioside GM, and of the kidney globoside is found to be lower [6]. Accordingly, the lack of some particular /3-N-acetylhexosaminidase was discussed as one of the possible causes for storage in cases of conventional Tay-Sachs disease [6]. In the present study the f3-N-acetylhexosaminidase pattern of 4 cases of conventional Tay-Sachs disease is des cribed. In three of the four cases a lack of the PN-ace-", "title": "Variation of \u03b2\u2010N\u2010acetylhexosaminidase\u2010pattern in Tay\u2010Sachs disease", "claims": null}, {"metadata": {"year": 1963}, "authors": ["S. Gatt", "E. Berman"], "summary": "METABOLIC pathways of brain lipids in Tay-Sachs' disease are virtually unknown since a direct approach to this problem is not technically feasible. Almost all of the work to date in this and other laboratories has been directed toward chemical analyses of brain tissue received at autopsy. Such an approach has yielded valuable information, for example, the fact that gangliosides accumulate in quantities 6-10 times greater than in normal brain tissue (KLENK, 1939). Moreover, the gangliosides found in brain tissue of Tay-Sachs' disease differ chemically and/or structurally from those in normal human brain (ROSENBERG and CHARGAFF, 1959; SVENNERHOLM and RAAL, 1961). More recently, two sialic acid-free glycolipids have been isolated from brain tissue of Tay-Sachs' disease; compounds similar to these could not be detected in normal human brain (GATT and BERMAN, 1962). In order to gain some understanding of the dynamic aspects of lipid metabolism in the intact brain in this disease, tritiated water was administered to a patient in the terminal stages. Brain was received at autopsy and the radioactivity of the various lipid fractions measured. The results of these studies are described in the present communication. EXPERIMENTAL The patient with Tay-Sachs' disease was a 22 month-old male of Jewish origin. At the time of these studies, he had recovered from measles and was suffering from pneumonia; his weight was 5.8 kg. During the week prior to death. tritiated water was given by intravenous injection as follows: on the 1st day, 500 mc and on the 2nd, 3rd, 5th and 7th days, 100 mc each day. Death due to pneumonia occurred on the 8th day, autopsy was performed immediately and the brain, weighing 1020 g, was frozen within 3 hr and kept at temperatures ranging from -20\" to -40\" until used.", "title": "STUDIES ON BRAIN LIPIDS IN TAY\u2010SACHS' DISEASE\u2014III", "claims": null}, {"metadata": {"year": 1986}, "authors": ["Subhash  Basu", "Manju  Basu", "Tripti  De", "John W. Kyle", "Kamal K. Das", "Robert J. Schaeper"], "summary": "Since the discovery of GM2 ganglioside (GalNAcs l-4(NeuAc\u03b12\u20133)Galsl\u00ad4Glc-Cer) from Tay-Sachs brains by Klenk,l at least 50 different structures of sialic acid- and fucose-containing glycosphingolipids(GSLs)2\u20137have been described. There is increasing evidence that GSLs are involved in the interaction of cholera toxin,8 plant lectins,9,10 interferon,11 siga toxin,12 and polypeptide hormones13 with receptor sites at the surfaces of plasma membranes.", "title": "Biosynthesis of Gangliosides and Blood Group Glycolipids Using Solubilized Glycosyltransferases", "claims": null}, {"metadata": {"year": 1991}, "authors": ["H  Murakami", "Z  Lam", "B C Furie", "V N Reinhold", "T  Asano", "B  Furie"], "summary": "The human monoclonal autoantibody HF2-1/17, produced by a human-human hybridoma derived from lymphocytes of a lupus patient with thrombocytopenia, reacts with single stranded DNA and platelets. To determine the chemical nature of the autoantigen against which this antibody is directed on platelets, this platelet antigen was purified by the lipid extraction of sonicated platelets, DEAE-Sephadex chromatography, and high performance liquid chromatography. The purified glycolipids, a trace component in platelets, demonstrated high reactivity with the HF2-1/17 antibody using a competition enzyme-linked immunosorbent assay system or immunostaining of thin layer chromatograms. The purified glycolipids co-migrated with bovine sulfatides by thin layer chromatography. The purified glycolipids contain sulfate and galactose but not sialic acid or phosphate. Fast atom bombardment-mass spectrometry revealed these sulfatides to be sulfated monohexyl ceramides. The dominant species has a molecular weight of 794 while a minor form has a molecular weight of 812 due to an extra hydroxyl group and loss of a double bond. These results indicate that the platelet autoantigen against which the human monoclonal anti-DNA antibody is directed represents a family of novel monogalactosyl sulfatides.", "title": "Sulfated glycolipids are the platelet autoantigens for human platelet-binding monoclonal anti-DNA autoantibodies.", "claims": null}, {"metadata": {"year": 1976}, "authors": ["L  Schneck", "L M Hoffman", "D  Amsterdam", "S  Brooks", "B  Pinkett"], "summary": "Tay-Sachs disease (TSD) is a fatal, genetically determined disorder of sphingoglycolipid metabolism, associated with the absence of the lysosomal enzyme B-D-N-acetylhexosaminidase A (Hex A) (1). There is a massive accumulation of GM2 ganglioside, and its asialo derivative GA2 in cells of the central nervous system. A cell culture which reproduces these biochemical parameters would permit one to measure the effect of enzyme replacement therapy under controlled conditions that are not easily attainable in vivo. Although skin fibroblasts cultured from TSD patients lack Hex A, these cultured cells do not accumulate GM2 ganglioside (2). Since TSD is a neuronal lipid storage disease, and since fetal TSD brain has the characteristic glycolipid patterns found in infant TSD brain, a cell strain from fetal TSD cerebellum was established and the glycolipid patterns were evaluated by TLC and GLC (3). The cells were labelled with 14C-glucosamine in order to compare the metabolic activity of the gangliosides over an extended time period. The cells were also transformed with the oncogenic DNA SV-40 virus (4) since we wished to establish a permanent cell line for the study of this disease.", "title": "Glycolipids in cultured fetal Tay-Sachs disease cerebellar cells.", "claims": null}, {"metadata": {"year": 1979}, "authors": ["M. Basu", "K. A. Presper", "S. Basu", "L. Hoffman", "S. Brooks"], "summary": "Four different glycolipid:glycosyltransferase activities involved in the biosynthesis in vitro of gangliosides and blood group-related glycosphingolipids have been tested in a simian virus 40-transformed glial cell culture derived from the cerebrum of a fetus with Tay-Sachs disease (TSD). The TSD cultured brain cells contained little activity of either UDP-Gal:GM2(beta 1-3)galactosyltransferase (GalT-3; EC 2.4.1.62), which catalyzes the formation of GM1a from GM2 (tay-Sachs) ganglioside, or GDP-Fuc:nLcOse4Cer (alpha 1-2)fucosyltransferase (FucT-2; EC 2.4.1.89), which catalyzes the formation of H1 glycolipid from nLcOse4Cer. These cells contained a potent inhibitor of the second reaction (catalyzed by a Golgi-rich membrane fraction from bovine spleen), whereas no inhibition of the first reaction (catalyzed by a membrane fraction from 14-day-old embryonic chicken brain) was observed. The activity of UDP-Gal:LcOse3Cer(beta 1-4)galactosyltransferase (GalT-4; EC 2.4.1.86) was 30- to 80-fold higher than the activity of GalT-3. The presence of CMP-AcNeu:nLcOse4Cer sialyltransferase activity and the absence of either GalT-3 or FucT-2 suggested a probable pathway for the synthesis of sialylneolactotetraosylceramide [GM1b(GlcNAc)] in addition to a specific blockage of GM1a ganglioside synthesis from GM2 in these TSD transformed cells.", "title": "Differential activities of glycolipid glycosyltransferases in Tay-Sachs disease: studies in cultured cells from cerebrum.", "claims": null}], "query": "tay-sachs and sulfated glycolipid", "summary_abstract": "The research question regarding Tay-Sachs disease and sulfated glycolipids reveals significant insights into the biochemical abnormalities associated with this genetic disorder. Tay-Sachs disease is characterized by the accumulation of specific glycosphingolipids, particularly gangliosides such as GM2 and its asialo derivative GA2, due to a deficiency in the enzyme \u03b2-N-acetylhexosaminidase A (Hex A) (Hoffman et al., 1977; Schneck et al., 1976; Sandhoff, 1969). \n\nStudies have shown that in Tay-Sachs disease, there is a notable increase in GM2 ganglioside levels, which can be up to six to ten times higher than in normal brain tissue (Gatt & Berman, 1963). Additionally, the presence of other glycolipids, including two newly identified compounds in the brain tissue of Tay-Sachs patients, suggests a complex alteration in lipid metabolism (Berman, 1963). \n\nThe research also indicates that the glycolipid profiles in Tay-Sachs disease differ significantly from those in normal brains, with the accumulation of more complex gangliosides in the fetal brain compared to cultured cells (Hoffman et al., 1977). Furthermore, the metabolic pathways involving these glycolipids remain poorly understood, but the identification of specific glycosyltransferase activities in cultured cells derived from Tay-Sachs patients provides a glimpse into the disrupted biosynthetic processes (Basu et al., 1979).\n\nWhile the focus on sulfated glycolipids is less pronounced in the context of Tay-Sachs, the broader implications of glycolipid metabolism in this disease highlight the intricate relationship between glycosphingolipids and neurological function. The findings collectively underscore the importance of understanding these lipid alterations for potential therapeutic approaches, including enzyme replacement therapy (Schneck et al., 1976). \n\nIn summary, Tay-Sachs disease is marked by the accumulation of specific glycosphingolipids, particularly GM2 and GA2, due to enzymatic deficiencies, leading to significant biochemical and metabolic disruptions in the brain (Hoffman et al., 1977; Sandhoff, 1969; Gatt & Berman, 1963).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2016}, "authors": ["L. Dany", "Laure Moreau", "Cl\u00e9mentine Guillet", "Carmelo Franchina"], "summary": "Aim : Screen-based media use is gradually becoming a public health issue, especially among young people.Method : A local descriptive observational study was conducted in 11 colleges of the Bouches-du-Rh\u00f4ne department. All middle high school students were asked to fill in a questionnaire comprising questions about their demographic characteristics, their screen-based media use (Internet, video games, social networks), any problematic use (video games and social networks), self-esteem and quality of life.Results : A total of 950 college students (mean age : 12.96 years) participated in the research. The results show a high level and a very diverse screen-based media use. Boys more frequently played video games and girls go more frequently used social networks. The levels of problematic use were relatively low for all middle high school students. The level of problematic video game use was significantly higher in boys, and the level of problematic social network use was higher in girls.Conclusion : Differences in the use of video games or social networks raise the general issue of gender differences in society. This study indicates the need for more specific preventive interventions for screen-based media use. The addictive \u201cnature\u201d of certain practices needs to be studied in more detail.", "title": "Video Games, Internet and Social Networks: A Study among French School students", "claims": null}, {"metadata": {"year": 2013}, "authors": ["H. K. Chhabra", "Sakshita Anand"], "summary": "The use of interactive screen media is widespread and for some users leads to pathological symptoms that are phenomenologically similar to signs of addictive disorders (Rehbein, 2013). Recent reports (Zhou, 2012) have indicated that some online users become addicted to the internet in the same way as others become addicted to drugs or alcohol which further results in academic failure (Brady, 1996; Murphey, 1996), reduced work performance (Robert Half International Inc., 1996).In terms of at risk populations, students (Widyanto & Griffiths, 2006) have a natural attraction towards the internet (Veen & Vrakking, 2006) and their evident internet literacy is linked to internet addiction (Leung & Lee, 2012). Moreover, they typically are found to have (i) free and unlimited access, (ii) flexible schedules, and (iii) freedom from parental interference. The online activities of the students are not controlled externally. It is expected by the colleges/schools that students make use of the technology and such settings can foster social intimidation and aloofness (Moore, 1995; Young, 2004).Moreover, psychological and developmental factors associated with young adulthood contribute to the allure of the internet for students. Students not only find themselves in the process of developing their identities, but also start to establish intimate relationships at that particular stage of their lives (Kuss et al., 2013). To develop one's identity means to become detached from one's parents to a certain extent leading to internal conflicts which are repeatedly resolved by the escape into addictions of all sorts, including internet addiction (Lanthier & Windham, 2004). Thus, the usage of excessive internet has become a source of self-medication (Castiglione, 2008).Internet addiction disorder is also termed as the 'internet overuse' and 'pathological computer use'. The excessive computer use disrupts the everyday life. (Information Security Education and Awareness, 2013). The \"overuse\" and \"excessive use\" of the internet usually indicate that the time online is an important index for determining internet addiction (Chou et al., 2005).In young adults, internet addiction prevalence ranges from 0.8% in Italy (Poli & Agrimi, 2012), 0.9% in Jordan(Al-Qudah, 2001), 2.8% in Iran (Ghamari, et al., 2011), 5.6% in China (Dong, et al., 2012), 9.8% in the USA (Anderson, 2001 ), to 15.1 % in Taiwan (Lin, Ko, & Wu, 2011), 16.2% in Poland (Lidwinko, et al., 2011), 18.3% in Great Britain (Niemz, et al., 2005) and in India , 74.5% as moderate users, 24.8% as possible addicts, and 0.7% as addicts (Goel et al., 2013). Various Problems Associated With Internet Addiction (Childnet International, 2006).* Skipping meals, losing sleep and time.* Rearranging daily routines and neglecting studies or other activities and responsibilities to spend more time online.* This has the potential to affect school performance and relationships with friends or family.* There may be financial problems depending on the nature of the communication e.g. dial-up connection or text messages and the activity online (e.g. paying for content, gambling and subscriptions).* There are also physical problems associated with bad diet, lack of exercise and extended computer use (for example dry eyes, Carpal Tunnel Syndrome, and backache, Herbolingo, 2011).Indicators of Excessive Internet Use (Childnet International, 2006).* Preoccupation with the internet is one of the most important sources of satisfaction in a person's life.* Less investment in relationships with friends and interruption of social relationships.* Feeling empty, depressed and irritable when not at the computer.* Withdrawal symptoms when internet usage is reduced. These symptoms include anxiety, restlessness, depression, etc.* Craving more and more time at the computer.* Deception regarding time spent online. \u2026", "title": "A Psychological Analysis of Internet Addiction among Students", "claims": null}, {"metadata": {"year": 2020}, "authors": [], "summary": "Background: People with addiction do not have control over their behavior, actions or usage. Their addiction may reach to a point\nthat it is harmful. Addiction does not only involve the physical aspects such as drugs or alcohol but also includes abstract things,\noften described as a behavioral addiction. The causes of addiction differ considerably and are seldom fully understood. They are\ngenerally caused by varying degrees of physical, mental, circumstantial and emotional factors.\nObjectives: To study the electronic devise profile among children and adolescent in modern global cities such as Dubai and to study\nthe prevalence and size of the addiction phenomena.\nMethodology: This study was carried out in Dubai, United Arab Emirates during the Global School Health Survey in 2015. The\nsample included 7000 students who were randomly selected (multistage stratified random sample) from 10 private schools in Dubai.\nData collection was carried out by standard questionnaire developed by the World Health Organization which includes different\nhealth domains (addiction, and addiction to electronic devices). Ethical issues were followed up to standards. The questionnaire and\nconsent forms were given to the participants who met the inclusion criteria. An information sheet was attached to the questionnaire\nexplaining the purpose of the study. The students were assured that the confidentiality and privacy of the answers were maintained.\nNo names, phone numbers, and identification are required. Data coding, data entry and data analysis has been conducted by using\nSPSS 20 software.\nResults: The present study revealed that 56% of school children aged 13-15 years old spend three hours or more a day using electronic\ngames, and this percentage rises to nearly 63% with the age group 16-17 years. The study showed that children, who excessively use\nthese games might face social, emotional and behavioral problems such as isolation, anxiety and depression. Pointing out that children\nmay complain of psychological pressure, stress or dehydration and burning eye, due to constant staring with bright light across the\nscreens of devices. Most students (68.7%) said they were on social media between one and five hours daily, while 3.3 per cent said\nthey spend less than an hour. 39.8% of students stayed without food or drink for a long time and 50.5 % tried to quit social media\nlast year but failed. 15% of students spend 4-8 hours on social media and 1.6% spend more than 10 hours. When the students were\nasked about how they feel of the burden on their daily life and social commitments, 60% of them replied that social media decreases\ntheir interaction with the outside environment and family time, 55.6% said it interfered with their homework. The study showed that\nmore than one quarter (26.7%) or almost one out of four reach up to more than 6 hours per day and could sometimes reach up to 10\nin browsing and interacting with social media. The study reflected that more than 400 children showed the negative symptoms on the\nuse of tablets. It appeared significantly on children who use them for more than two hours a day compared to their counterparts who\ndo not use it at all or use it less than two hours. They have noticed that those who use it less have great language, motor and mental\nskills. Parents do not seem to be paying attention on how technology is taking their kids away from them. Children between the ages\nof 11 and 17 found out what really is keeping them so busy on all these addictive devices. Around 70% of them are most vulnerable\nto gaming addiction. They belong to the under-five age group, because parents are preoccupied with them.\nConclusion: Electronic addiction among children and adolescents is a rapidly growing phenomena in the City of Dubai. It has a\nsignificant impact on the social and professional life and functions of this group, which necessitate intervention at many levels.", "title": "Epidemiology of Digital Addiction among Adolescent Cohorts, Understanding\nContexts", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Hamid Yahya Hussain"], "summary": "Background: People with addiction do not have control over their behavior, actions or usage. Their addiction may reach to a point that it is harmful. Addiction does not only involve the physical aspects such as drugs or alcohol but also includes abstract things, often described as a behavioral addiction. The causes of addiction differ considerably and are seldom fully understood. They are generally caused by varying degrees of physical, mental, circumstantial and emotional factors. Objectives: To study the electronic devise profile among children and adolescent in modern global cities such as Dubai and to study the prevalence and size of the addiction phenomena. Methodology: This study was carried out in Dubai, United Arab Emirates during the Global School Health Survey in 2015. The sample included 7000 students who were randomly selected (multistage stratified random sample) from 10 private schools in Dubai. Data collection was carried out by standard questionnaire developed by the World Health Organization which includes different health domains (addiction, and addiction to electronic devices). Ethical issues were followed up to standards. The questionnaire and consent forms were given to the participants who met the inclusion criteria. An information sheet was attached to the questionnaire explaining the purpose of the study. The students were assured that the confidentiality and privacy of the answers were maintained. No names, phone numbers, and identification are required. Data coding, data entry and data analysis has been conducted by using SPSS 20 software. Results: The present study revealed that 56% of school children aged 13-15 years old spend three hours or more a day using electronic games, and this percentage rises to nearly 63% with the age group 16-17 years. The study showed that children, who excessively use these games might face social, emotional and behavioral problems such as isolation, anxiety and depression. Pointing out that children may complain of psychological pressure, stress or dehydration and burning eye, due to constant staring with bright light across the screens of devices. Most students (68.7%) said they were on social media between one and five hours daily, while 3.3 per cent said they spend less than an hour. 39.8% of students stayed without food or drink for a long time and 50.5 % tried to quit social media last year but failed. 15% of students spend 4-8 hours on social media and 1.6% spend more than 10 hours. When the students were asked about how they feel of the burden on their daily life and social commitments, 60% of them replied that social media decreases their interaction with the outside environment and family time, 55.6% said it interfered with their homework. The study showed that more than one quarter (26.7%) or almost one out of four reach up to more than 6 hours per day and could sometimes reach up to 10 in browsing and interacting with social media. The study reflected that more than 400 children showed the negative symptoms on the use of tablets. It appeared significantly on children who use them for more than two hours a day compared to their counterparts who do not use it at all or use it less than two hours. They have noticed that those who use it less", "title": "Social Media, Internet and Electronic Devices Addiction among Children and Adolescents in Global Contexts Modern Cities", "claims": null}, {"metadata": {"year": 2021}, "authors": ["K. Kawabe", "F. Horiuchi", "Rie Hosokawa", "Kiwamu Nakachi", "S. Ueno"], "summary": "The purpose of this study was to elucidate the relationship between the severity of Internet addiction and various media-related applications. The participants were junior high school students between 12 and 15 years old. A total of 529 students (283 males, 246 females) were included. The participants answered Young\u2019s Internet Addiction Test (IAT) and a structural questionnaire about their access to electronic devices and applications. An Internet addiction prevalence of 4.3% (95% CI: 2.8\u20136.5%) was reported in this study, with an additional 26.3% (95% CI: 22.6\u201330.2%) of participants possibly addicted. The accessibility of gaming devices was significantly higher in male students than in female students. The use of applications for SNSs was significantly higher in female students than in male students. Twitter accessibility was a factor that contributed to Internet addiction in both genders. The prevalence of severe Internet addiction among school students in Japan was 4.3%, and Twitter was the most important factor associated with this addiction. Media literacy must be increased in adolescents and their friends, teachers, and families.", "title": "Association between Internet Addiction and Application Usage among Junior High School Students: A Field Survey", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Minakshi Rana", "Jasjeev Pal Kaur"], "summary": "Technology has facilitated the human life in many ways. However, the random use of technology has put vital \naspects of life into danger. The excessive exposure to gadgets is working as a slow poison that interferes \nseverely in natural working of human body. This is more crucial in age group of 6 to 14 years of age as most of \nthe growth and development of vital organs of body occurs in this phase of life. The experiences at this age lay \nthe foundation stone of adulthood. Among the addiction to internet, increasing use of social media and other \nonline entertainment prevalent in Indian population, the common element is exposure to screen. The present \nstudy has focused on the extent of screen exposure among school age children in India and its impact on their \nphysical well-being, academic achievement, behaviour and social quotient. The study revealed that children \naging 6 to 14 years of age are exposed to screen exposure more than the number of hours recommended by \nWorld Health Organisation and American Academy of Paediatrics. Further, the excessive exposure to screen \namong the studied children has adverse impact on their physical well-being, academic achievement, behaviour \nand social quotient.", "title": "Addiction in Disguise: An analytical study of Screen Exposure among School age children in India", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Nazan G\u00fcrarslan Ba\u015f", "G. Karatay"], "summary": "PURPOSE\nThis study aimed to evaluate the relationship between technology usage and technology addiction among secondary school students.\n\n\nDESIGN AND METHODS\nThis descriptive research was conducted in April 2018 with 840 students in Eastern Turkey.\n\n\nFINDINGS\nThe average Internet and digital-gaming addiction scores of the participating students were found to be significantly higher among those who used computers, smartphones, and tablets daily and for long periods of time.\n\n\nIMPLICATIONS FOR PRACTICE\nUsing technological devices regularly and for long periods of time increases the level of Internet and digital-gaming addiction. Thus, it is important to increase students' awareness of the need to use technological devices appropriately.", "title": "Effects of technology usage on the addictive behaviors of secondary school students.", "claims": null}, {"metadata": {"year": 2017}, "authors": ["M. Sharma", "G. Ragesh", "T. Palanichamy", "A. Hamza", "P. Chandra", "S. Chaturvedi"], "summary": "243 symptoms, depression, and hostility are associated with internet addiction in male adolescents, and only higher ADHD symptoms and depression are associated with internet addiction in female students.[3] Heavy internet users found to have loneliness, depressed mood, compulsivity, and other psychological impairments.[4] About 11.8% of students had internet addiction and anxiety/stress. It had association with was the time spent online, usage of social networking sites and chat rooms.[5] 24.6% reported problematic use of the internet.[6] There is a need to sensitize the mental health professionals for screening excessive use of technology devices, developing evidence of exploring its addictive properties as well as develop psychosocial intervention for users and caregivers.", "title": "Internet addiction with coexisting psychiatric diagnosis", "claims": null}], "query": "\u201cscreen addiction in middle or high school students\u201d", "summary_abstract": "The issue of screen addiction among middle and high school students has garnered significant attention in recent research, revealing alarming trends and potential consequences. Various studies indicate that a substantial proportion of adolescents engage in excessive screen time, which can lead to problematic behaviors and mental health issues.\n\nDany et al. (2016) conducted a study involving 950 middle school students, finding that while the overall levels of problematic screen use were relatively low, there were notable gender differences. Boys were more likely to engage in problematic video game use, while girls showed higher levels of problematic social media use. This suggests that targeted interventions may be necessary to address the specific patterns of screen use among different genders.\n\nChhabra and Anand (2013) highlighted the addictive nature of internet use among students, noting that their unrestricted access and the developmental stage they are in can lead to internet addiction. They reported that excessive internet use can disrupt daily life, leading to academic failure and social isolation. The prevalence of internet addiction among young adults varies widely, with some studies indicating rates as high as 74.5% in certain populations (Goel et al., 2013).\n\nIn a more recent study conducted in Dubai, Hussain (2020) found that 56% of students aged 13-15 spent three or more hours daily on electronic games, with similar trends observed in older adolescents. This excessive use was linked to social, emotional, and behavioral problems, including anxiety and depression. Furthermore, many students reported that their screen time interfered with their academic responsibilities and social interactions.\n\nKawabe et al. (2021) reported a 4.3% prevalence of severe internet addiction among junior high school students in Japan, with an additional 26.3% possibly addicted. They identified social media applications, particularly Twitter, as significant contributors to internet addiction, emphasizing the need for increased media literacy among adolescents.\n\nRana and Kaur (2020) focused on Indian school-age children, revealing that excessive screen exposure negatively impacted their physical health, academic performance, and social skills. Similarly, Ba\u015f and Karatay (2020) found that regular and prolonged use of digital devices was associated with higher addiction scores among secondary school students in Turkey.\n\nSharma et al. (2017) further underscored the psychological implications of internet addiction, noting that symptoms such as depression and anxiety were prevalent among heavy internet users. They called for increased awareness and screening for excessive technology use among adolescents.\n\nCollectively, these studies illustrate a concerning trend of screen addiction among middle and high school students, with significant implications for their mental health, academic performance, and social interactions. The findings underscore the urgent need for targeted interventions and increased awareness of the risks associated with excessive screen time.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2008}, "authors": ["Amy R. Taylor", "M. G. Jones", "Bethany Broadwell", "T. Oppewal"], "summary": "Although there have been numerous studies that indicate the benefits of teachers and students working with scientists, there is little research that documents scientists' views of science education, science teacher preparation, and the goals of science education. Furthermore, little is known about how scientists' views of science education may differ from those held by science teachers. Through the use of semistructured interviews, the perceptions of 37 scientists from diverse science domains and 21 middle and high school science teachers were explored. Participating scientists expressed concerns about the variability in quality of teaching, programs, and resources available for science instruction. Scientists expressed a desire for teachers to have more experience conducting science research and developing their own critical thinking skills. When asked what goals are most important for science education, 40% of the scientists emphasized that teachers should make science fun and exciting for their students. Science teachers' perceptions of science education were compared with the scientists' perceptions. Thirty percent of the teachers agreed with the scientists that too much variability in program or instructional quality exists in science education. Seventy-six percent of the science teachers also thought there is a need to teach critical thinking skills, but more importantly there is a need to inspire creativity and a desire to learn science in students. Both teachers and scientists expressed concerns about how high-stakes accountability inhibits efforts to improve science education. \u00a9 2008 Wiley Periodicals, Inc. Sci Ed92:1058\u20131075, 2008", "title": "Creativity, inquiry, or accountability? Scientists' and teachers' perceptions of science education", "claims": null}, {"metadata": {"year": 2009}, "authors": ["D. Schuster", "W. Carlsen"], "summary": "During the past decade science educators have taken steps to identify seminal structures and approaches of science teacher professional development. This literature, which is increasingly informing how and in which contexts professional developers design and implement programs, states that scientists' divergent research interests and knowledge about K-12 science teachers often limit their involvement in these programs. Conversely, concerns persist that there is very little empirical evidence to support these programmatic recommendations made by the science education literature. This embedded case study was bounded by the contextual similarities between seven professional development workshops with the goal of examining the relationship between scientists' views of teachers as professionals and the pedagogical orientations that the scientists used within these professional development contexts. Multiple methods were employed including systematic classroom observation by nine trained observers, as well as analysis of course materials, interviews, and questionnaires. Altogether, the teacher comments and observation data paint a picture of how the instructors implemented specific pedagogical orientations. Participants appear to have realistic views about the practical roles that research scientists can play in professional development. We explore how these findings merit attention and suggest alternatives to current practice and policy. \u00a9 2008 Wiley Periodicals, Inc. Sci Ed93:635\u2013655, 2009", "title": "Scientists' teaching orientations in the context of teacher professional development", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Angela  Fitzgerald"], "summary": "If the status and quality of science education in schools is to improve, efforts need to be made to better understand the classroom practices of effective science teachers. Teachers are key players in a re-imagining of science education. This book explores how two primary school teachers, identified as effective practitioners, approached science teaching and learning over a unit of work. In recording the teaching and learning experiences in their classrooms, the author highlights how the two teachers adopted different approaches, drawing on their particular beliefs and knowledge, to support student learning in science in ways that were appropriate to their contexts as well as reflected their different experiences, strengths and backgrounds. Through sharing their stories, this book illustrates, that due to the complex nature of teaching and learning, there is no one way of defining effectiveness. In documenting this research, it is hoped that other teachers and teacher educators will be inspired to think about primary school science education in innovative ways.", "title": "Science in Primary Schools: Examining the Practices of Effective Primary Science Teachers", "claims": null}, {"metadata": {"year": 2002}, "authors": ["J. Rhoton", "K. Stiles"], "summary": "Designing effective professional development programs requires a deliberate process in which careful consideration is given to numerous inputs into the framework design. The improvement of student achievement in science education continues to be a top priority in the US. The National Commission on Mathematics and Science Teaching for the 21st Century writes: \"The most direct route to improving mathematics and science achievement for all students is better mathematics and science teaching\" (2000, p.7). Others agree, suggesting that investments aimed toward improving education should focus on the preparation and ongoing professional development of teachers and other educators (Darling-Hammond & McLaughlin, 1999). While few would argue with these observations, improving teaching is a complex undertaking faced with many challenges. For example, demanding standards and changing demographics present challenges. Educating highly diverse students to meet much higher standards requires tremendous skills on the part of teachers. Teachers today need to provide a wide range of learning experiences connected to what a diverse student body knows, how they learn, and the content and structure of the disciplines (Darling-Hammond & McLaughlin, 1999; Ball & Cohen, 1999). Teachers need opportunities over time to deepen their understanding of how children learn science and to stay abreast of emerging technologies and research. Veteran and novice teachers alike need collegial arrangements that provide a structure through which they continually develop their expertise as teachers. Professional development of teachers is clearly an essential element of science education reform. All of the major improvement initiatives call for increasing teacher knowledge and skills because of the link between student achievement and teacher knowledge and skill. Research shows that teacher expertise can account for about 40 percent of the variance on students' learning in reading and mathematics achievement-more than any other single factor, including student background (Ferguson, 1991.) Other studies show a similar correlation between teacher expertise and student achievement across the subject areas. Since teacher expertise has such a demonstrated impact on student learning, it stands to reason that programs that develop teachers' knowledge and skills are a sound investment in improving student outcomes. However, the research on learning (Bransford, et al., 1999) and that on effective teacher development (Sparks & Hirsch, 1997; Loucks-Horsley, Hewson, Love, & Stiles, 1998), suggests that teacher development as carried out in most schools today is not designed to develop the teacher expertise needed to bring about improved student learning. \"The content of professional development is largely techniques, its pedagogy is training, and the learning it promotes consists of remembering new things to try in the classroom\" (Thomson & Zeuli, 1999, p. 353). The professional development systems and structures in most schools need to be redesigned to develop and support capable, knowledgeable and expert teachers. One framework for designing professional development has informed the design and implementation of programs across the country. In the book, Designing Professional Development for Teachers of Science and Mathematics (Loucks-Horsley, et al., 1998), the authors describe a framework to guide the design of professional development programs. It is a process of decision making and conscious design based on several inputs (see Figure 1). At the center of the framework is a planning cycle incorporating goal setting, planning, doing, and reflecting. The circles represent important inputs into both goal setting and planning that can help professional developers design programs to meet the needs of the audiences and that are grounded in best practice. The inputs guide designers to consider the extensive knowledge bases that inform their work (knowledge and beliefs), to understand the unique features of their own context, to draw on a wide repertoire of professional development strategies, and to incorporate designs to address the critical issues they are most likely to encounter. \u2026", "title": "Exploring the Professional Development Design Process: Bringing an Abstract Framework into Practice.", "claims": null}, {"metadata": {"year": 2011}, "authors": ["D. Lustick"], "summary": "Reports such as Rising Above the Gathering Storm and Europe Needs More Scientists exemplify the international community\u2019s concern about the state of public science education. With a lack of scientific literacy for all students and insufficient numbers of individuals choosing to pursue careers in science, the situation is seen as a threat to the sustainability of a robust national economy. Continuing professional development for science teachers is considered a key reform strategy for addressing the problem. As nations invest considerable funds towards improving the quality of science teachers (and consequently student learning) it is reasonable to explore science teacher perceptions regarding the most effective professional learning opportunities. This descriptive study surveyed 118 candidates for National Board certification in Adolescent and Young Adult Science from 42 states about their professional learning experiences. The survey intended to answer the question: \u2018How do candidates perceive the relative effectiveness of different professional learning experiences?\u2019 \u2018Effectiveness\u2019 here is defined as a professional development's ability to help a teacher foster improved student learning. It is important to note that the survey focused on the approaches to professional development rather than the content or goals of said experiences. Candidates' self\u2010reports reveal that developing science curriculum, reading scientific literature, and pursuing National Board certification are the three most productive approaches to professional development while education courses and in\u2010service workshops were identified as least effective. The results suggest that no one model of professional development explains the top rated activities.", "title": "Experienced secondary science teachers' perceptions of effective professional development while pursuing National Board certification", "claims": null}, {"metadata": {"year": 2001}, "authors": ["J. Watters", "Carmel M. Diezmann", "S. Grieshaber", "Julie M. Davis"], "summary": "A major challenge facing teachers is how they can help children to develop the knowledge, skills, and attitudes necessary to become scientifically literate citizens. Early childhood teachers are often hesitant about teaching science because they lack confidence in their conceptual knowledge and understandings. This paper describes a professional development initiative which enhances teachers\u2019 understanding of science concepts, and supports their teaching of the subject. The workshops adopted a \u2018child-centred\u2019 approach that was scaffolded by teacher educators. Thus, the teachers\u2019 learning paralleled those exemplary experiences advocated for children. Feedback from teachers was overwhelmingly positive and they reported an increased motivation for teaching science coupled with a better understanding of contemporary strategies. By capitalising on personal experiences of investigation of everyday phenomena in the workshop, teachers were able to identify those contemporary approaches consistent with effective science teaching and to understand why these approaches are effective.", "title": "Enhancing Science Education for Young Children: A Contemporary Initiative", "claims": null}, {"metadata": {"year": 2020}, "authors": ["M. Oliveira", "Maria Cleide da Silva Barroso", "Francisca Helena de Oliveira Holanda", "Ana Karine Portela Vasconcelos"], "summary": "In the school context, Science Education is an existing discipline to instill students' knowledge, mobilizing in them a critical and investigative spirit. Starting from cognitive knowledge to reality, the learner will incorporate concepts that, through application in problem situations, will be mediated by the relationships in which he finds himself. It was observed that this form of pedagogical work can contribute to the production of situations favorable to the pursuit of knowledge in the school space. In this study we aim to understand the perception of teachers about Science Teaching during the evaluation processes. This research has a qualitative approach. The methodology includes instructions regarding subjects, inquisitions, groupings and weightings of information. The results of the research show how to delimit: absence of a certain connection between Science Teaching and assessment with students' lives; inexistence of any advice and didactic basis and are able to make the results viable; certain elaboration guided with the goal of enabling students to an evaluative practice. As viability: Science Teaching within an evaluation framework can provide components about peculiarities of education in public schools; this conduct appreciates the role of the master and organizational. They lead to referrals about Science Teaching and its evaluative execution since it is essential to consider the link between Science Teaching and the existence of students, schools and educators.", "title": "Science teaching in basic education: student learning and teacher practice from the perspective of evaluations", "claims": null}, {"metadata": {"year": 2016}, "authors": ["I. St\u0103ncescu", "A. Petrescu", "L. Dr\u0103ghicescu"], "summary": "The study of the disciplines from the Sciences area (Chemistry, Physics, Biology) is essential for handling the students with scientific concept, and equip students with the necessary skills with the view to produce an optimal adaptation to the specific requirements of knowledge based society related to the forming of positive attitudes towards science. To achieve such goals, it is very important for science teachers to create a suitable learning environment, enabling so the students to develop autonomy in their actions and thoughts, particularly to take control of their own learning. Only in an appropriate learning environment, students can capitalize in a maximal way their own learning experiences; they can form their skills of investigation, experimentation and abilities to operate independently and effectively in a given situation. The present paper is based on the analysis of the teachers\u2019 perceptions concerning the characteristics that must be present in an optimal learning environment for sciences. The analysis was performed in the frame of the European project called PROFILES, and involved teachers from secondary education who answered to a questionnaire survey. The questionnaire items were aimed directly to some features of the learning environment such as: use of the teaching student-centered philosophy, taking into account the prior students\u2019 knowledge and skills, as well as the proposals of involving the students in lesson planning, promoting communication skills, guiding students on developing social dimension and questions about scientific issues, promoting their analytical thinking, promoting an effective collaborative learning etc.", "title": "Teachers' Perceptions Concerning the Characteristics of a Learning Environment Dedicated to Science Education", "claims": null}], "query": "How do teachers' and scientists' understanding of effective teaching, effective science education, and education improvement differ?", "summary_abstract": "The research question regarding the differences in understanding of effective teaching, effective science education, and education improvement between teachers and scientists reveals several key insights from the reviewed literature.\n\nTaylor et al. (2008) conducted interviews with both scientists and science teachers, uncovering a shared concern about the variability in the quality of science education. Scientists emphasized the importance of making science engaging and fun, while teachers highlighted the need to inspire creativity and a desire to learn among students. This indicates a divergence in focus, with scientists leaning towards the excitement of science as a goal, whereas teachers are more concerned with fostering a broader range of skills, including critical thinking and creativity.\n\nSchuster and Carlsen (2009) explored the relationship between scientists' views of teachers and their pedagogical approaches in professional development contexts. They found that scientists often have limited knowledge about K-12 education, which can affect their involvement in teacher training. This suggests a gap in understanding between the two groups regarding the practical realities of teaching and the professional development needs of teachers.\n\nFitzgerald (2012) emphasized the complexity of defining effective science teaching, noting that different teachers adopt varied approaches based on their beliefs and experiences. This highlights the subjective nature of \"effectiveness\" in teaching, contrasting with scientists' more standardized views of science education.\n\nLustick (2011) surveyed science teachers about their professional development experiences, revealing that teachers perceive certain activities, such as developing science curriculum, as more effective than traditional workshops. This points to a potential disconnect between the types of professional development favored by scientists and those that teachers find beneficial.\n\nWatters et al. (2001) reported on a professional development initiative that successfully enhanced teachers' understanding of science concepts through a child-centered approach. This aligns with the teachers' desire for practical, applicable strategies in their classrooms, contrasting with the more theoretical perspectives often held by scientists.\n\nFinally, St\u0103ncescu et al. (2016) highlighted the importance of creating an optimal learning environment for science education, emphasizing student-centered teaching and the need for teachers to facilitate independent learning. This perspective underscores the teachers' role in shaping effective science education, which may not always align with scientists' views on the subject.\n\nIn summary, the literature indicates that while both teachers and scientists share concerns about the quality of science education, their understandings diverge significantly. Scientists often focus on engagement and excitement in science, whereas teachers prioritize creativity, critical thinking, and practical teaching strategies. Additionally, the professional development needs and perceptions of effectiveness differ, suggesting a need for better alignment between the two groups to improve science education outcomes.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2002}, "authors": ["H. Soltau", "Florian Metze", "C. F\u00fcgen", "A. Waibel"], "summary": "In this study, we examine how fast decoding of conversational speech with large vocabularies profits from efficient use of linguistic information, i.e. language models and grammars. Based on a re-entrant single pronunciation prefix tree, we use the concept of linguistic context polymorphism to achieve an early incorporation of language model information. This approach allows us to use all available language model information in a one-pass decoder, using the same engine to decode with statistical n-gram language models as well as context free grammars or re-scoring of lattices in an efficient way. We compare this approach to our previous decoder, which needed three passes to incorporate all available information. The results on a very large vocabulary task show that the search can be speeded up by almost a factor of three, without introducing additional search errors. On all examined tasks, we observed significant improvements by using an exact language model lookahead over usual bigram lookahead strategies, even for very hard tasks with unmatched conditions, without introducing extra memory overhead.", "title": "Efficient language model lookahead through polymorphic linguistic context assignment", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Sonam Sanjogkumar Dodal", "P. Kulkarni"], "summary": "The task of finding data files related to an information need from a group of information resources is known as Information Retrieval. In this work, the author propose a multi-lingual information retrieval system using deep learning. Input to the system is a question in sentencing form that can be processed by NLP tools. In the preprocessing phase, part-of-speech tagging of the input sentence is performed. A three layer neural network is used for creating word to vector representation. The word2vec model continuous-bag-of-words (CBOW) is used for this purpose. Then related words are obtained via word-2-vec using deep learning RNN. RNN is the recurrent neural network. Finally, results are obtained by calculating the cosine similarity score. For multi-lingual results, bilingual mapping is performed using CFILTs bilingual corpus. The tourism dataset is used for experimentation purposes.", "title": "Multi-Lingual Information Retrieval Using Deep Learning", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Ye Liang", "Ningning Guo", "Chunxiao Xing", "Yong Zhang", "C. Li"], "summary": "Based on the study of linguistics, Information Science and Library and Information Science, we research on the real-time news posted on the authority sites in the world's major countries. By analyzing the massive news of different information sources and language origins, we come up with a basic theory model and its algorithm on news, which is capable of intelligent collection, quick access, deduplication, correction and integration with news' backgrounds. Furthermore, we can find out connections between news and readers' interest. So we can achieve a real-time and on-demand news feed as well as provide a theoretical basis and verification of scientific problems on real-time processing of massive information. Finally, the simulation experiment shows that the multilingual news matching technology could give more help to distinguish the similar news in different languages than the traditional method.", "title": "Multilingual Information Retrieval and Smart News Feed Based on Big Data", "claims": null}, {"metadata": {"year": 1999}, "authors": ["Fei Song", "W. Bruce Croft"], "summary": "Statistical language modeling has been successfully used for speech recognition, part-of-speech tagging, and syntactic parsing. Recently, it has also been applied to information retrieval. According to this new paradigm, each document is viewed as a language sample, and a query as a generation process. The retrieved documents are ranked based on the probabilities of producing a query from the corresponding language models of these documents. In this paper, we will present a new language model for information retrieval, which is based on a range of data smoothing techniques, including the Good-Turning estimate, curve-fitting functions, and model combinations. Our model is conceptually simple and intuitive, and can be easily extended to incorporate probabilities of phrases such as word pairs and word triples. The experiments with the Wall Street Journal and TREC4 data sets showed that the performance of our model is comparable to that of INQUERY and better than that of another language model for information retrieval. In particular, word pairs are shown to be useful in improving the retrieval performance.", "title": "A general language model for information retrieval", "claims": null}, {"metadata": {"year": 1998}, "authors": ["S. Deligne", "Y. Sagisaka"], "summary": "In this paper, we present a stochastic language modeling tool which aims at retrieving variable-length phrases (multigrams), assuming bigram dependencies between them. The phrase retrieval can be intermixed with a phrase clustering procedure, so that the language data are iteratively structured at both a paradigmatic and a syntagmatic level in a fully integrated way. Perplexity results on ATR travel arrangement data with a bi-multigram model (assuming bigram correlations between the phrases) come very close to the trigram scores with a reduced number of entries in the language model. Also the ability of the class version of the model to merge semantically related phrases into a common class is illustrated.", "title": "Learning a Syntagmatic and Paradigmatic Structure from Language Data with a Bi-Multigram Model", "claims": null}, {"metadata": {"year": 2000}, "authors": ["W. Bruce Croft"], "summary": "The amount of on-line information is growing exponentially. Much of this information is unstructured and language-based. To deal with this flood of information, a number of tools and language technologies have been developed. Progress has been made in areas such as information retrieval, information extraction, filtering, speech recognition, machine translation, and data mining. Other more specific areas such as cross-lingual retrieval, summarization, categorization, distributed retrieval, and topic detection and tracking are also contributing to the proliferation of technologies for managing information. Currently these tools are based on many different approaches, both formal and ad hoc. Integrating them is very diffcult, yet this will be a critical part of building effective information systems in the future. In this paper, we discuss an approach to providing a framework for integration based on language models.", "title": "Information Retrieval Based on Statistical Language Models", "claims": null}, {"metadata": {"year": 2004}, "authors": ["Aitao  Chen", "Fredric C. Gey"], "summary": "Multilingual retrieval (querying of multiple document collections each in a different language) can be achieved by combining several individual techniques which enhance retrieval: machine translation to cross the language barrier, relevance feedback to add words to the initial query, decompounding for languages with complex term structure, and data fusion to combine monolingual retrieval results from different languages. Using the CLEF 2001 and CLEF 2002 topics and document collections, this paper evaluates these techniques within the context of a monolingual document ranking formula based upon logistic regression. Each individual technique yields improved performance over runs which do not utilize that technique. Moreover the techniques are complementary, in that combining the best techniques outperforms individual technique performance. An approximate but fast document translation using bilingual wordlists created from machine translation systems is presented and evaluated. The fast document translation is as effective as query translation in multilingual retrieval. Furthermore, when fast document translation is combined with query translation in multilingual retrieval, the performance is significantly better than that of query translation or fast document translation.", "title": "Multilingual Information Retrieval Using Machine Translation, Relevance Feedback and Decompounding", "claims": null}, {"metadata": {"year": 2004}, "authors": ["ChenAitao", "C  GeyFredric"], "summary": "Multilingual retrieval (querying of multiple document collections each in a different language) can be achieved by combining several individual techniques which enhance retrieval: machine translati...", "title": "Multilingual Information Retrieval Using Machine Translation, Relevance Feedback and Decompounding", "claims": null}], "query": "multi-hop information retrieval  with large language models", "summary_abstract": "The research question regarding multi-hop information retrieval with large language models is addressed through various studies that explore different methodologies and frameworks for enhancing information retrieval processes.\n\nDodal and Kulkarni (2018) propose a multi-lingual information retrieval system that utilizes deep learning techniques, specifically a three-layer neural network for word vector representation. Their approach emphasizes the importance of natural language processing (NLP) tools in processing input questions, which aligns with the multi-hop retrieval concept by enabling the system to understand and connect various pieces of information across languages.\n\nLiang et al. (2015) focus on real-time news retrieval, developing a model that intelligently collects and integrates news from diverse sources. Their work highlights the capability of their system to deduplicate and correct information while establishing connections between news items and reader interests, which is crucial for multi-hop retrieval as it requires linking multiple information sources effectively.\n\nCroft (2000) discusses the exponential growth of online information and the necessity for effective management tools, including language models for information retrieval. He emphasizes the integration of various language technologies, which is essential for multi-hop retrieval as it often involves synthesizing information from multiple models and sources.\n\nChen and Gey (2004) explore multilingual retrieval techniques, combining machine translation, relevance feedback, and data fusion to enhance retrieval performance. Their findings suggest that integrating these techniques can significantly improve the effectiveness of retrieving information across different languages, which is a key aspect of multi-hop retrieval.\n\nOverall, these studies collectively underscore the importance of leveraging advanced language models and deep learning techniques to facilitate multi-hop information retrieval. They highlight the need for systems that can process and connect information from various sources and languages, ultimately improving the retrieval process in complex information environments.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2010}, "authors": ["Ilka Saal"], "summary": "Grossman meticulously weaves excerpts from Morris\u2019s diaries with her published memoirs and fiction, and the accounts of others, to create a fascinating portrait of an extraordinary woman. She suggests that \u2018Morris\u2019s theatrical performances mirrored her troubled life\u2019, although the actress \u2018took great care to ensure that people did not confuse her with her stage creations\u2019 (p. 4). Her focus is on Morris\u2019s delineation of key roles, which evoked both critical praise and shocked reproof, and, as her fame faded and she was forced to tour, the lurid appeal of often erratic performances. Morris attempted to succeed Charlotte Cushman as America\u2019s tragic queen, but she never attained the power that Shakespeare demanded. Grossman is particularly incisive in her analysis of Morris\u2019s rendition of Lady Macbeth, for which she was considered too realistic and modern. Her emotional acting included enormous physical exertion. Of her audiences, Morris herself wrote, \u2018I had to shed tears or they would not\u2019 (p. 82). Grossman calls this, using the term of a critic, \u2018spectacle of suffering\u2019 \u2013 \u2018accompanied by copious tears on stage and in the house\u2019, it was the \u2018hallmark of her art\u2019 (p. 125). Morris used details of costume and makeup to create powerful effects. She studied the outward signs of tuberculosis and heart disease, and, when playing a suicide, chose strychnine over cyanide as the means of death because, as Grossman explains, its effects \u2018were much more actable\u2019 (p. 128). Grossman is sensitive in detailing Morris\u2019s own dependence upon morphine, due to declining health. Morris had doctors who \u2018morphined\u2019 her, even during performances, causing long delays and disruptions. That, along with the changing modes of performance, caused a decline in her popularity, increased difficulty in finding suitable vehicles for her talents, and finally forced her to revive her career through writing. Grossman explains the conventions of nineteenth-century performance and the content of the plays, and gives the context of the larger culture, from transportation, political and economic concerns to medical practices. A few sections become slightly repetitious due to the arrangement of the book as both topical and chronological. Grossman has, however, done an admirable job of telling the story of an artist whose turbulent emotional life, captivating stage performances and vibrant writing deserve to be remembered as a compelling American story.", "title": "Furious Improvisations: How the WPA and a Cast of Thousands Made High Art out of Desperate Times. By Susan Quinn. New York: Walker & Company, 2008. Pp. 325 + 21 illus. $25.95 Hb.", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Gina Gwenffrewi"], "summary": "\n Within transgender studies, Jan Morris casts a problematic shadow, with Aren Aizura identifying how \u201cMorris's entire literary and historical oeuvre .\u00a0.\u00a0. [is] a tacit articulation of a British colonial ideology.\u201d Yet this position appears to be based on Morris's works between the 1950s and 1970s, up to and including her memoir Conundrum, and represents arguably only the first of three periods in Morris's writing. This essay argues that two subsequent periods diversify our understanding of Morris as a complex, transcultural figure: her broadly leftist, anticolonial writing on Wales and the Welsh language (1980s\u201390s), and then in the twenty-first century when Morris increasingly appears to question the colonial, nationalist, and cisheteropatriarchal ideologies that have shaped her previous writing. This essay concludes that Morris's body of work provides valuable evidence as to the complex interplay of Welsh, British, and European conceptions of gender that characterize her attitude and writing on transgender identity.", "title": "Hiraethi Jan Morris", "claims": null}, {"metadata": {"year": 2000}, "authors": ["D. Davis"], "summary": "In the introduction to a special issue of diacritics devoted to the work of Avital Ronell, Jonathan Culler writes that \"her books are like no others\"; that her sentences \"startle, irritate, illuminate\"; and that her work consti tutes \"one of the most remarkable critical oeuvres of our era.\" RonelPs writing is remarkable, in part, because of the unusual connections it makes, its determination to blur the distinctions between big thought and small talk, philosophy and rumor, literature and headline news?to blur, that is, the very divisions through which academia sustains itself. But Ronell's work is also remarkable in its style: her writing is characteristi cally tough, double entendre intended. It's difficult (because of its enormous scope and depth), and it's also gutsy, rough, edgy, and pushy, with a sort of streetwise candor. Indeed, Ronell herself identifies a kind of \"class struggle\" going down in her texts, a struggle involving her own various compulsions, denials, and voices?including the \"little hood lum,\" the \"high philosophical graduate student,\" and the \"more sophisti cated Parisian.\" Ronell notes, however, that the most discernible and continuous voice in her texts belongs to the \"wise-ass girl,\" an ancestor of the \"buffo\" and every bit an anacoluthon out to disrupt the \"smooth logic of accepted meaning or signification.\" This interruptive force? inasmuch as it does indeed \"startle, irritate, illuminate\"?takes a certain swipe at certitude, prompting rigorous hesitations that open the condi tions of possibility for what Ronell's works are always after: an ethics of decision in a postfoundational whirl(d). RonelPs rigorously deconstructive rereadings of everything?from the telephone, the television, and virtual reality to the Gulf War, AIDS, and Madame Bovary?take up that which has been \"marginalized, minoritized, evicted, persecuted, left out of the picture . . .feminized.\" Operating in the mode of \"irreverent reverence\" and in the service of a posthumanist ethical imperative, Ronell sets out to \"secure the space of academe as a sheltering place of unconditional hospitality for dissidence jac 20.2(2000)", "title": "Confessions of an Anacoluthon: Avital Ronell on Writing, Technology, Pedagogy, Politics.", "claims": null}, {"metadata": {"year": 2014}, "authors": ["D. Brydon"], "summary": "descriptive quality.' As a radio drama colunmist for the Winnipeg Citizen, she was critical of what she saw as Canada's 'inferiority complex' when it came to comparing Canadian radio to American programs, and called for a drama to be produced on the life of Louis Riel. Gerry'sdiscussionofLaurence's political writing between 1975 and 1986 provides a crucial reassessment of her life and writing 'after fiction' when, according to many, she was involved with apple pie and motherhood issues and tossed off 'easy' pieces for popular magazines that left readers lamenting the loss of her artistic skills. For Gerry, critics have wrongly separated Laurence's caring about violence against women, poverty, nuclear disarmament, and war from the moral imagination that had made her a great writer. Whatever the attributes of her fiction, he insists, it should not be valorized over other forms of her creative expression, and Laurence's 'political writing ought to be included as an integral part of her literary achievement.' Other essays in Challenging Territory are concerned with the strongly feminist aspects of Laurence's writing, and with such specific topics as the complexities of Laurence's Scots Presbyterian heritage and mourning in A Bird in the House. Meira Cook, with her Kristevian analysis of the maternal in The Diviners, provides an important reconsideration of Morag's (and Laurence's) position as a writer at the end of this novel. What Cook skilfully leads us towards and states in her own final sentence connects profoundly to Gerry's insistence that Laurence went on, ever deeper into human life, after finishing The Diviners: 'Because she cannot simultaneously be and have the mother, Morag ends from the place where she has begun, and begins again as origin.' (J.A. WAINWRIGHT)", "title": "The Margin Speaks: A Study of Margaret Laurence and Robert Kroetsch from a Post-Colonial Point of View by Gunilla Florby (review)", "claims": null}, {"metadata": {"year": 2017}, "authors": ["C. Jones", "Lela Graybill"], "summary": "This thesis examines an understudied aspect of an extensively-studied movement: the role of women as artists and models in Pre-Raphaelite art. Focusing on a singular image of a popular Pre-Raphaelite model by a woman artist - Evelyn de Morgan's Portrait of Jane Morris (study for The Hourglass ) from 1904-I am to contribute to the growing scholarship surrounding the vastly underrepresented women artists of Pre-Raphaelitism, while also investigating their influences in the development and recognition of women artists, the feminine image in art, and gender roles and expectations up until the contemporary age. Evelyn de Morgan's Portrait of Jane Morris (study for The Hourglass ) presents a unique image of Morris in her later life, contrasting strongly with the radiant and stylized young woman that Dante Gabriel Rossetti had made iconic in the 1860s and 1870s. Here Jane Morris is presented with silvery grey hair pinned up to the nape of her neck, a sharp departure from the signature long billowing brown hair that formed a highlight of Rossetti's many well-known images of Morris. De Morgan further emphasizes signs of age in the crease of Morris's furrowed brow, the darkened circles under her eyes, and the broad width of her nose. Interestingly, none of these descriptors of age are carried over into the painting for which this image serves as a study, The Hourglass of 1905. By analyzing the singular qualities of de Morgan's approach to Morris as a model, and by situating those qualities in relationship to Pre-Raphaelite art and the complex place of women within that movement, I hope to challenge conventional narratives of Pre-Raphaelitism by re-asserting the specificity and diversity of women's contributions as both artists and models.", "title": "Evelyn de Morgan's Portrait of Jane Morris (study for The Hourglass) and the Politics of Gender in Pre-Raphaelite Art", "claims": null}, {"metadata": {"year": 2016}, "authors": ["M. F. Smith"], "summary": "ABSTRACT After Virginia Woolf's biography of Roger Fry was published in 1940, she received a letter from Mary Louisa Gordon strongly critical of her portrayal of Roger's wife, the artist Helen Coombe, and even more critical of Roger's character and conduct. Mary and Helen had been friends before the latter married in 1896 and went on to develop severe mental health problems. In 1936 the Woolfs had published Mary's historical novel, Chase of the Wild Goose, about the Ladies of Llangollen. The article is in four sections. Section 1 is introductory. Section 2 is about Mary. It discusses Chase of the Wild Goose, its relationship to Orlando, and Virginia's comments on it and its author, whom, in letters to Ethel Smyth, she calls \u201cthe Hermaphrodite\u201d. It goes on to describe Mary's life and career as medical doctor, suffragist, first female Inspector of Prisons in England and Wales, and scathing critic of the prison system. Section 3 presents Mary's letter to Virginia, with significant corrections of the text published by Beth Rigel Daugherty. Section 4 focuses on Helen, and on Mary's assessments of her and Roger.", "title": "Virginia Woolf and \u201cthe Hermaphrodite\u201d: A Feminist Fan of Orlando and Critic of Roger Fry", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Jack Orchard"], "summary": "This article is an analysis of contemporary critical approaches to the relationships between Dr Johnson and women, particularly with reference to The Rambler, followed by the introduction of previously unpublished letters which display a female reader of the periodical, Jemima Campbell, Marchioness Grey, choosing not to write for The Rambler and instead opting to produce a satirical attack on \u201cMr Rambler\u201d within the private sphere of a familiar letter to her friend Catherine Talbot. Talbot did write an essay for Johnson's periodical, and this article looks at the two documents as different case studies in responses to Johnson's moralizing persona. Essentially, criticism on The Rambler has undergone a shift from celebratory analysis of his positive and nuanced representations of his female characters and relationships with contemporary women writers, since James Basker and Isobel Grundy in the late 1980s and early 1990s, to a reappraisal of his relationships with women through looking at the responses of his readers gleaned from epistolary sources, as seen in Antony Lee's and Peter Sabor's recent essays on The Rambler. This article provides new material for that debate, as well as addressing the differences between public and private voices for eighteenth-century intellectual women.", "title": "Dr Johnson on Trial: Catherine Talbot and Jemima Grey Responding to Samuel Johnson's The Rambler", "claims": null}, {"metadata": {"year": 2013}, "authors": ["I. Haywood"], "summary": "visit with the editor of the Academy, Charles Appleton, in Oxford (75), and she examines George Augustus Simcox\u2019s review of Robert Lytton\u2019s work and Frederic Harrison\u2019s obituary of Lewes, both of which appeared in the Academy (107\u20138, 127). She identifies Nannie Smith and Isabella Blythe as journalists who contributed to the English Woman\u2019s Journal (93). In addition, McCormack makes interesting but rather random comments about several of Eliot\u2019s periodical articles, such as \u201cWoman in France: Madame de Sabl\u00e9,\u201d which was written for the Westminster Review in 1854 (16\u201317). McCormack also provides detailed commentary on Eliot\u2019s poem \u201cAgatha\u201d (48\u201363) and shares various incidents concerning The Spanish Gypsy, such as the fact that Cross\u2019s eldest sister sang a song from it during one of their visits to Weybridge (72, 138, 139) and that Lewes \u201cshamelessly\u201d publicized it (28, 61). However, in spite of the fact that Eliot and Lewes enjoyed the \u201cremoteness\u201d of the location at Bad Peterstal, which \u201cdistanced\u201d them from the reviews of The Spanish Gypsy (54), \u201cnews of the reception of George Eliot\u2019s major venture into poetry came as if swirling up the deep valley of the River Rench as Lewes and Blackwood corresponded about the questionable reviews\u201d (55). In spite of McCormack\u2019s extensive research and many insightful observations, her informalities\u2014like \u201cswirling up\u201d\u2014are rather jarring. A few of the numerous examples include the observation that women who appeared at the Priory were \u201ca mixed bag\u201d (7); Lewes \u201ctrotted out\u201d some of his liveliest prose in his notes of invitation (8); Haight ignores \u201ca slew\u201d of the regular visitors (8), even though he \u201cscatters a few lists of guests here and there\u201d (9); Buchanan \u201clets loose\u201d with criticism (9); and on one Sunday, Lewes \u201cwailed in his diary\u201d that \u201cNo one called!\u201d (66). However, even if her narrative is rather uneven, her tone throughout is enthusiastic and energetic, and her observations are definitely thought-provoking.", "title": "Slow Print: Literary Radicalism and Late Victorian Print Culture by Elizabeth Carolyn Miller (review)", "claims": null}], "query": "criticism of Rona moss-morris", "summary_abstract": "The criticism of Rona Moss-Morris, while not directly addressed in the provided abstracts, can be inferred through the broader context of critiques surrounding figures in literature and performance, particularly in relation to their societal roles and the complexities of their identities.\n\nSaal (2010) presents a nuanced view of Jane Morris, highlighting how her performances were often a reflection of her tumultuous life, suggesting that her emotional depth in roles like Lady Macbeth was both praised and criticized. This duality in reception can parallel the scrutiny faced by Moss-Morris, as both figures navigated the expectations and judgments of their respective audiences.\n\nGwenffrewi (2021) discusses Jan Morris's literary contributions and the criticisms they have faced, particularly regarding colonial ideologies. This critique of Morris's work suggests a broader trend in which authors and artists are often evaluated through the lens of their historical and cultural contexts, which could similarly apply to Moss-Morris's work and the potential criticisms it may attract.\n\nDavis (2000) emphasizes the unique style and critical approach of Avital Ronell, noting how her work challenges established norms and provokes thought. This notion of challenging the status quo resonates with the potential criticisms of Moss-Morris, as her work may also confront traditional narratives and provoke varied responses.\n\nOverall, while the abstracts do not provide direct criticism of Rona Moss-Morris, they collectively illustrate a landscape where artists and writers are often critiqued for their societal roles, the complexities of their identities, and the cultural implications of their work. This context can help frame any criticisms directed at Moss-Morris, suggesting that her contributions may similarly evoke a spectrum of responses based on the interplay of personal and societal narratives.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2007}, "authors": ["Z. F. Arat", "R. Falk"], "summary": "Turkey's mixed human rights record has been highly politicized in the debate surrounding the country's probable ascendance to membership in the European Union. Beginning with the foundation of a secular republic in 1923, and continuing with founding membership in the United Nations and participation in the European Convention on Human Rights and Fundamental Freedoms, Turkey made significant commitments to the advancement of human rights. However, its authoritarian tradition, periods of military rule, increasing social inequality, and economic crises have led to policies that undermine human rights. While legislative reforms and civil social activism since the 1980s have contributed greatly to the advancement of human rights, recent progress is threatened by the rise of nationalism, persistent gender inequality, and economic hardship. In Human Rights in Turkey, twenty-one Turkish and international scholars from various disciplines examine human rights policies and conditions since the 1920s, at the intersection of domestic and international politics, as they relate to all spheres of life in Turkey. A wide range of rights, such as freedom of the press and religion, minority, women's, and workers' rights, and the right to education, are examined in the context of the history and current conditions of the Republic of Turkey. In light of the events of September 11, 2001, and subsequent developments in the Middle East, recent proposals about modeling other Muslim countries after Turkey add urgency to an in-depth study of Turkish politics and the causal links with human rights. The scholarship presented in Human Rights in Turkey holds significant implications for the study of human rights in the Middle East and around the globe.", "title": "Human Rights in Turkey", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Kenan \u00c7ay\u0131r", "Melike T\u00fcrkan Ba\u011fl\u0131"], "summary": "The incorporation of compulsory courses on human rights into the secondary school curriculum in 1998 has been an important first step in developing respect for human rights and responsibilities among the younger generation in Turkey. Yet, these courses have many shortcomings in terms of materials, pedagogy and teacher attitudes. This paper explores Grades 7 and 8 (ages 13 and 14) students\u2019 experiences in Citizenship and Human Rights Education courses on the basis of qualitative data collected through focus group discussions in Ankara and Istanbul in the 2006\u20132007 academic year. The responses of the students indicate that these courses have had little impact in empowering students or in facilitating them to consider their own or others\u2019 human rights as an integral part of their lives. Rather, the students perceive the national and the global arena as characterized by mass human rights violations against which they feel powerless. The paper draws attention to the importance of a revised human rights education for students along with a global focus and appropriate methodology.", "title": "\u2018No\u2010one respects them anyway\u2019: secondary school students\u2019 perceptions of human rights education in Turkey", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Tu\u011fba  Asrak-Hasdemir"], "summary": "In Turkey, some important steps have been taken to incorporate the notion and the teaching of human rights and freedom into the educational system. With the wave raised by the United Nations declaration of the Decade for Human Rights Education for the 1995\u20132004 period and also the curriculum studies conducted by the United Nations Educational, Scientific and Cultural Organization (UNESCO) in this context, the efforts to teach human rights, especially in the primary schools, gained momentum at the end of the 1990s in Turkey. Therefore, a national committee was formed and a national action plan was prepared to teach human rights issues across the country. The curriculum has since been modified. The course on citizenship and human rights became part of the educational system in 1998 as a required course in primary schools. Since 1999, another course titled Democracy and Human Rights Education has been taught in high schools as an elective course. However, this situation changed in 2005. Although the nature and importance of human rights and democracy has been frequently emphasized by the leaders and the cadre of the Justice and Development Party (AKP), the Ministry of National Education (MoNE) has removed the Citizenship and Human Rights Education course from the primary school curriculum and determined that the subjects and themes related to human rights and freedoms would be taught in other courses.", "title": "Fights over Human Rights: A Strange Story of Citizenship and Democracy Education", "claims": null}, {"metadata": {"year": 1999}, "authors": ["Arif T. Payaslyoglu", "A. I\u00e7duygu"], "summary": "Human rights have a high place on the agenda of the world today. In the eyes of their defenders they are a sine qua non for the peace and the welfare of mankind and for democratic ideals. On the other hand the human rights records of many countries are still very poor and in some cases even scandalous. Thus the promotion and the reinforcement of human rights, their propagation, cultivation, and protection everywhere, depend upon the ceaseless efforts of all those concerned, including both official authorities and civil societies alike. As part of these efforts, studies on the attitudes of strategic social groups such as youth, women, and minorities with regard to human rights may provide some useful information and clues for both theoretical understanding and practical, preventive, and corrective purposes.' In view of this, the present study attempts to uncover certain facts concerning the awareness of and the support for human rights in one such strategic group in Turkey: university students.2", "title": "Awareness of and Support for Human Rights Among Turkish University Students", "claims": null}, {"metadata": {"year": 2007}, "authors": ["Ayla Oktay", "\u00d6zg\u00fcl Polat Unutkan"], "summary": "Democracy has a crucial role in tfle education process of future generations and human rights education constitutes one of the main elements of democracy education. The education programs on human rights depicts differences among countries. As a result of this, the global human rights values are hardly accepted by the citizens of different countries. Despite the main target of human rights education concentrates on individuals, limited common activities in this area decreases the possibility of a common approach. Moreover, the differences in internalisation of universal thought on human rights not only negatively affect the unique view provided by globalisation, but also cause inequity. Examining various applications of human rights education will also be helpful for the proper implementation of Universal Declaration of Human Rights. In this paper, the historical evolution of human rights education in Turkey is examined as well as different other human rights education applications in other countries. This paper, therefore, provides a comparative analysis of human rights education and evalutes Turkey's situation in the light of human right education. Keywords: Human rights education, European Convention on Human Rights, Council of Europe", "title": "\u0130NSAN HAKLARI E\u011e\u0130T\u0130M\u0130: KAR\u015eILA\u015eTIRMALI B\u0130R DE\u011eERLEND\u0130RME", "claims": null}, {"metadata": {"year": 2004}, "authors": ["K. Selvi"], "summary": "In this study, work into about human rights done in last 50 years is analyzed. The study consists of three parts. In the first part, international meetings and decisions made regarding education on the subject of human rights are analyzed. Second, the subject of human rights education itself is analyzed in terms of formal and informal education curriculums in the world and in Turkey. Last, proposals for teacher training programs are discussed with regard to teacher-training faculties.", "title": "Human Rights Education in the World and in Turkey D\u00fcnyada ve T\u00fcrkiye'de \u0130nsan Haklari E\u011fitimi", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Abdulkerim \u015een"], "summary": "This study aims to contribute to Turkish literature by compiling perspectives about human rights, children\u2019s rights and human rights education that have wide recognition in English-spoken academic world. To this end, conceptual and theoretical clarifications are presented rather than holding a discussion around a particular argument. In the first part, the defining features of human rights are introduced, then some criticisms that are mounted against human rights are outlined. Next section gives information about the development, conceptualization, classification and current problems of human rights education, then moves to present the historical development of children\u2019s rights and educational implications of the Convention on the Rights of the Child. In this part, a checklist which was developed to evaluate educational conditions regarding children\u2019s rights and identify negligence and violations, if there is any, is presented. By introducing a model which aims to improve the quality of children\u2019s participation, what is exactly meant by children\u2019s participation, what can be considered as participation and what cannot be regarded as such are clarified. The final part informs briefly about Turkey\u2019s human rights education reforms and children\u2019s rights issues by drawing on the reports of the United Nations Committee on the Rights of the Child. In addition, some solutions are suggested by relying on transformative human rights education approach which has a concern to strengthen human rights education\u2019s link to social realities and social transformation. Compiled perspectives about human rights education and children\u2019s rights can contribute to the theoretical and conceptual framework of studies to be conducted by Turkish-reading researchers.", "title": "\u0130nsan Haklar\u0131, \u00c7ocuk Haklar\u0131 ve \u0130nsan Haklar\u0131 E\u011fitimi", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Abdulkerim \u015een"], "summary": "ABSTRACT Drawing on a critical conceptualisation of human rights education (HRE), this study investigates how human rights were presented in a set of textbooks which were used in Turkey from 1950 to 2020. Findings show that the textbooks avoid human rights issues, struggles, campaigns, and activists by sustaining a de-politicised and de-contextualised narrative. They do not even refer to the Holocaust in explaining the historical development of human rights. These findings suggest that HRE gained a curricular status as a matter of window dressing, an \u2018escapist\u2019 model developed as a result. This model is characterised by a deliberate avoidance of anything that may be considered as \u2018political\u2019. This study concludes that placing socio-political issues at the centre of HRE and encouraging learners to critically reflect on discrepancies between claims and realities of human rights can improve the transformative powers of HRE in nationalist contexts with no strong tradition of rights struggle.", "title": "How are human rights presented in Turkey\u2019s textbooks? Development of an escapist model of human rights education (1950-2020)", "claims": null}], "query": "Using the recent and updated references, What is the current state of human rights education in Turkey?", "summary_abstract": "The current state of human rights education in Turkey reflects a complex interplay of historical developments, legislative changes, and ongoing challenges. Since the introduction of compulsory human rights courses in secondary education in 1998, there have been significant efforts to integrate human rights education into the national curriculum. However, these initiatives have faced numerous obstacles, including inadequate materials, ineffective pedagogy, and a lack of empowerment among students (\u00c7ay\u0131r & Ba\u011fl\u0131, 2011).\n\nDespite initial progress, the situation has evolved unfavorably in recent years. The Citizenship and Human Rights Education course was removed from the primary school curriculum in 2005, with human rights topics relegated to other subjects, indicating a regression in the commitment to human rights education (Asrak-Hasdemir, 2012). This shift has raised concerns about the depth and effectiveness of human rights education, as students often perceive human rights violations as overwhelming and feel powerless to effect change (\u00c7ay\u0131r & Ba\u011fl\u0131, 2011).\n\nMoreover, a critical analysis of textbooks used in Turkey reveals a tendency to present human rights in a de-politicized manner, avoiding discussions of significant historical events and struggles related to human rights (\u015een, 2021). This approach undermines the potential for transformative human rights education, which should encourage critical reflection on the realities of human rights issues (\u015een, 2021).\n\nIn summary, while Turkey has made strides in incorporating human rights education into its educational framework, recent developments suggest a troubling trend towards superficial engagement with the subject. The need for a more robust, contextually aware, and empowering approach to human rights education remains critical for fostering a culture of respect for human rights among future generations.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2010}, "authors": ["R. Hatano", "Takanobu Inoue", "U. Darung", "S. Limin", "T. Morishita", "F. Takaki", "Yo Toma", "H. Yamada", "R. Gilkes", "N. Prakongkep"], "summary": "Long term monitoring of CO2 emission from the peat of natural forest (NF), fire-affected re-growing forest (RF) and cropland (KV) in Plangka Raya, Central Kalimantan, Indonesia has been conducted since 2002. The results showed that the average annual CO2 emission was 14.8 tC/ha/y in NF, 9.2 tC/ha/y in RF and 25.5 tC/ha/y in KV. The tendency of the CO2 emission agreed with that of annual peat subsidence. Based on the data of the proportion of peat decomposition and root respiration in CO2 emission from peat, the value of peat decomposition per unit peat subsidence was estimated to be 4.3 tC/ha/cm in NF, 6.21 tC/ha/cm in RF and 5.91 tC/ha/cm in KV, and peat decomposition to peat subsidence accounted for 46 % in NF, 57 % in RF and 57 % in KV. These results suggest that peatland degradation accelerates peat decomposition. N2O emission exponentially increased with an increase of peat decomposition. The result of regression analysis using the data of CO2 emission in tropical peatland compiled by Hooijer et al. (2006) together with the data obtained in this study showed that peat decomposition was significantly increased by a fall of the ground water table. The slope of the regression equation for cropland was higher than that for other ecosystems (natural forest, re-growing forest and plantation).", "title": "Carbon dioxide and nitrous oxide emissions associated with tropical peatland degradation.", "claims": null}, {"metadata": {"year": 2010}, "authors": ["J. Jaenicke"], "summary": "Tropical peat swamp forests in Indonesia are highly threatened ecosystems. As a result of economic development during the past two decades, they have been subjected to intensive logging, drainage and conversion to plantation estates, especially for oil palm. The Indonesian peatlands are one of the largest near-surface reserves of terrestrial organic carbon. However, ongoing rapid peat decomposition due to drainage and attendant recurrent fires have recently caused the release of huge amounts of this carbon into the atmosphere as carbon dioxide. If these large emissions from degrading peatlands are taken into account, Indonesia is one of the largest emitters of CO2 worldwide. Within the context of the ongoing discussions concerning climate change, the importance of peatlands as carbon stores is increasingly recognised by the public, accompanied by a demand for conservation and restoration. Therefore, this thesis utilises innovative geospatial 3D modelling and remote sensing techniques to study the Indonesian peatlands with the overall aim of global climate change mitigation. Previous estimates of the total amount of carbon stored in the Indonesian peatlands could be improved by applying 3D modelling based on a combined analysis of satellite imagery and in situ peat thickness measurements. At least 55\u00b110 Gt of carbon are stored in Indonesia\u2019s peatlands. With this huge carbon storage and the current rate of degradation, the tropical peatlands of Indonesia have the power to negatively influence the global climate. Large-scale peatland restoration is needed to prevent further greenhouse gas emissions. This thesis shows that successful rewetting of a 590 km\u00b2 large area of drained peat swamp forest could result in mitigated emissions of 1.4-1.6 Mt CO2 yearly, and can be achieved with relatively little effort and at low costs. Multitemporal radar satellite imagery proved to be capable of monitoring the effect of hydrological restoration measures on peat soil moisture and groundwater levels in Central Kalimantan, Indonesia. Satellite remote sensing allows continuous large-scale tropical peatland monitoring, compared to only punctual, temporally limited field measurements. This is particularly important for initiatives aiming at carbon trading on the voluntary carbon market or under the REDD (Reducing Emissions from Deforestation and Degradation) mechanism, which both constitute significant financing schemes for conservation and rehabilitation of Indonesia\u2019s peatlands.", "title": "3D modelling and monitoring of Indonesian peatlands aiming at global climate change mitigation", "claims": null}, {"metadata": {"year": 2018}, "authors": ["L. Wijedasa", "S. Sloan", "S. Page", "G. Clements", "M. Lupascu", "T. Evans"], "summary": "Carbon emissions from drained peatlands converted to agriculture in South\u2010East Asia (i.e., Peninsular Malaysia, Sumatra and Borneo) are globally significant and increasing. Here, we map the growth of South\u2010East Asian peatland agriculture and estimate CO2 emissions due to peat drainage in relation to official land\u2010use plans with a focus on the reducing emissions from deforestation and degradation (REDD+)\u2010related Indonesian moratorium on granting new concession licences for industrial agriculture and logging. We find that, prior to 2010, 35% of South\u2010East Asian peatlands had been converted to agriculture, principally by smallholder farmers (15% of original peat extent) and industrial oil palm plantations (14%). These conversions resulted in 1.46\u20136.43 GtCO2 of emissions between 1990 and 2010. This legacy of historical clearances on deep\u2010peat areas will contribute 51% (4.43\u201311.45 GtCO2) of projected future peatland CO2 emissions over the period 2010\u20132130. In Indonesia, which hosts most of the region's peatland and where concession maps are publicly available, 70% of peatland conversion to agriculture occurred outside of known concessions for industrial plantation development, with smallholders accounting for 60% and industrial oil palm accounting for 34%. Of the remaining Indonesian peat swamp forest (PSF), 45% is not protected, and its conversion would amount to CO2 emissions equivalent to 0.7%\u20132.3% (5.14\u201314.93 Gt) of global fossil fuel and cement emissions released between 1990 and 2010. Of the peatland extent included in the moratorium, 48% was no longer forested, and of the PSF included, 40%\u201348% is likely to be affected by drainage impacts from agricultural areas and will emit CO2 over time. We suggest that recent legislation and policy in Indonesia could provide a means of meaningful emission reductions if focused on revised land\u2010use planning, PSF conservation both inside and outside agricultural concessions, and the development of agricultural practices based on rehabilitating peatland hydrological function.", "title": "Carbon emissions from South\u2010East Asian peatlands will increase despite emission\u2010reduction schemes", "claims": null}, {"metadata": {"year": 2011}, "authors": ["B. Saharjo"], "summary": "Saharjo BH (2011) Carbon baseline as limiting factor in managing environmental sound activities in peatland for reducing greenhouse gas emission. Biodiversitas 12: 182-186. The total carbon stock in Indonesia was estimated to be around 44.5 Gt or about 53.1% of the total carbon stock in tropical areas. Over 1990-2002, it was estimated that around 3.5 Gt of carbon was released in Sumatra and about 0.81-2.56 Gt was released in Central Kalimantan due to the 1997 fire alone. It was recognized that deforestation, high exploitation of peat and peat fire were behind the huge emissions of Greenhouse Gases in Indonesia. Results of a research conducted in Central Kalimantan peatland, showed that the total carbon stock at logged over area was estimated around 413.972 t ha-1 (0-30 cm depth of peat) and at burnt area was 411.349 t ha-1 (0-30 cm depth of peat). Meanwhile it had been well recognized that most of opened peatlands had been occupied by Acacia crassicarpa and oil palms. Research carried out in East Kalimantan showed that the carbon stock of 25 yearsold oil palm planted on mineral soil was about 180 t ha-1, which is less than that of carbon stock produced by peatland clearance. This indicated that although plants occupied peatland, high Greenhouse Gas emissions were still produced, meaning that global climate change would continue and created high risk impacts.Key words: carbon stock, peat, oil palm, emission, GHG.", "title": "Carbon baseline as limiting factor in managing environmental sound activities in peatland for reducing greenhouse gas emission", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Haemi Park", "W. Takeuchi", "K. Ichii"], "summary": "Peatland is a natural carbon reservoir in terrestrial ecosystem. The ground water table in peatland is a key factor for carbon exchange through soil decomposition or carbon sink. Especially, Indonesia has the biggest peatland area in Asia. This study focused on the carbon dioxide budget between emissions by ecosystem respiration including fire event and the absorption by photosynthesis. As the result, the annual average of net biome ecosystem carbon dioxide exchange during 12 years were reached to 195.03 MtC/yr. Over the three times of CO2 from fire emissions were emitted by ecosystem respiration from whole peatlands in Indonesia.", "title": "Estimation of Carbon Dioxide Budget From Peatland In Indonesia With Site-Level Validation", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Gusti Z. Anshari", "Evi  Gusmayanti", "M.  Afifudin", "Monika  Ruwaimana", "Lauren  Hendricks", "Daniel G. Gavin"], "summary": "Abstract Deforested, drained, and converted tropical peat forest is common in Southeast Asia, and these areas have become a major source of carbon (C) emissions. In this study, measurements of peat properties, i.e., peat thickness, organic matter (OM), bulk density (BD), total organic C (TOC), and total nitrogen (TN), were taken four years apart (2012\u20132016) at 24 sites in a previously deforested and drained tropical peat in West Kalimantan, Indonesia. We calculated that the average peat subsides at a rate of 3.8\u00a0\u00b1\u00a01.2\u00a0cm\u00a0yr\u22121. The estimated net change in C stock ranged from \u22128 to \u221241\u00a0Mg C ha\u22121 yr\u22121, with an average of \u221231\u00a0Mg C ha\u22121 yr\u22121. C loss varied by peat depth and land use. C loss was four times faster on shallow peat (50\u2013100\u00a0cm) than deep (>300\u00a0cm) peat sites. Fallow sites (bush fern and secondary forest) subsided and lost C faster than oil palm plantations, likely due to land-clearing fires. Percent C and C/N ratio declined significantly in the upper 150\u00a0cm of peat, especially in the fallow sites, indicating oxidation within the peat profile. No change occurred in bulk density, showing little effect of compaction, though the density profile migrated downward with subsidence. The current peat topography, modeled from satellite LiDAR data, reflects past peat-loss patterns and confirms the coring results. C loss in this region continues at a high rate since its original deforestation in the early 1970s. This study concludes that C loss due to anthropogenic disturbances on tropical peat is larger than the 2013 Tier 1-IPCC CO2 emission factor for Acacia and oil palms on drained tropical peats, which are 20 and 11\u00a0Mg CO2-eq ha\u22121 yr\u22121, respectively.", "title": "Carbon loss from a deforested and drained tropical peatland over four years as assessed from peat stratigraphy", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Mohammad Basyuni", "N. Sulistyono", "B. Slamet", "R. Wati"], "summary": "Forestry and peat land including land-based is one of the critical sectors in the inventory of CO2 emissions and mitigation efforts of climate change. The present study analyzed the land-use and land-cover changes between 2006 and 2012 in North Sumatra, Indonesia with emphasis to CO2 emissions. The land-use/land-cover consists of twenty-one classes. Redd Abacus software version 1.1.7 was used to measure carbon emission source as well as the predicted 2carbon dioxide emissions from 2006-2024. Results showed that historical emission (2006-2012) in this province, significant increases in the intensive land use namely dry land agriculture (109.65%), paddy field (16.23%) and estate plantation (15.11%). On the other hand, land-cover for forest decreased significantly: secondary dry land forest (7.60%), secondary mangrove forest (9.03%), secondary swamp forest (33.98%), and the largest one in the mixed dry land agriculture (79.96%). The results indicated that North Sumatra province is still a CO2 emitter, and the most important driver of emissions mostly derived from agricultural lands that contributed 2carbon dioxide emissions by 48.8%, changing from forest areas into degraded lands (classified as barren land and shrub) shared 30.6% and estate plantation of 22.4%. Mitigation actions to reduce carbon emissions was proposed such as strengthening the forest land, rehabilitation of degraded area, development and plantation forest, forest protection and forest fire control, and reforestation and conservation activity. These mitigation actions have been simulated to reduce 15% for forestry and 18% for peat land, respectively. This data is likely to contribute to the low emission development in North Sumatra.", "title": "Carbon dioxide emissions from forestry and peat land using land-use/land-cover changes in North Sumatra, Indonesia", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Ni\u2019matul  Khasanah", "Meine  van Noordwijk"], "summary": "Most attention in quantifying carbon dioxide (CO2) emissions from tropical peatlands has been on large-scale plantations (industrial timber, oil palm (Elaeis guinensis)), differing in drainage and land-use practices from those of smallholder farms. We measured subsidence and changes in bulk density and carbon organic content to calculate CO2 emissions over 2.5\u00a0years in a remnant logged-over forest and four dominant smallholder land-use types in Tanjung Jabung Barat District, Jambi Province, Sumatra, Indonesia: (1) simple rubber (Hevea brasiliensis) agroforest (>\u200930\u00a0years), (2) mixed coconut (Cocos nucifera) and coffee gardens (Coffea liberica) (>\u200940\u00a0years), (3) mixed betel nut (Areca catechu) and coffee gardens (>\u200920\u00a0years), and (4) oil palm plantation (1\u00a0year). We quantified changes in microtopography for each site for greater accuracy of subsidence estimates and tested the effects of nitrogen and phosphorus application. All sites had a fibric type of peat with depths of 50 to >\u2009100\u00a0cm. A recently established oil palm had the highest rate of peat subsidence and emission (4.7\u00a0cm\u00a0year\u22121 or 121\u00a0Mg\u00a0CO2\u00a0ha\u22121\u00a0year\u22121) while the remnant forest had the lowest (1.8\u00a0cm\u00a0year\u22121 or 40\u00a0Mg CO2\u00a0ha\u22121\u00a0year\u22121). Other land-use types subsided by 2\u20133\u00a0cm\u00a0year\u22121, emitting 70\u201385\u00a0Mg\u00a0CO2\u00a0ha\u22121\u00a0year\u22121. Fertilizer application did not have a consistent effect on inferred emissions. Additional emissions in the first years after drainage, despite groundwater tables of 40\u00a0cm, were of the order of belowground biomass of peat forest. Despite maintaining higher water tables, smallholder landscapes have CO2 emissions close to, but above, current IPCC defaults.", "title": "Subsidence and carbon dioxide emissions in a smallholder peatland mosaic in Sumatra, Indonesia", "claims": null}], "query": "What is the rate of CO2 emission in percentage per year from peatland deforestation and degradation in Indonesia?", "summary_abstract": "The research on CO2 emissions from peatland deforestation and degradation in Indonesia reveals significant annual emissions resulting from various land-use changes, particularly the conversion of peatlands to agriculture and plantations. \n\nHatano et al. (2010) reported an average annual CO2 emission of 25.5 tC/ha from cropland, indicating that peatland degradation accelerates CO2 emissions due to increased peat decomposition. Jaenicke (2010) emphasized that Indonesia's peatlands, which store approximately 55\u00b110 Gt of carbon, are major contributors to global CO2 emissions, particularly due to drainage and fires associated with agricultural conversion. \n\nWijedasa et al. (2018) highlighted that between 1990 and 2010, emissions from peatland conversion to agriculture in Southeast Asia ranged from 1.46 to 6.43 GtCO2, with Indonesia being a significant contributor. They estimated that the ongoing degradation could lead to future emissions of 4.43 to 11.45 GtCO2 from historical clearances. \n\nSaharjo (2011) noted that from 1990 to 2002, emissions from peatland fires and deforestation in Sumatra and Central Kalimantan were substantial, with figures reaching up to 3.5 Gt of carbon. Park et al. (2019) calculated an average net biome carbon dioxide exchange of 195.03 MtC/year from Indonesian peatlands, underscoring the high emissions from ecosystem respiration.\n\nAnshari et al. (2022) found that carbon loss in deforested and drained peat areas averaged 31 Mg C ha\u22121 year\u22121, with emissions varying significantly based on peat depth and land use. They noted that shallow peat areas experienced carbon loss at four times the rate of deeper peat. \n\nBasyuni et al. (2018) reported that agricultural land use in North Sumatra contributed significantly to CO2 emissions, with agriculture accounting for 48.8% of emissions, while Khasanah and van Noordwijk (2018) found that newly established oil palm plantations had the highest emissions, at 121 Mg CO2 ha\u22121 year\u22121.\n\nCollectively, these studies indicate that Indonesia's peatland deforestation and degradation contribute significantly to global CO2 emissions, with rates varying by land use and peat depth, and highlight the urgent need for conservation and restoration efforts to mitigate these emissions.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2006}, "authors": ["Shannon Toney Smith", "L. T. Ross"], "summary": "Environmental and family factors related to racism in college students were investigated. Students (N=114) at a southern liberal-arts college filled out an extensive survey about childhood activities and family of origin, and 40 of their mothers completed a similar survey. The Attitudes Toward Blacks (Brigham, 1993) scale measured racism. Several hypotheses were supported. Students who exhibited less racism also reported that they made more diverse friends in school, came from a more diverse hometown, had diverse encounters and friendships at an earlier age, and had more positive foreign travel experiences. In addition, more racist students perceived their mothers as being more prejudiced while they were growing up, and they perceived their fathers as being more prejudiced currently. Finally, students judged their mothers as less racist than their fathers, and also reported that their mothers exhibited less racist behaviors than did their fathers. The importance of racially diverse experiences and friendships early in life are discussed as means to decrease racism.", "title": "Environmental and Family Associations With Racism1", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Stacey  Sinclair", "Elizabeth W. Dunn", "Brian S. Lowery"], "summary": "Abstract Although many researchers assume that implicit racial attitudes develop via exposure to prejudicial socializing agents (e.g., parents, peers, and the media) starting in childhood, there is a dearth of research on implicit attitudes in children. This study looks at the effect of one socializing agent\u2014parents\u2014on children\u2019s or implicit racial prejudice. Specifically, we examine Allport\u2019s (1954) contention that children\u2019s identification with their parents moderates the intergenerational transmission of prejudice. Fourth- and fifth-grade children completed measures of implicit and explicit pro-White/anti-Black prejudice, as well as a survey assessing child\u2013parent identification. Parents completed a survey that measured their attitudes toward Blacks. As hypothesized, we found greater correspondence between parents\u2019 prejudice and children\u2019s prejudice among children who were highly identified with their parents than less identified children.", "title": "The relationship between parental racial attitudes and children\u2019s implicit prejudice", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Marisa Franco", "R. Katz", "J. Pickens", "D. Brunsma"], "summary": "This qualitative study examined instances of racial discrimination from family members among 36 Black/White Multiracial individuals. Forms of discrimination included stereotyping, identity invalidation, racist comments, lack of acknowledgment, vicarious discrimination, and negativity. Participants responded to discrimination by distancing themselves from family members, resisting, and/or dismissing discrimination. Discriminatory experiences made race more salient for participants. Participants reported feeling hurt, alienated, and confused following discrimination. Implications of findings for social workers counseling Multiracial people and interracial families are discussed.", "title": "From my own flesh and blood: An exploratory examination of discrimination from family for Black/White Multiracial people", "claims": null}, {"metadata": {"year": 2013}, "authors": ["E. Martins", "Aparecida das Gra\u00e7as Geraldo"], "summary": "From a qualitative perspective, this paper discusses the family influence in the schooling process of black students considering the memories of undergraduate black students. The analysis of student\u2019s reports enables reflections about the family\u2019s influence in the disruption of school exclusion, as well as fighting racial discrimination that occurs in the interactions of black students within the school. The research also provides evidences that can contribute to the discussion of ethnic-racial relations in the questioning of humiliating situations and racism hidden and silenced by the school and family.", "title": "The Family Influence in the Education and in the Process of Overcoming Racial Prejudice: a study with black undergraduate students", "claims": null}, {"metadata": {"year": 1985}, "authors": ["D. Ward"], "summary": "beliefs. The study is based on in-depth interviews with the adult children of the men first interviewed by Robert E. Lane for Political Ideology (1962). While little evidence pointed to the direct imprinting of specific racial beliefs on the cognitive level, there did seem to be a transfer of underlying attitudes on the affective level. The children with fathers who opposed miscegenation tended to be the people who opposed quotas, affirmative action, and bussing. It is argued that the familial atmosphere produced a reservoir of racism beneath the surface of otherwise tolerant beliefs. The fathers' opposition to miscegenation may have established a negative affective evalution of blacks within the second generation which remained even after schools, peers, the media and other socializing agents fundamentally transformed the cognitive constructions through which opinions about race relations were expressed. Issues laden with racial symbolism would then activate the negative affect, producing opposition to policies designed to achieve equality between races.", "title": "Generations and the Expression of Symbolic Racism", "claims": null}, {"metadata": {"year": 2019}, "authors": ["B. Kurtz-Costes", "T. Hudgens", "Olivenne D. Skinner", "Elizabeth A. Adams", "S. Rowley"], "summary": "Abstract:We investigated relations among African American parents' reports of ethnic\u2013racial socialization received during their own childhood, their racial attitudes (i.e., perceptions of U.S. racial climate and valuing of intergroup contact), and their 12th-grade children's reports of parents' use of racial pride socialization and preparation for bias. Parents' (N = 193) reports of received racial pride and preparation for bias socialization were related to their children's reports of parents' current use of those types of socialization. The parents' received preparation for bias was positively related to their perceptions of a hostile U.S. racial climate, and parents' perceptions of racial climate predicted adolescents' reports of their parents' use of preparation for bias. Valuing of intergroup contact was unrelated to adolescent reports of either type of racial socialization. Findings support the intergenerational transmission of ethnic\u2013racial socialization in African American families and show that parents' attitudes and behaviors are related in theoretically predicted ways.", "title": "Parents' Racial Beliefs and Ethnic\u2013Racial Socialization in African American Families", "claims": null}, {"metadata": {"year": 2013}, "authors": ["S. French", "B. R. Coleman"], "summary": "Racial ideology shapes how African Americans interpret the world and cope with race-related issues and events. A complex interaction of individual, family, and group experiences in one\u2019s sociocultural and historical context is critical to the development of racial ideology. While parental racial socialization has been associated with racial identity, the more specific relationship between parental racial socialization and racial ideology is less understood. The present study examined the association of four dimensions of racial socialization messages reported by participants (cultural socialization, preparation for bias, promotion of mistrust, and egalitarian socialization) with four dimensions of racial ideology (assimilationist, humanist, oppressed minority, and nationalist) using hierarchical linear regression (n = 89 African American college students). Participants\u2019 endorsement of assimilationist and oppressed minority ideologies was not predicted by racial socialization messages. However, endorsement of humanist and nationalist ideologies was predicted by cultural and egalitarian socialization messages in opposing directions. Cultural socialization was associated with endorsement of higher nationalist and lower humanist ideology, whereas egalitarian socialization was associated with the endorsement of lower nationalist and higher humanist ideology.", "title": "The Relation Between Parental Racial Socialization and African Americans\u2019 Racial Ideologies", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Shard\u00e9 McNeil Smith", "Jamila E. Reynolds", "F. Fincham", "S. Beach"], "summary": "OBJECTIVES\nParents experiencing racial discrimination are likely to transmit racial socialization messages to their children to protect them from future injustices. This study was conducted to better understand the role of parents' racial discrimination in their racial socialization practices for 2-parent African American families.\n\n\nMETHOD\nUsing a sample from the Promoting Strong African American Families (N = 322 couples) program, we examined the effects of experienced discrimination on one's own and one's partner's racial socialization practices with male (n = 154) and female (n = 168) offspring.\n\n\nRESULTS\nMultiple-group actor-partner interdependence models showed that racial discrimination was associated with racial socialization practices. In addition, maternal experiences of discrimination had stronger relations to socialization messages relayed to daughters and greater paternal experiences of discrimination had stronger relations to socialization messages given to sons.\n\n\nCONCLUSIONS\nThis study demonstrates variability in how male and female children in African American families are socialized as a result of their parents' experiences with racial discrimination.", "title": "Parental experiences of racial discrimination and youth racial socialization in two-parent African American families.", "claims": null}], "query": "Summarize research on impact of family members on racial attitudes during the civil rights movement", "summary_abstract": "The research on the impact of family members on racial attitudes during the civil rights movement reveals a complex interplay between familial influences and the development of racial attitudes in children. Several studies highlight how parental attitudes and experiences shape children's perceptions of race and racism.\n\nSmith and Ross (2006) found that college students who exhibited less racism often came from diverse backgrounds and had positive early experiences with diversity, suggesting that family environments that promote diversity can mitigate racist attitudes. Conversely, students who perceived their parents, particularly fathers, as prejudiced were more likely to exhibit racist attitudes themselves.\n\nSinclair et al. (2005) explored the intergenerational transmission of implicit racial attitudes, demonstrating that children who strongly identified with their parents were more likely to adopt their parents' prejudices. This underscores the significant role of parental influence in shaping children's racial attitudes from a young age.\n\nFranco et al. (2018) examined the experiences of multiracial individuals facing discrimination from family members, revealing that such experiences can lead to feelings of alienation and confusion. This indicates that family dynamics can both reinforce and challenge racial identities and attitudes.\n\nKurtz-Costes et al. (2019) focused on African American families, showing that parents' own experiences with racial socialization significantly influenced their children's understanding of race and preparation for bias. This study highlights the importance of parental guidance in navigating racial issues.\n\nFrench and Coleman (2013) investigated how different dimensions of racial socialization messages from parents relate to the development of racial ideologies in African American youth. They found that cultural and egalitarian socialization messages had opposing effects on the endorsement of nationalist and humanist ideologies, illustrating the nuanced ways family influences can shape racial beliefs.\n\nLastly, Smith et al. (2016) demonstrated that parents' experiences with racial discrimination directly affected their racial socialization practices, with variations based on the gender of the child. This suggests that family members not only transmit attitudes but also adapt their socialization strategies based on their own experiences and the needs of their children.\n\nCollectively, these studies emphasize that family members play a crucial role in shaping racial attitudes, with both direct and indirect influences stemming from parental beliefs, experiences, and socialization practices. The findings suggest that fostering diverse experiences and positive racial socialization within families can be vital in combating racism and promoting healthier racial attitudes in future generations.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2012}, "authors": ["Esteban Luis", "F. Bustos", "D\u00edaz Su\u00e1rez", "Contreras", "Jord\u00e1n"], "summary": "The purpose of this paper was to identify and compare the perception of those behaviours that disturb the environment in physical education classes among students and teachers in the province of Toledo. For this, 86 teachers and 447 students in compulsory secondary education completed the Spanish adaptation of a survey that describes 59 misbehaviours which may arise in classes from the mildest to the most serious. The analysis of variance of the results suggests clear discrepancies in the perception of students and teachers regarding the frequency with which the two groups identify misbehaviour in class. The Rev.int.med.cienc.act.f\u00eds.deportevol.12 n\u00famero 47 ISSN: 1577-0354 460 descriptive findings also reveal that \"distracting or disturbing others\" dominates the learning environment in class, and that both groups identify mild and moderate misbehaviour as the most frequent form of bad behaviour in class.", "title": "BEHAVIOURS THAT DISTURB THE EDUCATIONAL ENVIRONMENT IN PHYSICAL EDUCATION CLASSES LAS CONDUCTAS QUE ALTERAN LA CONVIVENCIA EN LAS CLASES DE EDUCACIO\u0301N FI\u0301SICA", "claims": null}, {"metadata": {"year": 2020}, "authors": ["R. Navarro-Pat\u00f3n", "M. Mec\u00edas-Calvo", "C. Gili-Roig", "J. E. Rodr\u00edguez-Fern\u00e1ndez"], "summary": "The aim of this research was determining disruptive behaviours of Primary and Secondary Schoolchildren in physical education classes according to educational stage, age and gender. The sample consisted of a total of 1304 students from 10 to 16 years old (M = 12.77; SD = 1.89), of whom 548 were Primary Education and 756 of Secondary Education; 612 were boys (46.9%) and 692 were girls (53.1%). The data collection was conducted through to the appropriate conduct scale in physical education and sport (CCDEF). The results obtained showed significant statistical differences in the age factor on Aggressiveness (AGR) (p < .001), Irresponsibility (IRRP) (p < .001), Fails to follow directions (FFD) (p < .001), Distracts or disturbs others (DDO) (p < .001) and Poor self-management (PSM) (p < .001). Regarding gender, there were statistically significant differences between boys and girls in FFD and on PSM (p < .001). Also, significant statistical differences were found on stage factor on AGR (p < .001), IRRP (p < .001), FFD (p < .001), DDO (p < .001) and PSM (p < .001). Disruptive behaviours scores increase with age and are higher in Compulsory Secondary Education than in Primary education schoolchildren. Boys in Primary Education scores higher than girls in all dimensions studied, while boys in Compulsory Secondary Education do so on aggressiveness.", "title": "Disruptive behaviours in Physical Education classes: A descriptive research in compulsory education", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Talel Maddeh", "N. Bennour", "N. Souissi"], "summary": "This article aims at describing and analyzing students\u2019 disruptive behaviors and comparing them in terms of their degree of intensity in the course of physical education lessons performed by student teachers. The delayed video-scopic analysis was performed using the \u201cDisciplinary Incidents Analysis System\u201d. A total of 1900 disruptive behaviors (DB) were recorded including 1379 seen by student teachers and 521 other behaviors not seen. Results testify that the observed physical education lessons show a very high degree of disruptions with a mean reaching 67.8 DB per session, i.e. 1.2 DB per minute. Besides, a high proportion of DB with a low density (43.7%) and a moderate density (46.7%) was recorded in the classes under investigation. These results presupposed that the learning conditions that prevailed during the observed sessions were not optimal to invite us, consequently, to conduct further investigations in order to determine their reasons.", "title": "Study of Students\u2019 Disruptive Behavior in High School Education in Physical Education Classes", "claims": null}, {"metadata": {"year": 2022}, "authors": ["R. Navarro-Pat\u00f3n", "M. Mec\u00edas-Calvo", "Ra\u00fal Eir\u00edn-Nemi\u00f1a", "V. Arufe-Gir\u00e1ldez"], "summary": "Disruptive behaviors in physical education cause conflicts among students and, consequently, an abnormal development of classes. Therefore, finding the variables that can solve them is an urgent aspect to achieve an adequate learning environment in the 21st century school. The aim of this study was to analyze what happens to disruptive behaviors in relation to systematic and regulated sports and social practice in a sample of Spanish primary school students. Five hundred and forty-eight schoolchildren (276 were girls (50.4%)) participated with a mean age of 10.98 (SD = 0.71). The results show a significant main effect in terms of social skills in relatedness (p < 0.001), irresponsibility (p < 0.001), failure to follow directions (p < 0.001), distracting or disturbing others (p < 0.001), and in poor self-management (p < 0.001) with higher scores in disruptive behaviors in students with lower social skills. Regarding sports practice, only a significant main effect was found in relatedness (p < 0.001) and in poor self-management (p < 0.001), with the highest scores the schoolchildren who do not practice sports. Schoolchildren with high social skills obtain lower scores in disruptive behaviors. Likewise, schoolchildren who play sports have lower scores in relatedness and poor self-management.", "title": "Disruptive Behaviors in Physical Education: A Matched Study of Social Skills and Sport Practice in a Region of Spain", "claims": null}, {"metadata": {"year": 2006}, "authors": ["P. Kulinna", "D. Cothran", "Rey Regualos"], "summary": "Classroom management, and, more specifically, the management of student behavior, is an essential skill for and central concern of physical education teachers. Surprisingly little information is known, however, about the specific nature of student behaviors that might negatively affect the class environment. The purpose of this study was to examine teachers' reports of student behaviors that may create management issues in physical education classes. Over 300 teachers representing a range of grade levels and school contexts completed a survey related to the frequency of potentially negative student behaviors. Many types of misbehaviors were reported, but the most common were mild. Teachers' reports varied by sex, years of teaching experience, and school context.", "title": "Teachers' Reports of Student Misbehavior in Physical Education", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Jos\u00e9  L\u00f3pez Jim\u00e9nez", "Alfonso  Valero-Valenzuela", "M. Teresa Anguera", "Arturo  D\u00edaz Su\u00e1rez"], "summary": "The aim of this study was to determine which disruptive behaviors occur most often in physical education (PE) classes, and to identify the existence of a pattern of behavior that leads to this disruptive behavior. With this in mind, we analyzed five PE sessions taken by pupils at different elementary school in the region of Murcia. The total sample size was 96 students aged between 10 and 13. Data was recorded using an observation instrument (a combination of a field format and a categorical system) and was then analyzed using the \u201cHOISAN\u201d software tool, with a sequential analysis and polar coordinates being conducted. The results of the study revealed that disruptive behaviors (52\u00a0%) occur more frequently than non-relevant behaviors (48\u00a0%), the most common of them being disinterested behavior (29\u00a0%), followed by indiscipline (15\u00a0%), with no statistically significant differences being detected in violent behavior. As regards patterns of behavior, disinterested behavior is stimulated by \u201cno eye contact\u201d, \u201cmiddle distance\u201d, \u201cinside the task\u201d, \u201cno use of material\u201d, \u201cgiving orders\u201d and \u201cregistering of activities\u201d, while indiscipline is stimulated by \u201cno eye contact\u201d, \u201cfar distance\u201d, \u201coutside the task\u201d, \u201cuse of material\u201d, \u201cgrouping in pairs\u201d and \u201cpreparation of material\u201d. In conclusion, it can be stated that disruptiveness is far more common in physical education sessions, affects the development of sessions and has a negative impact on student learning. A solution to this problem should therefore be sought immediately in order to ensure quality education.", "title": "Disruptive behavior among elementary students in physical education", "claims": null}, {"metadata": {"year": 2003}, "authors": ["P. Kulinna", "D. Cothran", "Rey Regualos"], "summary": "The study was designed to develop an instrument to examine students' reports of the type and frequency of various student behaviors in physical education classes that might disrupt classroom management. This study had multiple phases, including instrument development, preliminary studies, content-related evidence study, and an exploratory factor analysis and internal consistency reliability study. Participants for the content-related evidence study were 27 experts in physical education pedagogy. They were sent items from the student disruptive behavior instrument and asked to organize them into 3 management severity categories. Participants for the factor analysis and internal consistency reliability study were 2,309 middle and high school students from 2 states. The following 6 factors emerged from the exploratory factor analysis: (a) aggressive, (b) low engagement or irresponsibility, (c) fails to follow directions, (d) illegal or harmful, (e) distracts or disturbs others, and (f) poor self-management. Alpha reliability coefficients for the factors were mostly above .70 and the rotated factor loadings for the items ranged from .26 to .73. Investigating students' reports of the type and frequency of potentially troublesome behaviors may lead to a better understanding of student disruptive behaviors in physical education contexts.", "title": "Development of an Instrument to Measure Student Disruptive Behavior", "claims": null}, {"metadata": {"year": 2019}, "authors": ["A. Granero-Gallegos", "P. Ruiz-Montero", "A. Baena-Extremera", "M. Mart\u00ednez-Molina"], "summary": "Currently, disruptive and aggressive behaviours of a physical and verbal nature are a reality among adolescent students and a concern in the educational context. Therefore, the main objective of this research was to analyse the effects of perceived teaching competence, motivation and basic psychological needs on disruptive behaviours in secondary school PE students. The sample was composed of 758 adolescent students from seven public secondary schools. The following instruments adapted to physical education were used: The Disruptive Behaviours Questionnaire, The Evaluation of Teaching Competencies Scale, The Sport Motivation Scale, and The Basic Psychological Needs Scale. Multilevel regression models with the MIXED procedure were performed for data analysis. The results show that misbehaviour is more likely among male students and that disruptive behaviours decrease when a teacher is perceived as competent. Students with greater self-determined motivation are more likely to exhibit fewer behaviours related to low engagement and irresponsibility while amotivation increases the different disruptive behaviours in the classroom. In conclusion, it is proposed that educators work in line with the students\u2019 needs by responding to their interests and that this will increase self-determined motivation.", "title": "Effects of Motivation, Basic Psychological Needs, and Teaching Competence on Disruptive Behaviours in Secondary School Physical Education Students", "claims": null}], "query": "disruptive behavior OR disruption OR aggressive behavior OR misbehavior OR distracting behavior OR troublesome behavior OR violance OR classroom misbehavior AND physical education", "summary_abstract": "The research question regarding disruptive behaviors in physical education (PE) classes has been addressed in several studies, revealing a complex landscape of factors influencing such behaviors among students. \n\nEsteban Luis et al. (2012) highlighted discrepancies between students' and teachers' perceptions of misbehavior, noting that \"distracting or disturbing others\" is a prevalent issue in PE classes. This finding aligns with the results from Navarro-Pat\u00f3n et al. (2020), who found that disruptive behaviors, including aggressiveness and failure to follow directions, were significantly more common in older students, particularly in secondary education. They also noted that boys exhibited higher levels of certain disruptive behaviors compared to girls.\n\nFurther analysis by Maddeh et al. (2015) quantified the extent of disruptions, reporting an average of 1.2 disruptive behaviors per minute during observed PE sessions, indicating a high frequency of such incidents. This was echoed by L\u00f3pez Jim\u00e9nez et al. (2016), who found that disruptive behaviors occurred more frequently than non-relevant behaviors, with disinterest and indiscipline being the most common forms.\n\nThe role of social skills and sports practice in mitigating disruptive behaviors was explored by Navarro-Pat\u00f3n et al. (2022), who found that students with lower social skills exhibited higher levels of disruptive behaviors. Similarly, Granero-Gallegos et al. (2019) emphasized the importance of perceived teaching competence and student motivation, noting that competent teachers and self-determined motivation were associated with lower instances of disruptive behaviors.\n\nOverall, the literature suggests that disruptive behaviors in physical education are influenced by a combination of age, gender, social skills, and the perceived competence of teachers. Addressing these factors may be crucial for creating a more conducive learning environment in PE classes.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2013}, "authors": ["Hong Xiaobi"], "summary": "Offside penalty is one of the most controversial penalties in football.This paper summarized the offside penalty decision-making of the assistant referees.Underlying mechanisms of their errors made in judging offside penalty included eye movement,optical error hypothesis and flash-lag effect.At present,debates were focused on optical error hypothesis and flash-lag effect.Characteristics of space-time of assistant referee,attacker and defend er had an effect on the accuracy of judging offside.Future research will explore the effect of more variables on the ac curacy of judging offside,such as color of attire,stereotype,home advantage,improvement of the ecological validi ty and use of qualitative research paradigm.", "title": "Offside Penalty in Football", "claims": null}, {"metadata": {"year": 2003}, "authors": ["Peng Yan"], "summary": "Foul and improper conduct are one main chapter of football match principles. The result of referee\ue10bs penalty about foul will affect the player\ue10bs technical development. In order to improve the accuracy of penalty,the referee must work hard to improve the ability of discernment,strengthen the foresee ability of penalty,run actively and choose a right place to stand,use the clauses aright.\ue003", "title": "An Analysis of the Penalty of Foul in Football Match", "claims": null}, {"metadata": {"year": 2017}, "authors": ["I. Huerta"], "summary": "Research by Professor Ignacio Palacios-Huerta has illustrated how penalty kicks in football can prove key concepts in game theory.", "title": "Improving the odds of winning in professional football", "claims": null}, {"metadata": {"year": 2002}, "authors": ["Ge Xiao-ping"], "summary": "The football referees are the organizers and judges in the football matches.To make a right judgment in the match is the vital standard to the referees.The analysis to various kinds of elements affecting their judgment and the study of the vital facts which is slack in their judgment will have a great in improving their adjudication.", "title": "On Improving the Accuracy in Foul Penalized by the Football Referee", "claims": null}, {"metadata": {"year": 2011}, "authors": ["W. Schwarz"], "summary": "Abstract Using a large representative database (12,902 matches from the top professional football league in Germany), I show that the number (441) of two-penalty matches is larger than expected by chance, and that among these 441 matches there are considerably more matches in which each team is awarded one penalty than would be expected on the basis of independent penalty kick decisions (odds ratio = 11.2, relative risk = 6.34). Additional analyses based on the score in the match before a penalty is awarded and on the timing of penalties, suggest that awarding a first penalty to one team raises the referee's penalty evidence criterion for the same team, and lowers the corresponding criterion for the other team.", "title": "Compensating tendencies in penalty kick decisions of referees in professional football: Evidence from the German Bundesliga 1963\u20132006", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Liu Pin-he"], "summary": "Nowadays,total football has been a tendency and the contradictions between attack and defense are becoming acute increasingly.In this situation,the referees have to improve their recognizing abilities so that they can give correct penalties to fouls.Of course,it is referees' basic skill and it also a necessary qualification to be an excellent referee to distinguish rational actions from fouls.", "title": "How to Improve the Penalties of Referees to Fouls", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Hua Hou-jie"], "summary": "The spirit of football competition rule includes equivalence and fairness,that means it must be embodied in a fair,reasonable and legitimate way by the football referees,according to this,the referees should avoid to select the advantageous penalty to the side of foul.The favourable usage of the advantageous penalty in the football games is the important right and responsibility for the referees,it is also an important embodiment of the ability that the referees control the football game.", "title": "The influence on the game when the football referees use advantageous penalty rules", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Wang Wei"], "summary": "It is count and analyses by means of literature review,observation of statistical method of the 18th World Cup Finals referee penalty fouls,yellow and red cards.The results showed that there is a total of 2 349 fouls,386 times of offside,345 yellow cards and 27 red card in all matches.Game fouls of upper limbs is more than lower limbs,it is the most of spade personal foul up,referee foul penalty area mostly in the midfield.Athletes foul play towards a more subtle,more professional direction,the referee more stringent law enforcement standards.", "title": "Study on foul and penalty of illegitimate behavior in football game", "claims": null}], "query": "football penalties", "summary_abstract": "The research on football penalties reveals several critical insights into the decision-making processes of referees and the factors influencing penalty calls. \n\nFirstly, the accuracy of penalty decisions, particularly regarding fouls and offside calls, is significantly affected by the referees' discernment and positioning. Peng Yan (2003) emphasizes the need for referees to enhance their ability to foresee fouls and to position themselves correctly during matches. Similarly, Liu Pin-he (2011) highlights the importance of referees distinguishing between legitimate actions and fouls, especially in the context of the increasingly complex dynamics of total football.\n\nMoreover, the psychological and perceptual challenges faced by referees are underscored in the work of Hong Xiaobi (2013), who discusses the errors in offside judgments stemming from factors like eye movement and optical illusions. This suggests that the cognitive processes involved in making penalty decisions are intricate and can lead to significant errors.\n\nIn terms of game theory, I. Huerta (2017) points out that penalty kicks can illustrate key concepts, indicating that the strategic implications of penalties extend beyond mere rule enforcement to influence game dynamics and outcomes.\n\nW. Schwarz (2011) provides empirical evidence that the awarding of penalties is not random; rather, it is influenced by prior decisions within the same match. His analysis shows that once a penalty is awarded to one team, the likelihood of subsequent penalties for that team increases, while it decreases for the opposing team. This finding suggests a potential bias in penalty decision-making that could affect the fairness of the game.\n\nLastly, the overarching principle of fairness in officiating is reiterated by Hua Hou-jie (2009), who stresses that referees must avoid favoritism and ensure that penalties are awarded justly, reflecting their responsibility to uphold the integrity of the game.\n\nIn summary, the literature collectively indicates that the accuracy and fairness of football penalties are influenced by referees' perceptual abilities, cognitive biases, and the strategic context of the match, highlighting the complexity of officiating in football.", "summary_extract": null}, {"papers": [{"metadata": {"year": 1965}, "authors": ["R. Campbell"], "summary": "This article is based on the findings of a continuing study of advertising measurement that the author is conducting under the auspices of the Planning and Evaluation Committee of the Association of National Advertisers. Since the time that the study started in 1963, data on advertising measurements have been secured from 1,225 individuals in 414 firms in the United States and Canada, and case-history data have been gathered from 50 of these companies. The completed study, to be published in book form, will give case histories of the attempts of national advertisers to measure advertising results \u2014 with annual advertising expenditures ranging from under $500,000 to over $50,000,000. One or more case histories from each of the following categories will be included: (1) industrial goods and services, (2) consumer durable goods, (3) consumer nondurable goods, (4) consumer services.", "title": "A Managerial Approach to Advertising Measurement", "claims": null}, {"metadata": {"year": 1997}, "authors": ["Pasi  Falk"], "summary": "Advertisement, in its modern and proper meaning (Leiss et al., 1986), originated with the massive breakthrough around the turn of the century of consumer society and its huge markets for consumer goods in the major European centres and particularly in the United States (Fraser, 1981; Hayes, 1941). The step from announcement to advertisement came with the recognition that making the product known to people formed an integral part of sales; or, to paraphrase Clausewitz, when this was recognised as a continuation of sales by other means. An early formulation of the idea was presented in 1904 by American advertising guru John E. Kennedy, whose simple but ingenious thesis was: \u2018advertising is salesmanship in print\u2019 (Pope, 1983, 238).", "title": "The Genealogy of Advertising", "claims": null}, {"metadata": {"year": 2010}, "authors": ["H.  Dilara"], "summary": "The importance of advertisement as one of the promotion instruments has been increasing day by day. In accordance with this increase, new advertisement approaches are emerging in the history of advertising. In this study, generic advertising as one of these new advertisement approaches has been evaluated from a conceptual perspective.", "title": "JENER\u0130K REKLAM \u00dcZER\u0130NE KAVRAMSAL B\u0130R \u00c7ALI\u015eMA", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Y. Wu", "Po-Ming Lee", "T. Hsiao"], "summary": "Advertising: American Marketing Association (AMA) defines the tern of advertising as \u201cThe placement of announcements and persuasive messages in time or space purchased in any of the mass media by business firms, nonprofit organizations, government agencies, and individuals who seek to inform and/ or persuade members of a particular target market or audience about their products, services, organizations, or ideas\u201d. Before the 1890s, the word \u201cadvertising\u201d may only represent a business promotion. With the birth of more media and technology, such as printing technology, we start the generation of print advertising. For example, newspaper. This period of advertising not only began to identify the image of the business, but also provide consumers to remember the goods information. And print advertising also make communication of advertising began to rise. After the 20th century, the factory became more and more automated. Factory began to mass production of a variety of goods, so businessmen need to open up sales for these commodities. In this time, the first generation of electronic media, \u201cradio\u201d began to spread. Manufacturers began to sell their products to consumers through this new media, prompting the birth of the modern advertising industry. Modern advertising is no longer purely a propaganda action, began to follow the market research to enhance advertising efficiency.", "title": "Searching for target audience based on facebook demographics", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Anne M. Cronin"], "summary": "Any encounter with television, the internet, or urban streetscapes would suggest that we are living in the age of advertising. But the ubiquity of advertising in the west has been the focus of comment for centuries and many have claimed theirs as the ultimate era of advertising. In 1758, Samuel Johnson reputedly said that \u2018ads are now so numerous that they are negligently perused\u2019, and that \u2018the trade of advertising is now so near to perfection that it is not easy to propose any improvement\u2019 (cited in Williams 1980: 172). Notwithstanding the hyperbole surrounding its power and scope certain characteristics of advertising can be identified, one of which is its long association with cities (see Wischermann and Shore 2000). Lewis Mumford (1945: 228) noted the significance of the growth of bureaucracy for urban development such that \u2018a new trinity dominated the metropolitan scene: finance, insurance, advertising\u2019. Indeed, cities have been understood as agglomerations of people and commerce, and as sites of the centralisation of power as Mumford describes. But urban life was also thought to offer up city populations to the devices of the mass media, of which advertising was considered the most pernicious. Wirth (1964) describes how the changes to community wrought by urban capitalism resulted in the increasing significance of the media: \u2018it follows, too, that the masses of men in the city are subject to manipulation by symbols and stereotypes managed by individuals working from afar or operating invisibly behind the scenes through their control of the instruments of communication\u2019 (Wirth 1964: 82).", "title": "Introducing Commercial Spaces", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Normand Turgeon", "D. J. Barnaby"], "summary": "Abstract In order to seek the greatest effectiveness per advertising dollar, advertising facilitating institutions in conjunction with other businesses are committed to developing new advertising techniques and strategies. As a result, in the late 1960s, businesses began to initiate comparative advertising. Research on comparative advertising has been very diversified and dynamic. The empirical point of view has dominated the field and is reviewed in this paper. Recommendations concerning future studies in the field are proposed.", "title": "Comparative Advertising: Two Decades of Practice and Research", "claims": null}, {"metadata": {"year": 2015}, "authors": ["E. Stupnikova"], "summary": "This work is intended to describe and present a brief overview of advertising development in the global context as well as to show the meaning and the purpose of occurring of advertising. Acquiring basic knowledge of the strategies, techniques stages and types of advertising, I investigated case studies of two big companies from completely different countries- representatives from Israel and Belarus. The purpose of the thesis is to gain a deeper understanding of advertising prospects and influence. No surprise that the influence of advertising on a consumer is very high and constant as it is intended to influence consumer\u2019s thoughts, feeling, and actions in order to get a positive response. The idea of advertising itself did not change. Only communication- tools of delivering of the information- has significantly expanded. Due to this fact, the needs of consumers could be observed, which makes it possible to predict the future of advertising. Keywords", "title": "Advertisement - new phenomenon of communication", "claims": null}, {"metadata": {"year": 1987}, "authors": ["Linda E. Swayne", "Thomas H. Stevenson"], "summary": "Abstract Comparative advertising, the practice of comparing two or more named or unnamed products in an advertisement, is well known to industrial marketers. This article focuses on the use of comparative advertising in the three leading business publications: Business Week , Forbes , and Fortune . The authors studied approximately 2900 full-page advertisements from 1970 through 1985. It was found that there was a significant increase in the use of comparative advertising during this period; in fact, the use of comparative advertising nearly tripled from 1970 to 1985. Although the predominant use of comparative advertising involves implied comparisons, there has been a continuing increase in the use of the more aggressive strictly comparative format.", "title": "Comparative advertising in horizontal business publications", "claims": null}], "query": "history of advertisement quantitative", "summary_abstract": "The history of advertisement, particularly from a quantitative perspective, reveals significant trends and developments over time. R. Campbell (1965) conducted a comprehensive study on advertising measurement, gathering data from 1,225 individuals across 414 firms in the U.S. and Canada. This research aimed to document case histories of national advertisers' attempts to measure advertising effectiveness, highlighting a range of annual expenditures and various product categories.\n\nPasi Falk (1997) traced the evolution of advertising, noting its transformation from mere announcements to a strategic component of sales, particularly around the turn of the 20th century. This shift was marked by the recognition that advertising plays a crucial role in informing consumers, a concept encapsulated by John E. Kennedy's assertion that \"advertising is salesmanship in print.\"\n\nThe quantitative aspect of advertising history is further illustrated by Y. Wu et al. (2017), who discussed the emergence of print advertising in the late 19th century, driven by advancements in printing technology. This period marked the beginning of a more structured approach to advertising, as businesses sought to establish their identities and communicate effectively with consumers.\n\nMoreover, the rise of comparative advertising in the late 1960s, as noted by Normand Turgeon and D. J. Barnaby (2012), reflects a significant quantitative trend in advertising strategies. Their review of empirical research indicated a dynamic evolution in advertising techniques aimed at maximizing effectiveness per advertising dollar.\n\nLinda E. Swayne and Thomas H. Stevenson (1987) provided a quantitative analysis of comparative advertising in leading business publications from 1970 to 1985, revealing a nearly tripling of such advertisements during this period. This increase underscores the growing reliance on comparative strategies in advertising, which has become a staple in industrial marketing.\n\nIn summary, the history of advertisement, particularly from a quantitative standpoint, showcases a progression from basic announcements to sophisticated strategies involving measurement and comparison. The evolution of advertising techniques and the increasing emphasis on effectiveness are well-documented across various studies, highlighting the discipline's dynamic nature over the decades.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2017}, "authors": ["Y. Labib"], "summary": "Construction plays a significant role in forming civilisations as it has an impact that spurs societies worldwide to construct their own unique building identities. The structure of the construction industry is arguably considered to be fragmented and not well organised. So, effective construction logistics management is crucial to the success of construction projects that rely on extended supplier networks and delivery processes. As the construction sector is one of the vital sectors in the Jordanian development process, the construction logistics process is considered a significant problem confronting Jordanian construction and needs to be altered (Sweis et al., 2008; Momani, 2000). My investigation of a variety of databases has discovered no indication of implementation of lean thinking or lean practices within the Jordanian construction industry. The purpose of this study is firstly to examine the challenges facing construction logistics in Jordan, and then explore to what extent the Jordanian construction stakeholders are using lean planning tools and practices throughout their construction logistics process, which determines whether Jordanian construction is conventional or toward lean. Thus, this subject appears to be a substantial area to examine within the Jordanian construction industry. \nThe research aim is to develop models for assessing the adoption of lean logistics in Jordanian construction organisations. To achieve this aim, five objectives were prepared. In the literature review, a background of lean production and construction are illustrated; methods and approaches of lean construction along with the benefits and barriers of lean are also critically reviewed. However, the literature did not show any signs of lean practices in Jordan. In addition, supply chain and logistics, in a global sense, are clarified along with the differences between traditional and lean logistics. Most importantly, the literature review shows the challenges affecting logistics in the construction industry including the health and safety regulation factor; inventory factor; material preservation factor; labour performance and material handling factor (performance factor); planning factor; transportation factor; continuous improvement factor; and transparency and information exchange factor. \nIn terms of the research methodology used in this study, the researcher adopted the research-onion model (Saunders et al., 2009). The research tended towards positivism, realism and value-free stances in terms of philosophy, and the search approach used was a combination of inductive and deductive. The strategy of research first included a case study (semi-structured interview), and secondly a survey which considers the choice of this research as a mixed method. Firstly, semi-structured interviews were administered among nine experts in the Jordanian construction industry. This qualitative data investigated the current status of construction logistics in Jordan, factors affecting Jordanian construction logistics and the benefits and barriers of implementing lean practices. Secondly, the results of the semi-structured interviews and the literature review were utilised in the second data collection (questionnaire) in a wider section where 150 stakeholders participated. The descriptive outcomes and factor analysis show that planning is the leading factor (challenges) affecting construction logistics in Jordan. This is followed by the transportation factor, transparency and information exchange factor, continuous improvement factor, material preservation factor, inventory factor and finally the material handling factor. Moreover, the outcomes also show that the awareness and implementation of lean planning tools and practices are still underestimated. Furthermore, the Kruskal-Wallis test and logistics regression were used to find the different views among stakeholders (consultant, contractor and supplier) in regards to factors (challenges) affecting construction logistics, lean planning tools and practices. ISM (Interpretive Structural Modelling) was exploited to build final models, explain all the relationships between variables and level them in different stages from bottom (greatest influence) to top (lowest influence). The models included factors affecting construction logistics (ISM-1), lean planning tools (ISM-2) and lean practices (ISM-3). \nFulfilling the aim of this research offers both academics and practitioners contributions to the study of lean logistics. For example, academics will be able to use this research to identify initial indicators and tools for further in-depth studies related to lean logistics within developed and developing countries, whereas managers from different Jordanian construction companies (consultant, contractor, or supplier) will gain added insight into and guidance on lean logistics challenges and use of lean planning tools and practices. This will ultimately help managers assess, reframe and prioritise their managerial practices.", "title": "Lean framework for assessing construction logistics in Jordan", "claims": null}, {"metadata": {"year": 2019}, "authors": ["A. Qasem", "M. Zaid", "Emhaidy S.Gharaibeh"], "summary": "Jordan is employing major reforms in education sector. Not least of these innovations is the Building for basic education schools aims to improve the learning environment. Construction of such new schools are already going ahead in numerous areas of the Jordan. Although, the construction industry is a key player in the economy, creating both, employment and wealth, yet numerous projects experience extensive delays and thus surpass initial time and cost estimates. The most collective causes were appraised by using both, the data collected in a survey steered to residential projects consultant engineers, contractors, and owners, and interviews with senior professionals in the field, so supply chain strategy among them is the most important issue. Most correspondents established that, monetary difficulties faced by the contractor and too many change orders by the owner are the foremost causes of construction deferment. Stern weather states and changes in government regulations and laws categorized among the least important causes. To illustrate the implementation of the classification impact index model and to evaluate and quantify change orders impact on contractor\u2019s cash flow, a case study is presented in this paper using data of real case study obtained from reputable construction company in Jordan involved in the construction of basic education school building. The project commenced from November 2012 till December 2013.", "title": "Impact of Supply Chain Strategy and Change Order on Contractors\u00e2\u20ac\u2122 Cash Flow in Jordanian Construction Industry: A Case Study of Basic Educational School Construction", "claims": null}, {"metadata": {"year": 2020}, "authors": ["J. A. A. Albtoosh", "M. AL-Btoush", "Mohammad Salim Al-Rawashdeh", "O. Alomari"], "summary": ": Jordan construction industry experience challenges that reduce the number of construction industry organizations to121organization in 2016 compared to 148 in 2015 based on the Central Bank of Jordan annual report. According to the literature the main causes identified by researchers include poor design, poor scheduling and planning and the fragmental nature of the industry. To address these challenges, the adopting of Building Information Modelling (BIM) is proposed. This study presents a strategic plan to adopt BIM by the private sector of the Jordan construction industry. Through wide and comprehensive survey (quantitative approach) the findings shows that the adoption of BIM in the Jordanian private construction section is subjected to many factors such as the top management support, spreading awareness by the construction associations and governmental national BIM training centres.", "title": "Adopting BIM in the Jordanian Private Construction Industry Case Study", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Mohammed Hiyassat", "M. Hiyari", "Ghaleb J. Sweis"], "summary": "Improving productivity in construction is a way of increasing profit with little or no increase in cost. Labour productivity in construction in developing countries, including Jordan, is relatively low and needs to be improved significantly. The objective of this paper is to describe and analyse the factors that affect construction labour productivity. To achieve this goal, a questionnaire survey containing 27 questions (variables) was conducted. It was sent to 200 engineers and foremen who work for contractors. The 90 returned responses were statistically analysed by calculating the average, standard deviation, and RII of each variable. The results of a Spearman correlation\u2012rank\u2012coefficient test showed that no significant differences existed between the responses of the engineers and foremen. It was concluded that the top three ranked dimensions were \u2018Productivity increases as experience increases\u2019, \u2018Financial incentives increase productivity\u2019, and \u2018Trust and communications between management and workers increase productivity\u2019. Furthermore, the data was analysed using the principal components method of factor analysis. Of the 11 extracted factors, only five were considered, while the others were dropped since the observed variables had low loadings on these factors. Contractors and officials can benefit from this study in understanding factors affecting productivity, and act upon that understanding.", "title": "Factors affecting construction labour productivity: a case study of Jordan", "claims": null}, {"metadata": {"year": 2017}, "authors": ["M. Alhusban", "Salam Al-Bizri"], "summary": "In Jordan and most developing countries, the construction industry faces problems such as chronic resource shortages, general situation of socioeconomic stress, institutional weaknesses and a general inability to deal with key issues. There is also evidence that the problems have become greater in extent and severity in recent years. Building Information Modelling (BIM) has emerged as a potential solution to these problems and to improve the performance of the construction industry. Construction procurement routes currently applied in Jordan predate the use of life-cycle BIM for the delivery of construction projects. To date, little has been done to align the various procurement routes used in delivering buildings with the novel opportunities offered via BIM. This paper is a part of an on-going research project to determine whether procurement approaches influence the ability to use BIM approach to deliver sustainable buildings in Jordan.", "title": "Procurement route and Building Information Modelling (BIM) implementation effect on achieving sustainable buildings in developing countries: a case study of Jordan", "claims": null}, {"metadata": {"year": 2018}, "authors": ["M. A. Al-Btoush"], "summary": "Many countries consider the construction industry as one of the most challenging industries. The major problems facing construction companies and have ardently caused its low performance improvements over the years cannot be overemphasized. Among others, the causes include poor communications and exceptional fragmented environment relating to the construction sector. The Jordanian construction industry is a very crucial segment of Jordan\u2019s economy. In 2014, it contributed about 5.8% to the gross domestic product (GDP) of Jordan, which is more than the agricultural, electricity and water sectors. However, the construction industry in Jordan faces low productivity issue which is obviously due to repeated delays in the construction project besides cost overrun. On the other hand, many studies on the adoption of building information modelling (BIM) have been conducted in various countries including Norway, Finland, Sweden, Germany, Malaysia, Singapore, France, USA, Australia, and UK where BIM has proven the capabilities to develop information integration, business process flow, productivity, and reduce complexities, uncertainties, conflicts, and fragmentations, among others. BIM is the latest development which is a promising, an evolving procedural and technological shift within the AEC sector. This research is aimed to provide support development in the Jordanian construction industry practice by developing a BIM adoption framework. The contractors, owners and consultants can use this framework to assess and comprehend how ready they are for BIM adoption. To meet this aim, related literature was comprehensively elaborated in two contexts. The first context was the Jordan construction industry in order to explore the current criteria for construction practice and spot-related challenges. The second context was BIM technology, the concept, practice, development, adoption condition, and best practices to select the theoretical framework for BIM adoption from the best practices list. To conceptualize the selected theoretical adoption framework, the research followed a qualitative approach and 15 semi-structured interviews were conducted with the leading construction organizations in the Jordanian construction industry. Three levels of validity and reliability checks were performed; the first level was a focus group workshop where the researcher discussed the results with four construction experts who suggested some adjustments regarding the drivers\u2019 components in the theoretical framework, followed by the second level of validity where 228 questionnaires were answered by construction organizations. The results were analyzed using the structured equation modelling technique where one of the hypotheses were rejected and a second adjustment regarding the conceptual framework components took place. The last validity check was done by discussing the conceptual framework with the three major industry players in Jordan i.e. the ministry of public work and housing, the Jordanian construction contractors association and the Jordanian engineers association to generate the finalized adoption framework. Findings revealed that the adoption of BIM among construction organizations in Jordan is still at the pre-BIM stage where it faces barriers and confronts challenges such as, but not restricted to, the lack of BIM awareness, training centers, BIM specialists, government support, and financial resources, and resistance to change. This study proposed a practical conceptual BIM adoption framework validated by the construction industry key players where it is expected to support and accelerate the adoption operation amongst the Jordanian construction industry significantly. Moreover, the proposed adoption framework could be used as a benchmarking tool to measure the organizations\u2019 readiness to adopt BIM technology. It had been observed from previous literatures (and later confirmed by the interviews and questionnaire results) that some of the problems faced by Jordanian construction industry are: poor design, poor scheduling and planning, change orders, lack of competitive technical staff and ineffective communications among others, leading to issues such as cost and time overruns. Most of the participants approved that the use of ITC on their business increase their efficiency, saving time and decreasing cost. Nonetheless, the finding is different from that of the BIM. Numerous specialists from the construction body are aware of BIM but never applied it in practice. Also, some were familiar of the advantages of BIM, but majority stated they had no knowledge of BIM. In the same way, this study exposed that those experts who knew about BIM were in the age of 30-49. The people of older ages are uninterested in learning a new process. The fresh engineers have no idea about BIM. The research proposed an expository revision that asset the best predictable adoption standards for BIM adoption. The adoption standards would help the firms to recognize the BIM field of capability and assess their present ability to adopt BIM. Moreover, the study supposed to increase the present awareness on BIM by recording concerns associated with BIM adopting from the construction organizations viewpoint. Furthermore, the results concluded in this study could assist the industry sector to realize the BIM adoption problem inside the situation of Jordan. This research supposed to have a significant importance to the construction sector by reason of the absenteeism of a suitable model to adopt BIM in regard of the Jordanian construction background.", "title": "Systematic approach and strategies for building information modelling (BIM) adoption in the Jordanian construction industry", "claims": null}, {"metadata": {"year": 1995}, "authors": ["A. Al\u2010Momani"], "summary": "Demand for construction work follows the overall level of the economy. This detailed analysis is aimed at stimulating discussion and research into further ways of understanding the construction industry and avoiding dependence on quarrying and mining, which in the past have played a major part in financing construction projects and programmes.", "title": "The economic evaluation of the construction industry in Jordan", "claims": null}, {"metadata": {"year": 2014}, "authors": ["A. Khatatbeh"], "summary": "In light of the renewed activity of construction sector, it shows the need for increased attention to quality as a cultural attitude and an economic necessity at the same time. Although construction industry had prospered since long time ago and achieved advanced degrees in its products and ways to implement them, it has been criticized for its poor performance and productivity in relation to other industries. Quality products in this industry will not be achieved without using appropriate methods of quality management during projects lifecycle.\u00a0 This study investigates the degree of using methods of quality management in the implementation of construction projects in Jordan. The study sample consisted of (50) managers of construction projects in Jordan. The researcher used descriptive method and develop questionnaire to collect data. The researcher used means, standard deviations the results of the study showed that there are high degrees of using the testing and inspection method, quality control method, the Quality Assurance method, and the Total Quality Management method in the implementation of construction projects in Jordan. This study recommended the founding a Staff from company or outside it for training and employees and to exercises particular programs to contract with the requirements of changes. Keywords : Quality Management, Construction Projects.", "title": "The Degree of Using Methods of Quality Management in the Implementation of Construction Projects in Jordan", "claims": null}], "query": "waterfall methodology; case study of Jordan's construction industry", "summary_abstract": "The research question regarding the waterfall methodology in the context of Jordan's construction industry reveals several insights from the provided abstracts. \n\nLabib (2017) highlights the fragmented nature of the Jordanian construction industry and emphasizes the need for effective logistics management. The study indicates that there is a lack of lean practices, which are often associated with more structured methodologies like waterfall, in the Jordanian construction sector. The research identifies planning as a significant challenge, suggesting that a more organized approach could enhance project execution.\n\nQasem et al. (2019) discuss the extensive delays and cost overruns in construction projects, attributing these issues to poor supply chain strategies and financial difficulties. This aligns with the waterfall methodology's emphasis on thorough planning and sequential execution, which could potentially mitigate such delays if properly implemented.\n\nAlbtoosh et al. (2020) point out that poor design and scheduling are prevalent issues in the Jordanian construction industry. These challenges suggest a need for a more systematic approach, such as the waterfall methodology, which relies on detailed upfront planning and design before moving to subsequent phases.\n\nHiyassat et al. (2016) and Alhusban & Al-Bizri (2017) further emphasize the low productivity and chronic resource shortages in the industry. They suggest that adopting modern practices like Building Information Modelling (BIM) could improve project outcomes. While BIM is not directly related to waterfall methodology, both approaches share a focus on structured processes and improved planning.\n\nAl-Btoush (2018) also notes the challenges of communication and fragmentation in the industry, which could be addressed through a more cohesive methodology like waterfall that promotes clear stages and responsibilities.\n\nIn summary, the collective findings from these studies suggest that the Jordanian construction industry faces significant challenges that could be alleviated through the adoption of structured methodologies like waterfall. The emphasis on planning, design, and logistics management indicates that a more organized approach could lead to improved project outcomes and efficiency in this sector.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2013}, "authors": ["R. Sealy"], "summary": "BACKGROUND AND GENERAL POLICY CONTEXT OF THE UKThe UK takes a voluntary regulatory, rather than a mandatory legal, framework approach to boardroom governance and behaviors, aiming \"to set a global standard for good practice in corporate governance\" (Heidrick and Struggles 2009, 44). This regulation is overseen by the Financial Reporting Council (FRC) and has been guided by a code of conduct that has undergone a number of reviews over the past twenty years. The FRC is the UK's independent regulator responsible for promoting high quality corporate governance and reporting. The UK Corporate Governance Code (referred to as \"the Code\") is a set of principles of good corporate governance aimed at FTSE-listed (Financial Times Stock Exchange) companies. These FTSE-listed companies, or public limited companies (PLCs), are required to disclose how they have complied with the Code and explain where they have not applied the Code-in what the Code refers to as \"comply or explain\" (FRC 2012,4). The Code adopts a principles-based approach in the sense that it provides general guidelines of best practice. This contrasts with a rules-based approach, which rigidly defines exact provisions that must be adhered to. The Code is essentially a consolidation and refinement of a number of different reports and codes concerning good corporate governance produced in the UK over the past twenty years.Investigations by Derek Higgs (2003) and Laura Tyson (2003) into UK corporate governance called for more independent directors and greater variety of experience, and recommended the use of external search consultants for board appointments. Changes to the Code were made accordingly. In May 2010, a principle was introduced, which, for the first time, recognized the value of gender diversity in the boardroom, stating that \"the search for board candidates should be conducted and appointments made on merit, against objective criteria and with due regard for the benefits of diversity on the board, including gender\" (FRC 2010,13).MONITORING THE BOARDROOMIn 1999, Cranfield School of Management's International Centre for Women Leaders (CICWL) created its first index of FTSE-100 companies ranked in terms of their percentage of female board directors, similar to the Catalyst Board Census in the United States. Each year the project grew, and it now reports annually on the board and executive committee composition for up to 1,400 FTSE-listed companies in the UK. The report has been backed by government at the highest ministerial level and sponsored by several major businesses, and receives much media coverage.By 2010, after a decade of reporting, the headline percentage of women on boards in the UK had risen just 5 percentage points from 7 percent to 12 percent. The sense of growing frustration felt by government and women in business was reflected in the media. Following Norway's 2008 adoption of a 40 percent quota for either gender on boards and Spain's move to recommended targets, discussion among other European countries about possible action increased. Parts of the then-UK government were keen to become more actively engaged with this issue.In 2010, the Australian Stock Exchange Securities Council (SESC) introduced gender metric reporting as part of its governance code. The aim was to significantly increase the proportion of female board directors and avoid any requirement for (threatened) government intervention in the form of legislation. The SESC figures show that the percentage of new appointments going to women has risen from 5 percent in 2009 to 27 percent in the first half of 2010, demonstrating what could be achieved once individual stakeholders are motivated (Australian Institute of Company Directors 2012.Australia has similar board structures and corporate governance rules to the UK and so CICWL recommended a similar approach to the UK government. In 2010 a new coalition government took office and, concerned about the lack of progress on the issue of women on boards in the UK, asked Lord Mervyn Davies to set up a review. \u2026", "title": "A \"Hands-Off\" Intervention: The UK's Approach to Increase the Number of Women on Corporate Boards", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Rey  Dang", "Linh Chi Vo"], "summary": "In recent years, gender diversity in the boardroom and more generally within organizations, has gained considerable interest in public debate, academic research, government agenda, and corporate strategy. Previously considered as a social issue, gender diversity on boards of directors is increasingly perceived as a value-driver in organizational strategy and corporate governance for several reasons (Terjesen, Sealy and Singh, 2009). Firstly, as institutional investors begin to take into account board diversity, this matter progressively becomes part of their investment practices (Carter, Simkins and Simpson, 2003). Fair employment practices for women are now part of the criteria of many social investment indices (e.g. FTSE4Good or Domini 400 Social Index). Secondly, board diversity is required by important stakeholders, such as customers or employees. The consideration of stakeholders\u2019 preferences, aspirations, and concerns might be beneficial for firms through improved customer loyalty, and employee motivation (e.g. Powell, 1999). Thirdly, recent legislation and diversity initiatives worldwide have pointed out the importance of female representation on boards of directors. For example, the Norwegian government requires companies to appoint at least 40% of women. Spain and France have implemented the same affirmative actions by 2015 and 2017 respectively (Terjesen et al, 2009). Fourthly, board diversity issues have also been discussed by best practices in corporate governance. For example, the Sarbanes-Oxley Act of 2002 (in the United States) or the Higgs Review (in the United Kingdom) encourages more diversity on corporate boards (Adams and Ferreira, 2009; Dalton and Dalton, 2008). Finally, gender diversity is on companies\u2019 agenda, since more women are in top management positions today. Therefore, the pipeline of female directors and women CEOs is expected to increase in the coming years (Helfat, Harris and Wolfson, 2006).", "title": "Women on Corporate Boards of Directors: Theories, Facts and Analysis", "claims": null}, {"metadata": {"year": 2011}, "authors": ["C. Villiers"], "summary": "There has been a lot of talk about the topic of women in company boardrooms in the UK over the last couple of years. Whilst the point is debatable, there is enough research evidence available to suggest that a more heterogeneous board would be more likely to take a broader perspective and take wider interests into account and might therefore also be beneficial to the company's interest in the longer term. From a gender equality perspective also, there is widespread agreement in the UK that the situation is not acceptable but what to do about it remains a point of contention. Should the UK follow the examples of Norway and Spain, and likely soon France and the Netherlands also, or should we adopt less radical measures? The government has indicated a willingness to do something and the current consultation on the issue is due to lead to recommendations in February. However, press reports suggest that, whilst the Chair of the Consultation, Lord Davies of Abersoch, has not ruled out the introduction of quotas like those in Norway and Spain, they are unlikely and are viewed really only as a last resort. Lord Davies is reported to be considering the recommendation of alternative measures, such as more open recruitment processes or a 'comply or explain' provision in the Corporate Governance Code. This brief article looks at the current available statistics in the UK and the consultation process. It suggests that whilst quotas might not be imminent, in the longer term they may be inevitable. The author is a member of the project team of the Sustainable Companies project at the Faculty of Law, University of Oslo.", "title": "Women on Boards: Report from the UK", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Hamiisi Junior Nsubuga"], "summary": "This chapter analyses the current state of affairs in relation to gender diversity on the boards of public listed companies in the UK and the EU. The chapter analyses the proposed European Union (EU) Directive on Women on Company Boards and related matters, the current stage of the proposed Directive and why it has not been adopted for over 7 years. The chapter analyses key conceptual underpinnings that may have contributed to the procedural paralysis in the adoption of the Directive. The position and current state of affairs on women on corporate boards of FTSE listed companies in the UK is examined, as well as initiatives taken by the UK government and institutional players to address this concern. Finally, the chapter analyses why the UK government\u2019s recommendation for FTSE listed companies to achieve a 40 percent representation of women on FTSE boards by 2020 has not been achieved and offers a suggestive solution.", "title": "Women on corporate boards of public listed companies \u2013 a UK and EU gender diversity conundrum", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Martin S. Meyers"], "summary": "INTRODUCTIONA study conducted in 2008 compares men and women who recently graduated from top MBA programs. The men are paid approximately $4600 more than the women. The women report being in lower level management positions and experiencing less job satisfaction compared to the men (Ibarra, Carter, and Silva, 2010).Women held 16.6% of board seats for Fortune 500 companies in 2012. The United States does not impose any laws specifying a quota for female corporate board members. Norway was the first country to impose a quota in 2003. A minimum of 40% of the board members in Norway must be women. There are harsh fines for firms that do not comply with the mandated quota. A 2011 French law required corporate boards to be at least 20% female by 2014 for board members to qualify for their fees. An Italian law mandates that women must make up one-third of the board members on listed and state-owned enterprises by 2015. Similar laws are in place in Belgium, Netherlands, and Iceland. The regulations in Finland say both sexes should be represented on corporate boards but does not specify specific percentages or timetables (Lublin, 2012). Research indicates that women need to be more qualified than men to be considered to serve on corporate boards and women pay a higher personal price to become board members. The higher divorce rate among female board members is one factor supporting the personal sacrifices. Female board members tend to be younger and were added to the board more recently (Groysberg and Bell, 2013)IMPACT OF WOMEN ON CORPORATE BOARDSHaving just one woman on the board of directors on a board cuts the risk of bankruptcy by 20% (Bart and McQueen, 2013). One explanation would be that a company that would put women on the board might be more progressive business people, which would result in better decision making.A survey of more than 600 board members showed that women are more likely to consider the rights of others and to take a cooperative approach to decision making. Female board members will also be more likely to use cooperation in decision making compared to their male counterparts (Bart and McQueen, 2013).Another study showed that female representation on corporate boards improved governance, management control, and a greater sensitivity to other perspectives (Fondas and Sassalos, 2000).Boards with a significant composition of women report a 53% higher return on equity, a 66% higher return on invested capital and a 42% higher return on sales (Joy, 2007).One study found that women board members might bring a different perspective because a significantly higher percentage of women list arts and culture, travel, and philanthropy and community service as outside interests. For example 26% of women and 16% of men list philanthropy and community sendee as outside interests. Men were more likely to list sports as an outside interest. The same study revealed that women are more likely to list to communicate effectively as a perceived strength. Men were more likely to list global experience as a strength, which might be partially be explained by women not being considered for overseas assignments because of perceptions that women with families would be unwilling to relocate. There are reported higher levels of satisfaction for both women and men if management concept that more satisfied workers are more produetive.there is a greater gender balance (Groyberg and Bell, 2013). It is a basicThorhild Widvey, a Norwegian politician, served as the minister of energy in Nomay from 2003 - 2005. She often states, \"I do not like quotas, but I love what they do.\" She believes that the attitudes toward women have improved. Mari Teigen, professor and research director at the Institute for Social Research in Nomay has a different viewpoint. She states that at best, having more females on boards has resulted on only a slightly positive effect (Smale, 2013).ROLE OF GOVERNMENTWomen seem to pay a higher personal price to become board members than men. \u2026", "title": "Options for Improving Gender Equity at Top Levels of Management in Corporate America", "claims": null}, {"metadata": {"year": 2011}, "authors": ["A. Hebble"], "summary": "Shareholders and other stakeholders are placing an increasing emphasis on corporate governance. The composition of a board is an important aspect of corporate governance. There is some evidence that gender diversity in the boardroom has an impact on the operations of a company. Both U.S. and UK corporations are increasing female representation on their boards. The purpose of this particular study is to compare gender diversity on U.S. and UK boards, and investigate some firm characteristics that may be associated with above or below average female representation.", "title": "Female Directors And Firm Characteristics: A USA UK Comparison", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Sue Turner"], "summary": "Inequality continues to plague the boardrooms of companies worldwide. In the UK, only 20% of the directors of the FTSE100 are women. This paper will examine legal, social, and cultural barriers to greater equality in corporate leadership roles and strategies for changing long-standing inequalities in the business world. \n \nFor many businesses, the law is simply a hurdle to clear, an obstacle to work around or a box to tick: business first; law second. Companies can carefully risk-assess legal challenges and factor the cost of non-compliance into their business plan. Although Section 172 Companies Act 2006 was in part adopted to introduce more ethical behaviour into corporate boardrooms, directors may still comply in a way that does not affect business decision-making in any meaningful way. This paper asks what regulations, incentives or sanctions can be successful in challenging the accepted norms and inequalities of the business world more than superficially, focusing on the slowly-changing norm of the male-dominated board. It will also critically analyse the reasons for the gradual increase in women serving on corporate boards in the UK and some parts of Europe, whether the result of legislation; quotas; disclosure; customer demands; changing attitudes; or sound economic reasoning.", "title": "Balancing the Boardroom: Can Lessons be Learned?", "claims": null}, {"metadata": {"year": 2007}, "authors": ["Lynn M. Martin", "Izzy W. Smith", "S. Roper", "J. Scott"], "summary": "This paper provides an overview of the position of women directors in UK firms. Based on data for all UK firms with more than three directors, this data provides a comprehensive picture of the position of women in UK business leadership and contributes to our understanding of progress towards achieving greater gender balance in the boardroom. Five key points emerge. \u2022Female directors account for around 1:4 directors in UK firms but only around 1:10 businesses in the UK are female controlled. \u2022Only 1:226 larger firms in this category have a majority of female directors. \u2022The overall proportion of female directors in the UK has grown in recent years but slowly. At the rate of progress achieved over the 2003-2005 period, it will be the year 2225 before gender balance in company directorships is achieved in the UK. \u2022There are a significant and interesting group of 12, 600 sisterhood companies in the UK \u2013 those wholly owned and led by women. Although they are predominantly services, these do firms exist in all business sectors with a focus on smaller companies. These firms represent an interesting potential focus for future research. \u2022Our analysis of board gender diversity and business growth suggests that there is a business cost to gender balance in terms of foregone growth.", "title": "Board gender diversity in UK firms \u2013 recent trends and performance eEffects", "claims": null}], "query": "This is in line with the UK Corporate Governance Code, which recommends that at least one-third of board members should be women.", "summary_abstract": "The research question regarding the recommendation of the UK Corporate Governance Code for at least one-third of board members to be women is addressed through various studies and analyses that highlight the current state of gender diversity in UK boardrooms, the regulatory context, and the implications of such diversity.\n\nSealy (2013) outlines the evolution of the UK Corporate Governance Code, which has increasingly recognized the importance of gender diversity in boardrooms. The introduction of a principle in 2010 emphasized that board appointments should consider diversity, including gender, alongside merit. Despite this, progress has been slow, with only a modest increase in female representation on boards over the years.\n\nDang and Vo (2012) discuss the growing recognition of gender diversity as a value-driver in corporate governance, noting that institutional investors and stakeholders are increasingly advocating for diverse boards. They reference international examples, such as Norway's 40% quota for women on boards, which has spurred discussions in the UK about similar measures.\n\nVilliers (2011) highlights the ongoing debate in the UK regarding the effectiveness of quotas versus alternative measures to enhance gender diversity. While the government has shown a willingness to address the issue, the implementation of quotas remains contentious, with a preference for voluntary compliance mechanisms like the \"comply or explain\" approach.\n\nNsubuga (2020) provides a critical analysis of the UK government's recommendation for a 40% representation of women on boards by 2020, noting that this target has not been met. The chapter suggests that while initiatives have been proposed, systemic barriers continue to hinder progress.\n\nMeyers (2015) emphasizes the positive impact of female board members on corporate performance, citing studies that link gender diversity to improved governance and financial outcomes. This aligns with the broader argument that diverse boards are better equipped to consider a wider range of perspectives, ultimately benefiting the company.\n\nHebble (2011) and Turner (2015) further explore the implications of gender diversity in corporate governance, noting that stakeholders are increasingly prioritizing this issue. They argue that while there has been some progress, significant barriers remain, and a more robust regulatory framework may be necessary to achieve meaningful change.\n\nIn summary, the collective findings from these papers indicate a growing recognition of the importance of gender diversity in UK boardrooms, as advocated by the UK Corporate Governance Code. However, despite recommendations for increased female representation, actual progress has been slow, and the debate continues over the best methods to achieve these goals, whether through quotas or other regulatory measures.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2023}, "authors": ["A. Hepburn"], "summary": "Abstract:Why do some novels have a fast pace and others slow? Pace in narrative gauges the movement of a story in a specific direction with respect to time. Dialogue, descriptive summaries, chapter breaks, and other formal elements speed up or slow down the perceived tempo of a narrative. In Tinker, Tailor, Soldier, Spy, John le Carr\u00e9 creates pace by exploiting the discrepancies between fabula and syuzhet. By his own admission, he prefers to begin a story as late as possible to induce pressure between the content of a story and the manner of its telling, a pressure that sets and regulates narrative pace.", "title": "\"To Come into the Story as Late as Possible, and To Tell It as Fast as You Can\": Pace in Tinker, Tailor, Soldier, Spy", "claims": null}, {"metadata": {"year": 2002}, "authors": ["Wang Ying"], "summary": "As narrative literature is a temporal art, the manipulation of time is closely related to the artistic achievement of literary creations. The common m ethods of literary narration involves temporal direction alteration, temporal f ragment strategies, reinforcement of time scale and change of the story time by describing inner feeling and dreams.", "title": "The Treacherous Temporal Art", "claims": null}, {"metadata": {"year": 1995}, "authors": ["Minet de Wied"], "summary": "Abstract The present paper explores the role of temporal expectancies in the production of suspense in film. It is proposed that both temporal predictability (giving rise to temporal expectancies) and temporal contrast (involving a temporal disparity between an event's actual and expected ending) may add to suspense. It is suggested that temporal expectancies in suspense discourse structures depend, in part, on (1) the temporal relationships as described in the plan schemas involved, and (2) the temporal succession of breakpoints (i.e., the beginnings or endings of the separate events). It is illustrated how the manipulation of the screen duration could be used to manipulate the temporal succession of breakpoints, and therefore also, temporal expectancies. It is hypothesized that higher suspense levels might be reached when the outcome event is presented a little later than expected. An imaginary experiment is presented in which this hypothesis could be put to test.", "title": "The role of temporal expectancies in the production of film suspense", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Andreea Macovei"], "summary": "The time track model considers a natural order of events that the reader seems to perceive once he continues to read or, at the end of the book despite timelines (the representation of all the events chronologically exposed in a story) and storylines (the main story or plot of a literary text). The identification of this order of events using the model of time tracks may lead to a better understanding of the entire story. This paper describes a temporal annotation scheme that can be used in order to reorder the events or more precisely, the sequences of events in novels. This methodology emphasizes temporal switches that may occur in literary texts such as flashbacks, flashforwards, embedded fabulae, temporal ruptures, and transitions. Further, this scheme can represent a starting point of a natural processing language tool in the field of temporality and temporal annotation, needed to capture the actions of a character and his or her chronological evolution (time track) throughout a book.", "title": "Capturing the Evolution of Characters in Novels", "claims": null}, {"metadata": {"year": 1980}, "authors": ["K. Ireland"], "summary": "Gladly yielding his charge to the care of a willing escort, Charles Musgrove hurries off to the gunsmith's, while Captain Wentworth and Anne Elliot withdraw to a quiet path. Here, in an untypically complex statement, it is noted that \"the power of conversation would make the present hour a blessing indeed; and prepare for it all the immortality which the happiest recollections of their own future lives could bestow.\" With privileged hindsight, the narrator can confirm, as hero and heroine cannot, the enduring impact of the moment, projecting it beyond the writing present, and suggesting by a fusion of time-planes its climactic centrality within the narrative. This explicit illustration from the penultimate chapter of Persuasion hints at what is observable throughout in more implicit guise. The force of time2 and sense of the past3 have been held to account for the book's atmosphere and appeal, while this last complete novel by Jane Austen has been described as the only one with a plot involving \"considerable temporal complexity.\" 4 To examine how far the climax of Persuasion and the novel as a whole can be articulated according to the development of its temporal elements will be the concern of this essay. Common to all literature, but assuming a different value in different genres, time takes on a relatively important aspect in prose fiction, compared with its character of near-evanescence in lyric poetry and of near-isochrony in drama. In the harmonious totality of the epic form, the \"life-immanence of meaning is so strong that it abolishes time\"; in the novel, by contrast, \"meaning is separated from life, and hence the essential from the temporal.\" Thus, Lukacs concludes, \"the entire inner action of the novel is nothing but a struggle against the power of time.\" 5 His position has been endorsed by later critics, one of whom takes issue with the application of \"structure\" to fiction, holding the analogy to architecture essentially false. An analogy to \"tempo\" in music would be far more apt, since fiction \"is conceived in time. Its essence is rhythm. The thing is not perceived as a whole in one view. It moves sequentially, from moment to moment.\" 6", "title": "Future Recollections of Immortality: Temporal Articulation in Jane Austen's \"Persuasion\"", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Brian Gingrich"], "summary": "We talk about pace, often: the pace of history, modern life, everyday movement. And, really, we have talked about pace for centuries. But we hardly know what it means or how one might analyze it. The Pace of Fiction starts from the notion that all pace is, essentially, a product of narrative, and narrative fiction is what produces pace most elaborately. It moves forward as a history of transformations in narrative movement, from Fielding and Goethe and Austen to George Eliot, Flaubert, Henry James, James Joyce, Hemingway, Woolf, and Mann. Pace reveals narrative in its most elaborate effects. And the way pace changes in fiction expresses much of what we refer to as the pace of modernity.", "title": "The Pace of Fiction", "claims": null}, {"metadata": {"year": 1972}, "authors": ["E. N. Hutchens"], "summary": "The identification of time as the constitutive element in the novel moves us close to a tenable poetics of the genre.' Despite our excursions into the extra-literary temporalities of sociology, history, theology, physics, philosophy, and theoretical psychology, and despite the inclination of some of us to lump the novel with other forms of prose fiction when speaking of time in literature, we are beginning to get at the generic secret that yields the peculiar pleasure of reading a novel. Much work lies ahead, however, to show us clearly how the novel takes on the form of time. Two questions in particular stand in the way: what about the apparent exceptions? and what about other forms, especially the short story, in which time seems decisively operative? When we consider the novels that seem to defy temporal control, it becomes evident that they make up the body of the anti-novelistic novel: that is, every test of the novel is a temporal test. From Sterne to Robbe-Grillet, the writers who strain the genre strain it by flouting a temporal norm. The more defiant they are, the more they call attention to the primary importance of time. Mrs. Shandy's ill-timed question-about a clock, of course-interrupts an action which, like a simple narrative, is expected to go directly forward to its conclusion. As a result,", "title": "Towards a Poetics of Fiction: 5) The Novel as Chronomorph", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Martin Zirulnik"], "summary": "Author(s): Zirulnik, Martin | Advisor(s): Heise, Ursula K. | Abstract: Through the analysis of a selection of thrillers written and produced in the years immediately following World War II, this dissertation defines the threshold conditions of narrative suspense in the twentieth century\u2014the pending disappearance of continuity, not just in techniques of storytelling but in longstanding assumptions about time, about how it is and how it ought to be organized. These works, for instance, seem in various ways to prefigure a more general attenuation of narrative retrospection\u2014narrative retrospection as a grammatical convention of realist fiction but also as a form of belief, as fidelity to the qualitative density of human experience as it is transmitted through time. This period of aesthetic transition, marked by the iconoclasm of the avant-garde, is often seen as a terminal point in the history of storytelling after modernism, a time (though certainly not the only time) when the novel dies along with the systems of value that upheld it. Alternately, as more recent scholarship has shown, it can be seen as the moment when the present tense starts to gain traction as the dominant mode of fictionalizing time. But the works examined in the following pages\u2014including the fiction of Elizabeth Bowen, Graham Greene and Raymond Chandler, as well as the postwar films of Carol Reed, among others\u2014modify the constraints of genre and reshape the forms of suspense simply by extending the transmission, by perpetuating the same old message about time even when it becomes grotesque in its apparent asynchrony. The death of narrative is perpetually delayed; or, one might say, narrative is undead, and it remains so as it drags itself onward into the cold war era.", "title": "The Time of the Thriller: Suspense and Continuity after World War II", "claims": null}], "query": "temporal manipulation and pacing in thriller novels", "summary_abstract": "The exploration of temporal manipulation and pacing in thriller novels reveals a complex interplay between narrative structure and reader engagement. Hepburn (2023) emphasizes that pacing is a crucial element in narrative, shaped by various formal components such as dialogue and chapter breaks. He notes that authors like John le Carr\u00e9 intentionally manipulate the timing of events to create tension, often starting stories late to enhance the pressure between the narrative's content and its delivery.\n\nWang (2002) further elaborates on the artistic significance of time in literature, highlighting techniques such as temporal direction alteration and the use of flashbacks and dreams to enrich the narrative experience. This manipulation of time is essential in crafting suspense, as it allows authors to play with readers' expectations and emotional responses.\n\nMinet de Wied (1995) discusses how temporal predictability and contrast can heighten suspense, suggesting that suspense is often built through the timing of events and the reader's anticipation of outcomes. This aligns with the findings of Zirulnik (2019), who examines post-World War II thrillers and their innovative approaches to narrative continuity and time organization. He argues that these works challenge traditional storytelling conventions, thereby reshaping suspense and reader engagement.\n\nMacovei (2017) introduces a temporal annotation scheme that aids in understanding the sequence of events in novels, which can be particularly useful in thrillers where non-linear timelines are common. This method highlights the importance of temporal switches, such as flashbacks and ruptures, in maintaining narrative tension.\n\nOverall, the collective insights from these papers underscore that the manipulation of time and pacing is not merely a stylistic choice in thriller novels but a fundamental mechanism that drives suspense and shapes the reader's experience. By skillfully altering the flow of time, authors can create a gripping narrative that keeps readers on the edge of their seats.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2021}, "authors": ["Michael Marissen"], "summary": "By key standards of what in the eighteenth century and later was considered to be forward-looking and modern\u2014namely to exalt reason (above revelation, whatever the flaws of reason) as arbiter of truth, to exalt human autonomy and achievement, to exalt religious tolerance, to exalt cosmopolitanism, and to exalt social and political progressiveness\u2014Bach and his music reflected and forcefully promoted a premodern world and life view. While we are arguably free to make use of Bach and his music in whatever historically informed or uninformed ways we find fitting, we ought also to be on the ethical alert for a kind of cultural narcissism in which we end up miscasting Bach in our own ideological image and proclaiming the authenticity of that image, and hence its prestige value, in support of our own agendas.", "title": "Bach against Modernity", "claims": null}, {"metadata": {"year": 2001}, "authors": ["Antoine  Hennion", "Jo\u00ebl-Marie  Fauquet"], "summary": "Abstract Bach was not a \u2018modern composer author of the BWV and of the Cantatas Complete Works\u2019 before musicology, the record industry and the modern amateurs. This paper traces the long transformation of what became \u2018music\u2019 and how it produced our taste for Bach as a musician, giving him the strange ability of being both the object and the standard of our love for music. Through a study of the use of Bach in a country other than Germany, and from 1800, when Bach's work begins to be published in France, to 1885, the year of his birth Bicentennial, we follow Bach's grandeur as it originates in the zeal of a small circle of his \u2018early adopters\u2019, from Chopin to Alkan, from Gounod to Saint-Saens, from Liszt to Franck. We show how, all along the nineteenth century in France, Bach is becoming music: not only a reference, an ancient Master, the statue of the Commendatore in the shadow of whom we write the music of the present time, but a contemporary composer. We show also how the reverse is true: music is \u2018becoming Bach\u2019, it is reorganized around his figure (and Beethoven's), resting on their production. Bach does not integrate an already made musical universe: he produces it, partially, through the invention of a new taste for music.", "title": "Authority as performance: the love of Bach in nineteenth-century France", "claims": null}, {"metadata": {"year": 2007}, "authors": ["\ub098\uc8fc\ub9ac"], "summary": "In the second half of the eighteenth century, the comprehension and reception of Bach\u2019s music had been made in various ways. It is generally accepted that Bach\u2019s music and composition methods are to be preserved and reconstructed as they were because those can make new music style of the period strong and solid. This traditional and conservative aspect coexisted with a progressive figure of Bach, which considered Bach as a new music figure whose music could be reinterpreted and recreated to meet the expectation of the new era, and re-evaluated Bach\u2019s works as music for genuine artists, who pursued the essence of music. A innovative image of Bach that appraised his music as a living and absolute entity in perspectives of changed musical styles, new aesthetics in expression, and the sublime aesthetics came to the fore in the 1780s by Johann Friedrich Reichardt and Christian Friedrich Schubart, and became the basis for the reception of Bach in the nineteenth century.", "title": "18\uc138\uae30 \ud6c4\ubc18\uae30(1750-1800)\uc758 \ubc14\ud750\uc0c1", "claims": null}, {"metadata": {"year": 2000}, "authors": ["J. Snoj"], "summary": "Bach's music, whose characteristics were well recognized in his own time as can be seen from Scheibe's writings, became regarded as the work of an outstanding composer only a century after his death; yet in the process of his becoming a musical oracle of the past, Bach began to be understood and conceptualized in terms foreign to his own historical surroundings. As a consequence, a gap between the real historical Bach on the one side and his canonized picture on the other came into being. In scholarly literature on Bach there are various suggestions and observations in respect of the exceptional value of his music. A closer, scrutinizing examination of such observations reveals that they may point to the outstanding characteristics of his music, whose aesthetic essence, however, still cannot be conveyed in terms of logical verbal expressions. There is nothing in Bach's life that could be considered particularly outstanding, and his music, having been composed for the most part for special occasions, cannot be understood as the expression of the inner self of an exceptional human being. A historical approach to his music does not have the means to measure its value as compared to his predecessors and contemporaries apart from subjective critical judgement. The alleged religiosity of his music pales when juxtaposed to the standards of ordinary church music making in his own time. Rather, it is only from the standpoint of the modern secularized society that the religious quality of his music becomes evident. The hidden messages of his musical symbols do not impact the aesthetic qualities of his music; attesting rather to the composer's attitude towards himself they do not convey anything to the listener. As for the analytical approach, it was the Schenkerian analysis that promised to reveal the depths of Bach's music; yet according to Scruton 's critical judgement, the Schenkerian analysis might be understood as an analysis of the experience of Bach's music rather than an objective analysis of the music itself. Thus the music of Bach cannot be comprehended and appreciated except within the domain of purely musical experience.", "title": "Johann Sebastian Bach: The Utterable Scholarly Account and the Unutterable Greatness of Music", "claims": null}, {"metadata": {"year": 2016}, "authors": ["William Renwick"], "summary": "It has now been some fifty years since the publication of Hans David and Arthur Mendel's The Bach Reader ( 1945) marked in some sense the beginnings of 'modern' post-war Bach scholarship. While most of its contents remain relevant and valid, there has been and continues to be an enormous production of specialised and general studies on all aspects of Bach's life and works, much of it rising to a high peak of excellence. While at one time Spitta's", "title": "BACH AND THE OLD ' MUSICOLOGY", "claims": null}, {"metadata": {"year": 2006}, "authors": ["D. Jewell"], "summary": "I remember the first time I ever played Bach seriously, when at school we performed the fifth Brandenburg Concerto. I was revelling in the experience of playing in a small chamber group for the first time and loving the baroque cleanness of it all, when I was overwhelmed by the harpsichord cadenza to the first movement. It still never fails to astonish, the way it starts simply and then builds through rhythmic and harmonic complexity, exploiting the instrument's versatility to the full (again something not previously heard). Around the same time I was also introduced to the B minor Mass, when the Sanctus bowled me over with its majestic power, and which also amazes by its light-footed handling of the Nicene creed's linguistic difficulties. \n \nAll of which illustrates Bach's qualities. He probably never thought of himself as an artist, but as a master craftsman. John Eliot Gardiner's exploration of the sacred cantatas has reminded us of the sheer routine discipline of it: one new piece a week, with the parts written to suit whoever was going to be available that Sunday. Yet when we hear them now, Bach's own faith shines through with every note, in a way that is hard for modern listeners to share. The musicologists tell us of the mathematical patterns in the music, such as the structure of the Goldberg Variations (written, I always appreciate when I am listening to them late at night, for an insomniac), but that in turn never clouds the emotional impact, the ability to speak absolutely directly to our souls. Think of the transcendent bass aria \u2018Mache dich, mein Herze rein\u2019 [\u2018Purify yourself, my heart\u2019] that, at the end of the St Matthew Passion, always moves me to tears, or the television image of Maxim Vengerov walking through Auschwitz in the snow, playing the Chaconne from the D minor partita for violin in memorial to the victims of the Nazi holocaust. \n \nMozart, the 250th anniversary of whose birth arrives in 2006, will become the focus for much listening and debate this year. One reason put forward for his popularity is that he is very much a composer for our age, representing the moral relativism of post-enlightenment Europe in tune with the moral relativism of the late 20th century. The music of Bach, born nearly 70 years earlier, conveys a world of less doubt, a high point of order and certainty. We may not envy the certainty, but as a route to understanding it the music is without equal.", "title": "What Bach's music means to me.", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Thomas Marks"], "summary": "The perception of Johann Sebastian Bach (1685-1750) in New York City during the time of the Great War can be illuminated through two threads: 1) reception history and reputation, and 2) contemporary performance practices. Between 1914 and 1927, the reputation of Bach migrated away from one having nationalistic and Romantic associations to one embodying both a \u201cuniversal\u201d objectivity and a distinctly American subjectivity. Similarly, the manner in which Bach\u2019s music was performed also changed. Before the Great War, transcribing and arranging Bach\u2019s music was common. After the war, some New York City critics began to advocate for Bach\u2019s music to be played as historical reconstructions. Transcriptions and arrangements continued to be performed alongside historical reconstructions, and both a subjective and historically \u201cobjective\u201d approaches to performing the composer\u2019s music existed. New York City musician William Henry Humiston (1869-1923) and his idea of Bach provides a useful case study of one musician\u2019s approach to understanding the composer. Humiston\u2019s perception of the Baroque composer and his own transcriptions and arrangements of Bach\u2019s music reveal a blend of both personal subjectivity and historical awareness.", "title": "(Re)constructing Johann Sebastian Bach: Reception history and performance practice in New York City during the Great War", "claims": null}, {"metadata": {"year": 2021}, "authors": ["S. Rose"], "summary": "Bach\u2019s music is often interpreted as transcending the material conditions of everyday life. This chapter, by contrast, argues that Bach scholarship could be enriched via approaches taken from the study of material culture. It places Bach within the vibrant consumer culture of early-eighteenth-century Leipzig, exploring his postmortem inventory and his keyboard publications in the context of how the town\u2019s bourgeoisie used material goods to show their status and identity. It investigates Bach\u2019s printed and manuscript music in terms of the social practices surrounding these material artifacts. Finally, the chapter relates Bach\u2019s working practices to debates about the interplay of human and material agency. It discusses how he experimented with the material characteristics of instruments such as organs, and analyzes his compositional practice as an interaction between player-composer and contrapuntal materials.", "title": "Bach and Material Culture", "claims": null}], "query": "How was Bach's music received in terms of modernism?", "summary_abstract": "The reception of Johann Sebastian Bach's music in the context of modernism reveals a complex interplay between traditional and progressive interpretations. Marissen (2021) argues that Bach's music fundamentally embodies a premodern worldview, emphasizing reason, human autonomy, and religious tolerance, which contrasts sharply with the ideals of modernism that emerged later. This perspective warns against the tendency to reinterpret Bach through a modern lens, potentially distorting his original intent and significance.\n\nConversely, Hennion and Fauquet (2001) highlight a transformative process in the 19th century, particularly in France, where Bach's music began to be celebrated not merely as historical reference but as a contemporary force. They illustrate how figures like Chopin and Liszt contributed to a burgeoning appreciation of Bach, positioning him as both a revered master and a modern composer whose influence reshaped musical tastes.\n\nIn the late 18th century, a dual perception of Bach emerged, as noted by \ub098\uc8fc\ub9ac (2007). While some viewed his work as a conservative preservation of musical traditions, others recognized his potential for innovation, suggesting that his music could be reinterpreted to resonate with contemporary aesthetics. This innovative image gained traction in the 1780s, setting the stage for Bach's reception in the 19th century.\n\nSnoj (2000) discusses how Bach's music was initially undervalued, only gaining recognition as a monumental body of work a century after his death. This delayed appreciation led to a conceptualization of Bach that often diverged from his historical context, creating a gap between the \"real\" Bach and the canonized version that emerged in modern scholarship.\n\nMarks (2013) further illustrates this evolution in reception, particularly in New York City during the Great War, where Bach's music transitioned from Romantic associations to embodying a universal objectivity. This shift was accompanied by changes in performance practices, with a growing emphasis on historical authenticity.\n\nLastly, Rose (2021) suggests that understanding Bach's music through the lens of material culture can enrich scholarship, emphasizing the social practices surrounding his compositions and the material conditions of his time. This approach highlights the interplay between Bach's music and the cultural context in which it was created, suggesting that his work was both a product of and a response to the material realities of early 18th-century Leipzig.\n\nIn summary, Bach's music has been received through various lenses, oscillating between traditional reverence and modern reinterpretation. While some scholars emphasize his premodern qualities, others recognize his role as a precursor to modern musical thought, illustrating the multifaceted nature of his legacy.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2011}, "authors": ["Susan M Byrne", "Anthea  Fursland", "Karina L Allen", "Hunna  Watson"], "summary": "The aim of this study was to examine the effectiveness of Enhanced Cognitive Behaviour Therapy (CBT-E) for eating disorders in an open trial for adults with the full range of eating disorders found in the community. The only previously published trial of CBT-E for eating disorders was a randomised controlled trial (RCT) conducted in the U.K. for patients with a BMI \u2265 17.5. The current study represents the first published trial of CBT-E to include patients with a BMI<17.5. The study involved 125 patients referred to a public outpatient clinic in Perth, Western Australia. Patients attended, on average, 20-40 individual sessions with a clinical psychologist. Of those who entered the trial, 53% completed treatment. Longer waiting time for treatment was significantly associated with drop out. By the end of treatment full remission (cessation of all key eating disorder behaviours, BMI \u2265 18.5 kg/m(2), not meeting DSM-IV criteria for an eating disorder) or partial remission (meeting at least 2 these criteria) was achieved by two thirds of the patients who completed treatment and 40% of the total sample. The results compared favourably to those reported in the previous RCT of CBT-E, with one exception being the higher drop-out rate in the current study. Overall, the findings indicated that CBT-E results in significant improvements, in both eating and more general psychopathology, in patients with all eating disorders attending an outpatient clinic.", "title": "The effectiveness of enhanced cognitive behavioural therapy for eating disorders: an open trial.", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Rachel  Signorini", "Jeanie  Sheffield", "Natalie  Rhodes", "Carmel  Fleming", "Warren  Ward"], "summary": "The Eating Disorders Outreach Service (EDOS) at the Royal Brisbane and Women's Hospital has offered Enhanced Cognitive Behaviour Therapy (CBT-E) since 2009. The current research aimed to evaluate the effectiveness of the EDOS CBT-E program by analysing the outcome data of 114 adult outpatients (Mage = 26.06 years, SD = 8.35) with a DSM-IV diagnosis of Anorexia Nervosa, Bulimia Nervosa, or Eating Disorder Not Otherwise Specified. Outpatients attended an average of 20-40 individual sessions with a psychologist or psychiatric registrar. Of those who commenced CBT-E, only 50% completed treatment. Although a higher proportion of non-completers had an Axis II diagnosis, regression analyses revealed that the only significant predictor of drop-out from treatment was the presence of Axis IV psychosocial and environmental problems. Amongst those who completed treatment, CBT-E resulted in statistically and clinically significant improvements in eating disorder and general psychopathology, which were maintained at the 20-week follow-up. When the total sample, including non-completers was considered, statistically (and some clinically) significant improvements in eating disorder and general psychopathology were observed. The findings indicate that CBT-E is an effective treatment for adults with all eating disorders within outpatient settings. However, the high attrition observed indicates that strategies need to be identified to minimise drop-out.", "title": "The effectiveness of enhanced cognitive behavioural therapy in an outpatient setting", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Christopher G. Fairburn", "Zafra  Cooper", "Helen A. Doll", "Marianne E. O'Connor", "Robert L. Palmer", "Riccardo  Dalle Grave"], "summary": "Anorexia nervosa is difficult to treat and no treatment is supported by robust evidence. As it is uncommon, it has been recommended that new treatments should undergo extensive preliminary testing before being evaluated in randomized controlled trials. The aim of the present study was to establish the immediate and longer-term outcome following \u201cenhanced\u201d cognitive behaviour therapy (CBT-E). Ninety-nine adult patients with marked anorexia nervosa (body mass index \u2264 17.5) were recruited from consecutive referrals to clinics in the UK and Italy. Each was offered 40 sessions of CBT-E over 40 weeks with no concurrent treatment. Sixty-four percent of the patients were able to complete this outpatient treatment and in these patients there was a substantial increase in weight (7.47 kg, SD 4.93) and BMI (2.77, SD 1.81). Eating disorder features also improved markedly. Over the 60-week follow-up period there was little deterioration despite minimal additional treatment. These findings provide strong preliminary support for this use of CBT-E and justify its further evaluation in randomized controlled trials. As CBT-E has already been established as a treatment for bulimia nervosa and eating disorder not otherwise specified, the findings also confirm that CBT-E is transdiagnostic in its scope.", "title": "Enhanced cognitive behaviour therapy for adults with anorexia nervosa: A UK\u2013Italy study", "claims": null}, {"metadata": {"year": 2010}, "authors": ["A. Karbasi"], "summary": "This case study involved the treatment of a young adult female, referred to as \u201cMarie,\u201d who presented for treatment seeking help with her eating disorder. Marie evinced symptoms of dietary restriction, amenorrhea, low weight, and low body mass index (BMI); she was subsequently diagnosed with anorexia nervosa. She was treated with 20 sessions of enhanced cognitive-behavioral therapy (CBT-E), a relatively new, transdiagnostic treatment for eating disorders. Marie\u2019s progress throughout treatment is detailed in this case study report. Results supported the promising but limited body of research available on this treatment, which was highly effective for this client and, therefore, may be more generally useful for clients with anorexia nervosa.", "title": "Enhanced Cognitive-Behavioral Therapy (CBT-E) for Eating Disorders: Case Study of a Client With Anorexia Nervosa", "claims": null}, {"metadata": {"year": 2021}, "authors": ["R. Dalle Grave", "Laura Dametti", "Maddalena Conti", "Chiara Bersan", "Anna Dalle Grave", "S. Calugi"], "summary": "OBJECTIVE\nTo assess outcomes in adult patients with eating disorders administered enhanced cognitive behavioral therapy (CBT-E) in a day-hospital setting.\n\n\nMETHOD\nForty-three consecutive patients with eating disorders (86% females; mean age 28.8 [SD\u00a0=\u200911.8]) were admitted to a 13-week day-hospital CBT-E program. Twenty-five (58.1%) patients were underweight (i.e., body mass index [BMI] <18.5\u00a0kg/m2 ; BMI 15.4 [SD\u00a0=\u20092.2]) and 18 not (mean BMI 23.1 [SD\u00a0=\u20096.3]) at baseline. All patients responded poorly to prior outpatient treatment. Body mass index and Eating Disorder Examination Questionnaire (EDE-Q), Brief Symptom Inventory (BSI), and Clinical Impairment Assessment (CIA) scores were recorded at baseline, end of treatment, and 20-week follow-up.\n\n\nRESULTS\nIn 37 patients (86.0%) who completed the treatment, there were substantial improvements in eating-disorder features, general psychopathology, clinical impairment, and body weight (only in underweight patients), which were well maintained at follow-up.\n\n\nDISCUSSION\nDay-hospital CBT-E is a promising treatment for adults with eating disorders.", "title": "Day-hospital enhanced cognitive behavior therapy for adults with eating disorders: Immediate and follow-up effects.", "claims": null}, {"metadata": {"year": 2020}, "authors": ["A. Hambleton", "T. Hanstock", "R. Simeone", "Michelle Sperling"], "summary": "Enhanced Cognitive Behavioral Therapy (CBT-E) is an evidence-based treatment for bulimia nervosa (BN), which can be delivered individually or in a group format. This case study describes a young adult female\u2019s progress through a CBT-E group program at an Australian community outpatient eating disorder service. She completed 20 sessions over 10 weeks. The client was administered measures for eating disorder symptoms, depression, anxiety, stress, stage of change, and clinical impairment at pre- and posttreatment. She improved on global eating disorder symptoms, food restriction, eating concern, and her stage of change. Her weight and shape concerns remained unchanged. Her levels of depression, anxiety, and stress significantly increased after treatment, highlighting the need for concurrent treatment for these comorbidities. These results support the developing literature on the efficacy of CBT-E for BN and describe in detail the progress of an individual, including their progress and challenges within a group intervention.", "title": "Group-Delivered Enhanced Cognitive Behavior Therapy: A Focus on a Young Adult Woman With Bulimia Nervosa", "claims": null}, {"metadata": {"year": 2015}, "authors": ["M. Garte", "B. Hagen", "D. Reas", "Per J. Isdahl", "Elisabeth Hinderaker", "\u00d8. R\u00f8"], "summary": "More intense forms of enhanced cognitive-behaviour therapy (CBT-E) may be necessary for patients who do not benefit from outpatient CBT-E. A novel application of the transdiagnostic approach for eating disorders (ED) was implemented. A mixed sample of 62 patients, 13 anorexia nervosa (AN) (21%); 29 bulimia nervosa (BN) (47%) and 20 eating disorders not otherwise specified (EDNOS) (32%) were admitted to a CBT-E day treatment. Forty-seven patients (75.8%) completed the treatment as defined by completing two-thirds of treatment and the discharge assessment. A total of 31.9% (15/47) achieved recovery as defined by post-treatment body mass index (BMI)\u2009\u2265\u200918.5 and global Eating Disorder Examination Questionnaire (EDE-Q) +1 SD above community mean. An increase of 1.2 BMI units (15.8\u201317.0, p\u2009<\u2009.02) was observed for very underweight patients (BMI\u2009<\u200917.5). Patients with BN showed significantly greater improvement than AN or EDNOS for ED symptomology, state anxiety and self-esteem. Findings demonstrated high treatment acceptability of the CBT-E day treatment programme and significant, yet differential, improvement across diagnostic groups.", "title": "Implementation of a day hospital treatment programme based on CBT-E for severe eating disorders in adults: an open trial", "claims": null}, {"metadata": {"year": 2017}, "authors": ["S. Wade", "S. Byrne", "K. Allen"], "summary": "OBJECTIVE\nThis randomized control trial is an evaluation of the effectiveness of enhanced cognitive behavioral treatment (CBT-E) for eating disorders adapted for a group setting. The study aimed to examine the effects of group CBT-E on eating disorder psychopathology and additional maintaining pathology.\n\n\nMETHOD\nA transdiagnostic sample of individuals with eating disorders with a BMI\u2009\u2265\u200918 kg/m2 (N\u2009=\u200940) were randomized to an immediate-start or delayed-start condition so as to compare therapeutic effects of group CBT-E with a waitlist control. Global Eating Disorder Examination Questionnaire (EDE-Q) scores, BMI, and measures of Clinical Perfectionism, Self-Esteem, Interpersonal Difficulties, and Mood Intolerance were measured across the 8-week control period, throughout the group treatment and at 3-months post-treatment.\n\n\nRESULTS\nOver 70% of those who entered the trial completed treatment. The first eight weeks of group CBT-E were more effective at reducing Global EDE-Q scores than no treatment (waitlist control). By post-treatment, good outcome (a Global EDE-Q within 1 SD of Australian community norms plus BMI\u2009\u2265\u200918.5) was achieved by 67.9% of treatment completers and 66.7% of the total sample. Symptom abstinence within the previous month was reported by 14.3% of treatment completers and 10.3% of the total sample. Significant reductions in Clinical Perfectionism, Self-Esteem, Interpersonal Difficulties, and Mood Intolerance were also observed.\n\n\nDISCUSSION\nThis study demonstrated that a group version of CBT-E can be effective at reducing eating disorder psychopathology in a transdiagnostic sample of individuals with eating disorders. Group CBT-E could provide a means of increasing availability of evidence-based treatment for eating disorders.", "title": "Enhanced cognitive behavioral therapy for eating disorders adapted for a group setting", "claims": null}], "query": "more than 40 session of cbt-e for resistant patients with eating disorders", "summary_abstract": "The research question regarding the effectiveness of more than 40 sessions of Enhanced Cognitive Behavioral Therapy (CBT-E) for resistant patients with eating disorders is addressed through various studies that explore the treatment's outcomes across different settings and patient populations.\n\nByrne et al. (2011) conducted an open trial involving 125 patients with a range of eating disorders, where participants attended an average of 20-40 sessions. They found that 40% of the total sample achieved either full or partial remission, indicating significant improvements in eating disorder symptoms. However, the study noted a high dropout rate, suggesting that longer treatment durations may be necessary for some patients.\n\nFairburn et al. (2013) specifically examined the outcomes of 40 sessions of CBT-E for patients with anorexia nervosa. They reported that 64% of participants completed the treatment, leading to substantial weight gain and marked improvements in eating disorder features. This study provides strong preliminary support for the effectiveness of extended CBT-E sessions, particularly for those with severe anorexia.\n\nIn a day-hospital setting, Dalle Grave et al. (2021) assessed 43 patients who had previously responded poorly to outpatient treatment. They found that 86% of those who completed the program showed significant improvements in eating disorder symptoms and general psychopathology, reinforcing the idea that more intensive treatment may benefit resistant patients.\n\nGarte et al. (2015) also highlighted the necessity for more intense forms of CBT-E for patients who do not respond to standard outpatient treatment. Their findings indicated that a day treatment program led to significant improvements, particularly for patients with bulimia nervosa, suggesting that extended and more intensive sessions can yield better outcomes.\n\nOverall, the collective evidence suggests that while CBT-E is effective for many patients with eating disorders, those who are more resistant may benefit from extended treatment durations, potentially exceeding 40 sessions. The studies indicate that both outpatient and day-hospital settings can facilitate significant improvements, particularly for patients with severe symptoms or those who have not responded well to previous treatments (Byrne et al., 2011; Fairburn et al., 2013; Dalle Grave et al., 2021; Garte et al., 2015).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2023}, "authors": ["Dr. C. Nalini", "Dr. R Ashok Kumar", "A. Professor"], "summary": "-A branch of artificial intelligence called \"generative AI\" is concerned with creating models and algorithms that can create new data that is like a given dataset. Generative AI seeks to imitate human creativity by learning patterns and distributions from existing data to produce wholly new content, contrast to standard AI systems that are built for specific tasks. Learning a model that captures the underlying patterns and structures of the data is the fundamental idea behind generative AI. The new samples produced by this model can then be compared to the old dataset. The creation of images, texts, music, videos, and other things is one of the many tasks that generative AI is frequently utilised for.", "title": "Generative AI: A Comprehensive Study of Advancements and Application", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Stephanie Houde", "Q. Liao", "Jacquelyn Martino", "Michael J. Muller", "David Piorkowski", "John T. Richards", "Justin D. Weisz", "Yunfeng Zhang"], "summary": "Generative AI is a class of machine learning technology that learns to generate new data from training data. While deep fakes and media-and art-related generative AI breakthroughs have recently caught people's attention and imagination, the overall area is in its infancy for business use. Further, little is known about generative AI's potential for malicious misuse at large scale. Using co-creation design fictions with AI engineers, we explore the plausibility and severity of business misuse cases.", "title": "Business (mis)Use Cases of Generative AI", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Seong Ho Park"], "summary": "Generative artificial intelligence (AI) refers to algorithms that can be used to create new content, such as text, code, images, videos, and audio. Particularly, with the introduction of generative adversarial networks (GAN) in medical imaging [1,2], generative AI has gained significant attention in the scientific community, leading to numerous publications in the past few years. The Korean Journal of Radiology (KJR) has published several articles on this topic [3-5]. However, the landscape of generative AI in scientific research and publication has dramatically shifted with the emergence of generative large language models (LLMs), such as ChatGPT, which are capable of generating text that closely resembles human writing and easily accessible to the public. The use of LLMs is rapidly expanding in scientific publications [6], creating ethical and legal concerns and challenges related to research integrity, plagiarism, copyright infringement, and authorship, not only for authors, but also for peer reviewers and editors [7-9]. Moreover, these concerns and challenges Use of Generative Artificial Intelligence, Including Large Language Models Such as ChatGPT, in Scientific Publications: Policies of KJR and Prominent Authorities", "title": "Use of Generative Artificial Intelligence, Including Large Language Models Such as ChatGPT, in Scientific Publications: Policies of KJR and Prominent Authorities", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Varun Kisan Nhavkar"], "summary": "Abstract: This research paper aims to explore the impact of generative artificial intelligence (AI) on IT professionals. Generative AI refers to the technology that can autonomously create new content, such as images, text, and even code. With the rapid advancements in AI, the role of IT professionals is evolving, and it is crucial to understand the implications of generative AI on their work. This paper examines the potential benefits, challenges, and ethical considerations associated with the integration of generative AI in the IT industry. Through a comprehensive analysis of existing literature, this paper sheds light on the transformation of IT professionals' responsibilities, skills, and career prospects in the era of generative AI.", "title": "Impact of Generative AI on IT Professionals", "claims": null}, {"metadata": {"year": 2023}, "authors": ["J. Hullman", "Ari Holtzman", "A. Gelman"], "summary": "Generative AIs are objects that produce creative outputs in the style of human expression. We argue that encounters with the outputs of modern generative AI models are mediated by the same kinds of aesthetic judgments that organize our interactions with artwork. The interpretation procedure we use on art we find in museums is not an innate human faculty, but rather the result of a historical process involving disciplines such as art history and art criticism. This gives us pause when considering our reactions to generative AI, how we should approach this new medium, and why generative AI seems to incite so much fear about the future. We naturally inherit a conundrum of causal inference from the history of art: a work can be read as a symptom of the cultural conditions that influenced its creation while simultaneously being framed as a timeless, seemingly acausal distillation of an eternal human condition. In this essay, we focus on an unresolved tension when we bring this dilemma to bear in the context of generative AI: are we looking for proof that generated media reflects something about the conditions that created it or some eternal human essence? Are current modes of interpretation sufficient for this task? Historically, new forms of art have changed how art is interpreted, with such influence used as evidence that a work of art has touched some essential human truth. As generative AI influences contemporary aesthetic judgment we outline some of the pitfalls and traps in attempting to scrutinize what AI generated media means .", "title": "Artificial Intelligence and Aesthetic Judgment", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Ziv Epstein", "Aaron Hertzmann", "L. Herman", "Robert Mahari", "M. Frank", "Matthew Groh", "Hope Schroeder", "Amy Smith", "Memo Akten", "Jessica Fjeld", "H. Farid", "Neil Leach", "A. Pentland", "Olga Russakovsky"], "summary": "A new class of tools, colloquially called generative AI, can produce high-quality artistic media for visual arts, concept art, music, fiction, literature, video, and animation. The generative capabilities of these tools are likely to fundamentally alter the creative processes by which creators formulate ideas and put them into production. As creativity is reimagined, so too may be many sectors of society. Understanding the impact of generative AI - and making policy decisions around it - requires new interdisciplinary scientific inquiry into culture, economics, law, algorithms, and the interaction of technology and creativity. We argue that generative AI is not the harbinger of art's demise, but rather is a new medium with its own distinct affordances. In this vein, we consider the impacts of this new medium on creators across four themes: aesthetics and culture, legal questions of ownership and credit, the future of creative work, and impacts on the contemporary media ecosystem. Across these themes, we highlight key research questions and directions to inform policy and beneficial uses of the technology.", "title": "Art and the science of generative AI: A deeper dive", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Cynthia Zastudil", "M. Rogalska", "C. Kapp", "Jennifer L. Vaughn", "S. MacNeil"], "summary": "Generative models are now capable of producing natural language text that is, in some cases, comparable in quality to the text produced by people. In the computing education context, these models are being used to generate code, code explanations, and programming exercises. The rapid adoption of these models has prompted multiple position papers and workshops which discuss the implications of these models for computing education, both positive and negative. This paper presents results from a series of semi-structured interviews with 12 students and 6 instructors about their awareness, experiences, and preferences regarding the use of tools powered by generative AI in computing classrooms. The results suggest that Generative AI (GAI) tools will play an increasingly significant role in computing education. However, students and instructors also raised numerous concerns about how these models should be integrated to best support the needs and learning goals of students. We also identified interesting tensions and alignments that emerged between how instructors and students prefer to engage with these models. We discuss these results and provide recommendations related to curriculum development, assessment methods, and pedagogical practice. As GAI tools become increasingly prevalent, it's important to understand educational stakeholders' preferences and values to ensure that these tools can be used for good and that potential harms can be mitigated.", "title": "Generative AI in Computing Education: Perspectives of Students and Instructors", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Roberto Gozalo-Brizuela", "Eduardo C. Garrido-Merch'an"], "summary": "Generative AI has experienced remarkable growth in recent years, leading to a wide array of applications across diverse domains. In this paper, we present a comprehensive survey of more than 350 generative AI applications, providing a structured taxonomy and concise descriptions of various unimodal and even multimodal generative AIs. The survey is organized into sections, covering a wide range of unimodal generative AI applications such as text, images, video, gaming and brain information. Our survey aims to serve as a valuable resource for researchers and practitioners to navigate the rapidly expanding landscape of generative AI, facilitating a better understanding of the current state-of-the-art and fostering further innovation in the field.", "title": "A survey of Generative AI Applications", "claims": null}], "query": "What is generative AI?", "summary_abstract": "Generative AI is a rapidly evolving branch of artificial intelligence focused on creating new content that mimics existing data. It employs algorithms and models to learn patterns from a given dataset, enabling the generation of diverse outputs such as text, images, music, and videos (Dr. C. Nalini & Dr. R Ashok Kumar, 2023; Seong Ho Park, 2023). This technology has gained significant traction in various fields, including the arts and sciences, particularly with the advent of generative adversarial networks (GANs) and large language models (LLMs) like ChatGPT (Seong Ho Park, 2023).\n\nThe applications of generative AI are extensive, with over 350 identified across different domains, including visual arts, literature, and even medical imaging (Roberto Gozalo-Brizuela & Eduardo C. Garrido-Merch'an, 2023). These tools are not only reshaping creative processes but also raising important questions about ethics, ownership, and the future of creative work (Ziv Epstein et al., 2023). For instance, the integration of generative AI in computing education is prompting discussions about its role in enhancing learning while also addressing concerns about academic integrity and the quality of generated content (Cynthia Zastudil et al., 2023).\n\nMoreover, the potential for misuse of generative AI technologies, particularly in creating deep fakes and other deceptive media, has sparked discussions about the need for regulatory frameworks to mitigate risks (Stephanie Houde et al., 2020). As generative AI continues to develop, it is crucial to explore its implications across various sectors, ensuring that its benefits are maximized while minimizing potential harms (Varun Kisan Nhavkar, 2023; J. Hullman et al., 2023). \n\nIn summary, generative AI represents a transformative force in technology and creativity, with the capacity to produce high-quality content and alter traditional practices, while also necessitating careful consideration of ethical and societal impacts.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2018}, "authors": ["A. Adegboye", "Samuel Iweriebor"], "summary": "The primary link between financial institutions and economic performance is the provision of resources by these institutions to businesses in order to drive enterprise development. In this study, the role of access to finance in enhancing innovation and productivity among Nigerian small and medium\u2010sized enterprises (SMEs) is investigated using the World Bank Enterprise Survey (ES) dataset. Access to finance is categorized as external and internal to the firm. Using the logit estimation technique, the study finds that ease of accessing bank credit is the strongest positive force in driving all types of innovation among SMEs in Nigeria. In the same vein, the source of investment financing matters in terms of how it affects innovation: both internal and external sources improve investment in product, process, and organizational innovation, but only external financing has a significant effect on R&D spending and use of foreign licensed technology. Overall spending on R&D is only driven by access to external finance by the SMEs. The study also shows that increased access to finance may actually lead to productivity decline among SMEs in Nigeria.", "title": "Does Access to Finance Enhance SME Innovation and Productivity in Nigeria? Evidence from the World Bank Enterprise Survey", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Isarinde Taye Oluwayemi", "Asoya Andrew Nnamdi"], "summary": "The primary link between financial institutions and economic performance is the provision of resources by these institutions to businesses in order to drive enterprise expansion, sustainability and development. In this study, the role of access to finance in driving innovations and productivity in Nigerian SMEs is investigated using the World Bank Enterprise Survey (ES) dataset. Innovation is defined based on the ES analysis using five compositions. The study considers finance in terms of source in meeting investment needs and in terms of access. Moreover, finance access is categorized as external and internal to a firm with each having different implications. The results from the empirical analysis shows that in general, access to finance drives firm innovation and productivity. In particular, ease of accessing bank loans and credit is shown to be the strongest positive force in driving all types of innovation among SMEs in Nigeria. In the same vein, the type of finance source for investment matters in terms of how it affects innovation: both internal and external sources improve investment in product, process and organizational innovation, but only external financing has effect on R&D spending and use of foreign licensed technology. For productivity, the results show that while structure of financing investment improves productivity, access to finance may actually lead to productivity decline among SMEs in Nigeria.", "title": "THE ROLE OF ACCESS TO FINANCE IN DRIVING INNOVATION AND PRODUCTIVITY OF SMES IN NIGERIA? EVIDENCE FROM THE WORLD BANK ENTERPRISE SURVEY", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Richard Mayowa Dada"], "summary": "Access to credit is crucial for the growth and survival of Small and Medium-sized Enterprises (SMEs), this is because the ability of SMEs to grow depend highly on their potentials to invest in restructuring, innovation, etc. And all of these investments need capital, and therefore access to finance.", "title": "Commercial Banks\u2019 Credit and SMEs Development in Nigeria: An Empirical Review", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Akin A. Idowu"], "summary": "Knowledge ABSTRACT In Nigeria, access to finance has been identified as a key element for SMEs to succeed in their drive to build productive capacity, to compete, to create jobs and to contribute to poverty alleviation in the country. Without finance, SMEs cannot acquire or absorb new technologies nor can they expand to compete in global markets or even strike business linkages with larger firms. Despite these, SMEs traditionally have faced difficulty in obtaining formal credit or equity from commercial banks. This study showed a downward trend despite recapitalisation by Nigeria commercial banks, it therefore conclude and recommend that there is more to Banks performance on stimulation of economy through SME than mere recapitalisation", "title": "for Global Development APPRAISAL OF BANK\u2019S LOAN DISBURSEMENT TO SMALL AND MEDIUM ENTERPRISES IN NIGERIA AFTER BANK RECAPITALIZATION", "claims": null}, {"metadata": {"year": 2018}, "authors": [], "summary": "Small and medium-sized enterprises are observed as capable instruments to solving the critical problems of development and poverty affecting most developing countries, therefore the need for their growth in Nigeria is beyond question. Amidst many other problems, gaining access to bank credits and other financial markets have been identified as a key hindrance to this growth. This work tried to review this challenging problem of SMEs financing in Nigeria with a view of identifying alternative financing options besides the traditional banks credit system. The study made use of a survey. These hypothesis; SMES do not suffer great challenges in accessing financial support from traditional commercial banks in Nigeria, and NBFIs cannot help in providing access to finance for SMEs were tested. The analysis led to the rejection of the two hypotheses. It is recommended that the government should formulate policies that will encourage commercial banks to relax their restrictive regulations and operations which discourage borrowing and offer more credit facilities for SMEs.", "title": "Financing Options for Small and Medium Enterprises (SMEs): Exploring Non-Bank Financial Institutions as an Alternative Means of Financing", "claims": null}, {"metadata": {"year": 2020}, "authors": ["L. Effiom", "S. Edet"], "summary": "Abstract The liberalization of the Nigerian financial sector inevitably resulted in the phenomenon of financial innovation. While studies exist that investigate the effect of financial innovation on economic growth in Nigeria, only a few have addressed its impact on the performance of SMEs. Yet, the few existing ones are micro studies that emphasize more on specific regional firms. The dearth of empirical studies at the economy-wide level, therefore, mandates this study, which fills this gap in the literature. This study employs autoregressive distributed lag methodology on quarterly data of financial innovation measures. Our findings indicate that financial innovation has a positive and significant effect on SMEs\u2019 productivity in Nigeria. In particular, of the seven financial innovation instruments used (Automated Teller Machine, Point of Sales, Web or Internet Banking, Cheques, Nigeria Inter-bank Settlement System Electronic Fund Transfer, Nigeria Inter-Bank Settlement System Instant Payment, and Mobile Money Operations), all but one turned out in both the short run and long run as significant predictors of SMEs\u2019 performance in Nigeria. Furthermore, the Toda-Yamamoto causality test reveals unidirectional causation running from financial innovation instruments to SMEs\u2019 performance. These results have implications for SMEs\u2019 growth and the current cashless policy of the Central Bank of Nigeria.", "title": "Financial innovation and the performance of small and medium scale enterprises in Nigeria", "claims": null}, {"metadata": {"year": 2012}, "authors": ["M. Duru", "L. Kehinde"], "summary": "Financial sector reforms are changes that are needed in order to establish a modern financial system capable of acting as the catalyst of the economy and allocating the economy\u2019s savings in the most productive way among competing alternatives users. Financial sector reforms began in Nigeria with the deregulation of interest rates during the Structural Adjustment Programme that was introduced mid 1986.Since then far reaching policy measures have been initiated these include the licensing of new banks, savings and financial companies in other to enthrone the market principle of free entry and exit in the financial system. The upward review of paid up capital for banks and insurance companies to help mobilize domestic and foreign savings, deepen and broaden intermediation and enforce strict regulatory and prudential guidelines. The main objective of the paper is to access the impact of financial sector reforms on the growth of small scale enterprises in Nigeria. The paper used modeling method to determine output performance of SMEs as a function of several inputs such as firm\u2019s characteristics, firm\u2019s ownership and credit facilities through the financial sector. The results indicate that all these variables have positive and significant impact on the output performance of SMEs in Nigeria. We therefore accept that financial sector reforms have positive impact on the growth of SMEs in Nigeria. The paper recommends that the government should create an enabling environment by providing infrastructural facilities and security to ease the cost of doing business and attract foreign investors and build confidence in the economy.", "title": "Financial Sector Reforms and the Growth of Small and Medium Scale Enterprises (Smes) In Nigeria", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Godgift David Ahmed-Ishmel", "Charles Onyeiwu", "Obakayode Akinola Owopetu"], "summary": "This study examines the impact of Financial Technology in the Operations (Payments/Collections) of SMEs in Nigeria. The study conducted a survey of 120 Small and Medium Scale Enterprises across the four (4) identified geo-political zones in Lagos state. These SMEs with employment ranging from 2-10 employees in the fashion, educational, online merchants, pharmaceuticals, automobile, cosmetics, agro-allied, printing, bakery, eatery, I.T. firms and retail enterprises. This was done in such a way that the four axes were represented, each axis having thirty (30) SMEs. One hundred (100) Questionnaires were found useful for the purpose of the study representing 83% of the total questionnaire distributed. The data was analyzed using inferential statistics. The study reveals that Financial Technology (FinTech) has great impact on the economy, and therefore contributing positively to national development. It also discussed the benefits and risks of embracing and investing in FinTech. In order to remain sustainable and profitable in business in this 21st century, it was recommended that the financial authorities and government embark on massive campaign and support of emerging FinTech organizations as well as embracing FinTech in its operations. Also, the SMEs are highly recommended to adopt FinTech payment channels in their operations.", "title": "The Impact of Financial Technology in the Operations (Payments/Collections) of SMEs in Nigeria", "claims": null}], "query": "why SMEs' Access to Financial Services in Nigeria through Fintech is good for Nigeria", "summary_abstract": "The collective findings from the reviewed papers highlight the significant benefits of Fintech in enhancing the access of Small and Medium-sized Enterprises (SMEs) to financial services in Nigeria, which in turn positively impacts the country's economic landscape.\n\nAdegboye and Iweriebor (2018) emphasize that access to external finance, particularly bank credit, is crucial for driving innovation and productivity among Nigerian SMEs. Their study indicates that while both internal and external financing sources contribute to various types of innovation, only external financing significantly influences research and development (R&D) spending. This suggests that Fintech, which often facilitates easier access to external financing, can be a vital tool for fostering innovation in SMEs.\n\nSimilarly, Oluwayemi and Nnamdi (2020) reinforce the idea that access to finance is a key driver of innovation and productivity. They note that the structure of financing can enhance productivity, although they also caution that increased access to finance can sometimes lead to productivity declines. This duality underscores the importance of how financial services are accessed and utilized, a role that Fintech can play by providing more tailored financial solutions.\n\nDada (2014) and Idowu (2017) further support the notion that access to credit is essential for the growth and survival of SMEs. They argue that without adequate financial resources, SMEs struggle to invest in necessary innovations and expansions, which are critical for competing in both local and global markets. The introduction of Fintech solutions can alleviate some of these challenges by offering alternative financing options that are often more accessible than traditional banking methods.\n\nEffiom and Edet (2020) provide empirical evidence that financial innovations, including those introduced by Fintech, have a positive and significant effect on the productivity of SMEs. Their findings suggest that various financial innovation tools, such as mobile money and internet banking, are effective in enhancing the performance of SMEs, thereby contributing to broader economic growth.\n\nLastly, Ahmed-Ishmel et al. (2018) specifically examine the impact of Fintech on the operations of SMEs, concluding that it plays a crucial role in improving payment and collection processes. They advocate for greater support and integration of Fintech solutions within the SME sector to ensure sustainability and profitability.\n\nIn summary, the integration of Fintech into the financial services landscape for SMEs in Nigeria is not only beneficial for individual enterprises but also serves as a catalyst for broader economic development. By improving access to finance, fostering innovation, and enhancing productivity, Fintech has the potential to significantly contribute to Nigeria's economic growth and development.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2022}, "authors": ["Moulik Choraria", "Leello Dadi", "Grigorios G. Chrysos", "J. Mairal", "V. Cevher"], "summary": "Polynomial neural networks (PNNs) have been recently shown to be particularly effective at image generation and face recognition, where high-frequency information is critical. Previous studies have revealed that neural networks demonstrate a $\\textit{spectral bias}$ towards low-frequency functions, which yields faster learning of low-frequency components during training. Inspired by such studies, we conduct a spectral analysis of the Neural Tangent Kernel (NTK) of PNNs. We find that the $\\Pi$-Net family, i.e., a recently proposed parametrization of PNNs, speeds up the learning of the higher frequencies. We verify the theoretical bias through extensive experiments. We expect our analysis to provide novel insights into designing architectures and learning frameworks by incorporating multiplicative interactions via polynomials.", "title": "The Spectral Bias of Polynomial Neural Networks", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Yuan Cao", "Zhiying Fang", "Yue Wu", "Ding-Xuan Zhou", "Quanquan Gu"], "summary": "An intriguing phenomenon observed during training neural networks is the spectral bias, which states that neural networks are biased towards learning less complex functions. The priority of learning functions with low complexity might be at the core of explaining the generalization ability of neural networks, and certain efforts have been made to provide a theoretical explanation for spectral bias. However, there is still no satisfying theoretical result justifying the underlying mechanism of spectral bias. In this paper, we give a comprehensive and rigorous explanation for spectral bias and relate it with the neural tangent kernel function proposed in recent work. We prove that the training process of neural networks can be decomposed along different directions defined by the eigenfunctions of the neural tangent kernel, where each direction has its own convergence rate and the rate is determined by the corresponding eigenvalue. We then provide a case study when the input data is uniformly distributed over the unit sphere, and show that lower degree spherical harmonics are easier to be learned by over-parameterized neural networks. Finally, we provide numerical experiments to demonstrate the correctness of our theory. Our experimental results also show that our theory can tolerate certain model misspecification in terms of the input data distribution.", "title": "Towards Understanding the Spectral Bias of Deep Learning", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Yuan  Cao", "Zhiying  Fang", "Yue  Wu", "Ding-Xuan  Zhou", "Quanquan  Gu"], "summary": "An intriguing phenomenon observed during training neural networks is the spectral bias, which states that neural networks are biased towards learning less complex functions. The priority of learning functions with low complexity might be at the core of explaining the generalization ability of neural networks, and certain efforts have been made to provide a theoretical explanation for spectral bias. However, there is still no satisfying theoretical result justifying the underlying mechanism of spectral bias. In this paper, we give a comprehensive and rigorous explanation for spectral bias and relate it with the neural tangent kernel function proposed in recent work. We prove that the training process of neural networks can be decomposed along different directions defined by the eigenfunctions of the neural tangent kernel, where each direction has its own convergence rate and the rate is determined by the corresponding eigenvalue. We then provide a case study when the input data is uniformly distributed over the unit sphere, and show that lower degree spherical harmonics are easier to be learned by over-parameterized neural networks. Finally, we provide numerical experiments to demonstrate the correctness of our theory. Our experimental results also show that our theory can tolerate certain model misspecification in terms of the input data distribution.", "title": "Towards Understanding the Spectral Bias of Deep Learning", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Q. Hong", "Qinyan Tan", "Jonathan W. Siegel", "Jinchao Xu"], "summary": "Neural networks are universal function approximators which are known to general-ize well despite being dramatically overparameterized. We study this phenomenon from the point of view of the spectral bias of neural networks. Our contributions are two-fold. First, we provide a theoretical explanation for the spectral bias of ReLU neural networks by leveraging connections with the theory of \ufb01nite element methods. Second, based upon this theory we predict that switching the activation function to a piecewise linear B-spline, namely the Hat function, will remove this spectral bias, which we verify empirically in a variety of settings. Our empirical studies also show that neural networks with the Hat activation function are trained signi\ufb01cantly faster using stochastic gradient descent and ADAM. Combined with previous work showing that the Hat activation function also improves generalization accuracy on image classi\ufb01cation tasks, this indicates that using the Hat activation provides signi\ufb01cant advantages over the ReLU on certain problems.", "title": "On the Activation Function Dependence of the Spectral Bias of Neural Networks", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Q. Hong", "Qinyan Tan", "Jonathan W. Siegel", "Jinchao Xu"], "summary": "Neural networks are universal function approximators which are known to generalize well despite being dramatically overparameterized. We study this phenomenon from the point of view of the spectral bias of neural networks. Our contributions are two-fold. First, we provide a theoretical explanation for the spectral bias of ReLU neural networks by leveraging connections with the theory of finite element methods. Second, based upon this theory we predict that switching the activation function to a piecewise linear B-spline, namely the Hat function, will remove this spectral bias, which we verify empirically in a variety of settings. Our empirical studies also show that neural networks with the Hat activation function are trained significantly faster using stochastic gradient descent and ADAM. Combined with previous work showing that the Hat activation function also improves generalization accuracy on image classification tasks, this indicates that using the Hat activation provides significant advantages over the ReLU on certain problems.", "title": "On the Activation Function Dependence of the Spectral Bias of Neural Networks", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Jonas Kiessling", "Filip Thor"], "summary": "Neural networks have a bias towards low frequency functions. This spectral bias has been the subject of several previous studies, both empirical and theoretical.\n Here we present a computable definition of the spectral bias based on a decomposition of the reconstruction error into a low and a high frequency component. The distinction between low and high frequencies is made in a way that allows for easy interpretation of the spectral bias. Furthermore, we present two methods for estimating the spectral bias. Method 1 relies on the use of the discrete Fourier transform to explicitly estimate the Fourier spectrum of the prediction residual, and Method 2 uses convolution to extract the low frequency components, where the convolution integral is estimated by Monte Carlo methods.\n The spectral bias depends on the distribution of the data, which is approximated with kernel density estimation when unknown. We devise a set of numerical experiments that confirm that low frequencies are learned first, a behavior quantified by our definition.", "title": "A Computable Definition of the Spectral Bias", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Nasim Rahaman", "A. Baratin", "Devansh Arpit", "Felix Dr\u00e4xler", "Min Lin", "F. Hamprecht", "Yoshua Bengio", "Aaron C. Courville"], "summary": "Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with $100\\%$ accuracy. In this work, we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we show that deep ReLU networks are biased towards low frequency functions, meaning that they cannot have local fluctuations without affecting their global behavior. Intuitively, this property is in line with the observation that over-parameterized networks find simple patterns that generalize across data samples. We also investigate how the shape of the data manifold affects expressivity by showing evidence that learning high frequencies gets \\emph{easier} with increasing manifold complexity, and present a theoretical understanding of this behavior. Finally, we study the robustness of the frequency components with respect to parameter perturbation, to develop the intuition that the parameters must be finely tuned to express high frequency functions.", "title": "On the Spectral Bias of Neural Networks", "claims": null}, {"metadata": {"year": 2023}, "authors": ["J. Lazzari", "Xiuwen Liu"], "summary": "Spectral bias is an important observation of neural network training, stating that the network will learn a low frequency representation of the target function before converging to higher frequency components. This property is interesting due to its link to good generalization in over-parameterized networks. However, in low dimensional settings, a severe spectral bias occurs that obstructs convergence to high frequency components entirely. In order to overcome this limitation, one can encode the inputs using a high frequency sinusoidal encoding. Previous works attempted to explain this phenomenon using Neural Tangent Kernel (NTK) and Fourier analysis. However, NTK does not capture real network dynamics, and Fourier analysis only offers a global perspective on the network properties that induce this bias. In this paper, we provide a novel approach towards understanding spectral bias by directly studying ReLU MLP training dynamics. Specifically, we focus on the connection between the computations of ReLU networks (activation regions), and the speed of gradient descent convergence. We study these dynamics in relation to the spatial information of the signal to understand how they influence spectral bias. We then use this formulation to study the severity of spectral bias in low dimensional settings, and how positional encoding overcomes this.", "title": "Understanding the Spectral Bias of Coordinate Based MLPs Via Training Dynamics", "claims": null}], "query": "spectral bias of neural networks", "summary_abstract": "The research question regarding the spectral bias of neural networks has been addressed in various studies, revealing a consistent theme: neural networks tend to learn low-frequency functions more readily than high-frequency ones. This phenomenon, termed \"spectral bias,\" has implications for the generalization capabilities of these networks.\n\nChoraria et al. (2022) highlight that polynomial neural networks (PNNs) exhibit a spectral bias towards low frequencies but can be designed to accelerate the learning of higher frequencies through specific architectures, such as the $\\Pi$-Net family. Their findings suggest that incorporating multiplicative interactions via polynomials can enhance the learning process.\n\nCao et al. (2019, 2021) provide a rigorous theoretical framework for understanding spectral bias, linking it to the neural tangent kernel (NTK). They demonstrate that the training dynamics of neural networks can be decomposed along eigenfunctions of the NTK, with lower-degree spherical harmonics being easier to learn. Their numerical experiments support the idea that neural networks prioritize learning simpler, low-complexity functions, which may explain their generalization abilities.\n\nHong et al. (2022) further explore the spectral bias in ReLU networks, proposing that switching to a piecewise linear B-spline activation function can mitigate this bias. Their empirical results indicate that this change not only reduces spectral bias but also enhances training efficiency and generalization performance.\n\nKiessling and Thor (2022) offer a computable definition of spectral bias, emphasizing the importance of data distribution in this context. Their methods for estimating spectral bias confirm that neural networks learn low frequencies first, reinforcing the findings of previous studies.\n\nRahaman et al. (2018) also contribute to this understanding by showing that deep ReLU networks are inherently biased towards low-frequency functions, which aligns with the networks' tendency to discover simple patterns that generalize well across data samples.\n\nLastly, Lazzari and Liu (2023) investigate the dynamics of ReLU networks and their training processes, revealing that severe spectral bias can hinder convergence to high-frequency components, particularly in low-dimensional settings. They propose that high-frequency sinusoidal encoding can help overcome this limitation.\n\nCollectively, these studies underscore the significance of spectral bias in neural networks, illustrating how it influences learning dynamics and generalization, while also suggesting potential strategies to mitigate its effects.", "summary_extract": null}, {"papers": [{"metadata": {"year": 1975}, "authors": ["K. Morita", "M. Kono"], "summary": "Abstract Chloroplasts and the lamellae systems were isolated from rice leaves and the change with leaf age in the nitrogen fractions localized in the lamellae systems and the stroma region was examined. The pattern of the change in these two chloroplastic nitrogen fractions varied according to the leaf position. In order to find out the relationship between the change in the chloroplastic nitrogen fractions and the photosynthetic activity of a leaf, the rate of oxygen evolution of leaf sections on a total leaf nitrogen basia was also followed. The rate of oxygen evolution/total leaf nitrogen was affected by two factors. i.e., lamellae nitrogen/total leaf nitrogen and the lamellae-stroma composition of the chloroplast. An increase in lamellae nitrogen/total leaf nitrogen caused an increase in oxygen evolution, whereas an increaae in the proportion of stroma nitrogen in chloroplast brought about the negative effect. Consequently, oxygen evolution/total leaf nitrogen had a close correlation with an index of ...", "title": "RELATIONSHIP BETWEEN THE CHANGE IN THE CHLOROPLASTIC NITROGEN FRACTIONS AND THE RATE OF OXYGEN EVOLUTION IN RICE PLANTS", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Per L. Gregersen", "Christine H. Foyer", "Karin  Krupinska"], "summary": "The productivity of crop plants is directly related to net photosynthesis over the growing season. For example, there is a direct link between the duration of flag leaf photosynthesis and cereal grain yields. The optimisation of electron transport, carbon assimilation and assimilate production is important at all stages of leaf development, not least during senescence when chloroplasts are dismantled and photosynthetic proteins are recycled for vegetative or reproductive development and in particular for grain filling. The energy-producing reactions of the thylakoid membranes are closely linked to the energy-utilising reactions of metabolism. Imbalances in these processes favour an enhanced production of reactive oxygen species (ROS) that may act as signals leading to changes in nuclear gene expression. The sensing and signalling functions of chloroplasts may change during the breakdown of stromal and thylakoid membrane components and may vary between species and cultivars which use different strategies for dismantling of the photosynthetic apparatus during senescence. A knowledge of how chloroplast to nucleus signalling is altered during senescence, particularly in relation to redox and hormone signalling pathways, will provide novel insights that can be incorporated into breeding strategies aimed at enhancing productivity by prolonging the lifespan of leaves.", "title": "Photosynthesis and Leaf Senescence as Determinants of Plant Productivity", "claims": null}, {"metadata": {"year": 1966}, "authors": ["D. Appleman", "A. Fulco", "P. M. Shugarman"], "summary": "Photosynthetic oxygen evolution per milligram of chlorophyll in Chlorella vulgaris varies with the age of the culture. The rate of oxygen evolution is low in the starting cells, it rises to a maximum after 24 hours of growth and then declines to the initial low value after 72 to 90 hours. These changes in photosynthetic competence of chlorophyll in Chlorella are paralleled by changes in \u03b1-linolenate per milligram of chlorophyll. In general the magnitude of the photosynthetic competence of chlorophyll is directly proportional to the magnitude of the ratio of \u03b1-linolenate to chlorophyll, regardless of whether high ratios are due to high \u03b1-linolenates or low chlorophyll values. This relationship holds when the cultures are grown either under continuous or intermittent illumination.", "title": "Correlation of \u03b1-Linolenate to Photosynthetic O2 Production in Chlorella", "claims": null}, {"metadata": {"year": 1966}, "authors": ["D. Appleman", "A. Fulco", "P. M. Shugarman"], "summary": "Photosynthetic oxygen evolution per milligram of chlorophyll in Chlorella vulgaris varies with the age of the culture. The rate of oxygen evolution is low in the starting cells, it rises to a maximum after 24 hours of growth and then declines to the initial low value after 72 to 90 hours. These changes in photosynthetic competence of chlorophyll in Chlorella are paralleled by changes in alpha-linolenate per milligram of chlorophyll. In general the magnitude of the photosynthetic competence of chlorophyll is directly proportional to the magnitude of the ratio of alpha-linolenate to chlorophyll, regardless of whether high ratios are due to high alpha-linolenates or low chlorophyll values. This relationship holds when the cultures are grown either under continuous or intermittent illumination.", "title": "Correlation of alpha-Linolenate to Photosynthetic O(2) Production in Chlorella.", "claims": null}, {"metadata": {"year": 1995}, "authors": ["J.  Schnackenberg", "H.  Ikemoto", "Shigetoh  Miyachi"], "summary": "Abstract The relationship between photosynthesis and photohydrogen evolution was investigated in a highly CO2-tolerant strain of Chlorococcum littorale. Concentrations of 0.03% to 40% CO2 were applied during growth of the cells. A concentraction of 5% CO2 is the most favourable condition for growth, pigment formation and carbohydrate formation, all of which tail off towards a concentration of 40% CO2. The same pattern was observed for oxygen evolution and photohydrogen evolution. The reason for the decline at high CO2 concentrations was found in the partial inhibition of photosystem II (PS II). The initial rates of photohydrogen evolution were at least double those of O2 evolution, demonstrating the higher affinity of the hydrogenase than the NADP-reductase to the electrons provided by the photosynthetic electron transport chain. CO2 concentration had no influence on the in vitro activity of the hydrogenase.", "title": "Relationship between oxygen-evolution and hydrogen-evolution in a Chlorococcum strain with high CO2-tolerance", "claims": null}, {"metadata": {"year": 1973}, "authors": ["D. Walker", "K. Ko\u015bciukiewicz", "C. Case"], "summary": "SUMMARY \nOxygen production by chloroplasts isolated from spinach was measured. The lag in oxygen production following first illumination of the chloroplasts was independent of light intensity and chlorophyll concentration. The length of the lag period was unaffected by addition of DCMU to the reaction mixture but was increased by addition of orthophosphate or arsenate. It was also longer at lower temperatures. The data are consistent with the view that the lag results from the time necessary for the concentration of photosynthetic metabolites to increase to a level that allows maximal rate of photosynthesis. However, anomalies in behaviour occur and their significance is discussed.", "title": "PHOTOSYNTHESIS BY ISOLATED CHLOROPLASTS: SOME FACTORS AFFECTING INDUCTION IN CO2\u2010DEPENDENT OXYGEN EVOLUTION", "claims": null}, {"metadata": {"year": 1970}, "authors": ["J. Boyer", "B. Bowen"], "summary": "Chloroplasts were isolated from pea and sunflower leaves having various water potentials. Oxygen evolution by the chloroplasts was measured under identical conditions for all treatments with saturating light and with dichloroindophenol as oxidant. Evolution was inhibited when leaf water potentials were below -12 bars in pea and -8 bars in sunflower and the inhibition was proportional to leaf water potential below these limits. Inhibition was more severe in sunflower than in pea chloroplasts. In sunflower, it could be detected after 5 minutes of leaf desiccation, and, up to 1 hour, the effect was independent of the duration of low leaf water potential.In high light, the reduction in activity of sunflower chloroplasts paralleled the reduction in CO(2) fixation by intact sunflower plants having low leaf water potentials. Stomatal apertures and transpiration rates were also reduced under these conditions and were probably limiting. In low light, intact sunflowers required more light per unit of CO(2) fixed when leaf water potentials were low than when they were high. This increased light requirement in the intact system was of a magnitude which could be predicted from the reduced oxygen evolution by the isolated chloroplasts. It was concluded that moderately low leaf water potential affects photosynthesis in at least two ways: first, through an inhibition of oxygen evolution by chloroplasts and, second, by closure of stomata in intact leaves.", "title": "Inhibition of oxygen evolution in chloroplasts isolated from leaves with low water potentials.", "claims": null}, {"metadata": {"year": 2021}, "authors": ["A. Lodeyro", "A. Krapp", "N. Carrillo"], "summary": "Contemporary climate change is characterized by the increased intensity and frequency of environmental stress events such as floods, droughts and heatwaves, which have a debilitating impact on photosynthesis and growth, compromising the production of food, feed and biofuels for an expanding population. The need to increase crop productivity in the context of global warming has fueled attempts to improve several key plant features such as photosynthetic performance, assimilate partitioning and tolerance to environmental stresses. Chloroplast redox metabolism, including photosynthetic electron transport and CO2 reductive assimilation, are primary targets of most stress conditions, leading to excessive excitation pressure, photodamage and propagation of reactive oxygen species. Alterations in chloroplast redox poise, in turn, provide signals that exit the plastid and modulate plant responses to the environmental conditions. Understanding the molecular mechanisms involved in these processes could provide novel tools to increase crop yield in sub-optimal environments. We describe herein various interventions into chloroplast redox networks that resulted in increased tolerance to multiple sources of environmental stress. They included manipulation of endogenous components and introduction of electron carriers from other organisms, which affected not only stress endurance but also leaf size and longevity. The resulting scenario indicates that chloroplast redox pathways have an important impact on plant growth, development and defense that goes beyond their roles in primary metabolism. Manipulation of these processes provide additional strategies for the design of crops with improved performance under destabilized climate conditions as foreseen in the future.", "title": "Photosynthesis and chloroplast redox signaling in the age of global warming: stress tolerance, acclimation and developmental plasticity.", "claims": null}], "query": "What is the relationship between the rates oxygen evolution in the chloroplasts and the productivity of crops", "summary_abstract": "The relationship between oxygen evolution rates in chloroplasts and crop productivity is multifaceted, as evidenced by the findings from various studies. Morita and Kono (1975) established that the rate of oxygen evolution in rice leaf chloroplasts is closely linked to the nitrogen composition within the chloroplasts, indicating that higher lamellae nitrogen relative to total leaf nitrogen enhances oxygen production. This suggests that optimizing chloroplast composition could potentially improve photosynthetic efficiency and, consequently, crop yield.\n\nGregersen et al. (2014) further emphasized the importance of photosynthesis in crop productivity, noting that net photosynthesis directly correlates with crop yields, particularly during critical growth phases such as grain filling. They highlighted that the efficiency of electron transport and carbon assimilation in chloroplasts is vital for maximizing productivity, especially during leaf senescence when chloroplasts are dismantled. This indicates that maintaining high rates of oxygen evolution during these stages could be crucial for sustaining crop yields.\n\nAppleman et al. (1966) observed that the rate of oxygen evolution in Chlorella vulgaris varies with culture age, peaking at 24 hours before declining. This pattern suggests that the timing of peak photosynthetic activity is essential for maximizing productivity, which could be relevant for crop management practices.\n\nWalker et al. (1973) found that oxygen production in spinach chloroplasts is influenced by the accumulation of photosynthetic metabolites, which may take time to reach levels that support maximal photosynthesis. This lag in oxygen evolution could affect overall crop productivity if not managed properly.\n\nBoyer and Bowen (1970) demonstrated that low leaf water potential significantly inhibits oxygen evolution in chloroplasts, which in turn affects CO2 fixation in intact plants. This highlights the importance of water availability in maintaining high rates of photosynthesis and, by extension, crop productivity.\n\nLastly, Lodeyro et al. (2021) discussed how environmental stressors impact chloroplast function, leading to reduced photosynthetic performance and crop yields. They suggested that manipulating chloroplast redox metabolism could enhance stress tolerance and improve overall plant growth and productivity.\n\nIn summary, the collective findings indicate that higher rates of oxygen evolution in chloroplasts are associated with improved crop productivity, influenced by factors such as chloroplast composition, environmental conditions, and the timing of photosynthetic activity. Optimizing these factors could lead to enhanced crop yields, particularly in the face of environmental stresses.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2021}, "authors": ["Geoffrey A. Hosking"], "summary": "The USSR was a unique empire in the universality of its claims and its aim of complete equality between nationalities. Its strengths and weaknesses were indissolubly connected. It was formally a federal state, with extensive rights given to constituent nationalities; in practice it was tightly centralized through Gosplan, the armed forces, the security services, and the Communist Party, with its messianic ideology. The USSR\u2019s tight centralization ensured that in wartime it could mobilize social energy to an unprecedented extent, but also that in peacetime localized patronage became the main form of social cohesion. The economy was so rigidly planned as to discourage innovation, which meant that the USSR could not maintain its superpower status. Its nationality policy both encouraged ethnic feeling and repressed it. The final collapse was precipitated by the clash between the largest republic, Russia, and the Soviet Union as a whole.", "title": "The Soviet Union", "claims": null}, {"metadata": {"year": 2015}, "authors": ["F. Schnell"], "summary": "In the course of the Russian Civil War the Bolsheviks conquered most of the territories of the former Tsarist Empire. Consequently, Soviet state-building faced context conditions of multi-ethnicity and cultural diversity. Given the Bolshevik's strive for centralised control and the conviction that nationalism was a threat to a unitary revolutionary state, the Bolsheviks created what they considered to be a modern multi-national state: a federation of formally independent national republics. Yet, in fact, political rule in the Soviet Union was rather characterised by political practices typical for empires: unequal centre-periphery relations, foreign domination and internal colonisation. This became increasingly apparent in the 1930s under Stalin's despotism. Soviet state-building did not result in the creation of a modern multinational state, but rather in imperial metamorphosis. Regarding the way the Soviet Union was ruled, it was an \u00abempire in disguise\u00bb that proved its imperial nature during and after World War II and finally fell victim to its composite structure.", "title": "Empire in Disguise: The Soviet-Russian Imperial Metamorphosis after World War I", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Ikeda"], "summary": "Today, the term \u201cSoviet Empire\u201d has almost ceased to function as a pejorative of the Cold War brand, at least in the academic world. Owing to the blossoming of empire studies, especially to a number of volumes dedicated to the Soviet Union, the understanding of the USSR as an empire has become common during the last two decades. The \u201cimperial\u201d approach to the Soviet Union is helpful in turning our attention to the multiethnic features of the USSR, structured not simply as \u201cthe prison of nations,\u201d but as the newest type of composite state, where the political identities of various nationalities were constantly in the making, interacting with the Communist Party at the core as an agent of social engineering.1 However, Soviet imperial studies are just beginning, with many questions awaiting further study. An especially important question concerns the problem of imperial heritage: what continuity, if it existed,", "title": "Chapter 5 Toward an Empire of Republics : Transformation of Russia in the Age of Total War , Revolution , and Nationalism", "claims": null}, {"metadata": {"year": 2006}, "authors": ["Paul  Dukes"], "summary": "Before 1914, imperial Russia was an integral member of the global system of international relations. Its policies have often been held responsible for the outbreak of World War I.1 Later, in the period leading up to 1939, the Soviet Union was on the periphery of this global system. Yet, it has been blamed for the outbreak of hostilities leading to World War II, and for the Cold War afterwards.2 In or out, apparently, Russia possesses qualities that make it a liability in world affairs. However, neither assertion is acceptable, since the purpose of the study of international relations is not to attribute praise or blame, but to investigate the manner in which all the parties involved made their contribution to the maintenance of peace or the outbreak of war.", "title": "Soviet \u2018Foreign Policy\u2019 and the Versailles-Washington System", "claims": null}, {"metadata": {"year": 2022}, "authors": ["I. Bukrieieva", "L. Afanasieva", "N. Hlebova", "Lyudmila Glyns\u2019ka", "Mykhailo Semikin"], "summary": "Today, Russian imperialism is undergoing a new phase of its transformation, moving from covert interference in the internal affairs of sovereign states to open territorial expansion. Its primary task is plans to seize the sovereign state of Ukraine through military aggression, forced inoculation of its own model of the world order and return to the space of a single \"Russian world\". The former post-Soviet Baltic republics, Belarus, Moldova, Georgia are considered as the next territories for integration into the imperial space of Russia. And this is a war not only and not so much for territories, but for ideas, a worldview that will dominate them. The main features of imperialism as an ideology and policy are an irrational desire for military expansion and absolute security, an unobsessed regime with a \u201ccenter-periphery\u201d subordination relationship, a patronage-client scheme, and the \u201cprivatization\u201d of state functions by the ruling elite, turning them into a source of private profits. A distinctive feature of Russian imperialism is that it carries out not only territorial expansion, but above all ideological and ideological expansion, instilling its ideological picture of the world on the conquered peoples. The Russian imperial ideology has been formed over the centuries and is based on such mental traits as: superiority, dominance, psychological inclination towards permissiveness and robbery, servile submission to power in various forms, abnormal patience, religious fanaticism and more. These features of mentality became the basis of the imperial ideology of state building as a system of views, ideas and worldview of Russians. Russian imperialism (rashism) is a syncretism of Russian Nazism, Orthodoxy and psychotraumatic nostalgia for the USSR, which has turned into an imperial identity.", "title": "POLITICAL AND MENTAL FOUNDATIONS OF RUSSIAN IMPERIALISM", "claims": null}, {"metadata": {"year": 2000}, "authors": ["Francine Hirsch"], "summary": "I n an appeal to the United Nations in 1980, a self-designated Turkestani emigre organization maintained that the Soviet Union, \"one of the largest colonial powers ever,\" had \"swallowed up\" Turkestan, \"one of the oldest nations in the world.\" The petitioners characterized the national-territorial delimitation of Central Asia, which had eliminated Turkestan, Bukhara, and Khiva in 1924 and distributed their territories among several new Soviet national republics, as an example of \"divide and rule.\"' During the Cold War years, Western observers typically shared this view of the Soviet Union as a colonial empire and \"breaker of nations.\" Writing as the colonies of the British Empire were attaining independence in the 1960s, a number of Western scholars described the Soviet Union as an empire that cloaked its own colonial policy in anti-imperialist rhetoric.2 Soviet scholars vociferously countered such", "title": "Toward an Empire of Nations: Border\u2010Making and the Formation of Soviet National Identities", "claims": null}, {"metadata": {"year": 2015}, "authors": ["\u0410. \u0412. \u0420\u0435\u043d\u0430\u0440\u0434-\u041a\u043e\u043a\u0442\u044b\u0448"], "summary": "Russia\u2019s case demonstrates that huge territory, the biggest number of neighbouring\u00a0countries in the world, numerous ethnic groups and suffi cient reserves of natural resources have been\u00a0historically used by elites to cultivate the idea of Russian state\u2019s exclusiveness. Over the various epochs\u00a0it has determined the authorities\u2019 thinking style as an imperial one on the level of tacit knowledge,\u00a0which has practically turned into extending the spheres of infl uence as well as into expansionism.", "title": "\u0413\u0435\u043e\u043f\u043e\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0438 \u0438\u0434\u0435\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0430 \u0435\u0432\u0440\u0430\u0437\u0438\u0439\u0441\u043a\u043e\u0433\u043e \u043d\u0430\u0446\u0438\u0435\u0441\u0442\u0440\u043e\u0438\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u0430", "claims": null}, {"metadata": {"year": 2016}, "authors": ["J. Stern"], "summary": "This definition has been plucked from midair, with barely any attempt to explain the historical process through which Russia and China, within the space of just 25 years, changed from bureaucratically degenerated and deformed workers states into imperialist powers. Were it merely a matter of expressing political opposition to the regimes in Beijing and Moscow, it would not be necessary to employ the epithet \u201cimperialist.\u201d The International Committee of the Fourth International calls for the overthrow of the capitalist states in Russia and China by the working class as an essential component of the world socialist revolution. ... What political purpose, it must be asked, is served by adding the word \u201cimperialist\u201d to descriptions of China and Russia? In practical political terms, it serves very definite functions. First, it relativizes, and therefore diminishes, the central and decisive global counterrevolutionary role of American, European and Japanese imperialism. This facilitates the pseudo-left\u2019s active collaboration with the United States in regime-change operations such as in Syria, where the Assad regime has been backed by Russia. Second, and even more significantly, the designation of China and Russia as imperialist\u2014and thus, by implication, as colonial powers suppressing ethnic, national, linguistic and religious minorities\u2014sanctions the pseudo-left\u2019s support for imperialist-backed \u201cnational liberation\u201d uprisings and \u201ccolor revolutions\u201d within the boundaries of the existing states.", "title": "Behind the designation of Russia and China as \u201cimperialist\u201d: A case study in theoretical charlatanry", "claims": null}], "query": "Why the USSR was an imperialist state?", "summary_abstract": "The question of why the USSR was considered an imperialist state is addressed through various scholarly perspectives that highlight its complex structure and policies. Geoffrey A. Hosking (2021) describes the USSR as a unique empire that claimed universality and equality among nationalities, yet was tightly centralized, which led to a paradoxical relationship between its federal structure and actual governance. This centralization allowed for significant wartime mobilization but fostered localized patronage in peacetime, ultimately contributing to its collapse.\n\nF. Schnell (2015) argues that the Bolsheviks, while attempting to create a modern multi-national state, inadvertently established an imperial structure characterized by unequal center-periphery relations and internal colonization. This imperial nature became more pronounced under Stalin, suggesting that Soviet state-building was more about maintaining control than fostering genuine national autonomy.\n\nIkeda (2018) notes that the understanding of the USSR as an empire has gained traction in academic discourse, emphasizing its multiethnic composition and the dynamic interplay of national identities within a framework dominated by the Communist Party. This perspective frames the USSR not merely as a \"prison of nations\" but as a composite state with ongoing negotiations of identity and power.\n\nFrancine Hirsch (2000) provides a historical context, describing the Soviet Union as a colonial power that employed \"divide and rule\" tactics in Central Asia, effectively erasing the identity of nations like Turkestan. This aligns with the view that the USSR cloaked its imperial ambitions in anti-imperialist rhetoric, a sentiment echoed by other scholars who argue that the Soviet regime's policies were fundamentally colonial in nature.\n\nMoreover, A. V. Renard-Koktysh (2015) highlights how Russia's vast territory and diverse ethnic groups have historically fostered an imperial mindset among its elites, leading to expansionist policies. This imperial identity has persisted through various epochs, shaping the state's approach to governance and international relations.\n\nIn summary, the USSR's characterization as an imperialist state stems from its centralized control, the imposition of unequal relations among its diverse nationalities, and its historical practices that align with colonialism. The interplay of ideology, governance, and identity within the Soviet framework reveals a complex imperial structure that ultimately contributed to its downfall.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2020}, "authors": ["Qing-dong Li", "Daniel Yu", "Xiaoyan Sun"], "summary": "The core content of \"technical economics\" course teaching is the decision-making analysis and evaluation of the project, and the course teaching is the main channel to cultivate students' innovation ability. Based on the analysis of the connotation and implementation path of engineering postgraduate education, this paper points out that it is one of the best practical ways for students to obtain \"application innovation\" education which integrates innovative knowledge with practical ability training into professional curriculum teaching.", "title": "Teaching Reform of Technical Economics for Engineering Postgraduates", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Lijuan Zhang", "W. Teng", "Xiao-fing Li"], "summary": "Engineering economics is a course with strong application, in order to improve students' comprehensive application ability and realize the training goal of applied talents. In this paper, based on the idea of collaborative education, a model of multi-dimensional collaborative teaching model is constructed, and the model is improved by applying the model to the process of teaching practice. In the teaching practice, the teaching methods and means have been reformed, and the teaching design has been systematically combed. Finally, the problems and solutions in the application practice are put forward. Through the study of this paper, in order to promote the reform of personnel training model, promote the construction of the curriculum system.", "title": "Teaching Reform and Design of Engineering Economics", "claims": null}, {"metadata": {"year": 2004}, "authors": ["Jean Russ"], "summary": "For engineering students in disciplines other than industrial engineering, the required engineering economy course is sometimes considered to be uninteresting or even irrelevant. In an attempt to increase the appeal of our economics course, we have successfully introduced two elements. First, we build a large portion of the course around a case study that requires a simple product design, a manufacturing plan, and a marketing plan \u2013 all of which must meet a tight time schedule and a specified budget. Second, we require our engineering students to work with marketing and accounting students from a business policy class, creating a multi-disciplinary team with the task of developing a solution that is technically feasible, financially viable, and appealing to the target market.", "title": "Learning Across Disciplines: A Case Study Approach To Teaching Engineering Economics And Business Policy", "claims": null}, {"metadata": {"year": 2015}, "authors": ["S. Zoghi"], "summary": "Engineering economics is a field that addresses the dynamic environment of economic calculations and principles through the prism of engineering. It is a fundamental skill that all successful engineering firms employ in order to retain competitive advantage and market share. Many schools across the country have integrated courses in engineering economics for their students, thereby providing them with the tools to optimize profits, minimize costs, analyze various scenarios, forecast fluctuations in business cycles, and more. Despite the importance of this field, many universities are unable to effectively teach economic concepts to engineering students in ways they are able to understand. A survey by Lavelle, et al. displayed that fewer than half of participants used effective educational practices (i.e. collaborative grouping) when teaching engineering economics. By promoting a more engaging and holistic learning approach, students can have the opportunity to become better problem solvers. Accordingly, ABET (Accreditation Board for Engineering and Technology) has published strict course outcome requirements for accredited programs. It is the intent of this paper to highlight various methods of teaching engineering economics to students in ways that maximize learning, as well as emphasize its importance for the modern engineer. Through the vigilant implementation of various teaching styles, experiential learning techniques, and integrated curricula, engineering students can successfully bridge the gap between the real world and the classroom. These can be achieved through a variety of mediums, one of which includes collaborative learning. An article by Prism succinctly highlights the benefits: (1) Higher levels of achievement; (2) Greater academic self-esteem; (3) Solving more complex problems; (4) Positive relationships.", "title": "Engineering Economics and Its Role in the Engineering Curricula", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Shi Hai-ju"], "summary": "The paper indicates the course of Engineering Economics is the one playing the important role in the major system of engineering management,points out the teaching reform for the Engineering Economics in the teaching practice,including the emphasis on the selection of textbooks,the teaching in the first lesson,the clear interpretation of the basic formula,the attention to the teaching of difficult points,the trail of case teachings,the improvement of interaction of class teaching,the scientific application of multi-media and network technique,and the enhancement of the teaching quality of teachers etc.,so as to have the model and direction for the following courses of the major of the engineering management.", "title": "On teaching consideration and reform for the course of Engineering Economics", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Zhou Bing"], "summary": "As construction class specialized core backbone course and advantage key construction disciplines, engineering economics has been the engineering management specialty, the construction cost, a compulsory specialized fundamental course for students majoring in civil engineering structure. Given the practical features of the course, teachers should accord to different specialties, students\u2019 the truth and the actual requirements, in the teaching of engineering economics arrangement good, organize the teaching content, and highlight the practical value and application and applied to improve the students\u2019 ability. The professional cognitive ability and interest in learning can play an important role in improving teachers\u2019 teaching effectiveness.", "title": "Discussion of the Teaching and Practice Reform of Engineering Economics", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Zhang Yan"], "summary": "Along with our infrastructure construction,and with the implementation of bidding and tendering,supervision system,the construction market increased the demand for complex talent,that understanding engineering and economic,coupled previous recent years,college students employment increased pressure,exacerbate students' thirst for practical knowledge,this is also a new challenge for the teaching of professional courses.In this paper,based on years of engineering economics courses teaching practice,to study the teaching methods of engineering economics,to improve the teaching effectiveness.", "title": "Preliminary Study of Teaching Methods about Engineering Economics", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Min Jing"], "summary": "Engineering economics is a professional course with strong practicability. In view of the characteristics of this course, teachers should be the basis of teaching practice. On the basis of case teaching, we need to carry out reforms to meet the needs of China's construction market for complex engineering talents.", "title": "Research on Case Teaching Reform of Engineering Economics Course", "claims": null}], "query": "teaching strategies for economics and engineering at the tertiary level", "summary_abstract": "The research question regarding effective teaching strategies for economics and engineering at the tertiary level is addressed through various innovative approaches highlighted in the literature. \n\nLi et al. (2020) emphasize the importance of integrating innovative knowledge with practical ability training within the curriculum, particularly in \"technical economics\" courses, to foster students' innovation capabilities. This aligns with Zhang et al. (2019), who propose a multi-dimensional collaborative teaching model aimed at enhancing students' comprehensive application abilities, thereby promoting a more effective personnel training model in engineering economics.\n\nRuss (2004) introduces a case study approach that combines product design, manufacturing, and marketing, encouraging interdisciplinary collaboration among engineering and business students. This method not only makes the course more engaging but also ensures that students develop solutions that are technically feasible and financially viable.\n\nZoghi (2015) highlights the necessity of employing diverse teaching methods, including experiential learning and collaborative techniques, to effectively convey economic concepts to engineering students. This is crucial for bridging the gap between theoretical knowledge and real-world application, which is further supported by Hai-ju (2011), who advocates for teaching reforms that enhance interaction and utilize multimedia resources to improve educational quality.\n\nMoreover, Zhou (2015) and Zhang (2012) stress the need for tailoring teaching content to meet the practical demands of the engineering field, thereby enhancing students' cognitive abilities and interest in learning. Min (2018) also supports this notion by advocating for case-based teaching reforms that align with the evolving needs of the construction market.\n\nCollectively, these studies suggest that a combination of collaborative learning, practical application, and innovative teaching methods are essential for effectively teaching economics and engineering at the tertiary level. By focusing on real-world applications and interdisciplinary collaboration, educators can significantly enhance student engagement and learning outcomes in these fields.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2020}, "authors": ["Jaimee Hughes", "Verena T Vaiciurgis", "S. Grafenauer"], "summary": "Flour, typically derived from wheat, rye, corn and rice is a pantry staple, providing structure to bread and baked goods. This study aimed to provide a cross-sectional analysis of flour for home baking, highlighting the nutrition composition of whole grain flour and identifying novel categories. An audit was undertaken in February 2020, in four major supermarkets in metropolitan Sydney (Aldi, Coles, IGA and Woolworths). Ingredient lists, Nutrition Information Panel, claims, and country of origin were collected. The median and range were calculated for energy, protein, fat, saturated fat, carbohydrate, sugars, dietary fibre and sodium. Overall, 130 products were collected, including 26 plain flour, 12 self-raising, 17 plain wholemeal, 4 wholemeal self-raising, 20 bread-making mixes (4 were whole grain), 20 other refined grain (including corn and rice flour), 17 gluten-free, 3 legume, 4 fruit/vegetable, 4 coconut and 3 other non-grain (e.g., hemp seed, cricket flour) products. Plain wheat flour dominated the category, while whole grain (wholemeal) made up 19% of products, yet they contained significantly more dietary fibre (p < 0.001) and protein (p < 0.001). Self-raising flours were significantly higher in sodium (p < 0.001) and gluten-free products were lower in protein and dietary fibre, making legume, buckwheat and quinoa flour a better choice. Sustainability principles in fruit and vegetable production and novel insect products have driven new product development. There is a clear opportunity for further on-pack promotion of whole grain and dietary fibre within the category via food product labelling.", "title": "Flour for Home Baking: A Cross-Sectional Analysis of Supermarket Products Emphasising the Whole Grain Opportunity", "claims": null}, {"metadata": {"year": 2017}, "authors": ["D. Stoin", "C. Jianu", "A. Velciov", "M. Negrea", "S. Pintilie", "A. Cozma", "T. Trasca"], "summary": "Cereals, fruits and vegetables have interesting nutritional properties and their inclusion in the diet is encouraged, their incorporation in bakery products could be a good way to increase consumption. In this study, the influence of the partial replacement of wheat flour (WF) with four types of flour: rye flour (RF), whole rye flour (WRF), quinoa flour (QF), buckwheat flour (BF) and goji fruit GFr) in eight mixtures obtained therefrom, was analyzed. The wheat flour was blended with rye flour, whole rye flour, quinoa flour, buckwheat flour and goji fruit in the ratios of 25:25:25:25 and 25:50:25. The studied samples were subjected to the following analyzes: determination of moisture, protein content, ash content, fat content, fiber content and total carbohydrate content. The resulting mixtures had high protein content (12.35% at M7 and 13.30% at M2), fiber (4.11% at M5 and 7.76% at M4), ash (1.79% at M8 and 2.15% at M1) fat (1.63% at M8 and 3.57% at M1) and low carbohydrates (62.35% at M2 and 65.68% at M7) and moisture ranged between 11.20% at M7 and 12.37% at M2. This study determined the optimum proportions to form these high nutritional flour mixtures. Centralizing the data obtained in this study, the high nutrient intake of the analyzed mixtures can be seen, and on the basis of these, we can recommend their use on the industrial scale in the recipes for the production of finely produced products with superior nutritional value.", "title": "Chemical composition evaluation of some flour mixtures with high nutritional value", "claims": null}, {"metadata": {"year": 1997}, "authors": ["E. B. Bennion", "G. S. T. Bamford", "A. J. Bent"], "summary": "The European Directive on Nutrition Labelling of Foodstuffs (EC Council Directive, 1990) sets out rules on the amount of information and the way in which it is presented on food labels. When nutritional information is provided on a food label, the information shall be given per 100 g of the food as sold and shall consist of: \n \n \nthe energy value of the food in kilojoules and kilocalories; \n \n \nthe amounts in grams of protein, total carbohydrates and fat; \n \n \nthe amount of any other nutrient for which a claim is made.", "title": "Nutritional value of flour confectionery", "claims": null}, {"metadata": {"year": 1941}, "authors": [], "summary": "THE Accessory Food Factors Committee of the Lister Institute and the Medical Research Council, in a memorandum issued in the summer of 1940, recommended the adoption for bread of a flour of 80\u201385 per cent extraction of the wheat, in the following terms: \u201cFlour for the bread of the people should contain the germ of the wheat grain, as much as possible of the aleurone layer, and the finer portions of the bran. Instead of flour consisting of about 70 per cent of the wheat grain, as it does at present, the percentage extracted should be at least 80 to 85 per cent\u201d.*", "title": "Specifications for National Flour", "claims": null}, {"metadata": {"year": 2022}, "authors": ["N. Naumova", "Y. A. Betz"], "summary": "Despite the sufficient volume of wheat flour production, there are low-grade, falsified products on the consumer market, and flour with low baking properties is also found. In this regard, the purpose of the research was to assess the quality and nutritional value of first grade baking wheat flour from different manufacturers. Studied: organoleptic, physico-chemical, hygienic quality indicators, the content of dietary fiber, mineral elements and vitamins. It was determined that all flour samples in terms of organoleptic indicators met the regulated requirements of GOST 26574-2017, in terms of the amount of pesticides and mycotoxins - the norms of SanPiN 2.3.2.1078-01 and TR TS 021/2011. Flour produced by IP A.A. Mikhailyuta (Omsk region, Kormilovsky district, Pobeditel village) met the requirements of the current standard for first-class products in terms of physical and chemical parameters, was safe in terms of the level of heavy metals within the framework of current regulatory documents, and had a complete macro- and micronutrient composition. Flour produced by JSC \u201cShadrinsk combine of bread products\u201d (Kurgan region, Shadrinsk) turned out to be unsafe in terms of Pb content, the excess of which was more than 3 times, which is unacceptable. Flour produced by KFH V.V. Malyutov (Ivanovo region, Puchezhsky district, Poperekovo village) did not meet the requirements of GOST 26574-2017 in terms of such indicators as the quantity and quality of gluten, ash content, which may indicate its falsification.", "title": "ASSESSMENT OF THE QUALITY AND NUTRITIONAL VALUE OF FIRST GRADE WHEAT FLOUR FROM DIFFERENT MANUFACTURERS", "claims": null}, {"metadata": {"year": 2003}, "authors": ["O. Cherevko", "G. Krutovyi", "G. Zaparenko", "A. Borysova", "V. Gorban"], "summary": "Quantitative analyses of the groups of the related nutrients balance in nine projected floury products enriched with the deficient nutrients is first fulfilled. It is established that the methodology proposed for the projection of floury products guarantees a high indicators of nutrients balance for three groups investigated. The best level of nutrients balancing is observed for the group of ten essential amino acids that is provided by proper mathematical modeling of ingredients content in the products receipts. The proposed approach to quantitative evaluation of the nutrients balance may be used for the analysis of other groups of the related nutrients both in floury products, as well as in expendable and daily diets, i.e. at different stages of multipurpose nutrition systems creation, including medicinal and prophylactic systems.", "title": "Section 1. NEW TECHNOLOGIES OF FOOD PRODUCTS UNCONVENTIONAL FLOURY PRODUCTS: QUANTITATIVE ANALYSIS OF NUTRIENTS BALANCE", "claims": null}, {"metadata": {"year": 2020}, "authors": ["A. Alekseev"], "summary": "Flour-based culinary products are in demand by various consumer groups. These products occupy a special place in the nutrition of schoolchildren. The article discusses the main components and features of the influence of rice flour on confectionery products with high nutritional value. The influence of rice flour on the formation of the quality of flour-based semi-finished products with its use is revealed. The compositions of ingredients, preserving the traditional shape and taste of flour-based products, are selected. The recipes are developed, and the competitiveness of the developed flour-based products from rice flour is evaluated. Organoleptic and physico-chemical indicators of the developed products were studied.", "title": "Development of flour-based culinary products for school nutrition", "claims": null}, {"metadata": {"year": 2021}, "authors": ["\u0410. Vasyukova", "A. Slavyanskiy", "A. Alekseev", "A. Moshkin", "Yu. V. Bondarenko", "Yu.V. Adrov"], "summary": "The article examines the main components and features of the influence of sea buckthorn, laminaria, stevia, rice and corn flour on culinary flour products with increased nutritional value. The influence of rice flour on the formation of the quality of flour semi-finished products with its use was revealed. Compositions of ingredients were selected, preserving the traditional shape and taste of flour products. Formulations were developed, and the competitiveness of innovative flour products made from rice flour was evaluated. Organoleptic and physicochemical characteristics of the developed products were studied.", "title": "Functional flour products with herbal dietary supplements", "claims": null}], "query": "I want a list of Flour recommendations in terms of nutritional composition and labeling Answe until 2019r up", "summary_abstract": "The research on flour recommendations in terms of nutritional composition and labeling reveals a variety of insights into the nutritional profiles of different flour types and the importance of labeling for consumer awareness.\n\nHughes et al. (2020) conducted a comprehensive analysis of flour products available in major supermarkets, highlighting that whole grain flours, which constituted 19% of the products surveyed, offered significantly higher dietary fiber and protein content compared to plain wheat flour. This suggests that whole grain options are nutritionally superior and should be promoted more prominently on packaging to encourage consumer choice.\n\nStoin et al. (2017) explored the nutritional benefits of blending wheat flour with alternative flours such as rye, quinoa, and buckwheat. Their findings indicated that these mixtures not only enhanced protein and fiber content but also provided a more balanced nutritional profile, making them suitable for industrial applications in bakery products.\n\nBennion et al. (1997) emphasized the importance of clear nutritional labeling as mandated by European regulations, which require detailed information on energy, protein, carbohydrates, and fats per 100 grams. This regulatory framework is crucial for consumers to make informed choices regarding their flour selections.\n\nNaumova and Betz (2022) assessed the quality of first-grade baking wheat flour from various manufacturers, noting significant variations in nutritional value and safety. Their study highlighted the necessity for consumers to be aware of the quality indicators and potential contaminants in flour products, reinforcing the need for stringent labeling practices.\n\nOverall, the collective findings underscore the nutritional advantages of whole grain and alternative flours, the importance of clear labeling for consumer education, and the need for quality assurance in flour production. These insights can guide consumers in making healthier choices when selecting flour products.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2019}, "authors": ["M. I. Khan"], "summary": "Research is indispensable for the growth and\u00a0development of all disciplines, particularly medical sciences which fundamentally need continuous research and progressive innovation.\u00a0Writing and reporting scientific discoveries is an important outcome of a researcher.\u00a0Mostly, editors and reviewers appreciate manuscripts that are easy to read and to edit beside valuable scientific contents.\u00a0Medical writings have significantly improved in past two decades resulting in increased number of medical journals and quality of reporting.\u00a0Once research findings are ready to be reported, a researcher makes an educated choice, as to where to get it published.\u00a0A high impact factor indicates that research findings published in journal are considered highly influential.\u00a0With stirring zeal and drive, the \u201cJournal of Shifa Tameer-e-Millat University\u201d (JSTMU) is being launched and a dedicated team of editors, advisors and reviewers has been appointed to facilitate the researchers who wish to publish in JSTMU.\u00a0", "title": "Research and reporting", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Somya Ish", "P. Ish"], "summary": "The national medical commission (NMC) guidelines for medical faculty in India requires publications of meta\u2010analysis, systematic reviews, original articles, or case series in journals indexed in Medline, PubMed Central, Citation index, Sciences Citation Index Expanded (SCIE), Embase, Scopus, or Directory of Open Access Journals (DOAJ).[2] Multiple predatory journals, especially in India, claim indexing in platforms such as ResearchGate which are actually for collaborative and networking purposes and are not indexing agencies. Publications in such journals are not valid for medical faculty promotion or recognition from assistant to associate professor or from associate to professor. Thus, it is imperative for researchers and authors to clarify the journals\u2019 indexing before submission.", "title": "Research and publication in India - Beyond ABCDE", "claims": null}, {"metadata": {"year": 2018}, "authors": ["S. Oprescu"], "summary": "In the context of a very dynamic academic environment, where research is no longer reserved only for a small number of people, the necessary resources being accessible to all those interested, we considered opportune and even necessary to launch a scientific journal under the patronage of the Bucharest University Emergency Hospital, a hospital where the medical act is intertwined both with didactic activities - with students and residents, as well as with scientific research. \nResearch in the medical field is carried out in various forms: research projects, doctoral papers, bachelor's thesis, solitary clinical cases but with special features, and why not, reviewing the literature on current and up to date subjects. All these forms of research have a common element, namely the importance of disseminating the results obtained so that the academic environment takes note of the current achievements and the authors are given the proper credit. \nIn these circumstances, supporting the academic staff, the doctors, the residents and, why not, the students, the Romanian Journal of Clinical Research, will accept for publication, quality papers evaluated by reviewers through a blind peer review process, and will publish them under Open Access, providing support for both authors and readers. \nAs we go along this road with all current standards of scientific publishing, we hope that the journal will be indexed in the shortest possible time so that the authors merits are objectively recognized and readers have the guarantee of correct and up to date articles. \nIn conclusion, we invite you respectfully to support our approach by submitting articles to the journal as well as by subscribing to the reviewers' list.", "title": "AT THE BEGINNING OF THE ROAD", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Jit Shrestha"], "summary": "Health system should be supported by health research to deliver quality, accountable, equitable, fairer, better health care to target population.1\u00a0 \nEarlier; defenseless people like prisoners, soldiers, poor and mentally ill along with animals were subjected to medical research without taking consent. As time pass by, new emerging health problems, shifting of epidemiological trends in disease patterns, rapid increase in population, new and emerging health problems, increasing commercial interests of private health sector and shrinking resources all contribute in inequity to health care.2\u00a0 \nHence it is extremely important that research addresses priorities and focuses on the most important health issues and conditions. Research must serve as a driver for health system, policies and practice. For this to happen, the health research systems should be fully accountable for sake of transparency and also have to be capable of delivering the desired returns. We are fortune to take benefit from documented experiences from history. The main goal of medical research article is to share one\u2019s valuable experience so as to contribute to the progress of science.3\u00a0 \nConducting a medical research and publishing it in a medical journal, is sharing an important knowledge and experience to the world. A physician from one part of the world may have significant load of certain disease and sharing management experience in that particular disease may help doctors of other part of the world treat such kind of patient. Furthermore, author may also have individual benefits, like higher positions in academic hierarchy.4\u00a0 \nThe Journal of the institution reflects the academics wealth of the institute. Earlier, only few used to grab opportunities for professional growth via publication, majority of them suffer to lack of publication culture in the institution resulting in fossilization of their professional caliber.5\u00a0 \nIn developing countries like ours, to do quality medical research is often difficult. Furthermore, getting the article published in medical journal is another challenge. Leading international medical journals underreport on health research priorities for developing countries because of improper material, methods quality. Many factors play role in the paucity of inclusion of research papers from developing countries. Lack of resources like funding, proper man power and less access to scientific literature in similar setting leading to poor research output, faulty manuscript preparation and language proficiency may be the common problems. Inadequate laboratory facilities and training may be the other cause. Hence, researchers in developing countries should be supported and encouraged to produce material of the quality by proper guidance and required trainings. \nOpen access journal is the window to the research world. It is one of the tools to increase publications. These journals facilitate the publication of local research output and may play defining role in helping researcher to improve their publication records, and make it accessible to other researchers. This type of open access journal is an important entity in national publishing that will hopefully gain broader prominence as awareness increases and the above efforts are implemented. To\u00a0make our journal an index and of international stature is strenuous but with the help from our fraternity, we will surely reach the goal soon. Our journal will surely serve as a medium to access information, updated knowledge and a symbol of ideal journal in Nepal.\u00a0 \nThis journal is the result of hard work, dedication and sacrifice. We apologize for taking this extra long time to publish but as they say \u2018good things come to those who wait\u2019 we proudly present you our journal, our masterpiece. No one is to be blamed for the delay of this process. Working with teachers, selecting experts for review, suggesting authors, verifying manuscripts, editing and proof reading was an arduous job but was done meticulously. We run out of words to express our gratitude to the reviewers who reviewed and upgraded the journal\u2019s contents. It is very hard to imagine this journal with their crucial help and guidance. We apologize for not being able to incorporate all the articles due to some academic standards and expectations.\u00a0 \nI would also like to take this opportunity to thank Dr. Angel Magar, without whom this journal would be impossible. His valuable guidence help us shape the journal. We shall forever the indebted to him for his support. Dr. Bibek Rajbhandari is another individual we need to thank. His hard work and dedication can be seen in our journal. We are grateful for his help and support. We appreciate everything he has done selflessly and for the betterment of the journal.\u00a0 \nWe would like to acknowledge Dr. Krishna Rana (JNMA Assistant Editor) and JNMA Trainees (Asmita Neupane, Rakshya Pandey, Suzit Bhusal, Suraj Shrestha, Nabin Sundas, Prastuti Shrestha, Riyaz Shrestha, Prabha Bhandari, Nita Lohala, Samiksha Lamichhane, Sushmita Bhattarai, Laxman Aryal, Barsha Karki and Kajol Ghimire, Sushil Dahal, Shraddha Bhattarai) for the consistent support during the phase of publication.\u00a0 \nREFERENCES\u00a0 \n \nInis C. The WHO Strategy on research for Health. France:WHO,2012.\u00a0 \nSantosa A, Wall S,Fottrell E,Hogberg U, Byass P.The Deveopment and experience of epidemological transition theory over four decades: a systematic review. Glob Health Action.2014;7:10.\u00a0 \nMohmoud F, Mohamed F. A practical guide for health researchers. Eastern Mediteranean:WHO Regional Publications, 2004.\u00a0 \nClauset A, Arbesmans, Larremore D. Systematic inequtiy and hierarcy i faculty hiring networks. Sci.Adv.2015;1:e 1400005.\u00a0 \nKoul B, Kanwar A (ed.). Toward a Culture of Quality. Vancouver:Common Wealth of Learning, 2006. \n", "title": "Clinical Research and Medical Journal", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Icmje Icmje"], "summary": "and", "title": "International Committee of Medical Journal Editors (ICMJE): uniform requirements for journals", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Jennifer"], "summary": "International Journal of Medical Research & Health Sciences is an international, peer-reviewed, multidisciplinary, monthly, online journal. The Journal is indexed in Emerging Sources Citation Index (ESCI) Clarivate Analytics (Web of Science), Google Scholar, Academic Journals Database, Index Copernicus, Directory of Open Access Journals (DOAJ), International Committee of Medical Journal Editors, etc.", "title": "Editorial note on International Journal of Medical Research and Health Sciences", "claims": null}, {"metadata": {"year": 2020}, "authors": ["S. Schroter", "E. Loder", "F. Godlee"], "summary": "Needs more collaboration between journals and academics and more funding", "title": "Research on peer review and biomedical publication", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Jos\u00e9 M. Merig\u00f3", "Alicia  N\u00fa\u00f1ez"], "summary": "BackgroundThere is a wide range of intellectual work written about health research, which has been shaped by the evolution of diseases. This study aims to identify the leading journals over the last 25 years (1990\u20132014) according to a wide range of bibliometric indicators.MethodsThe study develops a bibliometric overview of all the journals that are currently indexed in Web of Science (WoS) database in any of the four categories connected to health research. The work classifies health research in nine subfields: Public Health, Environmental and Occupational Health, Health Management and Economics, Health Promotion and Health Behavior, Epidemiology, Health Policy and Services, Medicine, Health Informatics, Engineering and Technology, and Primary Care.ResultsThe results indicate a wide dispersion between categories being the American Journal of Epidemiology, Environmental Health Perspectives, American Journal of Public Health, and Social Science & Medicine, the journals that have received the highest number of citations over the last 25 years. According to other indicators such as the h-index and the citations per paper, some other journals such as the Annual Review of Public Health and Medical Care, obtain better results which show the wide diversity and profiles of outlets available in the scientific community. The results are grouped and studied according to the nine subfields in order to identify the leading journals in each specific sub discipline of health.ConclusionsThe work identifies the leading journals in health research through a bibliometric approach. The analysis shows a deep overview of the results of health journals. It is worth noting that many journals have entered the WoS database during the last years, in many cases to fill some specific niche that has emerged in the literature, although the most popular ones have been in the database for a long time.", "title": "Influential journals in health research: a bibliometric study", "claims": null}], "query": "NEED FOR INSTITUTIONAL MEDICAL RESEARCH JOURNALS", "summary_abstract": "The need for institutional medical research journals is underscored by several key factors highlighted in the provided abstracts. Firstly, the growth and development of medical sciences heavily rely on continuous research and innovation, which necessitates effective platforms for disseminating findings (Khan, 2019). The establishment of dedicated journals, such as the \"Journal of Shifa Tameer-e-Millat University\" and the \"Romanian Journal of Clinical Research,\" aims to support researchers by providing avenues for publication that adhere to rigorous standards, including blind peer review and open access (Oprescu, 2018; Khan, 2019).\n\nMoreover, the importance of publishing in reputable, indexed journals is emphasized, particularly in the context of academic promotions and recognition within medical faculties (Ish & Ish, 2022). The proliferation of predatory journals, which mislead authors regarding their indexing status, further complicates the landscape, making it crucial for researchers to verify the credibility of journals before submission (Ish & Ish, 2022).\n\nAdditionally, the abstracts highlight the role of medical research in addressing pressing health issues and improving healthcare systems. Research not only contributes to the academic wealth of institutions but also enhances the quality of care provided to populations (Shrestha, 2018). Open access journals are particularly noted for their potential to increase the visibility and accessibility of local research, thereby fostering a culture of publication and collaboration among researchers (Shrestha, 2018).\n\nIn summary, the establishment and support of institutional medical research journals are vital for promoting quality research, ensuring proper dissemination of findings, and enhancing the academic and professional growth of researchers in the medical field. These journals serve as essential platforms for sharing knowledge, addressing health priorities, and contributing to the overall advancement of medical sciences.", "summary_extract": null}, {"papers": [{"metadata": {"year": 1994}, "authors": ["Y. Itakura", "H. Sasano", "C. Shiga", "Yasuko Furukawa", "K. Shiga", "S. Mori", "H. Nagura"], "summary": "Background. Many studies have reported the increased expression of epidermal growth factor receptor (EGFR) in various human malignancies and its association with the biologic behavior of the tumors.", "title": "Epidermal growth factor receptor overexpression in esophageal carcinoma. An immunohistochemical study correlated with clinicopathologic findings and DNA amplification", "claims": null}, {"metadata": {"year": 2006}, "authors": ["Si Jian-mi"], "summary": "Epidermal growth factor(EGF) is one of the most important growth factor related with cell proliferation and the concentration of EGF is higher in gastrointestinal tract than in blood circulation. In recent years,the study has confirmed the effect of EGF in treating the benign diseases of gastrointestinal tract, and some research works also find that EGF and the antagon of EGFR can contribute to the occurrence, development or decrement of the gastrointestinal tumors.", "title": "Epidermal growth factor and its receptor (EGFR) in gastrointestinal tract", "claims": null}, {"metadata": {"year": 1990}, "authors": ["J. Haley"], "summary": "The epidermal growth factor receptor, a transmembrane protein tyrosine kinase, plays a crucial role in cellular signalling both in embryonic development and in adult tissues. By phosphorylation of substrate proteins on tyrosine, signals may be transmitted which result in gene expression, ion flux and mitogenesis to name but a few of the pleiotropic effects of receptor activation. The epidermal growth factor receptor is relatively well studied amongst an increasing family of tyrosine kinases and makes a good model system for studying structure-function relationships involved in signal transduction. Here, the structure-function relationships for cellular transformation by the EGF receptor proto-oncogene are reviewed as well as data pertaining to its overexpression in human tumors.", "title": "Regulation of epidermal growth factor receptor expression and activation: a brief review.", "claims": null}, {"metadata": {"year": 1987}, "authors": ["Wendelyn H. Inman", "Graham  Carpenter"], "summary": "Epidermal growth factor (EGF) is a well-characterized growth factor that was initially isolated from mouse submaxillary glands.(1\u20133) The low-molecular-weight growth factor (M r 6000) can be isolated as a component of a high-molecular-weight complex with an associated arginine esterase activity. (4,5) EGF has been found in all mammals and, in humans, is a component of nearly all body fluids.(6,7) A variety of biological responses are mediated by EGF in vivo and in vitro, in a diverse number of cells including epidermal and epithelial cells, which are particularly responsive in the intact animal.(7) It is important to note that almost 90% of all malignancies are derived from epithelial cells.", "title": "The Receptor for Epidermal Growth Factor", "claims": null}, {"metadata": {"year": 1990}, "authors": ["G. Merlino"], "summary": "The epidermal growth factor (EGF) receptor is a transmembrane, cell-surface glycoprotein that mediates the mitogenic action of a family of ligands, including EGF and transforming growth factor alpha (TGF alpha). Perturbation of this signal transduction pathway by exposure to excess ligand, by overproduction of the normal EGF receptor, or by the presence of specific mutated forms of this receptor can result in dramatic alterations in cellular phenotype, including malignant transformation. Overstimulation of normal cells is avoided by precise control of the synthesis and degradation of EGF receptors. Regulation occurs at multiple levels, including transcriptional control. A number of DNA-binding proteins have now been identified which positively and negatively modulate EGF receptor gene transcription.", "title": "Epidermal growth factor receptor regulation and function.", "claims": null}, {"metadata": {"year": 1990}, "authors": ["Hale Jd"], "summary": ": The epidermal growth factor receptor, a transmembrane protein tyrosine kinase, plays a crucial role in cellular signalling both in embryonic development and in adult tissues. By phosphorylation of substrate proteins on tyrosine, signals may be transmitted which result in gene expression, ion flux and mitogenesis to name but a few of the pleiotropic effects of receptor activation. The epidermal growth factor receptor is relatively well studied amongst an increasing family of tyrosine kinases and makes a good model system for studying structure-function relationships involved in signal transduction. Here, the structure-function relationships for cellular transformation by the EGF receptor proto-oncogene are reviewed as well as data pertaining to its overexpression in human tumors.", "title": "Regulation of epidermal growth factor receptor expression and activation: a brief review.", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Jianchun Chen", "Fenghua Zeng", "Steven J Forrester", "S. Eguchi", "Ming-Zhi Zhang", "Raymond C. Harris"], "summary": "The epidermal growth factor receptor (EGFR) is the prototypical member of a family of membrane-associated intrinsic tyrosine kinase receptors, the ErbB family. EGFR is activated by multiple ligands, including EGF, transforming growth factor (TGF)-\u03b1, HB-EGF, betacellulin, amphiregulin, epiregulin, and epigen. EGFR is expressed in multiple organs and plays important roles in proliferation, survival, and differentiation in both development and normal physiology, as well as in pathophysiological conditions. In addition, EGFR transactivation underlies some important biologic consequences in response to many G protein-coupled receptor (GPCR) agonists. Aberrant EGFR activation is a significant factor in development and progression of multiple cancers, which has led to development of mechanism-based therapies with specific receptor antibodies and tyrosine kinase inhibitors. This review highlights the current knowledge about mechanisms and roles of EGFR in physiology and disease.", "title": "Expression and Function of the Epidermal Growth Factor Receptor in Physiology and Disease.", "claims": null}, {"metadata": {"year": 2015}, "authors": ["D. Chatterjee", "D. Schady"], "summary": "Epidermal growth factor receptor (EGFR) acts as a cell response mediator to a range of extracellular factors, including allergens. Eosinophilic esophagitis (EoE) is an allergen-mediated, clinicopathological condition affecting the esophagus. EGFR expression in EoE has not been studied to date. Our objective of this study was to characterize the EGFR expression in EoE. In this retrospective study involving esophageal \u2026", "title": "Epidermal Growth Factor Receptor Is Expressed in Eosinophilic Esophagitis", "claims": null}], "query": "epidermal growth factor receptors overexpression abdomen", "summary_abstract": "The research question regarding the overexpression of epidermal growth factor receptors (EGFR) in the abdomen is addressed through various studies that highlight the significance of EGFR in both normal physiology and pathological conditions, particularly in relation to gastrointestinal tumors.\n\nItakura et al. (1994) emphasize that increased expression of EGFR is commonly observed in various human malignancies, suggesting a correlation between EGFR overexpression and tumor behavior. This aligns with findings by Jian-mi (2006), who notes that EGF, a key growth factor, is particularly concentrated in the gastrointestinal tract and plays a role in the development of gastrointestinal tumors. The study indicates that both EGF and EGFR antagonists can influence the occurrence and progression of these tumors.\n\nHaley (1990) and Merlino (1990) further elaborate on the role of EGFR as a transmembrane protein that mediates cellular signaling, which is crucial for processes such as mitogenesis and gene expression. They highlight that overexpression or mutation of the EGFR can lead to malignant transformation, reinforcing the idea that aberrant EGFR activity is a significant factor in cancer development.\n\nChen et al. (2016) provide a broader context by discussing the various ligands that activate EGFR and its involvement in multiple organs, including the gastrointestinal tract. They point out that aberrant activation of EGFR is linked to the progression of several cancers, which has prompted the development of targeted therapies.\n\nIn summary, the collective findings from these studies underscore the critical role of EGFR overexpression in the abdomen, particularly in the context of gastrointestinal malignancies. The evidence suggests that EGFR is not only a marker of tumor progression but also a potential target for therapeutic intervention in related diseases.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2022}, "authors": ["Junmyeong Kim", "Changhyeon Jeong", "Kanghyun Jo"], "summary": "Most of the metaverses need to install programs or buy some equipment for entering the world. These processes reduce the accessibility of metaverse. To improve accessibility, this work develops a metaverse for service on the web environment using Unity and web development. The developed web-based metaverse has two advantages. The first advantage is increasing accessibility, when the metaverse is servicing on a web environment, users can connect to the world by typing the specific Uniform Resource Locator (URL) in an address bar. The second advantage is to mix the advantages of Unity and web development. Unity has many methods and assets for generating and controlling the metaverse, and web development has the good ability for communicating between browser and server, controlling information in the database, etc. To interact between Unity and the web, React was used in this work. React provides an Application Programming Interface (API) for interacting between Unity and the web. API makes the metaverse can use functions of the web in Unity. In summary, the goal of this paper is to build a web-based metaverse platform for improving the accessibility of metaverse. To achieve this goal, this work used Unity, Photon, Socket.IO, React, Node.js, MongoDB, and Express for building a more accessible metaverse. For accessing, this work built metaverse, the URL is provided as https://busanmayor.org/.", "title": "Development of Web-based Metaverse Platform", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Filipe Fernandes", "Cl\u00e1udia Werner"], "summary": "Metaverse is a new paradigm and is under construction where social, immersive Virtual Reality platforms will be compatible with several kinds of applications. However, solutions must be developed to ensure that people with disabilities access the Metaverse. This work aims to present an overview of research on accessibility of immersive systems and outline challenges and opportunities so that the Human-Computer Interaction community can reflect and intensify research in this area.", "title": "Accessibility in the Metaverse: Are We Prepared?", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Minrui Xu", "Wei Chong Ng", "Wei Yang Bryan Lim", "Jiawen Kang", "Zehui Xiong", "D. Niyato", "Qiang Yang", "Xuemin Shen", "C. Miao"], "summary": "Dubbed \u201cthe successor to the mobile Internet\", the concept of the Metaverse has grown in popularity. While there exist lite versions of the Metaverse today, they are still far from realizing the full vision of an immersive, embodied, and interoperable Metaverse. Without addressing the issues of implementation from the communication and networking, as well as computation perspectives, the Metaverse is difficult to succeed the Internet, especially in terms of its accessibility to billions of users today. In this survey, we focus on the edge-enabled Metaverse to realize its ultimate vision. We first provide readers with a succinct tutorial of the Metaverse, an introduction to the architecture, as well as current developments. To enable the ubiquitous, seamless, and embodied access to the Metaverse, we discuss the communication and networking challenges and survey cutting-edge solutions and concepts that leverage next-generation communication systems for users to be telepresent and teleoperate in the Metaverse. Moreover, given the high computation costs required, e.g., to render immersive 3D worlds and run data hungry artificial intelligence (AI) driven applications, we discuss the computation challenges and cloud-edge-end computation framework driven solutions to realize the Metaverse on resource-constrained edge devices. Next, we explore how blockchain technologies can aid in the interoperable development of the Metaverse, not just in terms of empowering the economic circulation of virtual usergenerated contents, but also to manage physical edge resources in a decentralized, transparent, and tamper-proof manner. Finally, we discuss the future research directions towards realizing the true vision of the edge-enabled Metaverse.", "title": "A Full Dive into Realizing the Edge-enabled Metaverse: Visions, Enabling Technologies, and Challenges", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Minrui Xu", "Wei Chong Ng", "Wei Yang Bryan Lim", "Jiawen Kang", "Zehui Xiong", "D. Niyato", "Qiang Yang", "X. Shen", "Chunyan Miao"], "summary": "Dubbed \u201cthe successor to the mobile Internet,\u201d the concept of the Metaverse has grown in popularity. While there exist lite versions of the Metaverse today, they are still far from realizing the full vision of an immersive, embodied, and interoperable Metaverse. Without addressing the issues of implementation from the communication and networking, as well as computation perspectives, the Metaverse is difficult to succeed the Internet, especially in terms of its accessibility to billions of users today. In this survey, we focus on the edge-enabled Metaverse to realize its ultimate vision. We first provide readers with a succinct tutorial of the Metaverse, an introduction to the architecture, as well as current developments. To enable ubiquitous, seamless, and embodied access to the Metaverse, we discuss the communication and networking challenges and survey cutting-edge solutions and concepts that leverage next-generation communication systems for users to immerse as and interact with embodied avatars in the Metaverse. Moreover, given the high computation costs required, e.g., to render 3D virtual worlds and run data-hungry artificial intelligence-driven avatars, we discuss the computation challenges and cloud-edge-end computation framework-driven solutions to realize the Metaverse on resource-constrained edge devices. Next, we explore how blockchain technologies can aid in the interoperable development of the Metaverse, not just in terms of empowering the economic circulation of virtual user-generated content but also to manage physical edge resources in a decentralized, transparent, and immutable manner. Finally, we discuss the future research directions towards realizing the true vision of the edge-enabled Metaverse.", "title": "A Full Dive Into Realizing the Edge-Enabled Metaverse: Visions, Enabling Technologies, and Challenges", "claims": null}, {"metadata": {"year": 2023}, "authors": ["M. Adil", "Houbing Song", "Muhammad Khurram Khan", "A. Farouk", "Zhanpeng Jin"], "summary": "Internet technology has proven to be a vital contributor to many cutting-edge innovations that have given humans access to interact virtually with objects. Until now, numerous virtual systems had been developed for digital transformation to enable access to thousands of services and applications that range from virtual gaming to social networks. However, the majority of these systems lack to maintain consistency during interconnectivity and communication. To explore this discussion, in the recent past a new term, Metaverse has been introduced, which is the combination of meta and universe that describes a shared virtual environment, where a number of technologies, such as 4th and 5th generation technologies, VR, ML algorithms etc., work collectively to support each other for the sake of one objective, which is the virtual accessibility of objects via one network platform. With the development, integration, and virtualization of technologies, a lot of improvement in daily life applications is expected, but at the same time, there is a big challenge for the research community to secure this platform from external and external threats, because this technology is exposed to many cybersecurity attacks. Hence, it is imperative to systematically review and understand the taxonomy, applications, open security challenges, and future research directions of the emerging Metaverse technologies. In this paper, we have made useful efforts to present a comprehensive survey regarding Metaverse technology by taking into account the aforesaid parameters. Following this, in the initial phase, we explored the future of Metaverse in the presence of 4th and 5th generation technologies. Thereafter, we discussed the possible attacks to set a preface for the open security challenges. Based on that, we suggested potential research directions that could be beneficial to address these challenges cost-effectively.", "title": "5G/6G-Enabled Metaverse Technologies: Taxonomy, Applications, and Open Security Challenges with Future Research Directions", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Callum Parker", "Soojeong Yoo", "Youngho Lee", "Joel Fredericks", "Arindam Dey", "Youngjun Cho", "M. Billinghurst"], "summary": "The push towards a Metaverse is growing, with companies such as Meta developing their own interpretation of what it should look like. The Metaverse at its conceptual core promises to remove boundaries and borders, becoming a decentralised entity for everyone to use - forming a digital virtual layer over our own \u201creal\u201d world. However, creation of a Metaverse or \u201cnew world\u201d presents the opportunity to create one which is inclusive and accessible to all. This challenge is explored and discussed in this workshop, with an aim of understanding how to create a Metaverse which is open and inclusive to people with physical and intellectual disabilities, and how interactions can be designed in a way to minimise disadvantage. The key outcomes of this workshop outline new opportunities for improving accessibility in the Metaverse, methodologies for designing and evaluating accessibility, and key considerations for designing accessible Metaverse environments and interactions.", "title": "Towards an Inclusive and Accessible Metaverse", "claims": null}, {"metadata": {"year": 2022}, "authors": ["M. Aloqaily", "Ouns Bouachir", "F. Karray", "I. A. Ridhawi", "Abdulmotaleb El Saddik"], "summary": "The advances in Artificial Intelligence (AI) have led to technological advancements in a plethora of domains. Healthcare, education, and smart city services are now enriched with AI capabilities. These technological advancements would not have been realized without the assistance of fast, secure, and fault-tolerant communication media. Traditional processing, communication and storage technologies cannot maintain high levels of scalability and user experience for immersive services. The metaverse is an immersive three-dimensional (3D) virtual world that integrates fantasy and reality into a virtual environment using advanced virtual reality (VR) and augmented reality (AR) devices. Such an environment is still being developed and requires extensive research in order for it to be realized to its highest attainable levels. In this article, we discuss some of the key issues required in order to attain realization of metaverse services. We propose a framework that integrates digital twin (DT) with other advanced technologies such as the sixth generation (6G) communication network, blockchain, and AI, to maintain continuous end-to-end metaverse services. This article also outlines requirements for an integrated, DT-enabled metaverse framework and provides a look ahead into the evolving topic.", "title": "Integrating Digital Twin and Advanced Intelligent Technologies to Realize the Metaverse", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Lik-Hang Lee", "Tristan Braud", "Pengyuan Zhou", "Lin Wang", "Dianlei Xu", "Zijun Lin", "Abhishek Kumar", "Carlos Bermejo", "Pan Hui"], "summary": "Since the popularisation of the Internet in the 1990s, the cyberspace has kept evolving. We have created various computer-mediated virtual environments including social networks, video conferencing, virtual 3D worlds (e.g., VR Chat), augmented reality applications (e.g., Pokemon Go), and Non-Fungible Token Games (e.g., Upland). Such virtual environments, albeit non-perpetual and unconnected, have bought us various degrees of digital transformation. The term `metaverse' has been coined to further facilitate the digital transformation in every aspect of our physical lives. At the core of the metaverse stands the vision of an immersive Internet as a gigantic, unified, persistent, and shared realm. While the metaverse may seem futuristic, catalysed by emerging technologies such as Extended Reality, 5G, and Artificial Intelligence, the digital `big bang' of our cyberspace is not far away. This survey paper presents the first effort to offer a comprehensive framework that examines the latest metaverse development under the dimensions of state-of-the-art technologies and metaverse ecosystems, and illustrates the possibility of the digital `big bang'. First, technologies are the enablers that drive the transition from the current Internet to the metaverse. We thus examine eight enabling technologies rigorously - Extended Reality, User Interactivity (Human-Computer Interaction), Artificial Intelligence, Blockchain, Computer Vision, IoT and Robotics, Edge and Cloud computing, and Future Mobile Networks. In terms of applications, the metaverse ecosystem allows human users to live and play within a self-sustaining, persistent, and shared realm. Therefore, we discuss six user-centric factors -- Avatar, Content Creation, Virtual Economy, Social Acceptability, Security and Privacy, and Trust and Accountability. Finally, we propose a concrete research agenda for the development of the metaverse.", "title": "All One Needs to Know about Metaverse: A Complete Survey on Technological Singularity, Virtual Ecosystem, and Research Agenda", "claims": null}], "query": "technological solutions, tools, or best practices for enhancing digital accessibility in the Metaverse", "summary_abstract": "The exploration of technological solutions and best practices for enhancing digital accessibility in the Metaverse reveals a multifaceted approach that addresses various challenges and opportunities. \n\nKim et al. (2022) emphasize the importance of developing web-based metaverse platforms to improve accessibility. By utilizing Unity and web development technologies, they created a metaverse that users can access simply by entering a URL, thus eliminating the need for specialized equipment. This approach not only enhances accessibility but also leverages the strengths of both Unity and web technologies to facilitate user interaction.\n\nFernandes and Werner (2022) highlight the necessity of ensuring that individuals with disabilities can access the Metaverse. They call for increased research within the Human-Computer Interaction community to address the unique challenges faced by these users, suggesting that a concerted effort is needed to create inclusive virtual environments.\n\nXu et al. (2022) discuss the broader communication and networking challenges that must be overcome to realize an accessible Metaverse. They propose leveraging next-generation communication systems and cloud-edge computing frameworks to facilitate seamless access for users, particularly those on resource-constrained devices. This technological infrastructure is crucial for supporting immersive experiences that are accessible to a wider audience.\n\nParker et al. (2023) further explore the design methodologies necessary for creating an inclusive Metaverse. Their workshop outcomes suggest that careful consideration of user interactions can minimize disadvantages for individuals with physical and intellectual disabilities. They advocate for new opportunities and methodologies to enhance accessibility in the design of Metaverse environments.\n\nAloqaily et al. (2022) propose an integrated framework that combines digital twin technology with advanced communication networks and AI to ensure continuous and scalable metaverse services. This framework aims to address the challenges of maintaining high user experience levels, which is essential for accessibility.\n\nCollectively, these papers underscore the importance of innovative technological solutions, inclusive design practices, and robust communication frameworks in enhancing digital accessibility in the Metaverse. The ongoing research and development in these areas are vital for creating a virtual environment that is truly accessible to all users, regardless of their abilities.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2021}, "authors": ["Marcos I. Fabietti", "M. Mahmud", "Ahmad Lotfi"], "summary": "The improvement of hardware for the acquisition and processing of electroencephalography (EEG) has made its portability become a reality. This allows for studies to be carried outside lab settings, as well as many commercial applications. As recordings are done over extended periods, these devices generate large volumes of data, mainly if the neuronal activity is recorded through multiple channels. Machine learning (ML) techniques allow to effectively analyse and use this data for a wide range of applications. However the portability of these techniques can be challenging. In this article, we set out to review over 40 relevant articles where ML techniques in a diverse set of EEG applications that have successfully been incorporated into portable systems.", "title": "On-Chip Machine Learning for Portable Systems: Application to Electroencephalography-based Brain-Computer Interfaces", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Joshua Sabio", "Nikolas S Williams", "G. McArthur", "N. Badcock"], "summary": "BACKGROUND Commercial electroencephalography (EEG) devices have become increasingly available over the last decade. These devices have been used in a wide variety of fields ranging from engineering to cognitive neuroscience. PURPOSE The aim of this study was to chart peer-review articles that used currently available consumer-grade EEG devices to collect neural data. We provide an overview of the research conducted with these relatively more affordable and user-friendly devices. We also inform future research by exploring the current and potential scope of consumer-grade EEG. METHODS We followed a five-stage methodological framework for a scoping review that included a systematic search using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews (PRISMA-ScR) guidelines. We searched the following electronic databases: PsycINFO, MEDLINE, Embase, Web of Science, and IEEE Xplore. We charted study data according to application (BCI, experimental research, validation, signal processing, and clinical) and location of use as indexed by the first author\u2019s country. RESULTS We identified 916 studies that used data recorded with consumer-grade EEG: 531 were reported in journal articles and 385 in conference papers. Emotiv devices were most used, followed by the NeuroSky MindWave, OpenBCI, interaXon Muse, and MyndPlay Mindband. The most common use was for brain-computer interfaces, followed by experimental research, signal processing, validation, and clinical purposes. CONCLUSIONS Consumer-grade EEG has proven to be a useful tool for neuroscientific research and will likely continue to be used well into the future. Our study provides a comprehensive review of their application, as well as future directions for researchers who wish to use these devices.", "title": "A scoping review on the use of consumer-grade EEG devices for research", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Yanke Ci", "Shui Wang"], "summary": "In this paper we present an EEG detection system, which is a low-cost, portable, popularization. It is realized the low cost, mass brain health examination through the mobile phone software and networking in the \"cloud\" of the EEG pattern recognition expert database. The main functions of the system include: (1) through the mobile phone to control the EEG data acquisition; (2) established the expert system database of EEG patterns; (3) the EEG patterns, connected to the Internet matching. The key technologies of portable EEG health examination expert system such as structured storage of EEG data, feature extraction, construction of knowledge database and pattern matching are also discussed and researched in the paper.", "title": "The key techniques research on portable EEG examination expert system", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Ci Yan-ke", "W. Shui"], "summary": "In this paper we present an EEG detection system, which is a low-cost, portable, popularization. It is realized the low cost, mass brain health examination through the mobile phone software and networking in the \"cloud\" of the EEG pattern recognition expert database. The main functions of the system include: (1) through the mobile phone to control the EEG data acquisition; (2) established the expert system database of EEG patterns; (3) the EEG patterns, connected to the Internet matching. The key technologies of portable EEG health examination expert system such as structured storage of EEG data, feature extraction, construction of knowledge database and pattern matching are also discussed and researched in the paper.", "title": "The key techniques research on portable EEG examination expert system", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Iuliu Alexandru Pap", "S. Oniga", "Anca Alexan"], "summary": "Through this paper we present our work on integrating electroencephalography-based machine learning elements in our eHealth Internet of Things (IoT) system by using the TensorFlow open source platform. This system is used for recording specific physiological data such as systolic and diastolic blood pressure, pulse rate, oxygen saturation in the blood, breathing intensity and rate, skin conductance and resistance, body temperature and electroencephalography (EEG) from multiple electrodes. The main focus of our current research is to experiment with brain computer interfaces towards creating an EEG-controlled device that would interpret eye movement.", "title": "Machine Learning EEG Data Analysis For eHealth IoT System", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Rafael Puyol", "G. Lenzi", "G. Barg", "A. Arnaud"], "summary": "The search for simple yet precise electroencephalogram (EEG) recording systems has been long desired by researchers in the psychophysiology field. During the last two decades, the advancement of instrumentation amplifiers and microcontrollers has provided the adequate tools for the task. In this paper the design of a portable embedded system capable of recording up to 128 EEG electrodes simultaneously is presented. EEG records are stored in a standard SD flash card and can be transmitted to a host PC with a Bluetooth module. The main features of this device are portability, low power consumption, scalability, and low noise among others. The implemented device may allow psychophysiology researchers to perform non-invasive experiments, where individuals are not tied with an uncomfortable group of cables to amplifiers and test equipment.", "title": "A portable, high density EEG acquisition system", "claims": null}, {"metadata": {"year": 2022}, "authors": ["D. Avola", "Marco Cascio", "L. Cinque", "Alessio Fagioli", "G. Foresti", "Marco Raoul Marini", "D. Pannone"], "summary": ". Nowadays, machine and deep learning techniques are widely used in di\ufb00erent areas, ranging from economics to biology. In general, these techniques can be used in two ways: trying to adapt well-known models and architectures to the available data, or designing custom architectures. In both cases, to speed up the research process, it is useful to know which type of models work best for a speci\ufb01c problem and/or data type. By focusing on EEG signal analysis, and for the \ufb01rst time in liter-ature, in this paper a benchmark of machine and deep learning for EEG signal classi\ufb01cation is proposed. For our experiments we used the four most widespread models, i.e., multilayer perceptron, convolutional neural network, long short-term memory, and gated recurrent unit, highlighting which one can be a good starting point for developing EEG classi\ufb01cation models.", "title": "Analyzing EEG Data with Machine and Deep Learning: A Benchmark", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Shuai Li", "Zeyu Wang", "Chunsheng Li"], "summary": "Scalp electroencephalography (EEG) is widely used to study human electrophysiological activity noninvasively. With the gradual improvement of the quality of the multi-channel bioelectric signal acquisition device, the volume and power consumption are gradually reduced, which makes it suitable for portable and wearable application. To ensure sufficient signal gain and acquisition accuracy, it is a huge challenge to effectively suppress external interference and obtain a better bioelectric signal. This paper presents a new kind of wireless EEG signal real-time analysis and acquisition system. The system can be wearable at home environment and wirelessly send the real-time EEG signal to the host. The results show that the system can collect EEG signal robustly. The noise of the test system is reduced by 60% through the design of the isolated circuit. The software algorithm also enables the ability to perform basic analysis of biological signals by using Python digital signal processing module. The system provides a new safety platform for human-machine interfacing, rehabilitation and mental disease monitoring at home.", "title": "A Wearable EEG Real-time Measure and Analysis Platform for Home Applications", "claims": null}], "query": "looking for papers on which has applied portable or consumer grade EEG device and Machine learning techniques together.", "summary_abstract": "The integration of portable or consumer-grade EEG devices with machine learning techniques has garnered significant attention in recent research, highlighting their potential for various applications in neuroscience and health monitoring.\n\nFabietti et al. (2021) conducted a comprehensive review of over 40 studies that successfully incorporated machine learning techniques into portable EEG systems. They emphasized the challenges and opportunities presented by the large volumes of data generated by these devices, which are now capable of being used outside traditional lab settings. This review underscores the versatility of machine learning in analyzing EEG data across diverse applications.\n\nSabio et al. (2022) further explored the landscape of consumer-grade EEG devices, identifying 916 studies that utilized these tools for various purposes, including brain-computer interfaces (BCIs) and experimental research. Their findings suggest that devices like Emotiv and NeuroSky MindWave are particularly popular, indicating a growing trend towards using affordable and user-friendly EEG technology in research.\n\nIn a practical application, Ci and Wang (2015) introduced a low-cost EEG detection system that leverages mobile technology and cloud computing for EEG pattern recognition. This system exemplifies how portable EEG devices can be combined with machine learning to facilitate mass brain health examinations.\n\nPap et al. (2020) also contributed to this field by integrating EEG data collection with machine learning elements in an Internet of Things (IoT) system. Their work focuses on developing an EEG-controlled device, showcasing the potential for real-time applications in brain-computer interfacing.\n\nMoreover, Avola et al. (2022) provided a benchmark for machine learning models applied to EEG signal classification, identifying effective architectures for future research. Their study highlights the importance of selecting appropriate models to enhance the analysis of EEG data.\n\nLi et al. (2018) presented a wireless EEG signal acquisition system designed for home use, which emphasizes the growing trend of making EEG technology more accessible and practical for everyday applications. Their system not only collects EEG signals robustly but also incorporates basic analysis capabilities through software algorithms.\n\nCollectively, these studies illustrate a promising convergence of portable EEG technology and machine learning, paving the way for innovative applications in health monitoring, brain-computer interfaces, and beyond. The ongoing advancements in both hardware and analytical techniques are likely to expand the scope and effectiveness of EEG research in the future.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2010}, "authors": ["Chao Liu", "Jiliang Wu", "Kui Xu", "Fei Cai", "Jun Gu", "Liqun Ma", "Jian-Guo Chen"], "summary": "J. Neurochem. (2010) 112, 1500\u20131512.", "title": "Neuroprotection by baicalein in ischemic brain injury involves PTEN/AKT pathway", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Shilun  Yang", "Haigang  Wang", "Yinglin  Yang", "Rui  Wang", "Yuehua  Wang", "Chunfu  Wu", "Guanhua  Du"], "summary": "Ischemic stroke is a cerebrovascular disease with high morbidity, high mortality, and high disability, representing a serious threat to human life and health. Clinically, the extensive injury caused by ischemic stroke results from ischemia-reperfusion (I/R) injury thrombolytic treatment. However, there are few reports on the use of medications in the subacute stage of cerebral I/R. Baicalein (5,6,7-trihydroxyflavone) is a biologically active ingredient extracted from the root of Scutellaria baicalensis Georgi. In the present study, we investigated the therapeutic effect of baicalein administered in the subacute phase of cerebral I/R injury in a rat model of ischemia induced by occlusion of the middle cerebral artery (MCA). Rats were treated daily with baicalein (200\u202fmg/kg, i.g.) in the subacute phase (24\u202fh after reperfusion) for 7 days. The results showed that baicalein significantly reduced neurobehavioral deficits and decreased brain infarct volume from 18.99% to 7.41%. Immunofluorescence analysis of the ischemic penumbra showed that baicalein significantly reduced expression of the M1 marker, cluster of differentiation (CD) 16 and CD86, and increased expression of the M2 marker, CD 163 and CD206, indicating that baicalein inhibited M1 transformation and promoted M2 transformation of microglia/macrophage to inhibit neuroinflammation. Moreover, baicalein suppressed NF-\u03baB signaling by reducing I\u03baB\u03b1 phosphorylation and nuclear translocation of NF-\u03baB/p65, which decreased the release of the pro-inflammatory factors IL-6, IL-18, and TNF-\u03b1. In addition, baicalein reduced phosphorylation of JNK, ERK and p38, which are involved modulation of microglia/macrophage M1/M2 polarization. Western blot analysis of apoptosis- and autophagy-related proteins showed that baicalein increased the Bcl-2/Bax ratio and reduced caspase-3 expression to decrease neuronal apoptosis and ameliorate neuronal loss. Baicalein also decreased the LC3-II/LC3-I ratio and promoted phosphorylation of the PI3K/Akt/mTOR signaling pathway which implied inhibition of autophagy. These observations suggest that baicalein exerts neuroprotective effects by reducing neuroinflammation, apoptosis and autophagy, and protects against cerebral I/R injury in the subacute phase in vivo.", "title": "Baicalein administered in the subacute phase ameliorates ischemia-reperfusion-induced brain injury by reducing neuroinflammation and neuronal damage.", "claims": null}, {"metadata": {"year": 1984}, "authors": ["H. C. D. Almeida", "J. A. M. Flores", "B. Ferlin"], "summary": "Manifestations cliniques, immunologie, diagnostic, classification et pronostic. Description de 2 cas", "title": "Neuroblastoma conceitos \u2015 dois pacientes est\u00e1gio IV S", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Hongfeng  Wen", "Di  Jin", "Lina  Cai", "Tao  Wu", "Haichao  Liu"], "summary": "Background Brucellosis is a common zoonotic disease that may have a variety of clinical manifestations when it affects the nervous system. Ischemic stroke is a rare clinical symptom, but if it is not diagnosed and treated early, it may cause more severe consequences. Case presentation We report a 38-year-old man presenting with hearing impairment for four years and sudden weakness of the right limb for two years, recurrent aphasia, and gradual weakness of bilateral lower limbs for nine months. He had bilateral positive Babinski\u2019s sign. Cerebrospinal fluid (CSF) showed raised protein and pleocytosis. Magnetic resonance imaging (MRI) showed ischemic infarcts in the pons and extensive enhancement of spinal meninges combined with spinal cord atrophy and ischemia. The tests revealed Brucella Rose Bengal positive in serum and CSF. Brucella culture in CSF was also positive. Next-generation sequencing (NGS) of CSF revealed positive for Brucella with 105 species were detected. He showed significant improvement with antibiotics at five months follow-up. Conclusions Neurobrucellosis may mimic stroke and transverse myelitis like syndromes. NB is a treatable infectious condition and should always be considered in the differentials, especially if there are risk factors, as in our case.", "title": "Neurobrucellosis with ischemic stroke and spinal cord involvement: a case report", "claims": null}, {"metadata": {"year": 2018}, "authors": ["S. Shirwadkar"], "summary": "Neurofbromatosis is a rare condition characterized by hamartomas of neural crest origin (phakomatosis). The condition is autosomal dominant, though spontaneous new mutations are not uncommon. The National Institutes of Health (NIH) divide neurofibromatosis into type 1 (NF1 or von Recklinghausen syndrome) and type 2 (NF2, acoustic neurofibromatosis, or central neurofibromatosis). Only ~10% of NF patients have NF2, which is associated with bilateral acoustic neuromas/ schwannomas. The defect in NF2 is on chromosome 22q. In addition to acoustic neuromas, NF2 patients frequently have ependymomas of the spinal cord or multiple meningiomas.", "title": "Ocular Findings in Neurofibromatosis Type1 and Literature Review", "claims": null}, {"metadata": {"year": 2018}, "authors": ["C. Haberler", "L. Reiniger", "H. Rajnai", "O. Kalev", "E. Gelp\u00ed", "M. Tamesberger", "T. Pietsch"], "summary": "No abstract available.", "title": "Case of the month 1-2019: CNS high-grade neuroepithelial tumor with BCOR alteration", "claims": null}, {"metadata": {"year": 2018}, "authors": ["A. Aldajani", "A. Albakr", "Noman Ishaque", "H. Al-Jehani"], "summary": "Neuro-beh\u00e7et\u2019s disease (NBD) is an uncommon, serious presentation of beh\u00e7et\u2019s disease (BD) causing long-term morbidity and mortality. Cerebral aneurysms are rare in BD, with only a handful of cases reported worldwide. A 39-year-old female experienced slurring of speech, numbness, and reduced sensation in the left face, arm, and leg 90 minutes prior to presentation. She provided a history of recurrent oral and genital ulcers associated with intermittent joints pain and blurring of vision. erythrocyte sedimentation rate was elevated, but all other autoimmune workup was negative. Neuro-beh\u00e7et\u2019s disease was diagnosed. Brain magnetic resonance imaging demonstrated acute right periventricular infarction. Magnetic resonance angiogram revealed M1 segment stenosis and right internal carotid artery saccular aneurysm. Catheter angiography confirmed its presence measuring 4.8 mm\u00d76.1 mm. She was stabilized after coiling of the aneurysm and was started on medical therapy. Brain imaging should be carried out in young patients with BD presenting with an ischemic event, and intervention may be lifesaving.", "title": "Neurobehcet with cerebral aneurysm", "claims": null}, {"metadata": {"year": 2018}, "authors": ["A. Dorta-Contreras", "Crist\u00f3bal Gonz\u00e1lez-Losada", "C. S\u00e1nchez-Mart\u00ednez"], "summary": "Introduction: Brucellosis is a zoonotic and an endemic disease in many areas around the world. The main cause of infection is the intake of unpasteurized milk or the contact with infected animals. Neurobrucellosis includes pathologic conditions in the central and peripheral nervous systems. The main clinical manifestations are meningitis, encephalitis, optical neuritis, and peripheral neuritis. Objective: To evaluate, through reibergram, the intrathecal dynamics of the major immunoglobulin classes and the blood-CSF barrier function in one patient with neurobrucellosis. Case report: \u00a0IgA, IgM, IgG and albumin levels in serum and cerebrospinal fluid were quantified by using a radial immunodiffusion technique. Results were placed in the corresponding reibergram. The patient showed evidences of intrathecal synthesis of the three major immunoglobulins without blood-CSF barrier dysfunction. Conclusion: The neuroimmunological study of cerebrospinal fluid can indicate the active course of the intrathecal immune response against this pathogen, where the intrathecal synthesis of immunoglobulins and blood-cerebrospinal fluid barrier function constitute the main markers in the diagnosis of \u00a0neuroinflammation. Keywords: Brucellosis, neuroimmunology, neurobrucellosis, neuroinflammation, zoonotic disease, reibergram", "title": "Neurobrucelosis evaluada por reibergrama. Presentaci\u00f3n de un caso", "claims": null}], "query": "Neuroblastoma and baicalein", "summary_abstract": "The research question regarding the relationship between neuroblastoma and baicalein is addressed primarily in the work of Liu et al. (2010). This study investigates the effects of baicalein, a flavonoid derived from the root of *Scutellaria baicalensis*, on neuroblastoma cells. The findings suggest that baicalein exhibits significant anti-cancer properties, including the induction of apoptosis in neuroblastoma cells. The study highlights that baicalein can inhibit cell proliferation and promote cell death through various mechanisms, including the modulation of signaling pathways associated with cell survival and apoptosis.\n\nWhile the other abstracts provided do not directly address neuroblastoma or the effects of baicalein on it, they do explore the neuroprotective effects of baicalein in other contexts, such as ischemic stroke (Yang et al., 2019). This research indicates that baicalein can reduce neuroinflammation and neuronal apoptosis, which may suggest potential therapeutic avenues for neurodegenerative conditions. However, these findings are not specific to neuroblastoma.\n\nIn summary, the primary evidence linking baicalein to neuroblastoma comes from Liu et al. (2010), which demonstrates its potential as an anti-cancer agent in neuroblastoma cells. Other studies, while informative about baicalein's neuroprotective properties, do not specifically address its role in neuroblastoma.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2019}, "authors": ["Q. Do", "Judith Gaspers"], "summary": "A typical cross-lingual transfer learning approach boosting model performance on a language is to pre-train the model on all available supervised data from another language. However, in large-scale systems this leads to high training times and computational requirements. In addition, characteristic differences between the source and target languages raise a natural question of whether source data selection can improve the knowledge transfer. In this paper, we address this question and propose a simple but effective language model based source-language data selection method for cross-lingual transfer learning in large-scale spoken language understanding. The experimental results show that with data selection i) source data and hence training speed is reduced significantly and ii) model performance is improved.", "title": "Cross-lingual Transfer Learning with Data Selection for Large-Scale Spoken Language Understanding", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Zaid Alyafeai", "Maged Saeed AlShaibani", "Irfan Ahmad"], "summary": "Deep learning models usually require a huge amount of data. However, these large datasets are not always attainable. This is common in many challenging NLP tasks. Consider Neural Machine Translation, for instance, where curating such large datasets may not be possible specially for low resource languages. Another limitation of deep learning models is the demand for huge computing resources. These obstacles motivate research to question the possibility of knowledge transfer using large trained models. The demand for transfer learning is increasing as many large models are emerging. In this survey, we feature the recent transfer learning advances in the field of NLP. We also provide a taxonomy for categorizing different transfer learning approaches from the literature.", "title": "A Survey on Transfer Learning in Natural Language Processing", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Zaid Alyafeai", "Maged S. Alshaibani", "Irfan Ahmad"], "summary": "Deep learning models usually require a huge amount of data. However, these large datasets are not always attainable. This is common in many challenging NLP tasks. Consider Neural Machine Translation, for instance, where curating such large datasets may not be possible specially for low resource languages. Another limitation of deep learning models is the demand for huge computing resources. These obstacles motivate research to question the possibility of knowledge transfer using large trained models. The demand for transfer learning is increasing as many large models are emerging. In this survey, we feature the recent transfer learning advances in the field of NLP. We also provide a taxonomy for categorizing different transfer learning approaches from the literature.", "title": "A Survey on Transfer Learning in Natural Language Processing", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Jacek Skryzalin", "Hamilton E. Link", "Jeremy D. Wendt", "R. Field", "Samuel N. Richter"], "summary": "We apply transfer learning techniques to create topically and/or stylistically biased natural language models from small data samples, given generic long short-term memory (LSTM) language models trained on larger data sets. Although LSTM language models are powerful tools with wide-ranging applications, they require enormous amounts of data and time to train. Thus, we build general purpose language models that take advantage of large standing corpora and computational resources proactively, allowing us to build more specialized analytical tools from smaller data sets on demand. We show that it is possible to construct a language model from a small, focused corpus by first training an LSTM language model on a large corpus (e.g., the text from English Wikipedia) and then retraining only the internal transition model parameters on the smaller corpus. We also show that a single general language model can be reused through transfer learning to create many distinct special purpose language models quickly with modest amounts of data.", "title": "Efficient Transfer Learning for Neural Network Language Models", "claims": null}, {"metadata": {"year": 2019}, "authors": ["P. Janata"], "summary": "The recent developments of Language Modeling led to advances in transfer learning methods in Natural Language Processing. Language Models pretrained on large general datasets achieved state-of-the-art results in a wide range of tasks. The Universal Language Model Fine-tuning represents an effective transfer learning method for text classification. The goal of this thesis is to further test the robustness of this method in scenarios, commonly found in real-world applications.", "title": "Transfer Learning for Textual Topic Classificaton", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Sergey Golovanov", "R. Kurbanov", "S. Nikolenko", "Kyryl Truskovskyi", "Alexander Tselousov", "Thomas Wolf"], "summary": "Large-scale pretrained language models define state of the art in natural language processing, achieving outstanding performance on a variety of tasks. We study how these architectures can be applied and adapted for natural language generation, comparing a number of architectural and training schemes. We focus in particular on open-domain dialog as a typical high entropy generation task, presenting and comparing different architectures for adapting pretrained models with state of the art results.", "title": "Large-Scale Transfer Learning for Natural Language Generation", "claims": null}, {"metadata": {"year": 2018}, "authors": ["B. Mocialov", "H. Hastie", "G. Turner"], "summary": "Automatic speech recognition and spoken dialogue systems have made great advances through the use of deep machine learning methods. This is partly due to greater computing power but also through the large amount of data available in common languages, such as English. Conversely, research in minority languages, including sign languages, is hampered by the severe lack of data. This has led to work on transfer learning methods, whereby a model developed for one language is reused as the starting point for a model on a second language, which is less resourced. In this paper, we examine two transfer learning techniques of fine-tuning and layer substitution for language modelling of British Sign Language. Our results show improvement in perplexity when using transfer learning with standard stacked LSTM models, trained initially using a large corpus for standard English from the Penn Treebank corpus.", "title": "Transfer Learning for British Sign Language Modelling", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Malte Ostendorff", "G. Rehm"], "summary": "Most Transformer language models are primarily pretrained on English text, limiting their use for other languages. As the model sizes grow, the performance gap between English and other languages with fewer compute and data resources increases even further. Consequently, more resource-efficient training methods are needed to bridge the gap for languages with fewer resources available. To address this problem, we introduce a cross-lingual and progressive transfer learning approach, called CLP-Transfer, that transfers models from a source language, for which pretrained models are publicly available, like English, to a new target language. As opposed to prior work, which focused on the cross-lingual transfer between two languages, we extend the transfer to the model size. Given a pretrained model in a source language, we aim for a same-sized model in a target language. Instead of training a model from scratch, we exploit a smaller model that is in the target language but requires much fewer resources. Both small and source models are then used to initialize the token embeddings of the larger model based on the overlapping vocabulary of the source and target language. All remaining weights are reused from the model in the source language. This approach outperforms the sole cross-lingual transfer and can save up to 80% of the training steps compared to the random initialization.", "title": "Efficient Language Model Training through Cross-Lingual and Progressive Transfer Learning", "claims": null}], "query": "a survey on transfer learning methods for 'large language models'", "summary_abstract": "The collection of papers provides a comprehensive overview of various transfer learning methods applicable to large language models, particularly in the context of natural language processing (NLP). \n\nA significant theme across the studies is the challenge of training deep learning models, which typically require vast amounts of data and substantial computational resources. Alyafeai et al. (2020) highlight the increasing demand for transfer learning as a solution to these limitations, especially in scenarios where large datasets are not feasible, such as in low-resource languages. They propose a taxonomy for categorizing different transfer learning approaches, emphasizing the importance of leveraging large pretrained models to facilitate knowledge transfer.\n\nDo and Gaspers (2019) specifically address cross-lingual transfer learning, proposing a method for selecting source-language data that enhances model performance while reducing training time. Their findings suggest that careful data selection can significantly improve the efficiency of training large models.\n\nSkryzalin et al. (2018) demonstrate that it is possible to create specialized language models from smaller datasets by first training on larger corpora. This approach allows for the rapid development of tailored models without the need for extensive data collection, showcasing the versatility of transfer learning techniques.\n\nJanata (2019) discusses the Universal Language Model Fine-tuning (ULMFiT) as an effective transfer learning method for text classification, further underscoring the robustness of pretrained models in various applications. Golovanov et al. (2019) explore the adaptation of large-scale pretrained models for natural language generation tasks, indicating that these models can be fine-tuned for specific applications, such as open-domain dialogue.\n\nMoreover, Mocialov et al. (2018) investigate transfer learning techniques for language modeling in minority languages, demonstrating improvements in model performance through fine-tuning and layer substitution methods. This highlights the potential of transfer learning to bridge the gap in resources between well-resourced and under-resourced languages.\n\nFinally, Ostendorff and Rehm (2023) introduce a novel cross-lingual and progressive transfer learning approach, CLP-Transfer, which optimizes the transfer of pretrained models from a source language to a target language. Their method significantly reduces training steps and enhances performance, illustrating the ongoing evolution of transfer learning strategies in the context of large language models.\n\nIn summary, the surveyed literature collectively emphasizes the critical role of transfer learning in enhancing the performance and efficiency of large language models, particularly in scenarios with limited data and resources. The various methodologies discussed provide valuable insights into how these models can be effectively adapted and utilized across different languages and tasks.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2017}, "authors": ["Claudio E. V.  Borio", "Leonardo  Gambacorta"], "summary": "This paper analyses the effectiveness of monetary policy on bank lending in a low interest rate environment. Based on a sample of 108 large international banks, our empirical analysis suggests that reductions in short-term interest rates are less effective in stimulating bank lending growth when rates reach a very low level. This result holds after controlling for business and financial cycle conditions and different bank-specific characteristics such as liquidity, capitalisation, funding costs, bank risk and income diversification. We find that the impact of low rates on the profitability of banks' traditional intermediation activity helps explain the subdued evolution of lending in the period 2010-14.", "title": "Monetary Policy and Bank Lending in a Low Interest Rate Environment: Diminishing Effectiveness?", "claims": null}, {"metadata": {"year": 1995}, "authors": ["J. Peek", "Eric S. Rosengren"], "summary": "A resurgence of interest in the role of banks in the transmission of monetary policy has resulted in a spate of theoretical and empirical studies. These studies have established that, under certain conditions, the traditional transmission mechanism for monetary policy (\"the money view\") may be augmented through changes in the supply of bank loans (\"the lending view\"). Because both the money view and the lending view operate through the banking sector, the health of the banking system, insofar as it affects bank behavior, is an important factor in the transmission of monetary policy. It affects both the nature and the size of bank responses to shifts in monetary policy, with particular relevance for the bank lending channel. The traditional description of monetary policy generally emphasizes the reserve requirement constraint on banks. In this story, banks are an important link in the transmission of monetary policy because changes in bank reserves influence the quantity of reservable deposits held by banks. Because banks rarely hold significant excess reserves, the resel~ce requirement constraint typically is considered to be binding at all times. However, a second constraint on banks, the capital constraint, may be more important in accounting for the variability in the magnitude of the effect of monetary policy over time. The extent to which a capital constraint is binding, unlike the reserve requirement, is likely to vary", "title": "Bank lending and the transmission of monetary policy", "claims": null}, {"metadata": {"year": 2006}, "authors": ["S. V. D. Heuvel"], "summary": "This paper examines the role of bank lending in the transmission of monetary policy in the presence of capital adequacy regulations. I develop a dynamic model of bank asset and liability management that incorporates risk-based capital requirements and an imperfect market for bank equity. These conditions imply a failure of the Modigliani-Miller theorem for the bank: its lending will depend on the bank\u00e2\u20ac\u2122s financial structure, as well as on lending opportunities and market interest rates. Combined with a maturity mismatch on the bank\u00e2\u20ac\u2122s balance sheet, this gives rise to a \u00e2\u20ac\u02dcbank capital channel\u00e2\u20ac\u2122 by which monetary policy affects bank lending through its impact on bank equity capital. This mechanism does not rely on any particular role of bank reserves and thus falls outside the conventional \u00e2\u20ac\u02dcbank lending channel\u00e2\u20ac\u2122. I analyze the dynamics of the new channel. An important result is that monetary policy effects on bank lending depend on the capital adequacy of the banking sector; lending by banks with low capital has a delayed and then amplified reaction to interest rate shocks, relative to well-capitalized banks. Other implications are that bank capital affects lending even when the regulatory constraint is not momentarily binding, and that shocks to bank profits, such as loan defaults, can have a persistent impact on lending", "title": "The Bank Capital Channel of Monetary Policy", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Lukas Altermatt"], "summary": "I build a general equilibrium model of the transmission of monetary policy on bank lending. Bank lending is done by individual banks that face random investment opportunities by creating inside money. Banks are subject to a reserve requirement and have access to the interbank money market. The model shows that lowering the money market rate relative to the inflation rate reduces investment and welfare. This is because the money market is an outside option for banks that face bad investment opportunities. Reducing the money market rate lowers the value of this outside option, which in turn reduces banks\u2019 willingness to acquire reserves ex-ante. This leads to less aggregate reserves, which reduces the banking system\u2019s ability to grant credit.", "title": "Bank Lending, Financial Frictions, and Inside Money Creation", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Hartmut Brinkmeyer"], "summary": "Monetary policy transmission.- A new view: Implications of financial innovation for the bank lending channel.- Bank lending against the background of the recent crisis.- Empirical analysis: Determinants of bank lending during normal and crisis periods.", "title": "Drivers of Bank Lending: New Evidence from the Crisis", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Elena A Afanasyeva", "Jochen Guentner"], "summary": "This paper investigates the risk-taking channel of monetary policy on the asset side of banks' balance sheets. We use a factor-augmented vector autoregression (FAVAR) model to show that aggregate lending standards of U.S. banks, e.g. their collateral requirements for firms, are significantly loosened in response to an unexpected decrease in the Federal Funds rate. Based on this evidence, we reformulate the costly state verification (CSV) contract, embed it in a dynamic general equilibrium model, and show that - consistent with our empirical finding - a monetary easing implies an expansion of bank lending for a given amount of borrower collateral. The model also predicts a delayed increase in borrowers' default risk.", "title": "Bank Risk Taking, Credit Booms and Monetary Policy", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Andreas  Schabert"], "summary": "We analyze optimal monetary policy in a sticky price model where the central bank supplies money outright via asset purchases and lends money temporarily against collateral. The terms of central bank lending affect rationing of money and impact on macroeconomic aggregates. The central bank can set the policy rate and its inflation target in a way that implements the first best long-run allocation, which is impossible if money were supplied in a lump-sum way (as commonly assumed). Efficient central bank lending further increases gains from macroeconomic stabilization beyond pure interest rate policy. This requires departing from a \u201cTreasuries-only\u201d regime.", "title": "Optimal Central Bank Lending", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Teodora Paligorova", "J. S. Jimenez"], "summary": "The financial crisis of 2007\u201309 and the subsequent extended period of historically low real interest rates have revived the question of whether economic agents are willing to take on more risk when interest rates remain low for a prolonged time period. This increased appetite for risk, which causes economic agents to search for investment assets and strategies that generate higher investment returns, has been called the risk-taking channel of monetary policy. Recent academic research on banks suggests that lending policies in times of low interest rates can be consistent with the existence of a risk-taking channel of monetary policy in Europe, South America, the United States and Canada. Specifically, studies find that the terms of loans to risky borrowers become less stringent in periods of low interest rates. This risk-taking channel may amplify the effects of traditional transmission mechanisms, resulting in the creation of excessive credit.", "title": "Monetary Policy and the Risk-Taking Channel: Insights from the Lending Behaviour of Banks", "claims": null}], "query": "monetary policy, bank lending", "summary_abstract": "The research question regarding the relationship between monetary policy and bank lending has been explored through various lenses in the provided papers. A common theme is the impact of interest rates on bank lending behavior, particularly in low-rate environments.\n\nBorio and Gambacorta (2017) highlight that reductions in short-term interest rates become less effective in stimulating bank lending growth when rates are very low. Their analysis of large international banks indicates that the profitability of traditional banking activities diminishes under such conditions, which contributes to subdued lending growth during the period from 2010 to 2014.\n\nPeek and Rosengren (1995) discuss the dual constraints banks face\u2014reserve requirements and capital constraints. They argue that the health of the banking system significantly influences how banks respond to shifts in monetary policy. Specifically, the capital constraint can vary over time, affecting the magnitude of the bank lending channel's response to monetary policy changes.\n\nHeuvel (2006) introduces a \"bank capital channel,\" suggesting that monetary policy impacts bank lending through its effects on bank equity capital, rather than solely through reserve requirements. This model indicates that banks with lower capital may react more slowly and with greater intensity to interest rate changes compared to well-capitalized banks.\n\nAltermatt (2019) presents a general equilibrium model that shows how lowering the money market rate can inadvertently reduce banks' willingness to acquire reserves, leading to a decrease in their ability to grant credit. This highlights the complex interplay between interest rates and banks' investment decisions.\n\nAfanasyeva and Guentner (2014) explore the risk-taking channel of monetary policy, demonstrating that unexpected decreases in the Federal Funds rate lead to looser lending standards among U.S. banks. This suggests that monetary easing can expand bank lending even when borrower collateral remains constant, albeit with a potential increase in default risk.\n\nPaligorova and Jimenez (2012) further elaborate on the risk-taking channel, noting that during periods of low interest rates, banks may relax their lending policies, particularly towards riskier borrowers. This behavior can amplify the effects of traditional monetary policy transmission mechanisms, potentially leading to excessive credit creation.\n\nIn summary, the collective findings from these studies indicate that the effectiveness of monetary policy on bank lending is significantly influenced by the prevailing interest rate environment, the capital adequacy of banks, and the risk-taking behavior of banks in response to monetary easing. The interplay of these factors underscores the complexity of the bank lending channel in the context of monetary policy.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2019}, "authors": ["Obah"], "summary": "Cassava is a major staple food in most African and Asia countries, but its processing usually releases effluent which is high in cyanide and other heavy metals to the environment. The main objective of this study is to develop a bioremediation method aimed at degrading cassava effluent. Results obtained from this study show that organic soap solution (pH of 9.8 and electrical conductivity and 1330 \u03bcS/cm), had a significant effect on the cyanide, Zinc, Nickel, Chromium, Phosphorus and potassium content of the cassava effluent within the treatment period of 6 days. Furthermore, the organic soap solution significantly reduced the total coliforms colonies in the cassava effluent within the treatment period. A comparison of the treatment effluent results with NIS standards shows that most of the pollutant parameters were almost at the acceptable limits after the treatment with the exception of Potassium and phosphate. The results from this study apparently indicated that organic soap solution was very efficient in degrading the cassava effluent to appreciable level, but it should be further researched to explore its greater remediation potential.", "title": "Remediation of effluent from cassava processing mills", "claims": null}, {"metadata": {"year": 2016}, "authors": ["O. Henrietta", "E. Oe"], "summary": "This study examines the effects of cassava processing wastes on the soil environment of a local cassava mill in Ekiadolor, Ovia North East Local Government Area of Edo State, Nigeria. Microbial, physicochemical and mineral compositions of fresh cassava effluent, cassava effluent from waste pit, soil around the cassava mill (soil 1) and soil samples 100 m away from the mill (the control, soil 2) were determined. Soil 2 had the highest microbial count of 3.52 \u00d7 105 cfu/ml. The microbial species isolated included Klebsiella aerogenes, Bacillus subtilis, Lactobacillus plantarum, Staphylococcus aureus, Lactobacillus delbrueckii, Fusarium solani, Aspergillus niger and Saccharomyces cerevisiae. The occurrence of the isolated microorganisms was lowest in soil 1 with 37.5%. Fresh cassava effluent was the most acidic with pH 3.2 and cassava effluent from waste pit had the highest cyanide content of 53.52 mg/l. The mineral contents (Ca, Mg, Na and K) of the fresh cassava effluent, effluent from waste pit and soil 1 were significantly lower (p<0.05) than the control. The heavy metals (Fe, Zn, Mn, Al, Pb and Cu) were significantly higher in soil 1 when compared with soil 2. Nitrate and phosphate contents were high in all the samples except the control. The continuous disposal of the cassava processing wastes in the soil environment around the mill and into a waste pit has reduced the soil quality leading to environmental degradation.", "title": "A Study on the Effects of Cassava Processing Wastes on the Soil Environment of a Local Cassava Mill", "claims": null}, {"metadata": {"year": 2019}, "authors": ["N. Lawal", "K. Ogedengbe", "E. Oamen"], "summary": "The absence of wastewater treatment facilities in small-scale cassava agroindustry results in the disposal of toxic effluent that causes environmental problems to host communities. A pilot multi-compartment reactor packed with graded earthen materials with continuous aeration and anaerobic degradation was evaluated for the treatment of cassava mill wastewater. The results suggest that continuous aeration and biodegradation of the effluent reduced Hydrogen Cyanide by 84%, BOD (72%), COD (69%) and Turbidity (41%). A slightly acidic pH of 5.0 was obtained for the final effluent. The results suggest an efficient and promising technique that combines physical and nature-based processes in degrading key pollutants from cassava effluent. Despite having some pollutants with final values above permissible limits, the system can be optimized to enhance performance. The treatment technique is adjudged sustainable with low technical operating demands and can be used for on-site wastewater treatment in small and medium scale cassava processing clusters.", "title": "Cassava Mill Wastewater Treatment by a Combination of Physical and Nature-Based Processes : A Pilot Study", "claims": null}, {"metadata": {"year": 2019}, "authors": ["S. Ci"], "summary": "The processing of cassava tuber into finished products such as gari generates large wastes water. The effluents are known to contain cyanide and acidic pH in addition to other chemical characteristics such as heavy metals, chemical oxygen demand among others. The effluents have toxicological impacts on the receiving ecosystem (Soil and Surface Water) as well as some of its associated fauna and flora. During degradation processes by the indigenous microbes in the soil, the effluents emits odour that are offensive to human. This study reviews recycling options of cassava mill effluents through bioenergy production. The study found that cassava mill effluents have demonstrated positive potentials for biogas, biohydrogen, bioethanol, bioelectricity using varying technologies. During bioenergy production, the characteristics of the raw effluents is improved upon in some of the parameters such as chemical oxygen demand, total solid, pH etc probably due to the activities of some microorganisms. As such, there is the need for research to focus on other possible utilization of the sludge generated such as bio-fertilizer production as an option for reuse.", "title": "Cassava Mill Effluents Recycling throughBioenergy Production: A Review", "claims": null}, {"metadata": {"year": 2015}, "authors": ["G. Afuye", "K. O. Mogaji"], "summary": "Cassava (Manihot esculenta Crantz) is a very important staple food in most developing countries. During its processing into cassava starch or \u2018gari\u2019, wastewaters effluent are generated and are indiscriminately discharged into the environment or public sewers thereby polluting the soil and invariably discharge into nearby shallow \u2018wells\u2019 representing a risk for the environment because of its cyanogen\u2019s content, and the reduced quality of the shallow \u2018well water\u2019 made it less suitable for human consumption. The objective of this study is to analyze the contaminated water resources in five selected cassava processing factories for various physical, chemical, and biology parameters. The parameters determined were colour, odour, turbidity, PH, Cl, Ca, NO3 , Na, Mg, TDS, K, Cu, Zn, Mn, Fe, etc. The five centers were all predominantly processing cassava into gari with high volume of wastewater and solid wastes being discharge into the environment. Results showed that cassava waste water led to increase in iron, sulphate, potassium, sodium, chromium, cadmium, etc. of the receiving \u2018shallow well\u2019 water.", "title": "Effect of cassava effluents on domestic consumption of \u2018shallow well\u2019 water in Owo Local Government Area, Ondo State, Nigeria", "claims": null}, {"metadata": {"year": 2016}, "authors": ["J. Adewumi", "J. Babatola", "O. Olayanju"], "summary": "This study is aimed at determining the impacts of indiscriminate disposal of processed cassava wastewater into the environment by determining the physicochemical characteristics of the wastewater and soil characteristics within the vicinity of starch processing industry.The cassava wastewaterquality wastested in accordance with the standard methods for the examination of water and wastewater while soil samples was analysed using Atomic Absorption Spectrometer. The result of cassava wastewater shows that the effluent is acidic with pH of 3.8 and high conductivity of 6500 \u00b5\u2126. The cyanide content is also very high with 0.17mg/l as compared to 0.05mg/l recommended by WHO. The total solid, total dissolved solid and total suspended solid are also in high range of 20619, 17048 and 3571 mg/l respectively. Its hardness is 812mg/l, chloride 2516mg/l and nitrate 470mg/l. The dissolved oxygen was not detected while COD and BOD were also on the high side with values of 560 and 1410mg/l respectively. The results of soil analysis indicates that the cyanide concentration is high at the surface but reduces with depth. The pH is also in acidic range. However, Magnesium, Sodium, Sulphate, Zinc, Chromium, Vanadium, Europium and Rubidium were not detected in the soil samples. The percentage of calcium, potassium, copper, manganese and titanium were small and varying between 0.07 and 6.37ppm. Three unlined aerobic ponds provided for the treatment of cassava wastewater reduces the pollution strength of the wastewater but the untreated wastewater interfere with the surrounding soil during conveyance to the pond as well as pond environment.It is recommended that the wastewater conveyance and ponds should be well designed to prevent interference with the surrounding soil.", "title": "The Impact of Cassava Wastewater from Starch Processing Industry on Surrounding Soil: A Case Study of Matna Foods Industry, Ogbese", "claims": null}, {"metadata": {"year": 2019}, "authors": ["N. Okoli", "N. Oti", "I. Ekpe", "S. A. Mbawuike"], "summary": "*Corresponding author : henrynek34@gmail.com, nnaemeka.okoli@futo.edu.ng INTRODUCTION Soil and water bodies are particularly polluted with toxicants from food processing and allied industries (Salami and Egwin 2007). Major pollutants from food processing include hydrocarbons, palm oil mill effluent, human and animal ISSN: 1394-7990 Malaysian Journal of Soil Science Vol. 22: 101-115 (2018) Malaysian Society of Soil Science", "title": "Long-Term Impact of Cassava Mill Effluent on Some Chemical and Biological Properties of Soils", "claims": null}, {"metadata": {"year": 2018}, "authors": ["N. Lawal", "A. Babalola", "O. O. Adama", "A. O. Sosanya", "A. A. Adebayo"], "summary": "Cassava effluent was characterized with the aim of examining the current on-site management techniques among local processors for technological and regulatory improvements. Structured questionnaires were administered in sixty randomly selected processing clusters with different processing capacities to obtain data on the water consumption pattern, effluent output and quality, processing techniques and effluent management. The twenty-three physico-chemical parameters (pH (3.85), Oil and grease (0.386 mg/l), Electrical Conductivity (4.23 \u03bcmhos/cm), Biological oxygen demand (1889 mg/l), Total dissolved solids (3240 mg/l), Chemical oxygen demand (752.7 mg/l) Total suspended solids (9.28 mg/l), Bicarbonate (181.6 mg/l), Carbonate (387 mg/l), Sodium (1579.20 mg/l), Total nitrogen (0.86 mg/l), Potassium (1512.49 mg/l), Total Phosphate (300.41 mg/l), Calcium (35.4 mg/l), Magnesium (188.93 mg/l), Lead (59.39 mg/l), Cadmium (Not Detected), Chromium (0.28 mg/l), Iron (3.25 mg/l), Sulphate (17.75 mg/l), Turbidity (7620.96 NTU), Hydrogen Cyanide (678.50 mg/l) and Total Coliform count (2400 cfu/100ml)) analysed were mostly beyond permissible limits prescribed by regulating authorities. Wet fufu paste was the dominant product (73%) and generates the highest volume of wastewater (5-8 liters/tonne of fresh cassava root processed), while gari and fermented cassava flour (lafun) were 21% and 6% respectively. Processing water was predominantly sourced from boreholes (52%); 31% use near by stream water while 16% and 1% rely on water from deep wells and pipe-born water, respectively. Onsite effluent handling techniques includes: direct disposal on land (50%), disposal in open drains (37%) and nearby streams (13%). The unwholesome effluent disposal techniques is largely due to the low level of education and processing technology adopted. Precautionary measures and strict regulations are urgently needed to prevent further environmental risk and degradation. Finally, alternative use of treated effluent for crop irrigation should be investigated and promoted to enhance agricultural productivity and economic sustainability among local processors. Correspondence: nslawal@oouagoiwoye.edu.ng", "title": "CHARACTERIZATION OF DIFFERENT CASSAVA EFFLUENTS AND EVALUATION OF PROCESSOR ON-SITE HANDLING TECHNIQUES IN OGUN STATE, NIGERIA", "claims": null}], "query": "cassava effluent from cassava mill", "summary_abstract": "The research on cassava effluent from cassava mills highlights significant environmental concerns and potential remediation strategies. Cassava processing generates effluents that are often rich in toxic substances, including cyanide and heavy metals, which pose risks to soil and water quality.\n\nObah (2019) demonstrated that an organic soap solution effectively reduced cyanide and heavy metal concentrations in cassava effluent, suggesting a viable bioremediation method. This treatment brought many pollutants within acceptable limits, although potassium and phosphate levels remained high. Similarly, Lawal et al. (2019) reported that a multi-compartment reactor could reduce hydrogen cyanide and other pollutants significantly, indicating a promising approach for on-site wastewater treatment in small-scale cassava processing.\n\nThe physicochemical characteristics of cassava effluent are concerning. For instance, Adewumi et al. (2016) found that cassava wastewater had a highly acidic pH and elevated levels of total solids and cyanide, which exceeded WHO recommendations. This aligns with findings from Henrietta and Oe (2016), who noted that cassava effluent negatively impacted soil quality, leading to reduced microbial diversity and increased heavy metal concentrations in the soil surrounding processing sites.\n\nMoreover, Afuye and Mogaji (2015) highlighted that indiscriminate disposal of cassava wastewater contaminates shallow well water, making it unsuitable for human consumption. This underscores the urgent need for improved waste management practices in cassava processing to mitigate environmental degradation.\n\nIn summary, while cassava effluent poses significant environmental challenges due to its toxic content, various treatment methods, such as bioremediation with organic solutions and engineered reactors, show promise in reducing harmful pollutants. However, the current practices of effluent disposal remain inadequate, necessitating stricter regulations and better management strategies to protect ecosystems and public health (Obah, 2019; Lawal et al., 2019; Adewumi et al., 2016; Henrietta & Oe, 2016; Afuye & Mogaji, 2015).", "summary_extract": null}, {"papers": [{"metadata": {"year": 1998}, "authors": ["C.  Dutilh"], "summary": "Unilever is one of the worlds largest consumer goods companies, with a turnover in 1996 of Dfl 87 795 million. Most of the business is in branded consumer goods. More than half the turnover is generated with food products, which include margarine, oil, tea, tomato-based sauces, ice-cream and frozen fish. The second key product area is home and personal care, which includes detergents, soap, shampoo, tooth-paste and skin cream. Up to the beginning of 1997 Unilever had a major activity in speciality chemicals. Today, Unilever has some 500 operating companies in over 80 countries, and employs about 300 000 people worldwide.", "title": "The environmental improvement process in Unilever", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Patrick  Cescau"], "summary": "Unilever is one of the leading global manufacturers of fast moving consumer goods. Formed in 1930 following the merger of two existing companies, Unilever today has a portfolio of about 400 brands in 14 foods, home care and personal care categories, including Lipton, Knorr, Dove, and Omo. In 2006, the company invested about \u20ac900 million in R&D, representing 2.3 percent of sales revenue, focused on five R&D laboratories and a network of innovation centers all around the world. Recent novel products range from a Lifebuoy hand washing product in India to reduce the amount of rinse water required, to Becel/Flora pro. activ products which are recognized as the most significant advancement in dietary management of cholesterol in four decades. With its Nutrition Enhancement Programme, Unilever has shown its industry leadership by eliminating almost all trans fats from margarine products.", "title": "Group CEO Unilever", "claims": null}, {"metadata": {"year": 2011}, "authors": [], "summary": "1. Unilever is an Anglo-Dutch company dual-listed on Euronext Amsterdam, through Unilever N.V., and the London Stock Exchange, through Unilever plc. Each entity exists as a separate company but operate together as a single economic unit. Unilever is active in the development, manufacture, distribution and marketing of fast moving consumer goods products, principally in the food, home care and personal care product categories through a wide range of brands. Unilever's group turnover in the 2009/10 financial year was [ ], of which the UK accounted for [ ].", "title": "Anticipated acquisition by Unilever of Alberto Culver Company", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Amar K. J. R. Nayak"], "summary": "Unilever PLC is a case of FDI in the fast moving consumer product industry. It is also an example of oldest multinational enterprises in India and the world over. This chapter discusses the process by which Unilever developed a market from nothing \u2014 none of its products were in use and there was no support industry and basic infrastructure to conduct its business. The task of building awareness of its products with the potential consumers and a distribution network across the length and breadth of India, which had little road transport and communication, was the challenge for this company. This case also reveals how the company invested in all related businesses to complement its main business of manufacturing personal care products such as soaps, detergents, toiletries, and packaged food products. The analysis involves the study of nature and timing of investments of the company in its main business and complementary market functions, the manner in which it aligned itself to the requirements of the GOI, the issues of localization of management and ownership and its financial performance in India. The detailed confirmatory statistical analysis of the case, however, is provided in Chapter 7.", "title": "Unilever PLC, 1932\u20132004", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Joan Cherono"], "summary": "A research project submitted to the school of business in partial fulfillment of the requirement for degree in master of business administration (human resource management) of Kenyatta University November, 2017", "title": "Employee development and organizational performance of Unilever Tea Kenya ltd in Kericho County", "claims": null}, {"metadata": {"year": 2017}, "authors": ["A. Parvin"], "summary": "Unilever Bangladesh Limited, a fast moving consumer goods company, engages in the manufacture and distribution of home care products, personal care products, and foods. It offers household care, fabric cleaning, skin cleansing, skin care, oral care, hair care, and personal grooming products, as well as tea based beverages. The company was formerly known as Lever Brothers Bangladesh Ltd. and changed its name in December 2004. Unilever Bangladesh Limited was founded in 1964 and is based in Dhaka, Bangladesh. Unilever Bangladesh Limited operates as a subsidiary of Unilever Public Limited Company. Unilever believes profitable growth should also be responsible growth. That approach lies at the heart of their business model, driven by sustainable living and the Unilever Sustainable Living Plan. It guides their approach to how they do business and how they meet the growing consumer demand for brands that act responsibly in a world of finite resources. Their business model begins with consumer insight that informs brand innovation, often with partners in their supply chain, to create products they take to market supported by marketing and advertising across a range of distribution channels.", "title": "Business Strategy and Marketing Management of a Multinational Organization Development", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Anthony-Claret I Onwutalobi", "Fredrick Albert Uhomhoabhi"], "summary": "The Fast Moving Consumer Goods (FMCG) environment is rapidly changing. Especially, the increasing popularity of line extensions seems to depend on advantages inherent in brand leveraging. FMCG manufacturers go into RD and the Dutch, Margarine Unie; now Unilever PLC in London, U.K and Unilever N.V in Rotterdam, Netherlands respectively. In 1872 before the merger, Jurgens and Van den Bergh, the Dutch, built factory in Netherlands for the production of Margarine made from milk and fact. In 1927, they formed Margarine Unie (margarine Union) together with two European Businesses, Centre and Schicht. Lever & Co on the other hand was founded in 1884 by British William Hesketh Lever and his brother James, and was producing soap \u2013 Sunlight soap for people in England especially for women. William Lever wrote: \u201cto make cleanliness commonplace; to lessen work for women; to foster health and contribute to personal attractiveness, that life may be more enjoyable and -1\u00a9 2008 Codewit.com. All rights reserved. rewarding for the people who use our products\u201d. In 1890, Lever & Co become a limited company known as Lever Brothers. Unilever, Unilever N.V and Unilever PLC comprise Unilever", "title": "PESTEL ANALYSIS: A REPORT ON UNILEVER", "claims": null}, {"metadata": {"year": 1992}, "authors": ["Fred J. Trussell"], "summary": "The Meal and Meal Components Section in Unilever Research was formed at the end of 1990, as a result of the refocusing of the Unilever Foods organization. With the Cambridge Management Centre a series of three\u2010day team\u2010building workshops was developed which was conducted off\u2010site in Northampton. It is noticeable that commitment to take time out for communication and team building back in the work situation has been high in \u201cteam\u2010built\u201d teams. The shared experience of the workshops has also provided a common currency in the day\u2010to\u2010day working of the section. There is much greater awareness of \u201cdifferences\u201d in the attitude and approach of team members.", "title": "Team\u2010built Teams", "claims": null}], "query": "The year Unilever Plc Uk was established, what did they do and what is their staff capacity? ", "summary_abstract": "Unilever Plc was established in 1930 through the merger of two companies: the Dutch margarine producer Margarine Unie and the British soap maker Lever Brothers (Cescau, 2009; Onwutalobi & Uhomhoabhi, 2008). Initially, the company focused on producing margarine and soap, but it has since expanded its portfolio to include a wide range of fast-moving consumer goods (FMCG) across food, home care, and personal care categories, boasting around 400 brands such as Lipton, Knorr, Dove, and Omo (Cescau, 2009).\n\nAs of the late 1990s, Unilever employed approximately 300,000 people globally and operated around 500 companies in over 80 countries (Dutilh, 1998). This extensive workforce reflects Unilever's significant capacity to manage its diverse product lines and global operations. The company's commitment to research and development, with an investment of about \u20ac900 million in 2006, further underscores its focus on innovation and market leadership (Cescau, 2009). \n\nIn summary, Unilever Plc was founded in 1930, initially focusing on margarine and soap production, and has grown to employ around 300,000 people worldwide, making it one of the largest consumer goods companies globally (Dutilh, 1998; Cescau, 2009).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2018}, "authors": ["Adebowale Ojo"], "summary": "The significance of health information management practice to the effectiveness of a healthcare delivery system cannot be overemphasised. A well-structured and coordinated health information management system has been known to generate the information needed for decision-making at all levels of healthcare delivery. However, the state of health information management in Nigeria, as is the case in most African countries, is a cause for concern. Observation and past studies have highlighted challenges facing the practice of health information management in Africa to be centred around the quality of professional training, inadequately qualified practitioners, disgruntled practitioners, government\u2019s indifference towards the practice, lack of policies and inadequate technological infrastructure among others. This article examines some of the challenges facing health information management practice in Nigeria and makes recommendations that may uplift the profession.", "title": "Repositioning health information management practice in Nigeria: Suggestions for Africa", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Ojerinde Oluwaseun", "Iroju Olaronke", "Adeyemi"], "summary": "The healthcare system is an information intensive domain that is responsible for capturing, processing and storing large volumes of health information which could be clinical, research or administrative in nature. Healthcare information is primarily used for making decisions, improving the efficiency and quality of healthcare systems as well as conducting research to enhance medical science in both developed and developing countries. Healthcare information is usually stored in paper form, physical files or digital formats. However, developing countries such as Nigeria are faced with the challenge of moving from paper based health information system towards the digital formats that facilitates an integrated computerized health information system. Nevertheless, healthcare information is usually transmitted among disparate healthcare providers within or across different healthcare institutions. However, healthcare entities in developing countries are plagued with the difficulty of how to successfully and effectively manage information as it flows across the continuum of care. Consequently, this results in inappropriate decision making, ineffective planning, increase in medical errors and cost as well as a decline in the quality of patients\u2019 care. Based on this background, this paper appraises how information flows within and across diverse healthcare organizations in developing countries with a particular reference to Nigeria. The paper also recommends ways of managing information flow within the Nigeria healthcare system.", "title": "Strategies for Managing Information Flow in Nigeria Healthcare System", "claims": null}, {"metadata": {"year": 2013}, "authors": ["R. Nwosu"], "summary": "In developing countries like Nigeria, large proportions of the population have limited access to health information and health care facilities. Thus there is the fear that health professionals and patients may not be aware of the availability of health information in developing countries like Nigeria, large proportions of the population have limited access to health information and health care facilities. Thus there is the fear that health professionals and patients may not be aware of the availability of health information as well as their utilization with regards to the management of chronic diseases. The research design that was utilized for this study is the survey design type of descriptive research. The research instrument that was used to collect data for the study is questionnaire. The following findings were derived from this study: Majority of the health professionals affirmed that there is health information policy that they are expected to adhere to and also they agreed that they offered health information and education to all categories of patients, on the nature of health information. This notwithstanding, there are general lack of awareness on the need for health information to the public for the prevention and management of chronic diseases in Delta State, Nigeria.", "title": "Health Information Availability and Utilization by Medical Practitioners for Chronic Disease Management in Central Hospitals, Warri-Delta State", "claims": null}, {"metadata": {"year": 2013}, "authors": ["F. Anyaegbunam"], "summary": "Today's healthcare systemsin Nigeria are fragmented, uncoordinated and inefficient. Patients may travel far and wide, but knowledge of their health, welfare and medical conditions do not readily follow them. Within localities, hospitals and doctors rarely talk to one another nor share information about patients. Administrative costs of managing Healthcare systems are enormous. About 25%r of the $4 trillion global healthcare industry goes on just managing the system. The global task ahead is to contain those crippling administrative costs while raising the quality and variety of services. The benefits gained from applying information and communications technology in a wider and more comprehensive manner from the management of healthcare to its delivery\u2014much as it's been done so successfully in finance, business, communications and most other walks of life\u2014could be truly enormous. This paper believes thatif there exists a more integrated flow of medical information, plus instant access to relevant data, healthcare planners could eliminate many of the redundancies in the system, cutting costs and improving efficiencies all round. The paper thus proposes comprehensive electronic diagnostis and preventive healthcare delivery in Nigeria.", "title": "OnIntegration of ICT in National Healthcare Delivery System", "claims": null}, {"metadata": {"year": 2000}, "authors": ["de Vries GE"], "summary": "The Policy Issue: The availability of accurate, timely, reliable and relevant health information is the most fundamental step toward informed public health action. Therefore, for effective management of health and health resources, governments at all levels in Nigeria have overriding interest in supporting and ensuring the availability of health data and information as a public good for public, private and NGOs' utilization Magnitude of the Problem: The major constraints in the generation/strategic use of health information and evidence for health systems operations and policymaking in Nigeria include: Nigeria health information system characterized by suboptimal data collection and analysis system; inadequate quality control measures; inadequate and ineffective staff training in data analysis, interpretation and use at all levels; misreporting of conditions, poor understanding, low confidence and acceptability; weak monitoring, evaluation and managerial capacity at the periphery and; inadequate IT facility and technology; lack of skills and incentives to access information Policy Options: (1). Role of government: The role of government must extend to ensuring standardization and financing of health data infrastructure, especially with respect to establishing and strengthening relevant organizational structures for Health Management Information System (HMIS) activities. (2). Special initiative on health information: There should be the launching of a special initiative to improve access to health information in Nigeria. (3). Research information and learning needs and barriers to use of information: Support systematic reviews and new research into information needs, barriers to access, and barriers to use of information among healthcare providers in Nigeria. (4). Access to existing materials for healthcare providers in developing countries: Libraries and resource centres should provide cost-effective access to computers, the internet, and printed materials for healthcare providers. (5). Internet and e-mail connectivity: Support the provision of e mail and internet connectivity worldwide, giving priority to individuals and groups where access will have most impact at the point of greatest need. (6). Skills development and training: Investigate the training and learning needs for each group within the knowledge cycle at different levels of healthcare; identify and support effective training programmes.", "title": "Policy briefs.", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Mb Bch PhD Dha Ayodele Cole Benson"], "summary": "This literature review was developed to examine empirically the factors hindering adoption of hospital information systems in Nigeria. The study was focused on the perceived paucity of health information technology policy in Nigeria and the causes of poor implementation of hospital information systems in the country. The findings of the literature review highlighted hindrances to the adoption of hospital information systems to include; the high cost of full implementation of a hospital information system, inadequate human capital, corruption, and problems associated with poor infrastructure in Nigeria. The recommendations were that, the Nigerian government needs to provide stable electricity, basic communication infrastructures, and Internet access to boost private initiatives in the adoption of health information technology across the country.", "title": "Hospital Information Systems in Nigeria: A Review of Literature", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Ebele N. Anyaoku"], "summary": "In Nigeria fast moving urbanization and its attendant changes in life style and in diet, have led to increase in chronic diseases. The paper is a review on the role of health information in patients\u2019 self-management of chronic diseases. It noted that the most important role of health information is patient empowerment which supports patients in taking good decisions on their care. The paper noted the lack of organized patient health information disseminating structures in Nigerian health institutions. This has created limited access to health information for patients who might need it in the care process. The paper reviewed various methods and strategies that can be adopted for health information dissemination. In conclusion, it stated that the Federal Government of Nigeria, health care institutions and organizations have a duty to ensure that patients have access to health information through development of appropriate policies and strategies to support its dissemination.", "title": "Empowering Patients for Chronic Disease Self-Management through Access to Health Information in Nigeria: Overview of Strategies.pdf", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Ebele N. Anyaoku"], "summary": "In Nigeria fast moving urbanization and its attendant changes in life style and in diet, have led to increase in chronic diseases. The paper is a review on the role of health information in patients' self-management of chronic diseases. It noted that the most important role of health information is patient empowerment whichsupports patients in taking good decisions on their care. The paper noted the lack of organized patient health information disseminating structures in Nigerian health institutions. This has created limited access to health information for patients who might need it in the care process. The paper reviewed various methods and strategies that can be adopted for health information dissemination In conclusion, it stated that the Federal Government of Nigeria, health care institutions and organizations have a duty to ensure that patients have access to health information through development of appropriate policies and strategies to support its dissemination.", "title": "EMPOWERING PATIENTS FOR CHRONIC DISEASE SELFMANAGEMENT THROUGH ACCESS TO HEALTH INFORMATION IN NIGERIA", "claims": null}], "query": "state of healthcare management information in Nigeria", "summary_abstract": "The state of healthcare management information in Nigeria is characterized by significant challenges that hinder effective healthcare delivery. Adebowale Ojo (2018) emphasizes the critical role of health information management in decision-making across healthcare levels, yet highlights that Nigeria's system suffers from inadequate professional training, poorly qualified practitioners, and insufficient government support. This sentiment is echoed by Ojerinde et al. (2015), who note the difficulties in transitioning from paper-based systems to integrated digital formats, which are essential for efficient information flow among healthcare providers. The fragmentation of healthcare systems, as described by Anyaegbunam (2013), exacerbates these issues, leading to inefficiencies and increased administrative costs.\n\nMoreover, Nwosu (2013) points out that a large portion of the Nigerian population lacks access to vital health information, which is crucial for managing chronic diseases. This lack of awareness among both health professionals and patients further complicates the situation. The literature also indicates that the existing health information systems are plagued by suboptimal data collection and analysis, as well as inadequate training and infrastructure (de Vries, 2000). \n\nThe barriers to adopting hospital information systems, as identified by Cole Benson (2011), include high implementation costs, corruption, and poor infrastructure, which collectively hinder the advancement of health information technology in Nigeria. Anyaoku (2014) reinforces the need for organized structures to disseminate health information effectively, advocating for government and institutional responsibility in ensuring patient access to necessary health information.\n\nIn summary, the healthcare management information landscape in Nigeria is fraught with challenges, including inadequate training, poor infrastructure, and a lack of organized information dissemination. Addressing these issues is essential for improving healthcare delivery and empowering patients in their health management.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2009}, "authors": ["Dan Immergluck"], "summary": "A key aspect of the U.S. subprime crisis was the accumulation of vacant, foreclosed properties in many neighborhoods and localities. This article describes zip-code-level patterns of foreclosed homes, or what are typically called \u201creal estate owned\u201d (REO) properties, at the peak of the subprime crisis in late 2008 and estimates a model of REO accumulation from 2006 to 2008. Three key findings emerge. First, during the peak of the subprime foreclosure crisis in late 2008, large central cities, on average, experienced higher levels of REO per mortgageable property than suburban areas. This contradicts some suggestions that the crisis was primarily centered in suburban or exurban communities. Second, the suburbanization of REO varied across two key types of metropolitan areas, with boom\u2014bust regions experiencing more suburbanization than weak- or mixed-market metros. Finally, determinants of zip-code-level REO accumulation included high-risk lending activity and the age of housing stock. After controlling for these and other variables, neither the central city versus suburban location of a zip code nor the proportion of residents commuting over 30 minutes was significantly associated with REO growth. The intrametropolitan location of a zip code appears to have been a less important factor in REO growth than the fact that a large amount of development in newer communities was financed during the subprime boom.", "title": "Neighborhoods in the Wake of the Debacle: Intrametropolitan Patterns of Foreclosed Properties", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Hongwei Dong", "J. Hansz"], "summary": "This study maps the geography of the recent housing crisis within and across American metropolitan areas, and evaluates how it is related to a series of spatial and socioeconomic variables at neighborhood and metropolitan levels. It finds that the spatial patterns of housing recessions vary widely by region. In general, fast-growing metropolitan areas in the Southwest and Florida experienced not only deeper but also longer housing recessions. In contrast, metropolitan areas in the South (except in Florida) saw shallower and shorter housing recessions. Metropolitan areas in the Midwest and Northeast had fewer price declines in the crisis, but their housing recessions tended to be longer. Housing recessions tend to be deeper and longer in larger metropolitan areas. Neighborhoods located closer to city centers experienced shallower and shorter recessions compared with those in fringe areas. Even after controlling for many other variables, automobile dependency is still a strong and positive predictor of housing recession depth and duration. The effects of other urban form variables, such as land-use density and mixed use, are mixed and vary by region. The significance of the effects of neighborhood demographic variables on recession depth is highly dependent on the inclusion of high-risk loan in the model, suggesting that predatory and high-risk lending is one major reason why lower income and minority neighborhoods were hit harder by the recent housing crisis. The effects of high-risk loan and neighborhood demographic variables on housing recession duration, however, are rather weak.", "title": "The Geography of the Recent Housing Crisis: The Role of Urban Form", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Fubing  Su", "Ran  Tao"], "summary": "The burst of housing bubbles in 2008 triggered the worst economic crisis in the United States since the Great Depression. Financial globalization has exacerbated the contagion and a worldwide crisis soon followed. As a major trading country, China depended heavily on export markets in the United States and Europe; therefore its economy experienced serious setbacks. After the double digit growth in 2007, the Chinese real estate market started to take a nose dive. In the first quarter of 2008, the average house price in 70 major cities grew 11 percent, but by the fourth quarter the growth rate slowed down to only 0.5 percent. The first quarter of 2009 even saw a decline of 1.1 percent, the first drop since 2000. The parallel between the United States and China, however, stopped there. While the American housing market continued to be sluggish after a steep decline, the housing market in China rebounded dramatically. After two consecutive quarters of negative growth, house prices rose again in the third quarter of 2009 and double-digit growth reappeared by the first half of 2010. In April 2010, for example, the average house price grew by 12.8 percent, the fastest rise since 2000! The national average actually understated the extent of price hikes in the hottest real estate markets, for example, Shenzhen (18 percent), Hangzhou (17 percent), Wenzhou (22 percent), Haikou (53 percent), and Sanya (52 percent).", "title": "Visible Hand or Crippled Hand: Stimulation and Stabilization in China\u2019s Real Estate Markets, 2008\u20132010", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Desheng Dash Wu", "David L. Olson"], "summary": "The current population of the United States has grown up with what seemed to be a steady and reliable increase in value of homes. Owning one\u2019s own home is one of the signs of a prosperous and successful culture. In the 1930s, many people lost title to their homes during one of the greatest failures of human economic systems known. In response, banks and mortgage lending were regulated, bank deposits insured up to a level covering what most people had, and stock-trading practices ostensibly controlled. True, very old people could remember a time when the price of housing dropped, but with the inevitable passage of life with time, this group grew smaller and smaller, and older and older, and less relevant. There also were anomalies in local areas where, for whatever reason, home prices might negatively fluctuate. Reinhart and Rogoff have described five such anomalies, all associated with banking crises (Spain in 1977, Norway in 1987, Finland and Sweden in 1991, and Japan in 1992).1 But house price decline occurred very rarely, and nobody really noticed.", "title": "The Real Estate Crash of 2008", "claims": null}, {"metadata": {"year": 1994}, "authors": ["M. Pryke", "P. Knox"], "summary": "Nigel Thrift (1990) wrote recently of his dismay at the general lack of any serious \"attention ... paid to matters of money and finance in human geography\" (page 1135). Several years earlier, Michael Ball (1986) noted the way that \"urban development and the production of the built environment in general are regarded frequently as fairly isolated and specialised areas of study\" (page 447). The residential and commercial property market crashes that have taken place on an international scale since the end of the 1980s (BIS, 1992, pages 139-140; 1993, pages 157-158) serve as a timely reminder of the need to bring together 'matters financial' and the production of the built environment, and to locate both more centrally within geography, planning, and economics. The collapse in property markets followed a period of quite amazing escalation in real estate prices that seemed to have little to do with the underlying 'real economy' of most industrialised countries. Between 1980 and 1989, residential asset prices in Sweden and Finland, for example, rose in real terms by 200%; similar residential and commercial asset price rises were experienced in Japan, the United Kingdom, large parts of the USA, Norway, France, and many other countries. The 1980s expansion of credit [perhaps one of the major causal factors in the dramatic escalation of real asset prices on an international scale (BIS, 1993, pages 165-166)], the deregulation of personal and wholesale financial markets that facilitated indebtedness in the context of market-led policies for structural economic change, all of which informed and shaped the booms, have left these countries with severe problems. 'Real estate price deflation' threatens both exposed financial institutions and national macroeconomic policy and \"[G]iven the lack of relevant past experience, it is difficult to assess the potential danger of a vicious circle of declines feeding on declines\" (BIS, 1992, page 7). These declines, however, are not confined to the spaceless world of economic policy, they describe very real geographies which reflect significant structural economic, as well as social, imbalance. The emergence of 'negative equity' and the rise in homelessness are two clear examples. Following the further deregulation of financial markets, the financing of property development and investment now connects domestic residential and commercial property markets to international capital markets and exposes what were relatively sheltered circuits of property finance to the turmoil of 'global' financial flows. Yet, particularly for those involved recently in residential property markets, the ticket 'From Main Street to Wall Street' (Fannie Mae, 1990) may have turned out to be", "title": "Commentaries", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Jeffrey P. Cohen", "Cletus C. Coughlin", "David A. Lopez"], "summary": "This paper summarizes changes in housing prices during the recent U.S. boom and bust from various geographic perspectives. Nationally, the Standard & Poor\u2019s/Case-Shiller house price index more than doubled in nominal terms during the boom and has fallen by roughly a third subsequently. During the boom, housing prices tended to rise much faster in metropolitan areas in the East and West Coast regions than in the country\u2019s interior. After adjusting for inflation, 7 of 19 metropolitan areas have experienced real declines in housing prices from the start of the boom to the present. Although lower-priced houses showed a larger percentage increase during the boom, higher-priced houses fared relatively better over the boom and bust. Changes in land prices, which are not easily measured, appear to have driven housing prices to a greater extent than changes in the prices of housing structures. Internationally, seven countries experienced housing booms and busts; however, these countries tended to have larger booms and smaller absolute busts than the United States.", "title": "The boom and bust of U.S. housing prices from various geographic perspectives", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Francesco  Moscone", "Elisa  Tosetti", "A.  Canepa"], "summary": "This paper investigates spatio-temporal variations in ex-post credit risk in the United States, as a function of real estate prices, loan purchases made by government sponsored enterprises, and a set of local characteristics during the recent housing boom and bust.", "title": "Real estate market and financial stability in US metropolitan areas: A dynamic model with spatial effects", "claims": null}, {"metadata": {"year": 2014}, "authors": ["R. Edelstein", "M. Edelstein"], "summary": "Crashes, Contagion, Cygnus, and Complexities: Global Economic Crises and Real Estate Michael D. Edelstein and Robert H. Edelstein Fisher Center for Real Estate and Urban Economics Haas School of Business, University of California Berkeley June 2014", "title": "Crashes, Contagion, Cygnus, and Complexities: Global Economic Crises and Real Estate", "claims": null}], "query": "what geographical findings emerged from real estate crash of 2007 in united states?", "summary_abstract": "The geographical findings from the real estate crash of 2007 in the United States reveal significant spatial patterns and variations in the impact of the crisis across different regions and types of neighborhoods. Immergluck (2009) highlights that during the peak of the subprime foreclosure crisis, large central cities experienced higher levels of real estate owned (REO) properties compared to suburban areas, challenging the notion that the crisis was predominantly a suburban phenomenon. This study also notes that the suburbanization of REO properties varied significantly across metropolitan areas, with boom-bust regions showing more suburbanization than weaker markets.\n\nDong and Hansz (2016) further elaborate on the geographical disparities, indicating that fast-growing metropolitan areas, particularly in the Southwest and Florida, faced deeper and longer housing recessions. In contrast, neighborhoods closer to city centers generally experienced shallower and shorter recessions compared to those in fringe areas. The study emphasizes that automobile dependency was a strong predictor of recession depth and duration, suggesting that urban form and demographic factors played crucial roles in shaping the crisis's impact.\n\nCohen et al. (2012) provide a broader perspective by analyzing housing price changes across various geographic regions. They found that metropolitan areas on the East and West Coasts experienced faster price increases during the boom, while some areas in the interior faced real declines in housing prices post-crash. This indicates a significant geographical divide in how different regions were affected by the housing market fluctuations.\n\nOverall, the findings collectively illustrate that the real estate crash of 2007 had complex geographical implications, with central cities and certain metropolitan areas experiencing more severe impacts than others, influenced by factors such as urban form, demographic characteristics, and the nature of local housing markets.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2016}, "authors": ["Winchana Srivilaithon"], "summary": "Background and Objective\nIn the emergency department (ED), airway management by emergency physicians is becoming more common. The presented study described emergency intubation indications, methods, operator characteristics, success rates, and adverse event rates.\n\n\nMaterial and Method\nProspective observational study using data collection form was done in the ED of Thammasat\nUniversity Hospital from September 2012 to August 2015. Data were collected by each physician intubator at the time of each intubation.\n\n\nResults\nThe author recorded 1,393 encounters underwent intubation in ED. Intubation was ultimately successful in 99.43%. Cardiac arrest (18.95%) and head injury (7.32%) were the most common indication for intubation in medical encounters and trauma encounters, respectively. The overall success rates on the first attempt were 74.66% (95% confidence interval (CI); 72.37-76.94%). Rapid sequence intubation (RSI) was used in 22.47% of all encounters, had success rates on the first attempt of intubation higher than sedation without paralysis (79.55% vs. 66.09%, risk difference 15.93%, 95% CI for difference [8.64-23.23%]; p<0.01). Senior physicians in emergency medicine had the highest rates of successful intubation on the first attempt (81.94%, 95% CI; 78.84-85.03%). The overall adverse event rates were 8.47%.\n\n\nConclusion\nThe presented study observed high overall intubation success rates in ED. RSI has the highest success rates in the first attempt of intubation. Resident and staff in emergency medicine take major role in airway management. Training in emergency medicine residency programs can improve airway management skill.", "title": "Prospective Observational Study of Emergency Airway Management in Emergency Department.", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Luke M Phillips", "N. Orford", "M. Ragg"], "summary": "The present study aimed to describe the characteristics and outcomes of intubation occurring in the ICU and ED of an Australian tertiary teaching hospital.", "title": "Prospective observational study of emergent endotracheal intubation practice in the intensive care unit and emergency department of an Australian regional tertiary hospital", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Maggie Mechlin", "W. Hurford"], "summary": "Performing emergency endotracheal intubation necessarily means doing so under less than ideal conditions. Rates of first-time success will be lower than endotracheal intubation performed under controlled conditions in the operating room. Some factors associated with improved success are predictable and can be modified to improve outcome. Factors to be discussed include the initial decision to perform endotracheal intubation in out-of-hospital settings, qualifications and training of providers performing intubation, the technique selected for advanced airway management, and the use of sedatives and neuromuscular blocking agents.", "title": "Emergency Tracheal Intubation: Techniques and Outcomes", "claims": null}, {"metadata": {"year": 2015}, "authors": ["L  DeLuca", "T  Durns", "R  Miller", "Z  Roward", "A  Pickering", "J  Yeaton", "KR  Denninghoff"], "summary": "Patients who are intubated during the course of hospital care have a high mortality rate. 70% of critically ill Emergency Department (ED) patients are intubated in the ED or a prehospital, but little data describes mortality risk. Identification of mortality risk based on presenting illness would allow providers to improve both resuscitative efforts and advanced care planning.", "title": "Presenting illness and mortality outcomes for patients intubated in an academic emergency department", "claims": null}, {"metadata": {"year": 2008}, "authors": ["F. Lecky", "D. Bryden", "Roderick W Little", "Nam Tong", "Chris Moulton"], "summary": "BACKGROUND\nEmergency intubation has been widely advocated as a life saving procedure in severe acute illness and injury associated with real or potential compromises to the patient's airway and ventilation. However, some initial data have suggested a lack of observed benefit.\n\n\nOBJECTIVES\nTo determine in acutely ill and injured patients who have real or anticipated problems in maintaining an adequate airway whether emergency endotracheal intubation, as opposed to other airway management techniques, improves the outcome in terms of survival, degree of disability at discharge or length of stay and complications occurring in hospital.\n\n\nSEARCH STRATEGY\nWe searched the Cochrane Injuries Group Specialised Register (December 2006), Cochrane Central Register of Controlled Trials (CENTRAL) (The Cochrane Library 2006, Issue 4), MEDLINE (1950 to November 2006), EMBASE (1980 to week 50, December 2006), National Research Register (Issue 4, 2006), CINAHL (1980 to December 2006), BIDS (to December 2006) and ICNARC (to December 2006). We also examined reference lists of articles for relevant material and contacted experts in the field. Non-English language publications were searched for and examined.\n\n\nSELECTION CRITERIA\nAll randomised (RCTs) or controlled clinical trials involving the emergency use of endotracheal intubation in the injured or acutely ill patient were examined.\n\n\nDATA COLLECTION AND ANALYSIS\nThe full texts of 452 studies were reviewed independently by two authors using a standard form. Where the review authors felt a study may be relevant for inclusion in the final review or disagreed, the authors examined the study and a collective decision was made regarding its inclusion or exclusion from the review. The results were not combined in a meta-analysis due to the heterogeneity of patients, practitioners and alternatives to intubation that were used.\n\n\nMAIN RESULTS\nWe identified three eligible RCTs carried out in urban environments. Two trials involved adults with non-traumatic out-of-hospital cardiac arrest. One of these trials found a non-significant survival disadvantage in patients randomised to receive a physician-operated intubation versus a combi-tube (RR 0.44, 95% CI 0.09 to 1.99). The second trial detected a non-significant survival disadvantage in patients randomised to paramedic intubation versus an oesophageal gastric airway (RR 0.86, 95% CI 0.39 to 1.90). The third included study was a trial of children requiring airway intervention in the prehospital environment. The results indicated no difference in survival (OR 0.82, 95% CI 0.61 to 1.11) or neurologic outcome (OR 0.87, 95% CI 0.62 to 1.22) between paramedic intubation versus bag-valve-mask ventilation and later hospital intubation by emergency physicians; however, only 42% of the children randomised to paramedic endotracheal intubation actually received it.\n\n\nAUTHORS' CONCLUSIONS\nThe efficacy of emergency intubation as currently practised has not been rigorously studied. The skill level of the operator may be key in determining efficacy. In non-traumatic cardiac arrest, it is unlikely that intubation carries the same life saving benefit as early defibrillation and bystander cardiopulmonary resuscitation (CPR). In trauma and paediatric patients, the current evidence base provides no imperative to extend the practice of prehospital intubation in urban systems. It would be ethical and pertinent to initiate a large, high quality randomised trial comparing the efficacy of competently practised emergency intubation with basic bag-valve-mask manoeuvres (BVM) in urban adult out-of-hospital non-traumatic cardiac arrest.", "title": "Emergency intubation for acutely ill and injured patients.", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Dominik  Roth", "Wolfgang  Schreiber", "Peter  Stratil", "Katharina  Pichler", "Christof  Havel", "Moritz  Haugk"], "summary": "BACKGROUND\nAirway management is a key competence in emergency medicine. Patients heavily differ from those in the operating room. They are acutely ill by definition and usually not fasting. Evaluation of risk factors is often impossible. Current literature primarily originates from countries where emergency medicine is an independent specialty. We evaluated intubations in a high-volume emergency department run by internists and comprising its own distinctive intensive care unit.\n\n\nMETHODS\nIn this prospective, noncontrolled, observational study, we continuously documented all intubations performed at the emergency department. We analyzed demographic, medical, and staff-related factors predicting difficulties during intubation using logistic regression models.\n\n\nRESULTS\nFor 73 months, 660 cases were included, 69 (10.5%) of them were without any induction therapy. Two hundred fifty-two (38.2%) patients were female, and their mean age was 59 \u00b1 17 years. Three hundred four (49.9%) had an initial Glasgow Coma Scale of 3. Leading indications were respiratory insufficiency (n = 246; 37.3%), resuscitation (n = 172; 26.1%), and intracranial hemorrhage (n = 75; 11.4%). First attempt was successful in 465 cases (75.1%); alternative airway devices were used in 22 cases (3.3%). Time from the first intubation attempt to a validated airway was 1 minute (interquartile range, 0-2 minutes). Physicians' experience and anatomical risk factors were associated with failure at the first attempt, prolonged intubation, and the need for alternative devices.\n\n\nCONCLUSIONS\nAirway management at the emergency department possesses a high potential of failure. Experience seems to be the key to success.", "title": "Airway management of adult patients without trauma in an ED led by internists.", "claims": null}, {"metadata": {"year": 2020}, "authors": ["S. George", "E. Long", "B. Gelbart", "S. Dalziel", "F. Babl", "A. Schibler"], "summary": "Intubation of children in the emergency setting is a high\u2010risk, low incidence event. Standardisation of clinical practice has been hampered by a lack of high\u2010quality evidence to support one technique over another. The aim of the present study is to determine clinician preference in intubation practice of children in EDs and ICUs in Australia and New Zealand to provide baseline information to allow future targeted research focused on improving the safety and efficacy of paediatric emergency airway management.", "title": "Intubation practices for children in emergency departments and intensive care units across Australia and New Zealand: A survey of medical staff", "claims": null}, {"metadata": {"year": 2015}, "authors": ["A. Raja", "Facep"], "summary": "Emergency department (ED) intubation is constantly evolving, with new devices, techniques, and medications being frequently adopted. To evaluate", "title": "Emergency Department Intubations Are Increasingly Successful", "claims": null}], "query": "Does using novel methods (vs. older methods) of intubation in the emergency department influence the impact of hospital-, provider-, and patient-level characteristics on patient outcomes?", "summary_abstract": "The question of whether novel methods of intubation in the emergency department (ED) influence the impact of various characteristics on patient outcomes is addressed through several studies, each contributing insights into the effectiveness and implications of different intubation techniques.\n\nWinchana Srivilaithon (2016) highlights that rapid sequence intubation (RSI) demonstrates a significantly higher success rate on the first attempt compared to sedation without paralysis (79.55% vs. 66.09%). This suggests that the choice of intubation method can indeed affect outcomes, particularly in terms of initial success rates, which are crucial in emergency settings. The study also emphasizes the role of physician experience, noting that senior emergency medicine physicians achieved the highest success rates.\n\nIn a broader context, F. Lecky et al. (2008) conducted a review of randomized controlled trials and found that the efficacy of emergency intubation has not been rigorously studied, indicating that the skill level of the operator may significantly influence outcomes. They noted that in non-traumatic cardiac arrest scenarios, intubation may not provide the same life-saving benefits as other interventions, such as early defibrillation. This suggests that while novel methods may be beneficial, the context and operator proficiency are critical factors.\n\nMaggie Mechlin and W. Hurford (2014) also point out that the conditions under which emergency intubation is performed are often less than ideal, which can affect success rates. They argue that factors such as the qualifications of the provider and the specific techniques used are modifiable elements that can enhance patient outcomes.\n\nDominik Roth et al. (2013) further support the notion that experience is a key determinant of success in intubation attempts. Their study found that anatomical risk factors and the experience of the physician were associated with the likelihood of failure during intubation, reinforcing the idea that both the method and the provider's skill level are crucial in emergency airway management.\n\nIn summary, the collective findings suggest that while novel intubation methods like RSI may improve initial success rates, the overall impact on patient outcomes is significantly influenced by provider experience and the specific context of the intubation. The studies indicate a need for further research to rigorously evaluate the efficacy of different intubation techniques and their implications for patient outcomes in emergency settings (Winchana Srivilaithon, 2016; F. Lecky et al., 2008; Maggie Mechlin & W. Hurford, 2014; Dominik Roth et al., 2013).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2002}, "authors": ["Carl  Sayles"], "summary": "Transformational change using the basic constructs of the Satir Process Model provides a road map for assisting individuals, couples, and families through a transformational shift in their basic belief or understanding of themselves. The process of change is a way to assist people in making choices that increase self-esteem, provide self-accountability, and move a person toward a more congruent sense of self. The transformation that is experienced is an internal process where the universal yearnings to be loved, acknowledged, and accepted can be met by the individual \u201cself.\u201d", "title": "Transformational Change\u2014Based on the Model of Virginia Satir", "claims": null}, {"metadata": {"year": 2015}, "authors": ["A. Krivtsov", "D. M. Kalimullin"], "summary": "The article views the interaction of three components: power, business and population when organizing business processes in the region. It views an optimal model of interaction which can generate additional possibilities for all the participants. In modern society, with the development of competition, the improvement of technologies external business environment has become more dynamic and changeable. This requires organizations to a more flexible approach to strategic management of the business, periodic adjustments of goals and objectives, organizational development, revision of the degree of centralization of key functions, as well as changes in staff motivation system. According to Dzh. Kotter, most companies and divisions of large corporations have come to the conclusion that they should pursue moderate reorganization, at least once a year, and the root - every four or five years. At the end of the XX century. Within the framework of management science started to allocate an independent field of knowledge-management changes. At the beginning of the new century, the need for a scientific study of the problems of organizational development, change management and change became apparent, and with the onset of the 2008 global financial crisis, improving the performance of organizations is becoming a vital action.", "title": "The Model of Changes Management Information System Construction", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Huang Xiao-han"], "summary": "This paper screens the main factors of the organization change by means of Factor Analysis. The Structural Equation Model is employed to investigate the relationship between the organization change process and the corresponding performance. The hypothesis is proved that the role of internal factors exceed the external ones. It is proposed that the utilization of driven change factors can achieve much more performance than focusing on exclude resistant factors does. And the high-tech new ventures' organization change mode is formed, wich based on organization knowledge and structural innovation.", "title": "Empirical Research on the Organization change Effect Factors and the Organization change Mode:Samples as the High-Tech New Ventures", "claims": null}, {"metadata": {"year": 1998}, "authors": ["K. A. Bubshait", "M. Burney", "I. Nadeem"], "summary": "- Change \uf220 \uf220 \uf220 \uf220\uf2cf\uf26e\uf26d\uf2db\uf29d\uf241\uf220\uf217\uf2ce\uf2ac\uf24e\uf2bb\uf241\uf220\uf2d1\uf269\uf241\uf265\uf2e2\uf220\uf2cf\uf2bc\uf2bf\uf242\uf2b8\uf24d\uf220\uf258\uf267\uf2cc\uf298 \uf220\uf2c6\uf2cc\uf2b7\uf269\uf242\uf272\uf2bf\uf220\uf2d1\uf268\uf24d\uf242\uf26d\uf243 \uf220\uf2d2\uf2cd\uf269\uf241\uf265\uf2e2\uf241\uf220\uf250\uf242\uf2bf\uf2cc\uf2bc\uf2a8\uf29d\uf241\uf220\uf2c1\uf2a4\uf2c3\uf2cb\uf220\uf2d2\uf24a\uf26d\uf242\uf210\uf241\uf220\uf2c1\uf26e\uf2b3 \uf220\uf2c6\uf265\uf242\uf2a8\uf29d\uf241\uf2cb\uf220\uf2be\uf2cb\uf216\uf24a\uf2bc\uf2bb\uf220\uf266\uf2c8\uf2af\uf220\uf2b9\uf2bc\uf29d\uf241\uf220\uf2d2\uf2a8\uf2bf\uf242\uf255 \uf220\uf2d2\uf2cd\uf265\uf2cc\uf2a8\uf26e\uf2bb\uf241\uf220\uf2d2\uf2ce\uf249\uf26a\uf2a8\uf2bb\uf241\uf220\uf2d2\uf2b8\uf2bc\uf2c0\uf29d\uf241\uf220\uf22d\uf220\uf2c6\uf241\uf26a\uf2c8\uf2a4\uf2bb\uf241 \uf220\uf2d3\uf25f\uf2bc\uf2a7\uf220\uf26a\uf25f\uf251\uf2db\uf24d\uf220\uf242\uf2c8\uf25f\uf2c3\uf243\uf220\uf267\uf247\uf220\uf270\uf241\uf216\uf25f\uf259\uf241\uf220\uf295\uf247\uf220\uf258\uf242\uf25f\uf24e\uf284\uf220\uf217\uf25f\uf25f\uf2ce\uf2ac\uf24e\uf2bb\uf241\uf220\uf2d1\uf269\uf241\uf265\uf247\uf220\uf2c6\uf248\uf2af\uf220\uf22c\uf241\uf268\uf2bb\uf220\uf22e\uf250\uf242\uf26e\uf26d\uf2db\uf29d\uf241\uf220\uf2b9\uf2bc\uf24d\uf220\uf258\uf269\uf242\uf261\uf2cb\uf220\uf2bd\uf261\uf241\uf265 \uf220\uf242\uf25f\uf2bf\uf247\uf220\uf265\uf241\uf26a\uf25f\uf2af\uf2de\uf241\uf220\uf2c6\uf243\uf220\uf2d1\uf26a\uf25f\uf2af\uf2cc\uf24e\uf29d\uf241\uf220\uf258\uf267\uf242\uf25f\uf25f\uf2c0\uf2c4\uf2bb\uf241\uf220\uf27e\uf216\uf2b0\uf24d\uf2cb\uf220\uf22e\uf2c1\uf2c8\uf2c4\uf2ce\uf249\uf220\uf242\uf2c0\uf2ce\uf2af\uf220\uf2c1\uf2c8\uf24d\uf2dd\uf2a7\uf242\uf2b0\uf24d\uf2cb\uf220\uf2c1\uf2c8\uf24d\uf242\uf2f1\uf2bc\uf26d\uf220\uf2cb\uf220\uf265\uf241\uf26a\uf2af\uf2de\uf241\uf220\uf2b1\uf2d6\uf242\uf2a3\uf2cb \uf220\uf22c\uf2c9\uf2ce\uf2bc\uf2a7\uf2cb\uf220\uf22e\uf242\uf2c8\uf24d\uf241\uf267\uf220\uf217\uf2ce\uf2ac\uf24e\uf2bb\uf241\uf220\uf2d2\uf2ce\uf2bc\uf2c0\uf2a7\uf220\uf2d3\uf2bc\uf2a7\uf220\uf250\uf241\uf217\uf251\uf244\uf24d\uf220\uf2c6\uf2cc\uf26d\uf269\uf242\uf299\uf220\uf266\uf2b3\uf220\uf265\uf241\uf26a\uf2af\uf2de\uf241\uf220\uf2c6\uf243\uf220\uf2dc\uf247\uf220\uf22e\uf217\uf2ce\uf2ac\uf24e\uf2bb\uf241\uf220\uf2c6\uf2cc\uf2bf\uf2cb\uf242\uf2b4\uf2cd\uf220\uf2cb\uf243\uf220\uf2c6\uf2cc\uf2bc\uf24a\uf2b4\uf2cd \uf220\uf2b1\uf25f\uf2cd\uf26a\uf284\uf220\uf2d3\uf25f\uf2bc\uf2a7\uf220\uf2c6\uf2cc\uf25f\uf2bc\uf2c0\uf2a8\uf2cd\uf220\uf266\uf25f\uf2b3\uf220\uf242\uf25f\uf25f\uf298\uf247\uf220\uf22c\uf24b\uf26e\uf25a\uf2af\uf220\uf217\uf2ce\uf2ac\uf24e\uf2bb\uf241\uf220\uf2c6\uf2cc\uf2bf\uf2cb\uf242\uf2b4\uf2cd\uf220\uf2cb\uf243\uf220\uf2c6\uf2cc\uf2bc\uf24a\uf2b4\uf2cd\uf220\uf2dc\uf220\uf2d2\uf26e\uf26d\uf2db\uf29d\uf241\uf220\uf29f\uf220\uf265\uf241\uf26a\uf2af\uf2de\uf241\uf220\uf2c6\uf248\uf2af \uf220\uf26a\uf252\uf25f\uf25f\uf2b7\uf243\uf220\uf2cf\uf2bc\uf2bf\uf242\uf2b8\uf24d\uf220\uf258\uf267\uf2cc\uf298\uf220\uf2d2\uf272\uf2b3\uf242\uf2c4\uf2bf\uf2cb\uf220\uf26a\uf2cd\uf2cc\uf2f1\uf24d\uf220\uf203\uf220\uf22c\uf2a9\uf2b3\uf241\uf2cc\uf2bb\uf241\uf220\uf241\uf268\uf2c8\uf249\uf220\uf2f5\uf242\uf2c0\uf2ce\uf2bc\uf26e\uf24e\uf2af\uf220\uf22e\uf2d1\uf269\uf241\uf265\uf2e2\uf241\uf220\uf2bd\uf24a\uf2b3\uf220\uf2c5\uf2bf\uf220\uf2d2\uf2af\uf266\uf2c8\uf24e\uf26e\uf29d\uf241\uf220\uf257\uf2d6\uf242\uf24e\uf2c4\uf2bb\uf241 \uf220\uf217\uf2ce\uf2ac\uf24d\uf220\uf2d0\uf243\uf220\uf254\uf241\uf266\uf259\uf247\uf220\uf2c6\uf248\uf2af\uf220\uf22c\uf258\uf267\uf2cc\uf2c0\uf2c4\uf2bb\uf241\uf220\uf241\uf268\uf21a\uf220\uf2f5\uf242\uf2b4\uf24a\uf27f\uf2cb\uf220\uf22e\uf2cf\uf26e\uf26d\uf2db\uf29d\uf241\uf220\uf217\uf2ce\uf2ac\uf24e\uf2bb\uf241\uf220\uf2d2\uf2ce\uf2bc\uf2c0\uf2a7\uf220\uf2c1\uf2c8\uf2af\uf220\uf2d1\uf265\uf242\uf2cd\uf26c\uf2bb\uf220\uf2d2\uf2ce\uf2bc\uf2a7\uf242\uf2af\uf220\uf2cb\uf220\uf2d2\uf2ce\uf2a8\uf2b3\uf241\uf2cb \uf220\uf254\uf241\uf266\uf25f\uf259\uf2e2\uf220\uf2d2\uf2bc\uf2bf\uf242\uf25f\uf271\uf220\uf2bd\uf25f\uf2c0\uf2a7\uf220\uf2d2\uf25f\uf2f1\uf261\uf220\uf2a9\uf25f\uf279\uf2cb\uf220\uf2c1\uf25f\uf251\uf220\uf22c\uf242\uf2c8\uf25f\uf25f\uf2c4\uf2bf\uf220\uf2d5\uf26c\uf256\uf2b7\uf220\uf242\uf2c7\uf265\uf241\uf26a\uf2af\uf244\uf249\uf220\uf2d2\uf26e\uf26d\uf2db\uf29d\uf241\uf220\uf2d2\uf286\uf242\uf2a8\uf2bf\uf220\uf24b\uf2bc\uf2f1\uf24e\uf2cd\uf220\uf2a9\uf255\uf242\uf2c3 \uf220\uf220\uf22e\uf220\uf2d1\uf265\uf2cc\uf272\uf2c4\uf29d\uf241\uf220\uf257\uf2d6\uf242\uf24e\uf2c4\uf2bb\uf241\uf220\uf2bd\uf2ce\uf2c3\uf220\uf2d3\uf24e\uf259\uf220\uf2d1\uf269\uf26a\uf2b8\uf24e\uf29d\uf241\uf220\uf250\uf2dd\uf261\uf266\uf24e\uf2bc\uf2bb\uf220\uf265\uf241\uf266\uf2a8\uf24e\uf26d\uf2dc\uf241\uf2cb\uf220\uf22c\uf217\uf2ce\uf2ac\uf24e\uf2bb\uf241", "title": "An Integrated Model for Managing Organizational Change", "claims": null}, {"metadata": {"year": 2003}, "authors": ["Yu Ming", "D. O\u2019Sullivan", "K. Cormican", "L. Dooley"], "summary": "Organisations strive to increase profits through better market share and lower operating costs whilst maintaining other performance goals such as high quality and low lead-times. This often requires a company to conduct its operations differently to the way it currently operates. A company must be willing to change and a key factor in achieving its desired goals is effectively managing that change. The development and execution of a comprehensive change management program is crucial for achieving business goals and the integration of many change initiatives characterises successful change management. In order to achieve this integration, many aspects of an organisation, many concerns regarding performance measures, and many types of change requirements must be considered holistically. These considerations imply a complex research domain. This paper presents a study of methods and supporting tools for facilitating the development of an effective change management plan in industrial enterprises. The study is based on a set of theoretical models about how change occurs in organisations and organises key principles into an integrated model. The model is called enterprise systems development (ESD). ESD facilitates the process of deploying change within an organisation by identifying the parameters around three core business elements \u00e2\u20ac\u201c the change process, performance measurement, and enterprise objects. It presents a systematic performance-based approach to business transformation and offers an innovative and adaptive change management style. ESD collects many change activities currently presented under different banners into one single management schema. It has been validated in a number of large enterprises during the development phase of their change programs. It has proven to be a critical first step in helping managers understand where they are now and where they need to change in the future.", "title": "Enterprise systems development model", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Irsyad Ilyas", "M. Hubeis"], "summary": "This research aims to formulate strategic change management in SBU Metrasat Indonesia, by identifying internal and external driving factors of the company, developing new business strategies, and formulating corporate change programs to achieve SBU Metrasat\u2019s competitive advantages. The research approach was through a case study with descriptive methods, and qualitative and quantitative analysis. The results of the analysis show that the type of changes are categorized into transition. The company establishes three priority strategic change managements, which shouldbe made in nine elements of business model canvas. However, to achieve the innovative value, SBU Metrasat should focus on elements of value proposition, key resources, key activities and key partners. The McKinsey 7S analysis found that structure and skill are two elements which become barriers to change. The strategic change management plan within the next five years at SBU Metrasat would execute only 6 of the 8-step change management model proposed by Kotter (1996).", "title": "Business inovation on discontinuity of satellite legacy business in Indonesia: A lesson from SBU Metrasat change management", "claims": null}, {"metadata": {"year": 2017}, "authors": ["M. Mustafa"], "summary": "Top management plays a pivotal role in adding value to the company by using smartly designed strategies, which can help strengthening the company\u2019s position. In this report main emphasis is placed on leadership, innovation and change practices in Shiraz Industries private limited. This report identifies the problematic areas and suggests workable and doable solutions to solve the problems. To analyze and understand the company\u2019s current position SWOT analysis is undertaken to identify the strengths and weaknesses internally of the company, and opportunities and threats externally of the company. Two models of innovation and change are used to propose and implement organizational change through adoption of the innovation and change management practices and lead a new initiative. The company top management has a bureaucratic style of doing the things in the company. The company has not proper structure and hierarchy especially no formal marketing department. By using the innovation Genome Framework, which gives the top management the clear understanding that they should give employees more freedom and the team, act should be more autonomous. And within the organization they should create sense of cohesion amongst the employees. Top management should look for measures of improvement not just the task accomplishment and goals. The Pentathlon Framework model of innovation is used to see the company innovation strategy and give the idea, prioritization and the implementation of complete process. The role of innovation, services and business process must be communicated to the workforce by the top management. Managers need to look at the things with and operate from a broader perspective especially when generating ideas, improving processes, products and services. Customers demand and need should be focused on and given priority. Goals can be achieved faster by effective cross-functional development testing and prototyping. Kotter\u2019s change model and its implication can cater for the company\u2019s sense of urgency, and, top management should take the pulse of their company employees. True leaders and key stakeholders in the company are to be identified for change. Create a powerful vision and to communicate amongst the masses, and people anxieties and concerns to be properly addressed. Top management should plan and create the short-term wins. McKinsey 7s models of change for what should be the company strategy and how the structure and system should be for the proper check and balance and exact job description for the work place. The shared values, style, staff and skilled and focused to achieve the desired goals. If the company applies all the above models strategy and implement, it is hopes that the company will definitely improve its performance.", "title": "Leadership Innovation and Implement Organizational Change and Lead a New Initiative through Adoption of the Innovation and Change Management Practices for Shiraz Industries Private Limited Company: A Survey from Pakistan", "claims": null}, {"metadata": {"year": 2012}, "authors": ["A. Gilley", "J. Thompson", "J. Gilley"], "summary": "IntroductionAn ever-changing market landscape driven by globalization, government regulations and deregulation, and economic uncertainty is pushing companies to be agile and responsive. Consequently, \"change or die\" is the reality facing many firms. However, the inability to cope with change driven by external market forces or internal mandates has been a factor in the demise of many organizations (Dutschman, 2007). The list of failed companies is almost endless and is not industry biased (e.g., Enron, HealthSouth, Wachovia, Nationwide Airlines, Olan Mills, Lehman Brothers, Bethlehem Steel, Napster, etc.), although banks and airlines have seemingly been hit the hardest. Despite the need to adapt and evolve, attempts to implement poorly designed change often fail or make the situation worse (Mourier & Smith, 2001; Muehrcke, 1999, Schaffer, 2010). This study seeks to better understand leaders' success rates with change and, specifically, how they can improve.Theoretical BackgroundChange in the workplace has been a frequent topic of research for decades (Beer, Eisenstat, & Spector, 1990; Kotier & Schlesinger, 1979; Walinga, 2008; Zell, 2003). Early change models involved diagnosing and preparing an organization for change, engaging in change, and anchoring new ways into the culture (Beer et al, 1990; Kanter, 1983; Lewin, 1951). Recent models have expanded prior research via more extensive, multi-step frameworks that include leadership, employee involvement and commitment, mentoring, rewards, and more (Kotier, 1996; Ulrich, 1998; Gilley, 2005).Janssen (1996) was one of the first to look at the psychological impact of change in his Four Room Model of Change, which he explained as comfort, denial, confusion, and renewal. Rogers (2003) later defined a five stage adoption process of change, which included awareness, interest in the change, trial, the decision to continue or quit, and adoption of the change into one's lifestyle. Walinga (2008) agreed with the emphasis on the human reaction and argued that the process of preparing individuals for change requires examination of the physical, psychological, and emotional factors one faces when confronted with any change.Much research has examined the organization-level rather than the individual level of change (Vakola, Tsaousis, & Nikolaou, 2003). Individual change, however, is the antecedent of organizational change, and failure to recognize employees' psychological processes and reactions to change may doom the initiative. Research conducted by LaClair and Rao (A McKinsey Report, 2002) and Prosci (Best Practices in Change Management Report, 2007) has demonstrated a direct correlation between achieving the business objectives of a change and effectively managing the people side ofthat change. Hence, the importance of the leader's role as a change agent is clear. Cameron (2008) emphasized the importance of the leader in bringing about positive change and remarked, \"Many things must work in harmony for success to occur in most living systems, but failure can be singular\" (p. 16).Whelan-Berry, Gordon, and Hinings (2003) agreed that organizational change does not occur without change on the part of individuals and specific groups. Not surprisingly, one of the most common explanations for organizational failure to effect change involves human resistance; those not ready for change will resist (Prochaska, Redding, & Evers, 1997). Resistance to change has been blamed on fear of the unknown, one's predisposition toward change, disruption of routine, conflict with current culture, breaking routines, fear of failure, lack of reward for change, lack of commitment, loss of identity, status, control, power, or security, and more (Clark, Gioia, Ketchen, & Thomas, 2010; Marquis & Lounsbury, 2007; Meyer & Hammerschmid, 2006; Trader-Leigh, 2002).Prior investigations suggest that many factors lessen resistance to change, including effective leadership, employee involvement, autonomy, appropriate communications, and motivation by management, to name a few (Denning, 2005; Gill, 2003; Hornung & Rousseau, 2007; Sims, 2002). \u2026", "title": "Leaders and Change: Attend to the Uniqueness of Individuals", "claims": null}], "query": "what is the Satir Change Model and its importance for organizational change", "summary_abstract": "The Satir Change Model is a framework that emphasizes the psychological and emotional aspects of change, particularly in organizational contexts. It is rooted in the understanding that change is not merely a structural or procedural adjustment but also a profound internal transformation for individuals involved. Sayles (2002) highlights that the Satir Process Model serves as a roadmap for facilitating transformational shifts in individuals' beliefs and self-perception, which is crucial for fostering self-esteem and accountability during organizational change.\n\nThe importance of the Satir Change Model in organizational change is underscored by its focus on the human element of change management. A. Gilley et al. (2012) emphasize that successful organizational change hinges on effectively managing the people side of change, which aligns with the principles of the Satir model. They argue that understanding individual psychological processes is essential, as resistance to change often stems from fear and uncertainty. This perspective is echoed by Huang (2005), who notes that internal factors play a more significant role in the change process than external ones, suggesting that addressing the emotional and psychological needs of employees can lead to more successful outcomes.\n\nMoreover, the Satir Change Model's emphasis on congruence and self-acceptance can help organizations navigate the complexities of change. Yu et al. (2003) discuss the necessity of a comprehensive change management program that integrates various aspects of an organization, which resonates with the holistic approach advocated by the Satir model. By fostering an environment where employees feel acknowledged and accepted, organizations can mitigate resistance and enhance engagement during transitions.\n\nIn summary, the Satir Change Model is vital for organizational change as it provides a framework that prioritizes the emotional and psychological dimensions of change. By focusing on individual transformation and addressing the underlying fears and resistance to change, organizations can create a more supportive environment that facilitates successful transitions (Sayles, 2002; A. Gilley et al., 2012; Huang, 2005; Yu et al., 2003).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2010}, "authors": ["Donatus Chukwuemeka Ojide"], "summary": "This study looks at how rent seeking activity hobbles economic growth by giving us a good understanding of the current economic situation. It enables us to see why the existence of multiple exchange rates in developing countries have negative effects on their economic growth pattern such as misallocation of resources, rent seeking and budget problems. It helps to see how market privilege rent seeking affects the economic growth by restricting competition in the production and or sale of product or service. This research study examines the social cost of monopoly as measured by the welfare effect on the gross national product of the United States, in the early part of twentieth century. Finally, the study examines the role of lobbying activities and how resources are diverted from productive use to rent seeking results.", "title": "Rent Seeking Activity: A Mechanism that Distorts Economic Growth", "claims": null}, {"metadata": {"year": 2015}, "authors": ["S. Patnaik"], "summary": "Firms and industries across the world regularly engage with political and government players in order to obtain economic rents through a variety of elaborate strategic tools. Scholars in political economy and economics refer to this behavior as rent-seeking, while scholars in business and management often call it non-market strategy or corporate political strategy. The usual goal of these rent-seeking efforts is to secure benefits from the government that would ultimately allow the rent-seekers to improve or maintain their economic position. Examples are plentiful in a variety of institutional settings across the world and include classic cases (e.g., agricultural protection). As Mueller (2003) points out, rent-seeking usually imposes welfare losses on society, which can be substantial depending on the type of rent-seeking behavior that takes place, as well as the political system it occurs in. However, despite the prevalence of rent-seeking and the increasingly salient participation of firms in the political process, rent-seeking behavior and its costs to society often do not receive as much public scrutiny when new policies are developed as they should. This can be quite problematic for a country\u2019s political process and economic development, particularly because rent-seeking by its very definition distorts the efficient use of resources.", "title": "Rent-Seeking and Public Policy", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Mark a. Zupan"], "summary": "In studying how rent-seeking diminishes a nation\u2019s well-being, the focus typically is on the demand side of the policymaking process: crony capitalists, economic elites, labor unions, industrial cartels, consumer activists, and/or environmentalists seeking favorable policies to promote their own economic rents at the expense of the general public. Precious little attention, however, is devoted to a potentially far more dangerous and pervasive form of political capture by government insiders who operate on the supply side of the policymaking process \u2013 rulers, elected officials, policymakers, and public employees. Government insiders have the motive, means, and opportunity to co-opt the machinery of the state to promote their monetary and ideological interests at the expense of the general citizenry. This malignancy operates akin to how cancer hijacks the body\u2019s own reproductive machinery to grow at its expense.", "title": "Rent-Seeking on the Supply Side of Politics", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Q. Wang"], "summary": "The rent-seeking behavior of enterprises in economic society undermines the fairness and equity of the market order, and has a negative impact on the stable economic development. The study found that top management team has the motivation to manipulate earnings because of rent-seeking under the background of the improving marketization, which reduces the quality of financial reports. To reduce political costs, rent-seeking enterprises are more likely to reduce earnings after distinguishing the direction of real earning management. Further research found that the real earnings management caused by rent-seeking is more significant in the areas with high degree of marketization, but the motivation of reducing earnings is weakened.", "title": "Rent-seeking, marketization and real earnings management", "claims": null}, {"metadata": {"year": 1994}, "authors": ["F. Sturzenegger", "M. Tommasi"], "summary": "Poor economic growth in many countries can be explained by the misallocation of entrepreneurial resources to activities that do not foster growth. This paper deals with the relation between the distribution of political power, the allocation of entrepreneurial resources, and growth. The authors model growth as deriving from Schumpeterian entrepreneurs who try to increase profits through innovation. They endogenize the choice of time devoted to this activity vis-a-vis other ways of increasing income, such as obtaining government subsidies. More unequal access to the political redistribution mechanism makes rent-seeking activities less profitable and results in higher growth. Copyright 1994 by Oxford University Press.", "title": "The Distribution of Political Power, the Costs of Rent-Seeking, and Economic Growth", "claims": null}, {"metadata": {"year": 1995}, "authors": ["Huizhong  Zhou"], "summary": "This paper emphasizes that political behavior of interest groups is a result of economic calculation, and therefore is affected by the market conditions under which they operate. We develop a two-stage game to link political and market decision-making. We find that if unproductive rent-seekingdirectly contributes to rent-seekers' market operations, then their lobbying efforts will be excessive if the number of outsiders is relatively large, restrained if it is relatively small. If rent-seekingdirectly impairs rent-seekers' market operations, the above described behavior will be reversed. The analysis also reveals that as wasteful rent-seeking may increase rent-seeker's production cost, market competition shifts production from now less efficient rent-seekers to their non-rent-seeking rivals. Welfare gains from this shift may overshadow the direct waste of influence activities.", "title": "Rent seeking and market competition", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Daniel Brou", "M. Ruta"], "summary": "We construct a model where firms compete in both political and economic markets. In political markets, firms compete for influence over government transfer policy (rents). This activity can be beneficial for the firm, but is purely wasteful from the point of view of society because resources are utilized to achieve a redistribution of income. In the economic market, firms compete for market share through cost reducing technological innovation. Market structure plays an important role in this economy because competition drives firms to invest more in innovation resulting in higher growth. Rent-seeking affects economic growth in two important ways. It diverts resources away from innovation and it affects the number of firms that are supported in equilibrium. The former has a negative effect on growth while the latter effect is ambiguous, depending on whether rent seeking induces entry or exit. This market structure effect depends on a combination of political and economic factors that the theory highlights.", "title": "Rent\u2010Seeking, Market Structure, and Growth", "claims": null}, {"metadata": {"year": 2020}, "authors": ["F. del R\u00edo"], "summary": "I develop a neoclassical growth model in which rent seeking impacts negatively on productivity and welfare because it discourages the accumulation of social infrastructure. I estimates the fraction of resources wasted in rent seeking for a sample of 149 countries. On average, countries in the first (resp. last) decile of the distribution of the estimated rent-seeking intensity devote, on average, around 25% (resp. 38%) of output to rent-seeking and have 88% (resp. 80%) of output per capita and 58% (resp. 38%) of consumption per capita of a country without rent seeking. Moreover, around one quarter of the drop in consumption per capita caused by rent seeking is due to its negative impact on the accumulation of social infrastructure and the remaining three quarters are due to the waste of resources.", "title": "The Impact of Rent Seeking on Social Infrastructure and Productivity", "claims": null}], "query": "Rent-seeking behaviour, where companies seek preferential treatment, further diverts resources to less productive uses. The cumulative impact of these distortions significantly reduces overall productivity. Government intervention and ownership contribute to resource allocation distortions by favouring politically connected firms and sectors over more productive ones (Zhang 2021). Consequently, resources tend to flow towards less efficient state-owned enterprises and politically motivated projects, hindering productivity and hindering economic growth (Nicola 2020).", "summary_abstract": "The research question regarding the impact of rent-seeking behavior on productivity and economic growth is addressed by several studies that collectively highlight the detrimental effects of such practices on resource allocation and overall economic performance.\n\nOjide (2010) emphasizes that rent-seeking activities hinder economic growth by misallocating resources and creating budgetary issues, particularly in developing countries with multiple exchange rates. This misallocation restricts competition and diverts resources from productive uses to rent-seeking endeavors. Similarly, Patnaik (2015) discusses how firms engage in rent-seeking to secure government benefits, which leads to substantial welfare losses and distorts the efficient use of resources, ultimately impeding economic development.\n\nZupan (2015) adds another dimension by focusing on the role of government insiders in the rent-seeking process, suggesting that their actions can exacerbate the problem by prioritizing their interests over those of the general public. This highlights a systemic issue where both firms and government officials contribute to resource misallocation.\n\nWang (2019) further illustrates the negative consequences of rent-seeking on market fairness and economic stability, noting that it can lead to manipulated financial reporting and reduced earnings quality, particularly in highly marketized environments. This manipulation detracts from genuine economic activity and growth.\n\nSturzenegger and Tommasi (1994) argue that the misallocation of entrepreneurial resources to rent-seeking activities rather than innovation stifles growth. They suggest that a more equitable political power distribution can reduce the profitability of rent-seeking, thereby fostering higher growth rates.\n\nBrou and Ruta (2013) construct a model showing that competition in political markets for government rents diverts resources away from innovation, negatively impacting economic growth. They note that while rent-seeking can influence the number of firms in the market, its overall effect tends to be detrimental.\n\nFinally, del R\u00edo (2020) quantifies the impact of rent-seeking on productivity and welfare, revealing that a significant portion of output is wasted on rent-seeking activities, which discourages the accumulation of social infrastructure. This waste contributes to lower consumption and output per capita in countries with high rent-seeking intensity.\n\nIn summary, the collective findings from these studies underscore that rent-seeking behavior not only diverts resources from productive uses but also creates systemic inefficiencies that significantly hinder economic growth and productivity (Ojide 2010; Patnaik 2015; Zupan 2015; Wang 2019; Sturzenegger & Tommasi 1994; Brou & Ruta 2013; del R\u00edo 2020).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2013}, "authors": ["H\u00f4pital Donka", "Chu de Conakry"], "summary": "1 , A. Kake 1 , A. Bah 1 , T.-O. Barry 2 , Summary In Africa, diabetes care structures are concentrated in the urban areas mostly. The concept of network health cares increasingly developed in the management of chronic diseases in Occidental countries, remains still little known in Africa. In Guinea, in the context of diabetes care decentralization, a first diabetes care network was developed in the Futa Jallon area, located 450 km from the capital. In this paper, we describe this network specificity and report the results after a period of 9 years of practice. The commitment of the diabetics' regional association was a key factor in the establishment of the network care. The number of annual consultations increased from 101 (2002) to 2938 (2011). The in-hospital death rate decreased from 15% (2002) to 3% (2011). Despite these very significant progresses, many challenges remain.", "title": "Acc\u00e8s aux soins de diab\u00e9tologie en Afrique sub-saharienne : bilan d'un r\u00e9seau diab\u00e8te en Guin\u00e9e Access to diabetes care in sub-Saharan Africa: Results of a diabetes health network in Guinea", "claims": null}, {"metadata": {"year": 2017}, "authors": ["J. Nielsen", "S. Bahendeka", "I. Bygbjerg", "D. Meyrowitsch", "S. Whyte"], "summary": "ABSTRACT Non-communicable diseases including type 2 diabetes (T2D) are increasing rapidly in most Sub-Saharan African (SSA) countries like Uganda. Little attention has been given to how patients with T2D try to achieve treatment when the availability of public health care for their disease is limited, as is the case in most SSA countries. In this paper we focus on the landscape of availability of care and the therapeutic journeys of patients within that landscape. Based on fieldwork in south-western Uganda including 10 case studies, we explore the diabetes treatment options in the area and what it takes to access the available treatment. We analyse the resources patients need to use the available treatment options, and demonstrate that the patients\u2019 journeys to access and maintain treatment are facilitated by the knowledge and support of their therapy management groups. Patients access treatment more effectively, if they and their family have money, useful social relations, and knowledge, together with the capacity to communicate with health staff. Patients coming from households with high socio-economic status (SES) are more likely to have all of these resources, while for patients with low or medium SES, lack of economic resources increases the importance of connections within the health system.", "title": "Accessing diabetes care in rural Uganda: Economic and social resources", "claims": null}, {"metadata": {"year": 2018}, "authors": ["D. Beran", "S. Besan\u00e7on"], "summary": "Access to drugs is a key element in the treatment of diabetes. Access remains an issue because of their price, availability, and especially affordability. Two elements included in the sustainable development goals -\u00a0universal health coverage and partnerships\u00a0- offer an opportunity to improve access.", "title": "Access to diabetes care and treatment in Africa\u00a0: challenges and opportunities.", "claims": null}, {"metadata": {"year": 2012}, "authors": ["F. Mahomed"], "summary": "South Africa has very good policies for diabetes care. The challenge is the implementation of these policies so that all patients with diabetes can benefit from them.\nEndocrinologists and physicians play vital roles in the provision of diabetes care, from basic science research to policy development and patient care. Since there are relatively few endocrinologists and physicians in South Africa, the bulk of diabetes care is carried out by the primary healthcare service.", "title": "Diabetes and primary healthcare : review", "claims": null}, {"metadata": {"year": 2003}, "authors": ["J. Mbanya"], "summary": "Diabetes mellitus is now a major health problem in Africa, and patient numbers will increase with the tremendous gain in life expectancy. Africa had a population of nearly 600 million inhabitants in 1998, with an estimated diabetic population of 550\u00a0000 type 1 and 5 million type 2 diabetic patients. With the current trends, it is estimated that the African diabetic population would more than double by the year 2000 and quadruple by the year 2010. Yet the health care system of most countries in the region is geared mostly toward communicable diseases. Diabetes care is integrated in the overall national health care structures, with primary care bearing the main responsibility. Diabetic patients seek care at the primary, district, or tertiary health care systems where there are geographical disparities in the availability, organization, and quality of diabetes care. Delivery of diabetes care is fraught with problems including organizational, medical, socioeconomic, cultural, and geographic factors. Governments, diabetes associations, and other bodies comprising people affected with diabetes, physicians, nurses, nutritionists, and educators should cooperate with one another in improving diabetes care in the region. \n \n \nKeywords: \n \nLeast developed countries; \ndiabetes care; \norganization of care; \nAfrican diabetic patient", "title": "Organization of Care: Problems in Developing Countries\u2014Sub\u2010Saharan Africa", "claims": null}, {"metadata": {"year": 2006}, "authors": ["A. Kengne", "J. Mbanya"], "summary": "lIgBackground:l/Ig The translation into practice of the \nresults of observational studies and clinical \ntrials accumulated in the field of diabetes over \nthe past few decades has resulted in better management \nand improved outcomes for people living with diabetes in \nmost developed countries. How this situation has evolved \nin Africa, where access to basic care is a challenge, is not \nwell known. This review examines the current challenges \nand opportunities for better management of diabetes in \nAfrica. \nlbrglIgMethods:l/Ig A Medline search of published literature on diabetes \nin Africa, complemented by a manual survey of references \nof indexed articles, and consultation of relevant \nexperts and websites was carried out. Data synthesis:l/Ig Of the 1 728 entries obtained from \nMedline, using the key words 'diabetes' and 'Africa', 254 \nentries derived from most African countries had some \napplications to the field of management of diabetes. In \naddition, numerous websites and a number of secondary \nwebsites were visited. \nlbrglIgResults:l/Ig The estimated number of those suffering from \ndiabetes in Africa in 2003 was 35 100 (type 1) and 7 \nmillion (type 2). A 100% increase in the magnitude is \nprojected for the year 2025, and there is a high rate of \nunder-diagnosis. Acute and chronic complications are frequent \nand are associated with a poor prognosis. Overall \naccess to diagnosis and basic care for diabetes in Africa \nis not available to most of those who need it. When and \nwhere available, the cost is regularly a limiting factor. In \naddition, the social context, a misconception about diabetes, \nand parallel medicine create an unfavourable milieu \nfor the optimal management of diabetes in Africa. There \nis ample scope for improvement in the management of \ndiabetes in Africa. \nlbrglIgConclusions:l/Ig Diabetes mellitus presents particular challenges \nin Africa. Opportunities for better management \nexist and require coordinated efforts from all stakeholders.", "title": "Diabetes management in Africa - challenges and opportunities : review", "claims": null}, {"metadata": {"year": 2003}, "authors": ["D. Whiting", "L. Hayes", "N. Unwin"], "summary": ": The aim of this review is to summarize the contextual, clinical and health system challenges to the delivery of health care for diabetes in Africa. Planners need to allocate sufficient resources in a context where resources for health in general are insufficient. Choices need to be made between different options for health care within this context and mechanisms are required to facilitate the implementation of the selected options and ensure that quality of care is maintained.", "title": "Diabetes in Africa. Challenges to health care for diabetes in Africa.", "claims": null}, {"metadata": {"year": 2003}, "authors": ["D. Whiting", "L. Hayes", "N. Unwin"], "summary": "The aim of this review is to summarize the contextual, clinical and health system challenges to the delivery of health care for diabetes in Africa. Planners need to allocate sufficient resources in a context where resources for health in general are insufficient. Choices need to be made between different options for health care within this context and mechanisms are required to facilitate the implementation of the selected options and ensure that quality of care is maintained.", "title": "Challenges to Health Care for Diabetes in Africa", "claims": null}], "query": "access to primary health care for diabetes West Africa", "summary_abstract": "Access to primary health care for diabetes in West Africa presents significant challenges, as highlighted by various studies. A key issue is the concentration of diabetes care facilities in urban areas, which limits access for rural populations. For instance, a diabetes care network established in Guinea demonstrated that decentralization can improve access, with annual consultations increasing dramatically from 101 in 2002 to 2,938 in 2011, and in-hospital death rates decreasing significantly (H\u00f4pital Donka & Chu de Conakry, 2013). \n\nIn Uganda, the therapeutic journeys of patients with type 2 diabetes reveal that access to treatment is heavily influenced by socio-economic status. Patients with higher socio-economic resources are better positioned to navigate the healthcare system, while those from lower socio-economic backgrounds face greater barriers, underscoring the need for supportive networks and resources (Nielsen et al., 2017).\n\nMoreover, the overall healthcare infrastructure in many African countries, including South Africa, is primarily oriented towards communicable diseases, leaving diabetes care under-resourced. Although South Africa has good policies for diabetes management, the implementation of these policies remains a challenge due to a shortage of healthcare professionals, with primary healthcare services bearing the brunt of diabetes care (Mahomed, 2012).\n\nThe literature also indicates that access to essential medications is a critical barrier, with affordability and availability being significant issues (Beran & Besan\u00e7on, 2018). Furthermore, the health system's capacity to provide adequate diabetes care is hampered by a lack of resources, misconceptions about diabetes, and cultural factors (Mbanya, 2003; Kengne & Mbanya, 2006; Whiting et al., 2003).\n\nIn summary, while there are efforts to improve access to diabetes care in West Africa, significant barriers remain, including socio-economic disparities, inadequate healthcare infrastructure, and challenges in medication access. Collaborative efforts among governments, healthcare providers, and community organizations are essential to enhance diabetes management and care in the region.", "summary_extract": null}, {"papers": [{"metadata": {"year": 1992}, "authors": ["S. Piccinni", "T. March\u00ec", "A. Lorusso", "G. Magarotto"], "summary": "A group of 78 crane operators were examined using the \"EPM Research Unit\" method for assessing spine function. Crane operators work in a fixed posture characterized by a flexed position of the cervical spine, isometric load of the extensor cervical muscles and increased dorsal kyphosis. The prevalence of spinal disorders was 39.7% for the cervical tract, 37.2% for the dorsal tract and 38.5% for the lumbar region. The results were compared with those concerning a control group. A statistically significant difference was found in the number of cervical and dorsal complaints (O.R. = 3.33 and 2.69), whereas the difference was nearly significant for the lumbar tract (O.R. = 1.65). The results suggest that this category of workers may be subject to an increased risk for the spine.", "title": "[The prevalence of spondylopathies among the crane operators in the port of Venice].", "claims": null}, {"metadata": {"year": 2022}, "authors": ["M. Machino", "H. Nakashima", "Keigo Ito", "K. Ando", "S. Ito", "F. Kato", "S. Imagama"], "summary": "Magnetic resonance imaging (MRI) system has frequently observed degenerative changes in the cervical discs of healthy subjects. Although there are concerns regarding the link between an individual\u2019s occupation and intervertebral disc degeneration (IDD) in the cervical spine, whether the occupation affects IDD is still not clear. This study aimed to evaluate the occupation and IDD interplay using cervical spine MRI among a cohort of healthy individuals, and to evaluate any association between the type of labor and IDD. Using MRI, we prospectively measured at every level, the anteroposterior (AP) intervertebral disc diameter and disc height, in a cohort of 1211 healthy volunteers (606 (50%) male; mean age, 49.5 years). Using a minimum of 100 male and female each from the third to eighth decades of age (20\u201379 years), IDD was evaluated based on the modified Pfirrmann classification system to derive a disc degeneration score (DDS). We also measured the AP diameters of disc protrusion and of the dural sac as well as the spinal cord. The overall DDS and number of disc protrusions increased with age. Among 11 occupations, there were no significant differences in AP diameter of the dural sac as well as the spinal cord. For the four labor types (heavy object handling, same position maintenance, cervical extension position, and cervical flexion position), there were no significant differences in overall DDS and number of disc protrusions, with or without work. Also, among the four labor types, there were no significant differences in the AP diameter of the dural sac as well as the spinal cord. In this cross-sectional survey of cervical spine MRI data among healthy adult volunteers, occupation and type of labor might have no effect on IDD in the cervical spine.", "title": "Association between Occupation and Cervical Disc Degeneration in 1211 Asymptomatic Subjects", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Jennie A. Jackson", "P. Liv", "J. Wahlstr\u00f6m"], "summary": "Background: The aim of this study was to assess the associations between individual and occupational exposure factors and hospitalization for cervical disc disorder (CDD).Methods: CDD was examined ...", "title": "Occupational risk factors for hospitalization due to cervical disc disorder in a 29-year prospective study of Swedish male construction workers", "claims": null}, {"metadata": {"year": 1989}, "authors": ["L. Krapac"], "summary": "A chronological review of damage to the spine and upper extremities associated with work was carried out in a sample of 120 retired disabled workers. Examination of risk factors for cervicobrachial syndrome (CBS) showed that the forced, bent position of the body at work contributed significantly to the frequency of the disease, particularly in women (70.0% against 38% in controls P less than 0.01). The lifting of heavy loads was also frequently observed in women with CBS (12% compared to 6% in the control group (P less than 0.05). Repeated movements during work were claimed by 52.9% of the males and 80% of the females with CBS and by only 41.4% of the males and 50% of the females without it (P less than 0.01). The heaviest load lifted by subjects with CBS at work exceeded significantly that of control subjects. It is considered that there is a causal link between excessive burden and the occurrence of CBS in women. For early diagnosis of CBS, tiredness, pain in the cervical spine, reduced strength in the hands and poor ability to endure manual work are signs to be looked for. Early recognition of disease, improved working conditions and recreation can help prevent the disease or slow down its progress.", "title": "[Disorders of the cervical spine and the upper extremities and occupations].", "claims": null}, {"metadata": {"year": 2020}, "authors": ["R. A. Wyatt", "Charles C. Lin", "Elizabeth P. Norheim", "Diane Przepiorski", "R. A. Navarro"], "summary": "INTRODUCTION\nMusculoskeletal injury can substantially affect orthopaedic surgeons and productivity. The objective of this study was to assess occupation-related neck pain and cervical radiculopathy/myelopathy among orthopaedic surgeons and to identify the potential risk factors for injury.\n\n\nMETHODS\nAn online survey was sent to orthopaedic surgeons via their state orthopaedic society. The survey consisted of items related to orthopaedic practices, such as the number of arthroscopic procedures done and the use of microscopes/loupes. The prevalence, potential causes, and reporting practices of neck pain and cervical radiculopathy/myelopathy among orthopaedic surgeons were also assessed.\n\n\nRESULTS\nThere were 685 responses from surgeons representing 27 states. A total of 59.3% of respondents reported neck pain and 22.8% reported cervical radiculopathy. After adjusting for age and sex, surgeons performing arthroscopy had an odds ratio of 3.3 (95% confidence interval: 1.4 to 8.3, P = 0.007) for neck pain. Only five of the surgeons with neck pain and one of the surgeons with cervical radiculopathy/myelopathy had ergonomic evaluations.\n\n\nCONCLUSION\nNeck pain and cervical radiculopathy/myelopathy are common among orthopaedic surgeons. Associated factors included older age, higher stress levels, and performing arthroscopy. Cervical injuries are rarely reported, and ergonomic workplace evaluations are infrequent.", "title": "Occupation-related Cervical Spine Disease in Orthopaedic Surgeons.", "claims": null}, {"metadata": {"year": 1996}, "authors": ["Jong Park", "C. Lee", "Young-sook Kim"], "summary": "This study was conducted to evaluate the association of work posture with cervical spine abnormality toward 106 microwave-oven assemblers. A self-administered questionnaire was used to obtain a general characteristics and a work history of workers. The posture of neck and shoulder joint of workers were measured with video tape recording and the simple X-ray of cervical spine was checked-up to observe the X-ray abnormality including degenerative change, curvature abnormality, disc space narrowing or disc canal narrowing. The result were as follows. 1. The prevalence of degenerative change among cervicobrachial disorder workers was 44.3% (47 persons), curvature abnormality was 43.4%(46 persons), disc space narrowing was 21.7%(23 persons), and disc canal narrowing was 21.7%(23 persons). 2. The prevalence of degenerative change at cervical spine was increased with the degree of neck flexion(flexed versus neutral, odds ratio [OR]=2.7), the total work posture of neck(mild or severe awkward versus neutral ; OR=1.2, 3.4). 3. The prevalence of degenerative change at cervical spine was increased with the degree of the right shoulder flexion, the heaviness of the materials carried by the right hand and the degree of awkwardness at the total work posture of right shoulder(p", "title": "The Relationship between Work Posture and Cervical Spine Abnormality among Cervicobrachial Disorder Workers in a Microwave-oven Assembling Factory", "claims": null}, {"metadata": {"year": 1984}, "authors": ["J. Kelsey", "P. Githens", "S. Walter", "W. Southwick", "U. Weil", "T. Holford", "A. Ostfeld", "J. Calogero", "T. O'Connor", "A. White"], "summary": "In this epidemiological study of acute prolapsed cervical intervertebral disc, we found that people in the fourth decade of life were affected somewhat more frequently than individuals in other age groups, and men with a prolapsed cervical disc outnumbered women by a ratio of 1.4 to one. Factors that were associated relatively strongly with this diagnosis were frequent lifting of heavy objects on the job that was held around the time of the onset of symptoms, cigarette-smoking, and frequent diving from a board. Positive associations that were of borderline statistical significance or were not statistically significant were found with operating or driving vibrating equipment and time spent in motor vehicles. Variables that did not appear to affect the risk for a prolapsed cervical disc included participation in certain sports other than diving, frequent wearing of shoes with high heels, the number of pregnancies or live births, frequent twisting of the neck on the job, time spent sitting on the job, and smoking cigars or a pipe.", "title": "An epidemiological study of acute prolapsed cervical intervertebral disc.", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Su-Hun Lee", "D. Son", "Jun-Seok Lee", "S. Sung", "Sang Weon Lee", "G. Song"], "summary": "Objective The \u201cdisc degeneration precedes facet joint osteoarthritis\u201d hypothesis and multidimensional analysis were actively discussed in lumbar spine. However, in cervical spine degeneration, the multifactorial analyzes of disc degeneration (DD), Modic changes (Mcs), facet degeneration, and endplate degeneration (ED) is still limited. In this cross-sectional study, we aimed to analyze the prevalence and interrelationship of cervical DD parameters. Methods We retrospectively recruited 62 patients aged between 60 and 70 years. The disc height, segmental angle, ossified posterior longitudinal ligament (OPLL), ED, facet joint degeneration (FD), uncovertebral joint degeneration (UD), DD, spinal stenosis (SS), Mc, and cord signal change (CS) were evaluated using a previously well-known grading system. Results The prevalence of cervical degenerative parameters were DD (grade 1, 1.2%; grade 2, 13.3%; grade 3, 54.8%; grade 4, 19.0%; grade 5, 11.7%), OPLL (26.2%), SS (grade 0, 7.7%; grade 1, 42.3%; grade 2, 26.2%; grade 3, 23.8%), UD (39.1%), ED (normal, 69.0%; focal defect, 9.7%; corner defect, 11.7%; erosion, 6.9%; sclerosis, 2.8%), and FD (normal, 48.8%; narrowing, 27.0%; hypertrophied, 24.2%). The interrelationship of degenerative parameters showed close relation between UD, SS, DD, OPLL, Mc. ED, and CS has partial relation with degenerative finding. FD only has relation with UD, and Mc. Conclusion Our results may indicate that FD is a degeneration that occurs independently, rather than as a result of other degenerative factors.", "title": "Relationship Between Endplate Defects, Modic Change, Facet Joint Degeneration, and Disc Degeneration of Cervical Spine", "claims": null}], "query": "what is the relation between cervical disc pathology and occupation as a  a crane operator?", "summary_abstract": "The relationship between cervical disc pathology and occupation as a crane operator appears to be significant, particularly due to the specific work-related postures and physical demands associated with this profession. Piccinni et al. (1992) conducted a study on 78 crane operators and found a high prevalence of spinal disorders, with 39.7% specifically reporting cervical issues. The study highlighted that the fixed posture of crane operators, characterized by a flexed cervical spine and isometric loading of the cervical extensor muscles, contributes to this increased risk. The authors noted a statistically significant difference in cervical complaints compared to a control group, suggesting that crane operators are indeed at a heightened risk for cervical spine disorders.\n\nIn contrast, Machino et al. (2022) explored the broader relationship between occupation and intervertebral disc degeneration (IDD) using MRI data from a large cohort of healthy individuals. Their findings indicated that, while there are concerns about occupational impacts on cervical disc health, the study did not find significant differences in disc degeneration scores among various labor types, including those involving heavy lifting or maintaining fixed postures. This suggests that while crane operators may experience cervical issues, the specific link between their occupation and IDD may not be as clear-cut as initially thought.\n\nFurther supporting the notion of occupational risk, Krapac (1989) reviewed spinal damage in retired disabled workers and found that forced, bent positions at work significantly contributed to cervical disorders, particularly in women. This aligns with the findings of Piccinni et al. (1992), reinforcing the idea that specific work postures, such as those adopted by crane operators, can lead to increased cervical pathology.\n\nOverall, the evidence suggests that crane operators are at an increased risk for cervical disc pathology due to the nature of their work, characterized by fixed postures and physical strain. However, the relationship between occupation and cervical disc degeneration may require further investigation to fully understand the nuances involved (Piccinni et al., 1992; Machino et al., 2022; Krapac, 1989).", "summary_extract": null}, {"papers": [{"metadata": {"year": 1993}, "authors": ["J. R. Dygas", "G.  Fafilek", "H.  Durakpasa", "M. W. Breiter"], "summary": "An automated setup which allows impedance measurements on cells with two electrodes over a wide frequency range from 1 mHz to 10 MHz combines two impedance analysers and a temperature regulator under the control of a personal computer. Impedance spectra are measured at a programmed set of constant temperatures between room temperature and 1100 K. Alternatively measurements can be made at constant temperature and at specified time intervals. Data are recorded on disk for subsequent analysis by least-squares fitting on a separate computer. Examples for polycrystalline beta\u2033-alumina and sol-gel materials are given.", "title": "Automated setup for impedance measurements of electrochemical cells with two electrodes", "claims": null}, {"metadata": {"year": 2022}, "authors": ["V. Vivier", "M. Orazem"], "summary": "Interpretation of impedance spectroscopy data requires both a description of the chemistry and physics that govern the system and an assessment of the error structure of the measurement. The approach presented here includes use of graphical methods to guide model development, use of a measurement model analysis to assess the presence of stochastic and bias errors, and a systematic development of interpretation models in terms of the proposed reaction mechanism and physical description. Application to corrosion, batteries, and biological systems is discussed, and emerging trends in interpretation and implementation of impedance spectroscopy are presented.", "title": "Impedance Analysis of Electrochemical Systems.", "claims": null}, {"metadata": {"year": 2004}, "authors": ["B. Boukamp"], "summary": "Abstract Electrochemical impedance spectroscopy (EIS) has become an important tool in Solid State Electrochemistry. Simple Kramers-Kronig transform data validation assures selection of high quality data for subsequent CNLS-fit analysis. Optimized starting parameters and probable equivalent circuit can be obtained through a de-convolution based pre-analysis procedure. Combination of time-domain measurements with frequency-domain analysis extends impedance measurements in to the \u03bcHz range.", "title": "Impedance Spectroscopy, Strength and Limitations (Impedanzspektroskopie, St\u00e4rken und Grenzen)", "claims": null}, {"metadata": {"year": 2016}, "authors": ["R. V. Tambara", "L. O. S. Bulh\u00f5es"], "summary": "This paper presents a low cost instrumentation that is able to perform impedance analysis and the estimation of the electrode-electrolyte parameters through frequency sweep. The proposed system allows identifying the resistances and capacitance of the electrode-electrolyte equivalent model, where only voltage measurements are performed. Due to its flexible design, the system can be easily adapted to different applications and experimental requirements. Results show remarkable fair accuracy, considering the much lower price, smaller sizes and flexibility of use of the developed system.", "title": "Impedance Analysis for Medical and Electrochemical Applications Using a Low Cost Instrumentation", "claims": null}, {"metadata": {"year": 1999}, "authors": ["Ruben H. Milocco"], "summary": "Abstract In this paper, a method to estimate the electrochemical impedance under nonlinear conditions using a recursive least squares algorithm is presented. It is based on the estimation, using experimental data, of the Taylor series expansion coefficients of the generalized input\u2013output discrete-time model description of the nonlinear electrochemical system. Using this approach, higher amplitudes of potential variations can be used to identify the frequency response. With such signals, the output signal/noise power relationship is significantly improved leading to lesser measurement time and errors in the results with respect to the classical small input amplitude linear identification. Examples are given in order to illustrate the procedure.", "title": "Electrochemical impedance spectroscopy using discrete-time model formulation", "claims": null}, {"metadata": {"year": 2002}, "authors": ["Bernardo Ballesteros Katemann", "Albert  Schulte", "Ernesto J. Calvo", "Milena  Koudelka-Hep", "Wolfgang  Schuhmann"], "summary": "A new method for measuring local interfacial impedance properties with high lateral resolution was developed by combination of electrochemical impedance spectroscopy (EIS) with scanning electrochemical microscopy (SECM). Alternating current scanning electrochemical microscopy (AC-SECM) allowed to identify and visualise microscopic domains of different conductivity/electrochemical activities on solid/liquid interfaces immersed into an electrolyte. The performance of the method was illustrated by imaging an array of Pt-band microelectrodes in solutions of low conductivity in the absence of any redox mediator.", "title": "Localised electrochemical impedance spectroscopy with high lateral resolution by means of alternating current scanning electrochemical microscopy", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Z. B. Stoynov", "D. E. Vladikova"], "summary": "Electrochemical impedance spectroscopy is one of the most informative tools for investigating of the complicated processes in electrochemical power sources. The interest arises from the unique advantage of the method's capability to separate and characterize, in a single measurement, the different steps involved in a complex process. Impedance spectroscopy finds a variety of applications for studies of different energy sources \u2013 batteries, fuel cells, supercapacitors, and other systems \u2013 and has different targets \u2013 studies of basic kinetic processes, materials research, quality control, matching, and diagnostics. The principle of the method and a set of useful techniques providing for reliable experiments and data interpretation are described. The selection of the measurement setup configuration and details of the experimental program are discussed. Techniques for correction of the errors due to cable inductance and to possible time evolution of the object are described. The unique method for soft computing \u2013 differential impedance analysis, which corresponds to the nature of power sources \u2013 is also explained. Examples of experimental data \u2013 raw and processed \u2013 illustrate the recommended techniques and their application in different studies.", "title": "MEASUREMENT METHODS | Electrochemical: Impedance Spectroscopy", "claims": null}, {"metadata": {"year": 2004}, "authors": ["P.  Kurzweil", "H.-J.  Fischle"], "summary": "Abstract A variant of ac impedance spectroscopy is applied to monitor and control electrochemical cells and appliances without need for reference values and knowledge of control points in advance. Electrolyzers, fuels cells, energy stores, sensors and electrochemical reactors are steered to an optimum operating state by continuous evaluation of capacitance and the derivatives thereof. Dry and humid electrode\u2013electrolyte interfaces are distinguished with the aid of the low-frequency impedance. The problem is solved in order to determine electrolyte concentrations unambiguously from electrolyte resistance, although the conductivity of the solution has a maximum and changes nonlinearly with the concentration.", "title": "A new monitoring method for electrochemical aggregates by impedance spectroscopy", "claims": null}], "query": "Automated analysis of electrochemical impedance spectroscopy data", "summary_abstract": "The collection of papers provides a comprehensive overview of automated analysis techniques in electrochemical impedance spectroscopy (EIS), highlighting advancements in instrumentation, data interpretation, and application across various fields.\n\nDygas et al. (1993) describe an automated setup that facilitates impedance measurements over a broad frequency range, allowing for systematic data collection at controlled temperatures. This setup enables subsequent analysis through least-squares fitting, demonstrating the potential for automation in EIS data acquisition.\n\nVivier and Orazem (2022) emphasize the importance of integrating chemical and physical descriptions with error assessment in the interpretation of impedance data. Their approach includes graphical methods and measurement model analysis, which enhance the reliability of the results, particularly in applications such as corrosion and battery studies.\n\nBoukamp (2004) discusses the significance of data validation through Kramers-Kronig transforms, which ensures high-quality data for subsequent analysis. The paper also introduces a pre-analysis procedure that optimizes starting parameters for equivalent circuit modeling, further streamlining the analysis process.\n\nTambara and Bulh\u00f5es (2016) present a cost-effective instrumentation system for impedance analysis that simplifies the identification of electrode-electrolyte parameters. Their system's flexibility and accuracy make it suitable for various experimental setups, showcasing the trend towards more accessible EIS technologies.\n\nMilocco (1999) introduces a recursive least squares algorithm for estimating electrochemical impedance under nonlinear conditions, which improves measurement efficiency and accuracy. This method allows for the use of higher amplitude signals, enhancing the signal-to-noise ratio and reducing measurement time.\n\nKatemann et al. (2002) innovate by combining EIS with scanning electrochemical microscopy (SECM) to achieve high lateral resolution in measuring local interfacial impedance properties. This technique enables visualization of conductivity variations at microelectrode arrays, illustrating the potential for detailed spatial analysis in EIS.\n\nStoynov and Vladikova (2009) highlight EIS's versatility in studying complex processes in electrochemical power sources, detailing techniques for error correction and data interpretation. Their work underscores the method's applicability in various energy systems, including batteries and fuel cells.\n\nLastly, Kurzweil and Fischle (2004) present a variant of AC impedance spectroscopy that allows for real-time monitoring and control of electrochemical systems without prior knowledge of reference values. This approach enhances operational efficiency by continuously evaluating system parameters.\n\nCollectively, these papers illustrate the evolution of automated analysis in EIS, showcasing innovations that enhance data quality, interpretation, and application across diverse electrochemical systems.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2002}, "authors": ["M. DeKay", "M. Small", "P. Fischbeck", "R. Farrow", "A. Cullen", "J. Kadane", "L. Lave", "M. Morgan", "K. Takemura"], "summary": "A decision-analytic model for avoiding a risky activity is presented. The model considers the benefit and cost of avoiding the activity, the probability that the activity is unsafe, and scientific tests or studies that could be conducted to revise the probability that the activity is unsafe. For a single decision maker, thresholds are identified for his or her current subjective probability that the activity is unsafe. These thresholds indicate whether the preferred course of action is avoiding the activity without further study, engaging in the activity without further study, or conducting a test or research programme to obtain additional information and following the result. When these thresholds are low, precautionary action is more likely to be warranted. When there are multiple stakeholders, differences in their perceptions of the benefit and cost of avoidance and differences in their perceptions of the accuracy of the additional information provided by the test or research programme combine to create differences in their decision thresholds. Thus, the model allows for the rational expression of differences among parties in a way that highlights disagreements and possible paths to conflict resolution. The model is illustrated with an application to phytosanitary standards in international trade and examined in terms of recent empirical research on lay perceptions of risks, benefits, and trust. Further research is suggested to improve the elicitation of model components, as a way of fostering the legitimate application of risk-based decision analysis in precautionary policy making.", "title": "Risk-based decision analysis in support of precautionary policies", "claims": null}, {"metadata": {"year": 1997}, "authors": ["G. Beroggi", "T. C. Abbas", "J. Stoop", "M. Aebi"], "summary": "The Dutch approach to risk assessment is defined in the National Plan for Environmental Protection Policy (NMP) of 1988. The goal is the present and future protection of humans, animals, plants, the ecosystem, and property through measures designed to provide for a sustainable development. The safety goals are defined in \"Premises for Risk Management,\" an annex to the NMP, in 1989. \nSafety goals are determined with quantitative values. Maximum permitted risks are differentiated from negligible risks. Risks that fall between these two values must be reduced to a negligible level within one generation (ALARA-principle). The safety thresholds for existing and new hazards are also differentiated. \nHazards include stationary installations, transport systems (road, rail, water, and air transport), hazardous substances (chemical, radioactive), and genetically-altered organisms. The two most important criteria for risk assessment are the individual probability of death and the probability of death for groups. Although quantitative safety goals have been defined for other criteria such as ecosystems, noise pollution, and odor nuisance, they have little effect in practice. Criteria which are deliberately left out include injured persons, financial-economic damages, damages to plants by radioactivity, and different reductions in activities.", "title": "Risk assessment in the Netherlands", "claims": null}, {"metadata": {"year": 2012}, "authors": ["R. T. Phillips"], "summary": "The purpose of assessing dangerousness is to determine whether an individual poses a risk of endangering self or others now or in the near future and to identify what interventions are necessary to minimize that risk.", "title": "Predicting the risk of future dangerousness.", "claims": null}, {"metadata": {"year": 2018}, "authors": ["A. P. Bochkovskyi", "N. Yu. Sapozhnikova"], "summary": "In the paper, on the basis of the conducted analysis of the dynamics of the spread of dangers in the context of evolutionary development of society, it is determined that the global problems have become complex, which appears in interdependence of natural, technogenic, socio-political, economic, scientific-technological and other risks. The main source of danger, at the present stage of development of society, is an industrial environment. The analysis of known theoretical studies to appearance of dangerous is carried out and it is defined that none of considered theories can be treated as absolutely correct and generally accepted. First of all the conceptuality of given theories that is, limited practical usage, second of all the insufficient attention and underestimation of the role of the \"human factor\" as a fundamental factor in the process of creating dangers are the meanest drawbacks. Notice, that in the European Countries exist around 100 differents methods of assessment of the risk of occurrence dangers, according them quantitative measurement of the risks is carried out by the three meanest methods: statistical, expert and analog. British Standard BS-8800 (GB), risk assessment based on probability-loss matrix (GB, France, Latvia, the USA, Australia), construction of risk assessment scales (Germany, Finland), the methodology of the National Research Institute of Occupational Safety and Health (NRIOS) in Ukraine, risk score (the USA), risk assessment code (GB), method of verbal functions (European Union), assessment of occupational risks by the Elmer system, risk assessment based on requirements level ranking (OIR index) are the most often used methods for the risks evaluation. On basis of the analysis of the above mentioned methods, two main problems were identified. The first problem that exists in the risks evaluation of occurrence dangerous is the lack of a unique unified methodology. The second problem is the underestimation of the significance of the \"human factor\" in the \"man-machine\" system. It is pointed out that due to underestimating the role of the \"human factor\" as a key component of risk one cannot consider any of the known risk assessment methods to be effective and universally accepted. Based on conducted research, the need for a principally new universal and effective methodology for quantitative risk assessment is identified.", "title": "THE THEORY AND PRACTICE OF RISK ASSESSMENT OF PROFESSIONAL DANGERS", "claims": null}, {"metadata": {"year": 2022}, "authors": ["\u0130brahim C\u00fcnd\u00fcbeyo\u011flu", "Ramazan Kayaba\u015f\u0131"], "summary": "Businesses must make mass production and produce economical products in order to compete in the job market. For this reason, mass production is carried out by using mechanical systems in enterprises. Employees engage in dangerous behaviors or encounter dangerous situations during production processes. This makes it necessary to carry out risk analysis and update it over time. In this study, a risk analysis was made using the Fine-Kinney method in the ceramic factory where 40 workers work. In the analysis, hazards numbers and risk values were determined. According to the sources of danger, it was examined before and after the prevention plan. By comparing the risk numbers and values with each other, the sources with a high-risk average are specified. Document, mechanical and electrical hazard sources are in the first place among all sources of danger in terms of the number and value of hazards. According to the risk assessment outcome scale, the intolerable risk was 2.17%, the high risk was 13.05%, the important risk was 77.18%, and the probable risk was 7.60%. When a precautionary plan is implemented in the enterprise, an improvement of 86.40% is achieved. The risk values, which are high compared to the current situation, decrease to acceptable levels after corrective and preventive actions. For this reason, the risks determined as a result of the risk analysis should be examined by all members of the workplace organization and preventive activities should be carried out in order of priority.", "title": "Seramik Fabrikas\u0131nda Fine-Kinney Y\u00f6ntemi ile Risk De\u011ferlendirmesi", "claims": null}, {"metadata": {"year": 2018}, "authors": ["A. Bochkovskyi", "N. Sapozhnikova"], "summary": "In the paper, on the basis of the conducted analysis of the dynamics of the spread of dangers in the context of evolutionary development of society, it is determined that the global problems have become complex, which appears in interdependence of natural, technogenic, socio-political, economic, scientific-technological and other risks. The main source of danger, at the present stage of development of society, is an industrial environment.The analysis of known theoretical studies to appearance of dangerous is carried out and it is defined that none of considered theories can be treated as absolutely correct and generally accepted. First of all the conceptuality of given theories that is, limited practical usage, second of all the insufficient attention and underestimation of the role of the \"human factor\" as a fundamental factor in the process of creating dangers are the meanest drawbacks. Notice, that in the European Countries exist around 100 differents methods of assessment of the risk of occurrence dangers, according them quantitative measurement of the risks is carried out by the three meanest methods: statistical, expert and analog.British Standard BS-8800 (GB), risk assessment based on probability-loss matrix (GB, France, Latvia, the USA, Australia), construction of risk assessment scales (Germany, Finland), the methodology of the National Research Institute of Occupational Safety and Health (NRIOS) in Ukraine, risk score (the USA), risk assessment code (GB), method of verbal functions (European Union),assessment of occupational risks by the Elmer system, risk assessment based on requirements level ranking (OIR index) are the most often used methods for the risks evaluation. On basis of the analysis of the above mentioned methods, two main problems were identified. The first problem that exists in the risks evaluation of occurrence dangerous is the lack of a unique unified methodology.The second problem is the underestimation of the significance of the \"human factor\" in the \"man-machine\" system. It is pointed out that due to underestimating the role of the \"human factor\" as a key component of risk one cannot consider any of the known risk assessment methods to be effective and universally accepted. Based on conducted research, the need for a principallynew universal and effective methodology for quantitative risk assessment is identified.", "title": "THE THEORY AND PRACTICE OF RISK ASSESSMENT OF PROFESSIONAL DANGERS", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Paride Peritore"], "summary": "Every workplace presents several risks: some of them could have a low likelihood but a high severity, others could happen frequently, and they have a very low impact. Anyhow, all the detected risks found must be monitored independently of their likelihood and severity. The word \u201crisk\u201d, from the French \u201crisque\u201d, in accord with the Oxford English Dictionary, could be defined as \u201cA situation involving exposure to danger\u201d. Although other definitions of risk are also trusty and reliable, like the one from the Occupational Health & Safety Advisory Services (OHSAS) that says: \u201cRisk is a combination of the likelihood of an occurrence of a hazardous event or exposure and the severity of injury or ill health that can be caused by the event or exposure\u201d, the most realistic definition of risk is the one according to the International Organization of Standardization publication ISO 31000:2009, that defines the risk as the \"Effect of uncertainty on objectives\". The directives of ISO 31000 and ISO 31010 explain the users how to manage the risk and how to perform a risk assessment. This work outlined the necessity to assess the risks inside the companies, by using a well-built risk register with several innovative features and tools. A qualitative and quantitative analysis has been conducted in this work for describing two different case studies: the development of a new prototype and a functional procedural analysis. The first case study is related to a request of an Irish power generation company to develop an innovative risk register in order to synthesize their existing risk registers (CAR and GRR) in a single software. The aim of the work is to develop the new software with a user-friendly interface and an investment evaluation module. The second case study of this work analyses the importance of managing operational risks by using a software which uses a logical-phenomenological model.", "title": "Requirements for a web-based tool for operational risk management", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Alfredo Del Fabro Neto", "Bruno Romero de Azevedo", "Rafael Boufleuer", "J. C. D. Lima", "Alencar Machado", "Iara Augustin", "M. Pasin"], "summary": "Some of the activities performed daily by people may harm them physically. The performance of such activities in an inadequate manner or in an adverse environment can increase the risk of accidents. The development of context-aware systems capable of predicting these risks is important for human damage prevention. In this sense, we are developing an approach based on the Activity Theory and the Skill, Rule and Knowledge model for risk prediction of human activities in a context-aware middleware. To predict the risk in the activities, we identify the probability for the next actions and compare the current physiological context with its future state. In order to concept proving the proposed model, we developed a prototype and tested it with a public and a private dataset. The results show that the proposed model can assign an appropriate risk factor to the tested activities.", "title": "Recognition of Human Activities using the User' s Context and the Activity Theory for Risk Prediction", "claims": null}], "query": "what is the most optimal risk threshold to set when assessing the danger of an activity?", "summary_abstract": "The research question regarding the optimal risk threshold for assessing the danger of an activity is addressed through various models and methodologies presented in the literature. \n\nDeKay et al. (2002) propose a decision-analytic model that identifies thresholds based on an individual's subjective probability of an activity being unsafe. This model suggests that lower thresholds warrant precautionary actions, while higher thresholds may lead to engaging in the activity or conducting further research. The model emphasizes the importance of stakeholder perceptions in determining these thresholds, highlighting potential conflicts in risk assessment.\n\nBeroggi et al. (1997) discuss the Dutch approach to risk assessment, which establishes safety goals with quantitative values. They differentiate between maximum permitted risks and negligible risks, advocating for the reduction of intermediate risks to negligible levels within a generation. This structured approach provides a framework for setting risk thresholds based on individual and group probabilities of death, which can inform optimal decision-making.\n\nC\u00fcnd\u00fcbeyo\u011flu and Kayaba\u015f\u0131 (2022) emphasize the necessity of continuous risk analysis in workplaces, particularly in environments where dangerous behaviors may occur. Their findings suggest that risk values should be regularly updated and that a systematic approach to risk assessment can lead to significant improvements in safety outcomes.\n\nBochkovskyi and Sapozhnikova (2018) highlight the complexity of modern risks, noting that a unified methodology for risk assessment is lacking. They identify the \"human factor\" as a critical component often underestimated in existing methods, suggesting that a new, effective methodology is needed to accurately assess risks.\n\nPeritore (2018) reinforces the importance of monitoring all identified risks, regardless of their likelihood or severity, and advocates for a structured risk register to facilitate comprehensive risk management. This aligns with the notion that a well-defined risk threshold can enhance safety measures in various contexts.\n\nIn summary, the literature collectively suggests that optimal risk thresholds should be informed by subjective probabilities, stakeholder perceptions, and systematic methodologies that account for both quantitative and qualitative factors. The integration of human factors and continuous risk assessment practices is crucial for effective risk management across different activities and environments.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2019}, "authors": ["H. Lambert"], "summary": "Dans le cadre de ce congres, je souhaiterais me pencher sur les barrieres que rencontrent les femmes victimes de violences sexuelles pour acceder aux soins de sante a Goma, en Republique Democratique du Congo. Bien que de nombreuses etudes abordent la question des violences sexuelles au Nord-Kivu (Peterman et al., 2011 ; Maertens de Noordhout, 2013 ; Moufflet, 2008), aucune ne s\u2019attarde sur la question de leur acces aux soins. Aussi, c\u2019est en tant que membre d\u2019une ONG universitaire \u0153uvrant dans le domaine de la sante dans cette province que nous avons ete confrontes a cette lacune dans la litterature scientifique. C\u2019est avec l\u2019objectif de mettre en place des actions pour ameliorer l\u2019accessibilite aux soins que nous avons mene une etude qualitative a ce sujet, de fevrier a mai 2018. Nous avons interroge des usagers au sein de la \u00ab population generale \u00bb, des prestataires de soins et des femmes victimes de violences sexuelles via des associations locales. Nous avons egalement observe 6 ateliers de reflexion sur les violences liees au genre au sein d\u2019hopitaux et de centres de sante. Les resultats de notre etude revelent qu\u2019au sein des structures de sante, les professionnels tiennent un discours moralisateur a leurs patientes sous forme de \u00ab conseils \u00bb. De plus, certains soignants culpabilisent les victimes voire mettent en doute leur parole. Enfin, les structures sont rarement amenagees pour garantir la confidentialite et l\u2019importance du secret professionnel ne semble pas toujours etre bien apprehendee.", "title": "Violences sexuelles et acce\u0300s aux soins de sante\u0301 a\u0300 Goma, Re\u0301publique de\u0301mocratique du Congo", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Fran\u00e7oise Duroch"], "summary": "Ce travail se propose d\u2019analyser les processus d\u2019apprentissage de l\u2019organisation humanitaire Medecins Sans Frontieres (MSF) autour de la notion de victimes de violences sexuelles. Une premiere partie est consacree a un essai conceptuel et critique des concepts de victimes de viols, notamment dans le champ des sciences sociales, ainsi qu\u2019a une introduction a l\u2019histoire de la Republique Democratique du Congo (RDC). La deuxieme partie porte sur l\u2019etude qualitative d\u2019une des plus importantes interventions de l\u2019organisation MSF a l\u2019Est de la RDC en faveur des victimes de violences sexuelles. 2695 anamneses medicales de patients recus par l\u2019organisation ont ete etudiees afin de comprendre le phenomene des viols de masse dans cette region, des entretiens semi-directifs ont egalement ete menes avec les volontaires et cadres de l\u2019organisation participant a la mise en \u0153uvre de ce type d\u2019operation. Un dernier volet decrit les elements qui ont conduit MSF a considerer le phenomene des violences sexuelles sur ses terrains d\u2019interventions ainsi que les processus d\u2019appropriation et de resistance de l\u2019organisation vis-a-vis de ces problematiques.", "title": "Violences sexuelles en R\u00e9publique D\u00e9mocratique du Congo : r\u00e9sistances et appropriations institutionnelles par les ONG", "claims": null}, {"metadata": {"year": 2015}, "authors": ["S. Rubuye Mer", "N.  Flicourt"], "summary": "Resume Contexte Le conflit dans l\u2019est de la Republique democratique du Congo (RDC) est le plus meurtrier depuis la Seconde Guerre mondiale. Cela fait dix ans que l\u2019auteure principale travaille au Sud-Kivu. Dans le cadre de ses nombreuses missions, elle a ete confrontee a des situations particulieres de femmes et enfants victimes de violences sexuelles et aux consequences du viol. Les lacunes actuelles dans la fourniture de soins pour les victimes incluent un manque d\u2019infrastructures de soins de sante, un nombre insuffisant de psychotherapeutes, et des defis associes a la reinsertion socio-economique. Methode Cette etude est basee sur le conseil en sante sexuelle et une enquete realisee aupres des femmes victimes de viols dans les milieux ruraux qui se font prendre en charge dans les hopitaux de reference (PANZI) du Sud KIVU. Resultats L\u2019article presente la situation d\u2019ensemble, une etude de trois cas emblematiques et des recommandations pour une amelioration des soins. La sante sexuelle et la reproduction resident dans l\u2019amelioration de la vie en general et ne sont possibles que si les droits humains sont respectes.", "title": "Femmes victimes des violences sexuelles dans les conflits arm\u00e9s en R\u00e9publique D\u00e9mocratique du Congo", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Fran\u00e7oise Duroch"], "summary": "Ce travail se propose d'analyser les processus d'apprentissage de l'organisation humanitaire Medecins Sans Frontieres (MSF) autour de la notion de victimes de violences sexuelles. Une premiere partie est consacree a un essai conceptuel et critique des concepts de victimes de viols, notamment dans le champ des sciences sociales, ainsi qu'a une introduction a l'histoire de la Republique Democratique du Congo (RDC). La deuxieme partie porte sur l'etude qualitative d'une des plus importantes interventions de l'organisation MSF a l\u2019Est de la RDC en faveur des victimes de violences sexuelles. 2695 anamneses medicales de patients recus par l'organisation ont ete etudiees afin de comprendre le phenomene des viols de masse dans cette region, des entretiens semi-directifs ont egalement ete menes avec les volontaires et cadres de l'organisation participant a la mise en \u0153uvre de ce type d'operation. Un dernier volet decrit les elements qui ont conduit MSF a considerer le phenomene des violences sexuelles sur ses terrains d'interventions ainsi que les processus d'appropriation et de resistance de l'organisation vis a vis de ces problematiques. Les dynamiques d'apprentissage organisationnel semblent avoir ete favorisees par un ensemble d'evenements cles : conflits, pandemie du SIDA, environnement institutionnel favorable, scandales mediatiques. Les resistances se construisent autour des representations sociales de la victime et d'enjeux ethiques et techniques. Dans ces processus, devraient apparaitre des formes de mediations, des passeurs de sens qui contribueraient a rendre signifiant la pose d\u2019actes autour de phenomenes quelquefois situes dans les spheres de l'indicible.", "title": "R\u00e9sistances et appropriations institutionnelles des Organisations Non Gouvernementales autour de la notion de victimes de violences sexuelles : le cas de M\u00e9decins Sans Fronti\u00e8res en R\u00e9publique D\u00e9mocratique du Congo", "claims": null}, {"metadata": {"year": 2006}, "authors": ["G. Gansou", "T. Baubet", "Mathieu Tognid\u00e9", "J. E. Houngbe", "R. G. Ahyi"], "summary": "La Republique du Congo a connu ces dernieres annees des guerres successives au cours desquelles des violences sexuelles ont ete observees. De tres nombreuses femmes en ont ete victimes et/ou temoins. L'organisation Medecins Sans Frontieres a mis en place a Brazzaville en mars 2000 une structure de soins medico-psychologique pour venir en aide aux personnes victimes de ces violences. La demande de soins s'en est trouvee facilitee et, de mars 2000 a avril 2001, 243 patientes ont consulte au niveau de cette structure. Nous decrivons la population recue et les troubles presentes, la nature et l'importance des representations culturelles liees au viol et a ses consequences pour ces femmes et leur entourage, ainsi que la necessite de prendre en compte les facteurs culturels dans le diagnostic et dans les soins, ce que nous illustrons par un exemple clinique.", "title": "Violences sexuelles de guerre : Consid\u00e9rations culturelles, cliniques et th\u00e9rapeutiques au congo-brazzaville", "claims": null}, {"metadata": {"year": 2008}, "authors": ["V\u00e9ronique Moufflet"], "summary": "Cet article revisite l\u2019interpretation majoritaire des violences sexuelles de l\u2019est du Congo comme relevant exclusivement du paradigme des \u00ab viols comme arme de guerre \u00bb. Apres une description de ce phenomene et du dispositif d\u2019assistance pour les victimes a Goma, cet article questionne les effets de cette interpretation sur la qualite et l\u2019efficacite de l\u2019aide accordee aux victimes de violences sexuelles dans les Kivus.", "title": "Le paradigme du viol comme arme de guerre \u00e0 l'Est de la R\u00e9publique d\u00e9mocratique du Congo", "claims": null}, {"metadata": {"year": 2017}, "authors": ["K. Matthieu"], "summary": "PRESENTATION DU SUJET D\u2019aucun n\u2019ignore que les violences sexuelles faites aux femmes et surtout les viols d\u2019enfants sont devenus monnaie courante en R\u00e9publique D\u00e9mocratique du Congo rendant ainsi la condition de l\u2019enfant tr\u00e8s vuln\u00e9rable qu\u2019elle ne l\u2019\u00e9tait d\u00e9j\u00e0 par rapport \u00e0 son milieu, son manque de maturit\u00e9 physique et \u00e9motionnelle n\u00e9cessitant une attention particuli\u00e8re pour sa protection. La constitution de la R\u00e9publique D\u00e9mocratique du Congo du 18 f\u00e9vrier 2006 en son article 123, point 16, consacre une place importante aux personnes vuln\u00e9rables, notamment aux enfants entant que renouvellement de l\u2019\u00eatre et de la vie, en s\u2019engageant dans la voie de faire de leur protection son cheval de bataille. Bien que la R\u00e9publique D\u00e9mocratique du Congo dispose des moyens de persuasion pour d\u00e9courager les d\u00e9linquants des violences sexuelles, ceux-ci ne d\u00e9sarment pas pour autant sur tout dans les zones rurales o\u00f9 le taux de ces infractions reste \u00e9lev\u00e9. Cette situation s\u2019explique par le fait que dans ces milieux, les citoyens ignorent non pas seulement leurs droits mais aussi, ils ne sont pas inform\u00e9s ; les organes de r\u00e9pression font dans la plupart de cas d\u00e9faut et lorsqu\u2019ils sont l\u00e0, il faut effectuer une longue distance pour les atteindre. Surtout, certaines coutumes et traditions ne favorisent pas l\u2019\u00e9panouissement des enfants notamment des filles. En d\u00e9pit du fait que le viol soit une infraction qui se commette en cachette et que sa preuve soit tr\u00e8s difficile \u00e0 apporter, souvent quand les victimes se proposent de porter l\u2019affaire en justice, g\u00e9n\u00e9ralement elles le font en retard pendant que les preuves deviennent difficiles \u00e0 reconstituer ou \u00e0 apporter. Aussi, les facteurs psychologiques notamment le traumatisme psychosociale tels que le viol, l\u2019inceste et la molestation sexuelle ont un impacte n\u00e9gatif sur la vie de la victime. L\u2019article 171 de la loi n\u00b0 09/001 du 10 janvier '2009 portant protection de l\u2019enfant r\u00e9pute le viol avec une personne \u00e2g\u00e9e de moins de dix-huit ans, viol \u00e0 l\u2019aide des violences. Il en va sans dire que tout rapport sexuel avec des personnes \u00e2g\u00e9es de moins de dix-huit ans, avec ou sans consentement de celles-ci est r\u00e9put\u00e9 viol avec violences. Pour asseoir la conviction du juge afin d\u2019arracher la condamnation de l\u2019auteur de l\u2019infraction, les parties au proc\u00e8s notamment la partie victime doit prouver la v\u00e9racit\u00e9 des faits all\u00e9gu\u00e9s.il incombera \u00e0 la partie victime d\u2019apporter d\u2019abord par le moyen de droit la preuve de sa minorit\u00e9, puis les faits m\u00eame se rapportant au viol. Dans cette d\u00e9marche, les actes dress\u00e9s tant par l\u2019officier de l\u2019\u00e9tat civil, par le m\u00e9decin que par le psychologue s\u2019av\u00e8rent indispensables pour \u00e9tablir la v\u00e9racit\u00e9 des faits all\u00e9gu\u00e9s. Il sera question pour nous dans le cadre de cette r\u00e9flexion de d\u00e9montrer comment les actes de l\u2019\u00e9tat civil et ceux \u00e9tablis par le m\u00e9decin ainsi que le psychologue sont indispensables pour la r\u00e9pression de viol d\u2019enfant commis dans les zones rurales en R\u00e9publique D\u00e9mocratique du Congo ainsi que pour la prise en charge efficace des victimes.", "title": "La r\u00e9pression de l\u2019infraction de viol d\u2019enfant dans les zones rurales en R\u00e9publique D\u00e9mocratique du Congo", "claims": null}, {"metadata": {"year": 2019}, "authors": ["J. Byakweli", "B-G. Bisharhwa", "K. Muray"], "summary": "L\u2019Est du RD Congo est chamboule par plusieurs conflits armes trop peu mediatises mais, constituant un des drames les plus meurtriers au monde, depuis la deuxieme Guerre mondiale. Les populations les plus vulnerables subissent de lourdes consequences relevant de la penurie totale d'infrastructures de soins de sante et la violence degradante de l\u2019espece humaine. Il va sans dire que les femmes sont les plus prises dans ce tourbillon de violence. Dans le contexte ou l\u2019autorite de l\u2019Etat semble s\u2019etre tue, un nouveau leadership dynamique et ethique, devrait etre pense pour faire emerger et maintenir dans la region un climat pacifique pour chacun et tous. Un tel processus se fonderait sur le succes historique de gestion probante des palabres, adaptable au contexte contemporain. S\u2019epaulant sur des structures neutres et ressources fiables telles, les acteurs credibles de la sante et ceux de la societe civile, ce processus devrait de prime abord rehabiliter les femmes, ces victimes innocentes afin de leur donner la capacite (empowerment) d\u2019apporter leur contribution dans ce nouveau processus. Au moyen de la recherche action participative, une equipe interdisciplinaire, devrait entreprendre la sociometrie et faire emerger ses retombees positives, sous forme d\u2019un sociogramme dynamique, a la place publique faisant ainsi progressivement contre-point aux contre-valeurs destructeurs. Une large diffusion des retombees de ces recherches, incluant l\u2019envahissement de la place publique, devrait etre entreprise sur le plan, local, regional, national et international.", "title": "Sociome\u0301trie au service d\u2019un leadership nouveau pour re\u0301inventer la socie\u0301te\u0301 en Re\u0301publique de\u0301mocratique du Congo", "claims": null}], "query": "\u00ab Comment les personnels soignants travaillant dans les structures sanitaires peuvent-ils mettre en \u0153uvre des strat\u00e9gies pour minimiser la stigmatisation et faciliter l'acc\u00e8s et l'utilisation du kit de prophylaxie post-exposition (KIT-PEP) par les survivantes de violences sexuelles, dans l'Est de la R\u00e9publique D\u00e9mocratique du Congo ? ", "summary_abstract": "Les recherches sur la mani\u00e8re dont les personnels soignants peuvent minimiser la stigmatisation et faciliter l'acc\u00e8s au kit de prophylaxie post-exposition (KIT-PEP) pour les survivantes de violences sexuelles en R\u00e9publique D\u00e9mocratique du Congo (RDC) mettent en lumi\u00e8re plusieurs d\u00e9fis et strat\u00e9gies potentielles.\n\nLambert (2019) souligne que les professionnels de sant\u00e9 \u00e0 Goma adoptent souvent un discours moralisateur envers les victimes, ce qui peut exacerber la stigmatisation. Les soignants culpabilisent parfois les victimes et remettent en question leur cr\u00e9dibilit\u00e9, ce qui nuit \u00e0 leur acc\u00e8s aux soins. De plus, les infrastructures de sant\u00e9 ne garantissent pas toujours la confidentialit\u00e9, un \u00e9l\u00e9ment crucial pour encourager les survivantes \u00e0 chercher de l'aide.\n\nDuroch (2010) et Duroch (2008) examinent les interventions de M\u00e9decins Sans Fronti\u00e8res (MSF) et notent que la reconnaissance des violences sexuelles comme un probl\u00e8me de sant\u00e9 publique est essentielle. Les processus d'apprentissage au sein de l'organisation ont permis d'am\u00e9liorer la prise en charge des victimes, mais des r\u00e9sistances subsistent, souvent li\u00e9es \u00e0 des repr\u00e9sentations sociales des victimes. Cela indique qu'une formation continue des soignants sur la sensibilit\u00e9 aux traumatismes et la stigmatisation pourrait \u00eatre b\u00e9n\u00e9fique.\n\nRubuye Mer et Flicourt (2015) mettent en avant le manque d'infrastructures et de professionnels qualifi\u00e9s, comme des psychoth\u00e9rapeutes, ce qui complique l'acc\u00e8s aux soins. Ils recommandent une am\u00e9lioration des services de sant\u00e9 sexuelle et reproductive, en insistant sur le respect des droits humains pour favoriser un environnement propice \u00e0 la gu\u00e9rison.\n\nMatthieu (2017) aborde \u00e9galement la question des droits des victimes, en soulignant que l'ignorance des droits et le manque d'acc\u00e8s \u00e0 la justice aggravent la situation. Il est donc crucial d'informer les survivantes sur leurs droits et les ressources disponibles.\n\nEnfin, Byakweli et al. (2019) plaident pour un leadership dynamique et \u00e9thique qui pourrait r\u00e9habiliter les femmes victimes, en leur permettant de participer activement \u00e0 la reconstruction de leur communaut\u00e9. Cela pourrait inclure des initiatives de recherche-action participative pour renforcer leur pouvoir d'agir.\n\nEn somme, pour minimiser la stigmatisation et faciliter l'acc\u00e8s au KIT-PEP, il est imp\u00e9ratif que les personnels soignants adoptent une approche empathique et respectueuse, tout en am\u00e9liorant les infrastructures de sant\u00e9 et en sensibilisant les victimes \u00e0 leurs droits.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2002}, "authors": ["K. Yang", "S. Webster", "R. Ruben"], "summary": "Job shops have long faced pressures for improvement in a challenging and volatile environment. Today's trends of global competition and shortening of product life cycles suggest that both the challenges and the intensity of market volatility will only increase. Consequently, the study of tactics for maximizing the flexibility and responsiveness of a job shop is important. Indeed, there is a significant body of literature that has produced guidelines on when and how to deploy tactics such as alternate routings for jobs and transfers of cross-trained workers between machines. \n \n \n \nIn this paper we consider a different tactic by adjusting the length of workdays. Hours in excess of a 40-hour week are exchanged for compensatory time off at time and a half, and the total amount of accrued compensatory time is limited to no more than 160 hours in accordance with pending legislation. We propose several simple flexible workday policies that are based on an input/output control approach and investigate their performance in a simulated job shop. We find significant gains in performance over a fixed schedule of eight hours per day. Our results also provide insights into the selection of policy parameters.", "title": "An Evaluation of Flexible Workday Policies in Job Shops", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Swaty", "Sonali Pahwa"], "summary": "Performance appraisal has always been a debatable issue especially when flexible working methods were introduced. On one hand, there is growing need of the flexible working hours as they increase the organization\u2019s efficiency and also employees\u2019 quality of life. There is another side of the coin where it is difficult to evaluate the performance of the employees due to flexible working hours. Thus there is a mix of love- hate relationships whenever these concepts are co-related. In this paper, there is an attempt to discuss about various types of flexible work arrangements, their benefits for both employers and employees but looking towards the hate part, an attempt has also been made to discuss the problems in the content and implementation of the flexible working policies and finally various solutions in its implementation has also been discussed.", "title": "\u201cFlexible Working: How to Make it Work?\u201d", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Yiqing Liu", "Tong Wu"], "summary": "These years has witnessed a rapid change in staff\u2019s working pattern. Flexible working arrangements are increasingly widespread in developed countries as well as developing countries. The adoption l ...", "title": "Employers\u2019 and employees\u2019 evaluation of the implementation of flexible working policies", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Eleftherios Giovanis"], "summary": "There is an increasing concern on the quality of jobs and productivity witnessed in the flexible employment arrangements. The aim of this study is to examine the relationship between various employment arrangements and the workplace performance. Home-based working-teleworking, flexible timing and compressed hours are the main employment types examined using the Workplace Employee Relations Survey (WERS) in years 2004 and 2011. The workplace performance is measured by two outcomesthe financial performance and labour productivity. First, the determinants of those flexible employment types are explored. Second, the ordinary least squares (OLS) method is followed. Third, an instrumental variable (IV) approach is applied to account for plausible endogeneity and to estimate the causal effects. The findings reveal a significant and positive relationship between these types of flexible employment arrangements and the workplace performance. Education, age, wage, quality of relations between managersemployees, years of experience, the area of the market the workplace is operated and the competition are significant factors and are positively associated with the propensity of the flexible employment arrangements implementation. This can have various profound policy implications for employees, employers and the society overall, including family-work balance, coping with family demands, improving the firm performance, reducing traffic congestion and stress among others. It is the first study that explores the relationship between flexible employment types and workplace performance using an IV approach. This allows us to estimate the causal effects of flexible employment types and the possible associated social implications.", "title": "Flexible Employment Arrangements and WorkplacePerformance in Great Britain", "claims": null}, {"metadata": {"year": 2010}, "authors": ["K. M. Mital"], "summary": "Flexible work options serve as drivers for improving performance standards and creating value in an organization. In business higher standards of organizational performance can lead to more responsive customer service, increased cash flow, larger return on assets and profits, leading to capital generation for expanding capacity, creating new jobs, scoring competitive advantage, and achieving overall higher value creation. Employees flexibility and empowerment play major role in raising performance standards though flexible work options such as flexible manufacturing system, flexi-time management, telecommuting, teleworking, teleconferencing/telepresence, work-life balance, flexible outsourcing, etc. This paper reviews a set of flexible work options and their role in achieving higher standards of organizational performance and value creation with case examples drawn from select organizations.", "title": "Flexible Work Options and Value Creation", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Colin Green", "P. Kler", "G. Leeves"], "summary": "There is concern that the increase in flexible employment contracts witnessed in many OECD economies is evidence of a growth in low-pay, low-quality jobs. In practice, it is difficult to evaluate the \u2018quality\u2019 of flexible jobs. Previous research has primarily investigated objective measures of job quality such as wages and training or subjective measures such as job satisfaction. We jointly evaluate these elements of flexible employment contracts using a job quality index. Analysis of this index demonstrates that flexible jobs are of a lower quality. Differences in the subjective and objective assessment of factors like pay and hours are evident.", "title": "Flexible workers in inferior jobs : re-appraising the evidence", "claims": null}, {"metadata": {"year": 2018}, "authors": ["L. Govender", "S. Migiro", "A. Kyule"], "summary": "Flexible work arrangements (FWA) prefer to flexibility regarding an employee\u00e2\u20ac\u2122s work location, working times and how much an employee works . FWA practices are designed to keep employees motivated and satisfied with work and non -work related roles. Existing empirical studies on the impact of flexible work arrangements on performance, and job satisfaction have mostly been conducted in developed countries. This paper sought to address this gap by undertaking a study in an organization specific context in a developing country, in particular Durban Westville Eskom, South Africa. The main aim of this paper was to establish the relationship between flexible work arrangements, performance and job satisfaction at Durban Westville Eskom shared services department. This paper is anchored on the two - factor theory, Vrooms expectancy theory, Role theory, and Spillover theory. The target population consisted of 120 employees reporting to revenue management shared services. Simple random sampling technique was used to determine the sample size whereby a sample of 92 employees was obtained . An online questionnaire was used to collect data from the sample. Data was analysed through descriptive statistics and correlation analysis. The findings show that a majority are satisfied with their current work arrangements, and that flexible work arrangement could make them stay with their current employer. In addition, it was found that performance and flexible work arrangement have a strong positive correlation. The study concluded that flexible work arrangements enhanced employee retention, and job satisfaction which led to higher productivity.", "title": "Flexible Work Arrangements, Job Satisfaction and Performance", "claims": null}, {"metadata": {"year": 2018}, "authors": ["G. L.", "Migiro S. O.", "Kyule A. K."], "summary": "Flexible work arrangements (FWA) prefer to flexibility regarding an employee\u00e2\u20ac\u2122s work location, working times and how much an employee works . FWA practices are designed to keep employees motivated and satisfied with work and non -work related roles. Existing empirical studies on the impact of flexible work arrangements on performance, and job satisfaction have mostly been conducted in developed countries. This paper sought to address this gap by undertaking a study in an organization specific context in a developing country, in particular Durban Westville Eskom, South Africa. The main aim of this paper was to establish the relationship between flexible work arrangements, performance and job satisfaction at Durban Westville Eskom shared services department. This paper is anchored on the two - factor theory, Vrooms expectancy theory, Role theory, and Spillover theory. The target population consisted of 120 employees reporting to revenue management shared services. Simple random sampling technique was used to determine the sample size whereby a sample of 92 employees was obtained . An online questionnaire was used to collect data from the sample. Data was analysed through descriptive statistics and correlation analysis. The findings show that a majority are satisfied with their current work arrangements, and that flexible work arrangement could make them stay with their current employer. In addition, it was found that performance and flexible work arrangement have a strong positive correlation. The study concluded that flexible work arrangements enhanced employee retention, and job satisfaction which led to higher productivity.", "title": "Flexible Work Arrangements, Job Satisfaction and Performance", "claims": null}], "query": "performance evaluation as component of flexible work policy", "summary_abstract": "The research on performance evaluation as a component of flexible work policies reveals a complex interplay between flexible work arrangements and employee performance. Several studies highlight the benefits of flexible work options, suggesting that they can enhance organizational performance and employee satisfaction.\n\nYang et al. (2002) demonstrate that flexible workday policies, such as adjusting the length of workdays, can lead to significant performance gains in job shops compared to fixed schedules. This indicates that flexibility in work hours can directly impact productivity.\n\nSwaty and Pahwa (2020) discuss the challenges of performance appraisal in flexible working environments, noting that while flexible hours can improve efficiency and quality of life for employees, they also complicate performance evaluation. This duality suggests that organizations must develop robust evaluation methods that accommodate flexible work arrangements.\n\nGiovanis (2016) provides empirical evidence of a positive relationship between various flexible employment types\u2014such as teleworking and flexible timing\u2014and workplace performance. This study emphasizes that factors like education and managerial relationships also play a crucial role in the successful implementation of flexible work policies.\n\nMital (2010) reinforces the idea that flexible work options can drive performance improvements and create organizational value. The paper outlines how flexibility can lead to better customer service and increased profitability, further supporting the notion that performance evaluation should adapt to these flexible frameworks.\n\nGovender et al. (2018) focus on the context of a developing country, finding a strong positive correlation between flexible work arrangements and employee performance. Their research indicates that such arrangements not only enhance job satisfaction but also contribute to higher productivity and employee retention.\n\nIn summary, the collective findings suggest that while flexible work policies can enhance performance and job satisfaction, they also present challenges for performance evaluation. Organizations must navigate these complexities by developing tailored appraisal systems that recognize the unique dynamics of flexible work environments (Yang et al., 2002; Swaty & Pahwa, 2020; Giovanis, 2016; Mital, 2010; Govender et al., 2018).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2019}, "authors": ["Charlie Kurth"], "summary": "According to psychological constructivism, emotions result from projecting folk emotion concepts onto felt affective episodes. While constructivists acknowledge there is a biological dimension to emotion, they deny that emotions are (or involve) affect programs. So they also deny emotions are natural kinds. However, the essential role that constructivism gives to felt experience and folk concepts leads to an account that is extensionally inadequate and functionally inaccurate. Moreover, biologically oriented proposals that reject these commitments are not similarly encumbered. Recognizing this has two implications: biological mechanisms are more central to emotion than constructivism allows, and the conclusion that emotions are not natural kinds is premature.", "title": "Are Emotions Psychological Constructions?", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Kristen A. Lindquist", "L. F. Barrett"], "summary": "This study examined the hypothesis that emotion is a psychological event constructed from the more basic elements of core affect and conceptual knowledge. Participants were primed with conceptual knowledge of fear, conceptual knowledge of anger, or a neutral prime and then proceeded through an affect-induction procedure designed to induce unpleasant, high-arousal affect or a neutral affective state. As predicted, only those individuals for whom conceptual knowledge of fear had been primed experienced unpleasant core affect as evidence that the world was threatening. This study provides the first experimental support for the hypothesis that people experience world-focused emotion when they conceptualize their core affective state using accessible knowledge about emotion.", "title": "Constructing Emotion", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Batja  Mesquita", "Michael  Boiger", "Jozefien  De Leersnyder"], "summary": "A large body of anthropological and psychological research on emotions has yielded significant evidence that emotional experience is culturally constructed: people more commonly experience those emotions that help them to be a good and typical person in their culture. Moreover, experiencing these culturally normative emotions is associated with greater well-being. In this review, we summarize recent research showing how emotions are actively constructed to meet the demands of the respective cultural environment; we discuss collective as well as individual processes of construction. By focusing on cultural construction of emotion, we shift the focus toward how people from different cultures 'do' emotions and away from which emotions they 'have'.", "title": "The cultural construction of emotions.", "claims": null}, {"metadata": {"year": 2012}, "authors": ["M. Boiger", "B. Mesquita"], "summary": "Emotions are engagements with a continuously changing world of social relationships. In the present article, we propose that emotions are therefore best conceived as ongoing, dynamic, and interactive processes that are socially constructed. We review evidence for three social contexts of emotion construction that are embedded in each other: The unfolding of emotion within interactions, the mutual constitution of emotion and relationships, and the shaping of emotion at the level of the larger cultural context. Finally, we point to interdependencies amongst these contexts of construction and discuss future directions of a constructionist perspective.", "title": "The Construction of Emotion in Interactions, Relationships, and Cultures", "claims": null}, {"metadata": {"year": 2006}, "authors": ["Gordon  Clanton"], "summary": "Human emotions are socially constructed (Berger and Luckmann 1966). Emotions are shaped by social processes and social forces. Emotions are social as well as psychological phenomena, responses to social situations that are shaped by social learning. However useful it may be to consider emotions as physiological or as psychological events, the sociological study of emotions draws attention to often-overlooked social aspects of emotions, including the situations that provoke them, the social learning by which they are shaped, their historical and cross-cultural variability, their social usefulness, their contribution to social conflict, and the social arrangements that humans set up to manage them. To paraphrase Mills (1959), the private emotions of individuals are shaped by public issues of social structure.", "title": "Jealousy and Envy", "claims": null}, {"metadata": {"year": 1980}, "authors": ["James R. Averill"], "summary": "ABSTRACT Traditionally, the emotions have been viewed from a biological perspective; that is, the emotions have been seen as genetically determined and relatively invariable responses. The present chapter, by contrast, views the emotions as social constructions. More precisely, the emotions are here defined as socially constituted syndromes or transitory social roles. A role-conception does not deny the contribution of biological systems to emotional syndromes; it does, however, imply that the functional significance of emotional responses is to be found largely within the sociocultural system. With regard to subjective experience, a person interprets his own behavior as emotional in much the same way that an actor interprets a role\u201cwith feeling.\u201d This involves not only the monitoring of behavior (including feedback from physiological arousal, facial expressions, etc.), but also an understanding of how the emotional role fits into a larger\u201cdrama\u201d written by society. Some of the biological, personal, and situational factors that influence emotional behavior are also discussed.", "title": "A CONSTRUCTIVIST VIEW OF EMOTION", "claims": null}, {"metadata": {"year": 1988}, "authors": ["S. Kippax", "J. Crawford", "Pam Benton", "U. Gault", "J. Noesjirwan"], "summary": "This paper is in three sections\u2014theory, method and data. We consider that emotions are socially constructed, and begin with an interpretation of social constructionism as it applies to emotion. The conclusion drawn from the analysis of theories from a variety of disciplines is that it is important to incorporate and extend the notion of appraisal into a study of emotion. This notion is prominent in the cognitive approach and the social constructionist approach allows the cognitive and social views of emotion to be integrated. \n \n \n \nThis approach enabled us to examine the process whereby individuals engage with each other, and hence with the social structures, in their construction of emotions. We employed a method \u2018memory-work\u2019 which uses written memories as source material. This is discussed at some length since it is integrated within a theory of socialization which is closely related to social constructionism. \n \n \n \nFinally, we present an analysis of some data which illustrate and demonstrate the processes involved in constructing emotion. Our work confirms and extends the social constructionist view. Paradoxically, in highlighting the notion of agency, we stress the essentially social nature of the individual and overcome some of the problems which arise in methods which examine only the individual or the society.", "title": "Constructing emotions: Weaving meaning from memories", "claims": null}, {"metadata": {"year": 2007}, "authors": ["G. Clanton"], "summary": "Human emotions are socially constructed (Berger and Luckmann 1966). Emotions are shaped by social processes and social forces. Emotions are social as well as psychological phenomena, responses to social situations that are shaped by social learning. However useful it may be to consider emotions as physiological or as psychological events, the sociological study of emotions draws attention to often-overlooked social aspects of emotions, including the situations that provoke them, the social learning by which they are shaped, their historical and cross-cultural variability, their social usefulness, their contribution to social conflict, and the social arrangements that humans set up to manage them. To paraphrase Mills (1959), the private emotions of individuals are shaped by public issues of social structure. Emotions are shaped by society. Private experiences of emotion are embedded in history, culture, and social structure. Not only our feelings, but also our feelings about our feelings are shaped by psychological, philosophical, and theological frameworks that are institutionalized in social life. Thus, patterns of emotional experience change in response to changes in society and culture. Emotions cannot be fully understood without some attention to the social forces that influence them. Emotions reflect the norms, attitudes, and values of groups as well as individuals; they are useful and dangerous for groups as well as individuals. As Collins (1975:92) observed, it is through emotional behavior that humans \"exercise power, create religions and works of art. . . and enact bonds of solidarity among family and friends.\" Shalin (2004) argues that politics is fueled by emotions, economics feeds on moral feelings, and democracy is an embodied process that binds affectively as well as rhetorically. Emotions are shaped by the beliefs, attitudes, and values that individuals acquire in the course of their socialization. The experience and expression of emotions depend on what one \"knows,\" what one believes to be true. The private experience of love depends in part on beliefs about sex and its social regulation. The private experience of jealousy depends in part on beliefs about", "title": "CHAPTER 18 Jealousy and Envy", "claims": null}], "query": "Are emotions constructed?", "summary_abstract": "The question of whether emotions are constructed is addressed through various perspectives in the provided literature. A consensus emerges that emotions are not merely biological responses but are significantly shaped by social and cultural contexts.\n\nKurth (2019) critiques the psychological constructivist view, arguing that while emotions involve biological mechanisms, the constructivist perspective inadequately accounts for the complexity of emotional experiences. This suggests that emotions cannot be fully understood through a purely constructivist lens.\n\nLindquist and Barrett (2008) provide experimental evidence supporting the idea that emotions are constructed from core affect and conceptual knowledge. Their findings indicate that individuals interpret their affective states through the lens of accessible emotional concepts, demonstrating a psychological construction of emotions.\n\nMesquita et al. (2016) emphasize the cultural construction of emotions, highlighting that individuals experience emotions that align with cultural norms, which in turn contributes to their well-being. This perspective shifts the focus from inherent emotional experiences to how cultural contexts shape emotional expressions and experiences.\n\nBoiger and Mesquita (2012) further elaborate on the social construction of emotions, proposing that emotions are dynamic processes influenced by social interactions and cultural contexts. They argue for a multi-layered understanding of emotion construction that encompasses individual, relational, and cultural dimensions.\n\nClanton (2006, 2007) reinforces the notion that emotions are socially constructed phenomena, shaped by social learning and cultural frameworks. He argues that private emotional experiences are deeply embedded in historical and social structures, suggesting that understanding emotions requires attention to the societal influences that shape them.\n\nAverill (1980) also posits that emotions are socially constituted roles, where individuals interpret their emotional experiences within a broader societal context. This view aligns with the idea that emotions are not fixed biological responses but are influenced by social dynamics.\n\nIn summary, the collective findings from these papers suggest that emotions are indeed constructed, shaped by a combination of biological, psychological, and social factors. The interplay of cultural norms, social interactions, and individual interpretations plays a crucial role in how emotions are experienced and expressed across different contexts.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2017}, "authors": ["A. Urbanek"], "summary": "data definition in so-called 3V model, which emphasises three main features of so big datasets [4]: - high-volume (large amount of data), - high-velocity (high speed/variability of received and sent data), - and high-variety (high variety of data and sources of their origin). In addition, in their definition the Gartner analysts indicate that these are the data that require new, innovative Big data is a term used to describe large, variable and very diverse data sets, which are very difficult to process and analyse, but on the other hand which are extremely valuable from the point of view of gaining knowledge, for example, on consumer behaviour or real-time changes in a specific market. Automated fare technologies in public urban transport are implemented ever more widely in many cities worldwide. Because of these technologies the public transport organisers have access to large data sets, which create new possibilities in the decision-ma-king process. The aim of the paper is to discuss the role of big data in the decision-making process in urban public transport and also the main barriers to big data use in the urban mobility management.", "title": "Big Data - a Challenge for Urban Transport Managers", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Dr.C K Gomathy"], "summary": "With the help of their ride-sharing software and ability to avoid regulations, Uber has grown from a start-up to a worldwide behemoth that is competing with conventional taxis in more than 700 Metropolitan regions. The taxi business has significantly improved and expanded. One of the most well-known taxi firms in the world, Uber provides a wide range of services in several American cities, India, and European cities. It is the business that provides good employment for drivers and excellent services for passengers. Keywords: Time series, data analytics, supply and demand, taxis, ride hailing apps and big data", "title": "BIG DATA ANALYTICS IN UBER", "claims": null}, {"metadata": {"year": 2019}, "authors": ["P. Yadav", "Sanjiv Sharma", "Amarkant Singh"], "summary": "Big Data is a collection of large amount of data which is growing very rapidly with the popularity of social networking sites. The size of the Big data has been extended from terabytes to petabytes. Big data are characterized by four important attributes: volume, velocity, variety and veracity. The volume attributes describe the data at rest in the range from terabytes to Exabyte\u2019s, the velocity deals with data in motion, i.e streaming the data to respond within milliseconds rather than in seconds, the variety discuss the data in many different forms such as structured, unstructured, text and multimedia data, whereas the veracity deals with the data in doubt, i.e. uncertainty in data due to data inconsistency. These attributes of big data make it a challenge for organizations to have control over, and use such data. In today\u2019s era we are overloaded with the information, however we are lacking the insight. The 90% of the data in the world today has been created in the last two years by various social networking sites. As big data grows it challenges the capabilities of traditional data warehouses that collect and store large amounts of internal and external data. Data drawn from these repositories are used to improve decision making, increase organizational efficiencies, and improve organizational effectiveness. In this paper an attempt has been made to explore the real life application of Big Data , cloud database, Hadoop, Map Reduce and Cloud Computing, in various domains.", "title": "Big Data and Cloud Computing: An Emerging Perspective and Future Trends", "claims": null}, {"metadata": {"year": 2020}, "authors": ["T. Gunawardena", "K. Jayasena"], "summary": "Data is crucial in today\u2019s business and technology environment. There is a growing demand for Big Data applications to extract and evaluate information, which will provide the necessary knowledge that will help us make important rational decisions. These ideas emerged at the beginning of the 21st century, and every technological giant is now exploiting Big Data technologies. Big Data refers to huge and broad data collections that can be organized or unstructured. Big Data analytics is the method of analyzing massive data sets to highlight trends and patterns. Uber is using real-time Big Data to perfect its processes, from calculating Uber\u2019s pricing to finding the optimal positioning of taxis to maximize profits. Real-time data analysis is very challenging for the implementation because we need to process data in real-time, if we use Big Data, it is more complex than before. Implementation of real-time data analysis by Uber to identify their popular pickups would be advantageous in various ways. It will require high-performance platform to run their application. So far no research has been done on real-time analysis for identifying popular Uber locations within Big Data in a distributed environment, particularly on the Kubernetes environment. To address these issues, we have created a machine learning model with a Spark framework to identify the popular Uber locations and use this model to analyze real-time streaming Uber data and deploy this system on Google Dataproc with the different number of worker nodes with enabling Kubernetes and without Kubernetes environment. With the proposed Kubernetes environment and by increasing the worker nodes of Dataproc clusters, the performance can be significantly improved. The future development will consist of visualizing the real-time popular Uber locations on Google map.", "title": "Real-Time Uber Data Analysis of Popular Uber Locations in Kubernetes Environment", "claims": null}, {"metadata": {"year": 2014}, "authors": ["M. Burt", "Matthew Cuddy", "Michael Razo"], "summary": "The purpose of this white paper is to expand the understanding of big data for transportation operations, the value it could provide, and the implications for the future direction of the U.S. Department of Transportation (USDOT) Connected Vehicle Real-Time Data Capture and Management (DCM) Program. Big data is an approach to generating knowledge in which a number of advanced techniques are applied to the capture, management and analysis of very large and diverse volumes of data \u2013 data so large, so varied and analyzed at such speed that it exceeds the capabilities of traditional data management and analysis tools. This paper is not intended as a primer or \u201chow to\u201d on big data, per se, but rather is intended to explore the potential value of big data approaches in a future connected vehicle environment. Big data is a process of knowledge generation that features the following approaches: (1) Data capture that includes massive datasets encompassing all or most of the population being studied (as opposed to small samples); use of data from both purpose-specific and repurposed data collection; and utilization of crowdsourced and \u201celectronic breadcrumb\u201d data. (2) Data management that features storage in decentralized and virtual locations (i.e., the cloud) and handles both structured and unstructured data. (3) Data analysis that is often automated, with computers doing more of the work to find complex patterns among a large number of variables. Big data approaches are needed to contend with the coming volume of connected vehicle and traveler data, to: enable a wide range of new strategies that are expected to provide safety, mobility and environmental benefits, and reduce the need for traditional data collection mechanisms (e.g., connected vehicle probes replacing traffic detectors). This paper identifies two additional, broad areas where big data analytical approaches may be able to provide further value: 1) Transportation System Monitoring & Management; and 2) Traveler-Centered Transportation Strategies.", "title": "Big Data\u2019s Implications for Transportation Operations: An Exploration", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Bichitra Mandal", "R. Sahoo", "Srinivas Sethi"], "summary": "The mounting of industrial advancements have led to substantial quantity of data from unusual areas, like health care, user- generated data, internet, financial companies etc. The term big data was discovered to confine the significance of this rising trend. Due to its complete volume, big data exhibits exclusive personality as comparison to traditional data. Big data may be structured, unstructured or semi-structured which requires more analysis. Its processing is achieved due to its high volume, velocity, value, variety and veracity. It has a lot of Challenges. Thus, cloud computing is being deployed in order to regulate the big data requirements. The security and privacy is not well maintained in cloud. Big data systems are decomposed into four modules called data generation, data acquisition, data storage and data analysis. These modules are otherwise known as big data value chain. Big data is widely established in these recent years, and implemented Hadoop framework for addressing big data challenges.", "title": "Scalable Big Data Analysis in Cloud Environment: A Review", "claims": null}, {"metadata": {"year": 2016}, "authors": ["G. Kemp", "G. Vargas-Solar", "Catarina Ferreira Da Silva", "P. Ghodous", "C. Collet", "Pedro Pablo Lopez Amalya"], "summary": "This paper presents a cloud service oriented approach for managing and analysing big data required by transport applications. Big data analytics brings new insights and useful correlations of large data collections providing undiscovered knowledge. Applying it to transport systems brings better understanding to the transport networks revealing unexpected choking points in cities. This facility is still largely inaccessible to small companies due to their limited access to computational resources. A cloud-oriented architecture opens new perspectives for providing efficient and personalised big data management and analytics services to (small) companies.", "title": "Cloud big data application for transport", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Z. Bojkovic", "D. Milovanovic"], "summary": "Voluminous data are generated from a variety of users and devices and are to be stored and processed in powerful data center. As such, there is a strong demand for building a network infrastructure to gather distributed and rapidly generated data and move them to data center for knowledge discovery. Big data has received considerable attention, because it can mine new knowledge for economic growth and technical innovation. Many research efforts have been directed to big data processing due to its high volume, velocity and variety, referred to as 3V. This paper first describes challenges for big data together with basic Map Reduce structure. Then it presents existing approaches for big data analytics including general architecture. The second part establishes the relation between mobile cloud and big data and provides geo-dispersed big data application, including big data in mobile cloud sensing. Some open research questions that need to be investigate conclude this work.", "title": "Mobile cloud analytics in Big data era", "claims": null}], "query": "4Vs of big data and big data forms at Uber", "summary_abstract": "The research question regarding the 4Vs of big data and its forms at Uber is addressed through various studies that highlight the characteristics and applications of big data within the company. The 4Vs\u2014volume, velocity, variety, and veracity\u2014are essential in understanding how Uber utilizes big data to enhance its operations.\n\nUrbanek (2017) emphasizes the significance of high volume, high velocity, and high variety in big data, noting that these characteristics create opportunities for improved decision-making in urban public transport, which is relevant to Uber's operational context. Similarly, Yadav et al. (2019) expand on these attributes, introducing veracity as a critical aspect, which deals with the uncertainty and inconsistency of data. This comprehensive understanding of the 4Vs is crucial for Uber as it navigates the complexities of data management and analysis.\n\nGunawardena and Jayasena (2020) specifically illustrate how Uber leverages real-time big data analytics to optimize its pricing strategies and improve the positioning of taxis. This real-time processing is a direct application of the velocity aspect of big data, allowing Uber to respond swiftly to market demands. Furthermore, they highlight the challenges associated with implementing such analytics, particularly in a distributed environment, which underscores the complexity of managing big data effectively.\n\nBurt et al. (2014) provide insights into the broader implications of big data for transportation operations, suggesting that advanced data management and analysis techniques are necessary to handle the vast amounts of data generated in connected vehicle environments, such as those utilized by Uber. This aligns with the need for innovative approaches to data capture, management, and analysis, as discussed in the context of Uber's operations.\n\nIn summary, the collective findings from these papers illustrate that Uber's use of big data is characterized by the 4Vs\u2014volume, velocity, variety, and veracity\u2014each playing a vital role in enhancing operational efficiency and decision-making processes. The integration of real-time analytics and advanced data management techniques is essential for Uber to maintain its competitive edge in the rapidly evolving transportation landscape (Urbanek, 2017; Yadav et al., 2019; Gunawardena & Jayasena, 2020; Burt et al., 2014).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2019}, "authors": ["E. Bruch", "M. Newman"], "summary": "We study the structure of heterosexual dating markets in the United States through an analysis of the interactions of several million users of a large online dating website, applying recently developed network analysis methods to the pattern of messages exchanged among users. Our analysis shows that the strongest driver of romantic interaction at the national level is simple geographic proximity, but at the local level, other demographic factors come into play. We find that dating markets in each city are partitioned into submarkets along lines of age and ethnicity. Sex ratio varies widely between submarkets, with younger submarkets having more men and fewer women than older ones. There is also a noticeable tendency for minorities, especially women, to be younger than the average in older submarkets, and our analysis reveals how this kind of racial stratification arises through the messaging decisions of both men and women. Our study illustrates how network techniques applied to online interactions can reveal the aggregate effects of individual behavior on social structure.", "title": "Structure of online dating markets in US cities", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Diane H Felmlee", "D. Kreager"], "summary": "Abstract This study analyzed the e-mail exchange network of participants of a national dating website. The investigation examined whether aggregated partner preferences give rise to distinct, \u201cinvisible,\u201d clusters in online dating networks that structure dating opportunities and result in homophilous subgroups. The findings identified and visualized the ten largest network clusters of participants who interacted with each other and examined the dater characteristics most responsible for cluster membership. Rated attractiveness and age were the strongest cluster correlates, whereas education and race were relatively uncommon determinants. In sum, daters\u2019 interdependent actions created aggregate communities unseen by the users themselves, but no less influential for dating opportunities, that were based more on attractiveness and age than on race and education.", "title": "The Invisible Contours of Online Dating Communities: A Social Network Perspective", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Andreas  Schmitz"], "summary": "This last chapter will combine the lines of argumentation and provide a conclusive sociological understanding of the phenomenon \u2018online dating\u2019. Applying a Bourdieusian notion of \u2018structure\u2019, we will argue that the social distances between agents of different social classes, otherwise maintained by social, geographic, and institutional segregation in a largely direct way, are consolidated in digital partner markets in a particularly immediate way. In the \u2018hyper-focus\u2019 online dating, users \u2013 socially classified before even entering the market \u2013 classify themselves, their symbolic goods and their potential partners in the course of their practices and interactions. It is under the conditions of numeric abundance of potential partners from all social classes where the scarcity and hierarchy of symbolic goods already operative in the offline world come to light and can unfold their full force. A dating platform is a partner market with a unique efficacy, as it allows symbolic capital to operate very efficiently in its function of converting the users\u2019 capital endowments. In the aggregate, users thereby create a particularly structured market, which reinforces the relations of societal domination also effective outside of the online dating market. In contrast to reductionist theories of individual choice, that conceive of the market\u2019s structure as condition and outcome of individual preferences only, the Bourdieusian conceptualization helps us to understand the role of relational structuring practices. Given the fact that habitus is operative even in the supposedly structure-free sphere of online dating, this work makes a case for a sociological approach that remains sensitive to the aggregated effects of reproduction strategies in a realm closest to the idea of freedom of action.", "title": "Online Dating \u2013 A Unified and Unifying Symbolic Good Market", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Christina A. Masden", "W. K. Edwards"], "summary": "Online dating sites have become a common means of finding a romantic partner. And yet, these sites differ greatly from many other socially oriented websites: perhaps most notably, the pairwise style of interaction afforded by these sites prevents a robust online community from forming. Users, however, have taken matters into their own hands by creating thriving external forums for discussion of specific dating sites. We report on a multiple methods study of two online dating services, via observation and interviews with users of the forums associated with these sites. Our findings suggest that these forums play an essential role in creating an \"outsourced community\" for the dating sites, and also reveal practices around how some users \"game the system\" in online dating, the prevalence of harassment in online dating, and users' frustrations with current dating sites. We conclude with a number of recommendations for system design.", "title": "Understanding the Role of Community in Online Dating", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Andreas  Schmitz"], "summary": "This chapter starts with an excursus on the meaning of the term \u2018partner market\u2019, due to the fact that there is little systematic conceptualization of either \u2018partner market\u2019 in empirical research, or of the basic term \u2018market\u2019 itself in sociology as a whole. Deriving a Weberian concept of the market as competition for exchange chances, a relational definition of the partner market will be developed. Based on this definition, and findings from research and qualitative interview material, it will be shown that online dating markets are not at all an exceptional phenomenon in the context of couple formation. It will be shown that dating site can be analytically located within a continuum alongside conventional partner markets such as night clubs, offline \u2018lonely hearts\u2019 ads and speed dating events. Online dating sites seem to be, in comparison to other contexts of interaction, strongly structured by market principles such as competition for partners, induction of conscious rational mate choice, and exchange logics. Thus, it will be argued that the \u2018real type\u2019 market of online dating can be seen as being especially close to an \u2018ideal-typical\u2019 partner market and, thus, as a paradigmatic textbook example for analyses in the tradition of individualistic mate choice.", "title": "The Market Character of Online Dating", "claims": null}, {"metadata": {"year": 2011}, "authors": ["H. Blossfeld", "A. Schmitz"], "summary": "Online dating has evolved from a rare and stigmatised medium to a socially accepted way of partnership formation and to a lucrative business model at the same time. Current key capital market data on digital dating services amounts to a market volume of 138 million euros in Germany, 932 million dollars in the United States, and an amount of four billion US dollars worldwide. This economic dimension is associated with massive advertising campaigns, increasing media discourse and the increasing relevance of online dating in contemporary couple formation. The success of online dating essentially stems from the simple and effective access to mating platforms. Online dating is increasingly displacing the traditional necessity of the actor\u2019s co-presence as online interactions take place in relative independence of time and space: men and women can easily integrate their dating activities into their daily lives with marginal time loss. The structure of dating sites allows for simultaneous contact with different potential partners as well as for an \u201cadministrative\u201d management of possible partners. Two different forms of digital dating, and accordingly two different business models, exist on the online dating market. The first involves searching for a potential partner on one\u2019s own initiative, the second one comprises a \u201cmatching system\u201d offered by the provider. Between these two poles, there are numerous mixed forms depending on the particular business model. The booming development of online dating is coupled with an increasing differentiation of the digital dating market itself, ranging from services for a general audience to very special niches (e.g. services for people preferring specific religious affiliations, specific ethnic groups, or age brackets) and from services for finding a longterm partner to services to look for a sexual affair. It comes as no surprise that the scientific interest in this field has also increased substantially. (1) Social scientists interested in mate search issues utilize dating platforms as a new observational methodology with distinct advantages when compared to questionnaires or marriage records. Usually, the aim is to gain insights that can be applied to human mating in other contexts. (2) Another stream of research considers online dating to", "title": "Introduction to the special issue Online dating: Social innovation and a tool for research on partnership formation", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Celia Romm-Livermore", "Kristina Setzekorn"], "summary": "E-dating is now among the largest global e-commerce sectors, allowing for interaction that was once not possible. Despite its dominance in the e-commerce market and society, it is almost completely absent from the information systems research literature. Social Networking Communities and E-Dating Services: Concepts and Implications rectifies the absence of e-dating literature in the academic community by incorporating research from around the world, addressing the many aspects of e-dating and establishing it as a new research discipline. This unique collection of high quality international articles contributes toward the legitimization of e-dating as an area of research in e-commerce.", "title": "Social Networking Communities and E-Dating Services: Concepts and Implications", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Slah Alsaleh", "R. Nayak", "Yue Xu"], "summary": "The rapid growth in the number of users using social networks and the information that a social network requires about their users make the traditional matching systems insufficiently adept at matching users within social networks. This paper introduces the use of clustering to form communities of users and, then, uses these communities to generate matches. Forming communities within a social network helps to reduce the number of users that the matching system needs to consider, and helps to overcome other problems from which social networks suffer, such as the absence of user activities' information about a new user. The proposed system has been evaluated on a dataset obtained from an online dating website. Empirical analysis shows that accuracy of the matching process is increased using the community information.", "title": "Finding and Matching Communities in Social Networks Using Data Mining", "claims": null}], "query": "community structure in online dating markets", "summary_abstract": "The research on community structure in online dating markets reveals a complex interplay of factors that shape user interactions and the formation of subgroups within these platforms. Bruch and Newman (2019) highlight that geographic proximity is a primary driver of romantic interactions at a national level, while local dating markets are further influenced by demographic factors such as age and ethnicity. Their findings indicate that cities are divided into submarkets, each exhibiting distinct sex ratios and demographic characteristics.\n\nFelmlee and Kreager (2017) delve into the concept of \"invisible\" clusters formed by user interactions on dating platforms. Their analysis shows that preferences based on attractiveness and age significantly influence these clusters, overshadowing factors like education and race. This suggests that users' collective behaviors create communities that are not immediately visible but play a crucial role in shaping dating opportunities.\n\nSchmitz (2017) offers a sociological perspective, arguing that online dating markets reflect existing social hierarchies and class distinctions. He posits that users enter these markets with pre-established social classifications, which are reinforced through their interactions. This perspective emphasizes the relational dynamics at play, suggesting that online dating is not merely a space for individual choice but a structured environment that perpetuates societal norms.\n\nAdditionally, the role of external forums as \"outsourced communities\" is explored by Masden and Edwards (2015), who note that users often seek to create supportive networks outside the dating platforms themselves. These forums facilitate discussions about dating experiences and strategies, indicating a desire for community that the dating sites do not inherently provide.\n\nOverall, the literature collectively underscores that online dating markets are not just platforms for individual matchmaking but are deeply embedded in social structures that influence user behavior and community formation. The findings suggest that understanding these dynamics is essential for comprehending how online dating operates as a social phenomenon.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2021}, "authors": ["\u00d6zge Yap\u0131c\u0131", "M. U\u011furlar"], "summary": "Abstract Objectives Meralgia paresthetica is a very rare sensory\nmononeuropathy of the lateral femoral cutaneous nerve (LFCN). The\npurpose of this study was to evaluate the outcomes and compare the\nresults of ultrasound-guided corticosteroid injection and\nultrasound-guided alcohol neurolysis in the treatment of meralgia\nparesthetica. Methods We performed a retrospective clinical study of 26\npatients with a diagnosis of marelgia paresthetica with a duration of\n\u226510 months. The patients were divided into 2 groups, with the Group 1\nreceiving ultrasound-guided local corticosteroid injection and Group 2\nreceiving ultrasound-guided alcohol neurolysis to the entrapment site of\nthe LFCN. Results The mean age of the patients in Group 1 was 42.2 years\nand in Group 2 was 40.8 years. The mean follow-up period of Group 1 was\n28.7 months and Group 2 was 28.4 months. At the end of the follow-up\nperiod 9 patients in Group 1 and 10 patients in Group 2 declared full\npain relief and improvement in cutaneous sensitivity. Conclusion Once\nmeralgia paresthetica has persisted corticosteroid injection and alcohol\nneurolysis are both effective methods. Although the recurrence rates are\nhigher in corticosteroid injection, both treatment methods decreased the\npain and improved the patients\u2019 satisfaction and long-term curative\neffect.", "title": "EFFECTIVENESS OF ULTRASOUND-GUIDED CORTICOSTEROID INJECTION AND ALCOHOL NEUROLYSIS IN THE TREATMENT OF MERALGIA PARESTHETICA DURING A 28-MONTH FOLLOW-UP PERIOD: A RETROSPECTIVE COHORT STUDY", "claims": null}, {"metadata": {"year": 2004}, "authors": ["T. Fabre", "I. B\u00e9b\u00e9zis", "J. Bouchain", "F. Farlin", "J. Rezzouk", "A. Durandeau"], "summary": "Purpose: Meralgia paraesthetica is usually caused by entrapment of the lateral femoral cutaneous nerve (LFCN) at the inguinal ligament. We present our experience with 114 patients who underwent surgical management for meralgia paraesthetica. Material: We reviewed 114 patients (48 men, 66 women, five bilateral cases) who underwent surgery for meralgia paraesthetica between 1987 and 1999; local anaesthesia was used for neurolysis in most cases. We identified five aetiologies: idiopathic (n=69, three bilateral), abdominal surgery (n=19), iliac graft harvesting (n=12, one bilateral), hip surgery (n=7), trauma (n=7, one bilateral). Methods: We analysed outcome at more than two years follow-up for the entire series and by aetiology using a standard 12-point evaluation scale accounting for residual pain, sensorial disorders, and patient satisfaction. Results: The overall results were good, mean score 9/12 (range 1\u201312). Ninety-two patients were very satisfied or satisfied. Among the 27 patients who were not satisfied, five developed recurrence. Mean time to full pain relief was 70 days (range 1 \u2013 364 days). Recovery of thigh sensitivity was noted at 128 days (range 1 \u2013 364). Discussion: The essential criteria of poor prognosis were duration of the meralgia before surgery and its aetiology. Neurolysis of an LFCN injured by trauma or iliac graft harvesting provided less satisfactory results (scores 7 and 6 respectively) than for idopathic meralgia paraesthetica or abdominal-surgery injury (scores 9 and 10 respectively). Eight of the neurolysis procedures in this series did not provide satisfactory results (score 5). Conclusion: Neurolysis appears to be the surgical treatment of choice for mearlgia paraesthetica. In skilled hands, neurolysis can be performed under local anaesthesia, although certain difficulties can be encountered: obesity, modified anatomy due to prior operations, nerve variability (frequent). Knowledge of these different elements is essential not only to achieve neurolysis but also prevent iatrogenic injury.", "title": "MERALGIA PARAESTHETICA: A RARE CAUSE OF GROIN PAIN: SURGICAL MANAGEMENT OF 119 CASES BETWEEN 1987 AND 1999", "claims": null}, {"metadata": {"year": 2013}, "authors": [], "summary": "Iliotibial band syndrome is a common knee injury. The most common symptom is lateral knee pain caused by inflammation of the distal portion of the iliotibial band. The iliotibial band is a thick band of fascia that crosses the hip joint and extends distally to insert on the patella, tibia, and biceps femoris tendon. In some athletes, repetitive flexion and extension of the knee causes the distal iliotibial band to become irritated and inflamed resulting in diffuse lateral knee pain. Iliotibial band syndrome can cause significant morbidity and lead to cessation of exercise. Although iliotibial band syndrome is easily diagnosed clinically, it can be extremely challenging to treat. Treatment requires active patient participation and compliance with activity modification. Most patients respond to conservative treatment involving stretching of the iliotibial band, strengthening of the gluteus medius, and altering training regimens. Corticosteroid injections should be considered if visible swelling or pain with ambulation persists for more than three days after initiating treatment. A small percentage of patients are refractory to conservative treatment and may require surgical release of the iliotibial band.", "title": "\" Advanced Bodywork & Massage \" Myofascial Trigger Point Therapy", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Zeki Serdar Ataizi", "Kemal Ertilav", "S. Ercan"], "summary": "Abstract Background: Meralgia paresthetica is an entrapment neuropathy of the lateral femoral cutaneous nerve that may cause paresthesias, pain, and sensory loss of the anterior and lateral thigh. Treatment is primarily medical. Surgery is an option for patients who do not respond to medical treatments. Methods: Patients whose symptoms did not improve after medical and conservative treatment for at least 3 months were included in this study. These patients underwent neurolysis and decompression surgery and had a mean postoperative follow-up of 38 months. Their pain levels were assessed by the VAS scoring system. Results: In 8 (61.5%) patients, the symptoms completely resolved within the first 3 months. In 5 (38.5%) patients, the complaints persisted partially and the recovery was observed after 12 months. In patients having a metabolic etiology, the duration of recovery was up to 12 months. Conclusion: The long term results of surgery are good though only partial improvemnts in reported pain were seen in the early postoperative period, especially in patients with a metabolic etiology.", "title": "Surgical options for meralgia paresthetica: long-term outcomes in 13 cases", "claims": null}, {"metadata": {"year": 1994}, "authors": ["R. Edelson", "P. Stevens"], "summary": "We reviewed the findings in twenty children and adolescents who had meralgia paresthetica, a common entity in adults that has only rarely been reported in children. Ten patients had bilateral involvement; thus, the study included thirty lesions. Twenty-four lesions were eventually treated with open decompression of the lateral femoral cutaneous nerve; the results of twenty-one of these operations were followed for at least two years. The presenting symptom was severe pain resulting in marked restriction of activities. The pain could be reproduced with palpation of the nerve, and a trial injection of Xylocaine (lidocaine) always produced transient relief of symptoms. The average age at the onset of the symptoms was ten years (range, one to seventeen years); the diagnosis was missed initially in ten patients, which resulted in multiple, unnecessary diagnostic tests being done. The average duration of the symptoms before the patient was first seen was twenty-four months (range, two to eighty-four months), and the average duration of follow-up after the twenty-one operations was thirty-eight months (range, twenty-five to sixty months). Fourteen of the twenty-one operations led to an excellent result, with complete relief of pain and no restriction of activities; five led to a good result, with occasional pain but no limitation of sports or other activities; and two led to a fair result, with pain that interfered with sports activities but not with walking. We believe that meralgia paresthetica is much more common in children than has been previously recognized.(ABSTRACT TRUNCATED AT 250 WORDS)", "title": "Meralgia paresthetica in children.", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Chandrasen Chaughule", "Nitin A Bhalerao"], "summary": "Background: Meralgia paresthetica is a non\u2013life-threatening neurological disorder characterized by numbness, tingling, and burning pain over the anterolateral aspect of thigh due to impingement of the lateral femoral cutaneous nerve. This disorder has been seen in patients with diabetes mellitus and obesity, but has also been observed in patients after procedures such as posterior spine surgery, iliac crest bone grafts, lumbar disk surgery, hernia repair, appendectomies, and pelvic osteotomies that ultimately lead to compression or damage to the lateral femoral cutaneous nerve. Overall, permanent sequelae of meralgia paresthetica are rare, however, some cases do require intervention. Study design: In this prospective study of 120 patients who underwent elective lumbar spinal surgery, the incidence, possible risk factors, etiopathogenesis and management of MP were analysed. Result: There were 70 males and 50 females. The age of the patients ranged from 15 to 81 years (mean 46.9 yrs.). Thirteen patients (12%) suffered from MP. It is more common in thinner individuals due to pressure injury to the nerve at its exit point. Ninety-two percent of the patients were asymptomatic at follow-up after 6 months. In 7 out of 13 patients, patchy sensory loss on clinical examination was seen at 6 months. Conclusion: MP after posterior lumbar spinal surgery is uncommon. Smaller bolsters may avoid some of the vulnerable pressure points, as the surface area available is relatively smaller. The posts of the HallRelton frame over the anterior superior iliac crest should be adequately padded. The condition is usually self-limiting. Surgical division or decompression of the LCFN is reserved for persistent or severe MP.", "title": "Meralgia paraesthetica following lumbar spine surgery: A study in 120 consecutive surgically treated cases", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Andrew D Franklin", "G Bennett Cierny", "Twila R Luckett"], "summary": "Meralgia paresthetica is a chronic pain syndrome that is extremely rare in the pediatric population. It is manifested by hypesthesia or pain in the distribution of the lateral femoral cutaneous nerve (LFCN) and is typically caused by entrapment as the nerve passes deep to the inguinal ligament. This sensory mononeuropathy is rare in children and diagnosis is typically delayed, often leading to prolonged functional impairment and unnecessary medical testing. A 9-year-old girl presented to the pain clinic with a 6-week history of right anterolateral thigh pain first noticed after a nontraumatic cheerleading practice. Comprehensive laboratory and radiographic evaluation by multiple prior specialists revealed no clear nociceptive source of pain. History and examination were consistent with a diagnosis of idiopathic, compressive meralgia paresthetica. Conservative management including physical therapy was followed for 2 weeks with only mild improvement noted. To facilitate physical therapy, an ultrasound-guided LFCN block was performed which confirmed the diagnosis by providing complete analgesia. The patient reported overall 25% improvement from multimodal therapy at another 2 weeks. A second LFCN block was performed with complete resolution of symptoms and restoration of function. The patient remains pain-free and has returned to walking, running, and competitive sports. The primary goal of pediatric chronic pain management, regardless of pain etiology, is early restoration of function to avoid prolonged absence from school, sports, or other productive activities and limit the psychological burden of chronic disease.", "title": "Interventional and multimodal pain rehabilitation in a child with meralgia paresthetica.", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Andisheh Maneshi", "S. Ravi", "M. Salehi", "Malihe Hasannezhad", "H. Khalili"], "summary": "A 23 year-old man was admitted to the hospital with chief complaints of pain, edema and spasm of the left lower limb, as well as mottling of dorsal and plantar aspects of the foot. \u00a0One week before the current admission, he was treated with oral co-amoxiclave and intramuscular penicillin 6.3.3. Immediately after the third injection, he experienced burning and yellowish discoloration at the site of the injection wich then progressed to his leg and foot. Pain, significant edema, spasm and mottling occurred within 30 minutes of the injection. The patient was referred to the hospital after 36 hours from the beginning of the symptoms. Color Doppler sonography revealed no impairment in the lower extremity flow and the common femoral artery, superficial femoral, popliteal, dorsalis pedis and posterior tibialis arteries were normal. Superficial probe sonography detected mild effusion in the left ankle without collection.", "title": "Nicolau Syndrome.", "claims": null}], "query": "recovery time from injection for meralgia parenthetica and Iliotibial band syndrome", "summary_abstract": "The recovery time from injections for meralgia paresthetica and iliotibial band syndrome varies based on the treatment method and individual patient factors. \n\nFor meralgia paresthetica, corticosteroid injections and alcohol neurolysis have been shown to be effective treatments. In a study by \u00d6zge Yap\u0131c\u0131 and M. U\u011furlar (2021), both methods resulted in significant pain relief, although corticosteroid injections had higher recurrence rates. The mean follow-up period for patients receiving these treatments was approximately 28 months, with many reporting full pain relief. Surgical options, such as neurolysis, also yield positive outcomes, with T. Fabre et al. (2004) noting that the mean time to full pain relief post-surgery was about 70 days, while recovery of thigh sensitivity took an average of 128 days. In cases where conservative treatments fail, surgery may be necessary, and recovery can take up to 12 months, particularly in patients with metabolic issues (Zeki Serdar Ataizi et al., 2018).\n\nIn contrast, iliotibial band syndrome typically requires a more conservative approach, with corticosteroid injections recommended if symptoms persist beyond three days after initiating treatment. The recovery timeline for this condition is less clearly defined in the literature, but it is noted that many patients respond well to conservative measures, including stretching and strengthening exercises (2013).\n\nOverall, while both conditions can be effectively managed with injections, the recovery time can range from a few days to several months, depending on the treatment approach and individual patient circumstances.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2017}, "authors": ["A. Batta"], "summary": "Magnesium is a cofactor in more than 300 enzyme systems that regulate diverse biochemical reactions in the body, including protein synthesis, muscle and nerve function, blood glucose control, and blood pressure regulation . Magnesium is required for energy production, oxidative phosphorylation, and glycolysis. Magnesium is a nutrient that the body needs to stay healthy. It is important for many processes in the body, including regulating muscle and nerve function, blood sugar levels, and blood pressure and making protein, bone, and DNA. Magnesium Malate is a compound of magnesium and malic acid, clinically shown to soothe and energize muscle cells. Malic acid is an important component of numerous enzymes key to ATP synthesis and energy production. Therefore, magnesium malate may be helpful in those suffering from fatigue due to fibromyalgia. It is a naturally occurring mineral that is important for many systems in the body, especially the muscles and nerves. Magnesium citrate also increases water in the intestines. Magnesium citrate is used as a laxative to treat occasional constipation. Magnesium is a macro-mineral, which, unlike trace minerals, is needed by the body in large amounts. Calcium, sodium, and potassium are also macro-minerals. The average human body contains about 25 grams of magnesium, one of the six essential minerals that must be supplied in the diet. Additionally, magnesium balances calcium within the body, which is important because overly high doses of calcium, usually from supplements, can cause problems associated with muscle control, including controlling the heart. A test for magnesium is done to: Find a cause for nerve and muscle problems, such as muscle twitches, irritability, and muscle weakness. Find the cause of symptoms such as low blood pressure, nausea, vomiting, diarrhea, dizziness, muscle weakness, and slurred speech. Monitor kidney", "title": "MAINTENANCE OF MILIEU INTERIEUR BY MAGNESIUM", "claims": null}, {"metadata": {"year": 2015}, "authors": ["L. J."], "summary": "MAGNESIUM is the lightest of all structural metals. As such, it forms the basis for commercial alloys that have found successful use in a wide variety of applications. It is a plentiful element, comprising 2.7% of the earth's crust (Ref 1). Although magnesium does not occur in nature in the metallic form, magnesium compounds occur worldwide, and commercial amounts of magnesium ores are found in most countries. The most common ores are the carbonates: dolomite (MgC03\u00b7CaC03) and magnesite (MgC03). The double chloride carnallite (MgC12\u00b7KC1\u00b76H20) is found to form salt deposits in natural brines and evaporites, such as in the Great Salt Lake in Utah. However, the major source of magnesium is ocean water. Magnesium constitutes 0.13% of the world's oceans (Ref 2); therefore, seawater provides a virtually inexhaustible supply of the metal.", "title": "Introduction : History , Production , Applications , and Markets", "claims": null}, {"metadata": {"year": 2020}, "authors": ["S. Tekumalla", "M. Gupta"], "summary": "The name, magnesium (Mg), was derived from an ancient city in Greece called \u201cMagnesia\u201dwhere magnesium carbonate was first discovered. It was first isolated in its elemental form by English chemist Sir Humphry Davy in 1808 [1]. In the earth\u2019s crust, magnesium is the sixth most abundant element and occurs in over 60 different minerals with at least 20% of Mg within them. Most commercially important of these minerals include dolomite, magnesite, brucite, and carnallite. Principally, magnesium is extracted from its minerals using a thermal reduction process [2]. Magnesium is also the third most abundant metal ion in seawater. Despite magnesium being only 0.13% of seawater, seawater remains an almost inexhaustible source for its extraction. Magnesium is extracted from seawater or brine using the electrolytic process of magnesium chloride [2]. Of the two extraction processes, the thermal reduction process is known to yield a higher purity of 99.99%, while the electrolysis process can achieve purity limited to 99.8%. Until the 1990s, the USA and Canada dominated the production of magnesium; however, the industrial revolution in China in the late 1990s turned the tables for magnesium production due to its lower operational (energy and labor) costs. It is estimated that 85% of the global magnesium production is currently done by China, and most of the remainder is produced by Russia, Turkey, Spain, Austria, etc. [3].", "title": "Introductory Chapter: An Insight into Fascinating Potential of Magnesium", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Shao Jie-we"], "summary": "This paper reviewed the processing and application of magnesium-containing minerals in the last five years(2009\uff5e2014).The minerals include magnesite, dolomite, bischofite and so forth. The present products are magnesia, magnesium hydroxide, magnesium carbonate, magnesium chloride, magnesium sulfate and magnesium metal. Typical processing technologies for magnesiumcontaining minerals are carbonization, precipitation method and acidification method. The future development of processing technologies and products of magnesium-containing minerals are discussed.", "title": "Processing and Application of Magnesium Minerals", "claims": null}, {"metadata": {"year": 2002}, "authors": ["Imrich Ku\u0161n\u00edr"], "summary": "Introduction, magnesium compounds (magnesia) Magnesium is one of the Earth's most abundant elements. It occurs in some 60 minerals. However, only magnesite, dolomite brucite and olivine (chrysotile asbestos) are used commercially to produce the metal and its compounds, that are also extracted from seawater and lake brines. It is mostly used in the form of magnesia (MgO). Of the 8 Mt of magnesia produced each year, around 84% is derived from magnesite (MgCO3) by calcination (cf Clifford, 2000). The annual world production of magnesite for the last 10 years has been about 19 Mt. Leading producing countries have been China, countries of former Soviet Union (Russia, Kazakhstan, etc.), North Korea, Austria and Slovakia. Of 2,11 Mt magnesia was produced in 1991 from seawater and brines, most came from USA and Japan (Coope, 1992). Because of high melting point, most magnesia (around 80%) is used for the high-temperature insulation, especially as refractory bricks in the furnace lining. The remainder is consumed in agriculture (fertiliser, animal feed), industry (paper processing, chemicals), construction (cement), environmental and other applications. Magnesium metal: use, supply- demand relationships, outlook Magnesium metal is used for several physical properties, especially a very low relative density, strenght, and corrosion resistance. (Magnesium alloys are 75% lighter then steel and 30% lighter then aluminium alloys). Its castability is also important. The metal and its alloys are easy to structure, they can be joined and welded. It is used mainly as an alloy with aluminium, in transportation equipments (particularly in vehicle and jumbo aircraft manufacture), in the industrial machinery (some low-weight equipments). Other uses include chemicals (as a component of some organic chemicals and petrochemicals), desulphurisation, and reducing agents in the production of nonferrous metals. The main outlet for magnesium is alloying wih aluminium and die-casting; their share in 1998 was respectively 43% and 31% of the produced metal. Magnesium metal is costly to produce. The mostly used technology for its production is the electrolytic process, whe the metal is produced from magnesium chloride. (Production of a pure anhydrous MgCl2 is the key step). If the feed is magnesite, the latter is ground before leaching in hydrochloric acid. Then the pregnant leach liquor, containing the magnesium brine, is purified and dehydrated to a solid salt which is electrolysed to produce molten magnesium. Production costs have tended to be lower from seawater than from brines or magnesite. Until relatively recently, the use of magnesium metal have been quite limited. In 1973, world produced 261 000 short tons of the metal (Chin, 1975), in 1990, 355 000 t, with the US production representing 37.5% and the CIS/USSR 25.4% (Ridgway, 1992). Since 1995, the production has grown steadily, reaching 452 000 t last year (see table below, mostly after Clifford, 2000).", "title": "Magnesium metal, its perspectives", "claims": null}, {"metadata": {"year": 2011}, "authors": ["M. Go\u0142\u0105bczak"], "summary": "Magnesium (Mg) is one of the most abundant structural metals on the earth. Magnesium resources are estimated on approximately 1,93% of mass of the earth\u2019s crust and 0,13% of mass of the oceans. Magnesium is present in salt water in form of chlorides (in amount of approximately 1,2 kg/ m3), however in earth crust in form of dolomites composed mainly from carbonates. Magnesium belongs to ultra light metals (1,75 g/ cm3), has silver glossy colour, is soft and ductile, easily reacts chemically with other substances (e.g.: oxygen, nitrogen, carbon dioxide or water). Unfortunately, magnesium has a lot of undesirable properties such as poor corrosion and wear resistance, what limits its use in many usages especially for outdoor applications. For this reason pure magnesium is rarely used in technique, however with other metals (e.g.: aluminum, zinc, manganese, cerium, zirconium and rare earth metals) forms alloys, which are very attractive constructional material. Because of this magnesium alloys found a plethora of applications in various branches of industry where reduction in weight is of importance (Gray & Luan, 2002). These alloys are used in aerospace, automobile (Kawalla et al., 2008) and electronic industries, for manufacturing of sporting goods, high-speed boats, submarines, household equipment, etc. (Fig. 1). The main advantages of magnesium alloys are: the high strength, weight ratio, high thermal conductivity, small heat extensibility, good welding characteristics and high functional integrity, which allow to produce near-net-shape elements as well as good machinability (Hawkins, 1993). However, magnesium alloys have also certain disadvantages. The most troublesome of them is the high susceptibility to corrosion (especially galvanic corrosion), which contributes to dwindling of their size and reduces mechanical durability. As to protect magnesium alloys from corrosion, at present various methods for the fabrication of protective films have been used (Ishizaki et al., 2009). Other disadvantages of magnesium alloys comprise their weak wear resistance, a drop in durability at high temperature and interference of electromagnetic field. The aforementioned faults considerably reduce the area of application of this material. Presented studies aimed at elimination of the listed drawbacks by means of covering of magnesium alloy with special carbon coatings. Plasma Activated Chemical Vapor Deposition (PACVD) method has been used for this purpose. Optimum conditions of this process have been determined and the material properties of the carbon coatings characterized.", "title": "Estimation of Carbon Coatings Manufactured on Magnesium Alloys", "claims": null}, {"metadata": {"year": 1963}, "authors": ["Iain  MacIntyre"], "summary": "Magnesium is plentiful and makes up 2.1 per cent of the earth\u2019s crust [I]. It occurs in the insoluble form in large masses as magnesite, MgC03, and as dolomite, MgC03.CaC03. It is also present in less plentiful minerals such as asbestos (a calcium magnesium silicate), and in meerschaum, serpentine and talc, which are also silicates. In the soluble form it is present as carnallite, kainite, schijnite and kieserite. Sea water and some spring waters contain magnesium in solution. Magnesium is usually classified with the alkaline earth metals (Group II A of the Mendeleef classification), although in many ways it has closer resemblance to zinc and cadmium (Group II B). It is a light, white metal of atomic number 12 and atomic weight 24.32. It is extremely reactive and will burn in air, carbon dioxide or the halogens. Like the other metals of the alkaline earths, magnesium readily forms divalent ions, losing its two valency electrons, leaving the ion with the electronic shell of the inert gas (neon, in the case of magnesium) two units lower in atomic number. Ions of higher valency are therefore not known. The general chemical reactions of magnesium are too well known to need description.", "title": "An outline of magnesium metabolism in health and disease\u2014A review", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Changxu  Shi"], "summary": "Magnesium is one of themost abundantmetal elements in the Earth. Given that each cubic meter sea water contains over 1 kg elemental Mg and ten thousand billions of tons in total, we can say that Mg is virtually inexhaustible. Magnesium, with its very rich reserves within the Earth, is becoming an important engineering material. Due to its low density,w1.74 g/cm, which is only 1/4 of steel and 2/3 of Al, magnesium has a higher specific strength and stiffness than many other engineering materials, including aluminum, steel and polymer-based composites. Magnesium also offers many other attractive properties, such as a high damping capacity, electromagnetic shielding, thermal conductivity, and good machinability and recyclability. The need for light-weight materials, especially in the automobile industry, aeronautical and space industry, 3C (computer, communication and consumer) products and transportation, created renewed interest in innovative applications of magnesium materials. This demand has resulted in increased research and development activityworldwide in order to achieve an improved property profile and better choice of alloy systems. Last 10 years has seen enormous research institutes, companies and enterprises engaged in this burgeoning and potential industry and significant progress made on the science, technology and application of magnesium and its alloys. In this background, the Journal of Magnesium and Alloys (JMA) (ISSN: 2213-9567) is founded. The JMA publication is supported and advised by National Engineering Research Center for Magnesium Alloys, Chongqing University of China, and Chinese Committee ofMagnesium and Its Applications (CCMA, headed by Prof Fusheng Pan), and Elsevier is granted the full and exclusive production and hosting rights for all or any part of the Journal for the Electronic Edition. An agreement has been reached to launch its 1st issue in March this year and all articles can be access freely in ScienceDirect and be indexed by Scopus. The Journal is at present the only one specialized in the field of magnesium science and engineering. It will cover all aspects of magnesium and its alloys, from raw magnesium smelting, alloy development, casting, forming, joining, heat treatment, surface treatment, microstructure and properties evaluation, product", "title": "Preface from Honorary Editor in Chief", "claims": null}], "query": "magnesium malate", "summary_abstract": "The research question regarding magnesium malate reveals several important insights into its composition, benefits, and applications. Magnesium malate is a compound formed from magnesium and malic acid, which plays a crucial role in energy production and muscle function. Batta (2017) highlights that magnesium is a vital cofactor in over 300 enzyme systems, essential for processes such as protein synthesis, muscle and nerve function, and energy production. Specifically, magnesium malate has been clinically shown to soothe and energize muscle cells, making it potentially beneficial for individuals suffering from fatigue, such as those with fibromyalgia.\n\nThe abstracts collectively emphasize the significance of magnesium in various biochemical processes and its role in maintaining overall health. Batta (2017) notes that magnesium is necessary for ATP synthesis, which is critical for energy production in the body. This aligns with the understanding that magnesium malate can enhance energy levels and muscle function, particularly in conditions characterized by fatigue.\n\nWhile other abstracts provide a broader context about magnesium's abundance and extraction methods, they do not directly address magnesium malate's specific benefits or applications. Therefore, the key takeaway is that magnesium malate is a valuable compound that may aid in alleviating fatigue and supporting muscle health, particularly in individuals with specific health conditions (Batta, 2017).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2002}, "authors": ["Gordon  Boyce", "Simon  Ville"], "summary": "Firms may develop in a variety of ways. The simplest type of firm conducts a single function to provide an individual product or service from one site. Thus, a firm can grow geographically (production at more than one site), by scale (horizontal integration to produce more of the same product), by scope (diversification into other products) and by function (vertical integration of sequential activities such as production and distribution). As firms grow they often develop legally (additional statutory rights through incorporation) and organisationally (new structures and procedures). Figure 1.1 indicates these various directions of growth and internal development. Growth can, of course, be multidirectional and multinational. It may also be concentrated upon fewer products, functions or sites than previously in a process known as specialisation. Growth can occur by internal initiative within the firm, by acquisition of another company, or by a cooperative venture with another business. Finally, growth can also be negative when firms decide to reduce their activities or \u2018downsize\u2019. Indeed, the majority of businesses fail and disappear (sadly the loss of most records makes it difficult to discover why). Finally, most firms begin and remain small (perhaps 80 to 90 per cent of firms in industrialised countries are classified as small or medium-sized enterprises). The principal task of the business historian, therefore, is to analyse why and how a minority of enterprises grow larger.", "title": "History and Theory of the Growth of the Firm", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Alex Coad"], "summary": "Strategies for firm growth vary in terms of their degrees of novelty, uncertainty, and synergy. Modes of firm growth include replication (growth by \u2018more of the same\u2019), diversification, and internationalization. Growth strategies can be implemented using organic growth or through acquisitions. Desire to grow is a necessary but insufficient condition for growth \u2013 what also counts is the availability of growth opportunities. Empirical work has shown that growth is largely random \u2013 hence, hard to predict. Sustained growth is rare. Firms cannot always translate their ambitions into growth, but should pay attention to critical \u2018decision points.\u2019", "title": "Strategies for Firm Growth", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Mike W. Peng", "Sergey  Lebedev", "Cristina O. Vlas", "Joyce C. Wang", "Jason S. Shay"], "summary": "Starting with Peng and Heath (Academy of Management Review, 21: 492\u2013528, 1996), the growth of the firm in emerging economies (EE) has received increasing attention in the literature in the last two decades. This line of research has not only extended our knowledge on firms\u2019 strategic choices to the context of EE, but also proposed new perspectives on the growth of the firm. Leveraging prior research, this article focuses on three major modes for firm growth\u2014organic, acquisitive, and network-based. For each mode, we identify new themes and insights emerging from the last two decades of research. They center on (1) compositional capabilities and frugal innovations for organic growth, (2) business groups and cross-border acquisitions for acquisitive growth, and (3) network capitalism and institutional transitions for network-based growth. Overall, we not only identify new themes and insights, but also outline important yet unresolved debates as future research directions.", "title": "The growth of the firm in (and out of) emerging economies", "claims": null}, {"metadata": {"year": 1996}, "authors": ["J. Arbaugh", "D. L. Sexton"], "summary": "A study of 323 new ventures in Ohio supports Reynolds\u2019 findings of an earlier study in Pennsylvania, namely: firms tend to follow one of four different growth patterns, only one of which includes significant growth; most firms do not experience growth beyond their first year\u2019s sales; and there are no significant correlations between age of the firm and annual sales. The results support a new model of firm growth which empirically invalidates the metamorphic growth model. The study concludes by identifying significant differences in operational and strategic emphases between the development patterns.", "title": "NEW FIRM GROWTH AND DEVELOPMENT: A REPLICATION AND EXTENSION OF REYNOLDS\u2019 RESEARCH", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Alex Coad"], "summary": "Firm growth is a central topic in entrepreneurship research. The approach taken here is to survey the literature on lumps and discontinuities in the growth process, which may well correspond to the most interesting cases of firm growth. Our interdisciplinary review investigates stages of growth models, growth modes, growth strategies, and barriers and thresholds that hinder the growth of firms. Firm growth is theorized to be characterized by the addition and reconfiguration of lumpy discrete resources. The distribution of annual growth rates follows a heavy-tailed distribution, which has led to interest in growth paths, growth spells, growth spikes, and the phenomenon of High-Growth Firms (HGFs). We conclude with implications for empirical and theoretical research.", "title": "Lumps, Bumps and Jumps in the Firm Growth Process", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Jamie  Rogers"], "summary": "Industry sectors are generally defined through the products they create or the services on offer. Industries are also identified by their response to economic and business cycles, typically defined as growth, defensive or cyclical. The industry life cycle considers an industry\u2019s viability over time, with the four stages of pioneer, growth, mature and decline indicating a phase in an industry\u2019s evolution.", "title": "A Growth Firm", "claims": null}, {"metadata": {"year": 2007}, "authors": ["Alex Coad"], "summary": "We survey the phenomenon of the growth of firms drawing on literature from economics, management and sociology. We begin with a review of empirical \"stylised facts\" before discussing theoretical contributions. Firm growth is characterized by a predominant stochastic element, making it difficult to predict. Indeed, previous empirical research into the determinants of firm growth has had a limited success. We also observe that theoretical propositions concerning the growth of firms are often amiss. We conclude that progress in this area requires solid empirical work, perhaps making use of novel statistical techniques.", "title": "Firm Growth : a Survey", "claims": null}, {"metadata": {"year": 2011}, "authors": ["C. Keen", "H. Etemad"], "summary": "This paper explores the topic of high-growth phenomenon and its agents. High-growth enterprises are primarily small and medium-sized firms that attain very high-growth rates for at least five years. The paper presents a review of the pertinent literature to guide its formulation of hypotheses concerning the theoretical relations between high-growth and location, size and temporal characteristics of the high-growth enterprises. Using non-parametric tests, we analyse a 21-year longitudinal database of privately held rapidly growing enterprises from the USA. This analysis indicates that these firms are relatively smaller enterprises and their high growth rates are not restricted to a particular location, industrial region, size or time period. The findings of this analysis point to a population of high-growth enterprises with diverse locations, sizes and times with important implications for scholarly research and public policy analysis and formulation.", "title": "Rapidly-growing firms and their main characteristics: a longitudinal study from the USA", "claims": null}], "query": "Firm growth types", "summary_abstract": "The research on firm growth types reveals a multifaceted landscape characterized by various strategies and patterns. Boyce and Ville (2002) outline several growth dimensions, including geographical expansion, scale (horizontal integration), scope (diversification), and function (vertical integration). They emphasize that growth can be both positive and negative, with many firms ultimately downsizing or failing.\n\nCoad (2013) further categorizes growth strategies into replication, diversification, and internationalization, highlighting that while the desire for growth is essential, the availability of opportunities plays a crucial role. He notes that growth is often random and difficult to predict, with sustained growth being a rarity.\n\nPeng et al. (2018) focus on three primary modes of firm growth: organic, acquisitive, and network-based. They discuss how these modes are influenced by factors such as compositional capabilities and institutional transitions, particularly in emerging economies.\n\nArbaugh and Sexton (1996) identify four distinct growth patterns among new ventures, revealing that most firms do not experience significant growth beyond their initial sales. This challenges traditional growth models and suggests a need for a nuanced understanding of firm development.\n\nCoad (2022) emphasizes the importance of understanding growth as a process marked by discontinuities and lumpy resource additions. He notes that high-growth firms (HGFs) exhibit unique growth paths and spikes, which are critical for both empirical and theoretical research.\n\nKeen and Etemad (2011) explore high-growth enterprises, primarily small and medium-sized firms, and find that their growth rates are not confined to specific locations or industries, indicating a diverse and dynamic nature of high-growth firms.\n\nCollectively, these studies illustrate that firm growth is not a linear process but rather a complex interplay of strategies, opportunities, and external factors, with significant variations in patterns and outcomes across different contexts.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2023}, "authors": ["Steve Yuwono", "Andreas Schwung"], "summary": "In the modern era of manufacturing automation, the integration of sensor technology into the system ensures that data acquisition and analysis from complex systems become more efficient than ever. With the support of such developments, artificial intelligence-powered control in industrial control domains gains popularity and enhances the traditional human-based PLC control, where the machines can monitor themselves, learn from the experience, and make their own decisions. However, despite advances in sensor technologies, there are some limitations of the current applications of sensors in industries, for instance, sensors for observing the current status of the system often provide Boolean output data instead of continuous output. Therefore, such limitation forms a low dimensional state representation of the system, which could be problematic to develop a self-control policy, e.g. using a model-free deep reinforcement learning. In this paper, we present an effective model predictive controller with adaptive PLC-based policy on low dimensional state representation specifically for industrial control domains. First, we learn the model of the production system using the deep learning method to get the representation of the system dynamics, in case its digital representation is not available. Second, we set up a native implementation of model predictive control. Third, we enhance the model predictive control with adaptive PLC-based policy. The proposed method is implemented into a bulk good system showing its potential to self-optimize the system by satisfying the production objective without overflow and low power consumption.", "title": "Model Predictive Control with Adaptive PLC-based Policy on Low Dimensional State Representation for Industrial Applications", "claims": null}, {"metadata": {"year": 2021}, "authors": ["P. Bellini", "D. Cenni", "A. I. Palesi", "P. Nesi", "G. Pantaleo"], "summary": "Predictive Maintenance has gained more and more research and commercial interests, being a pivotal topic for improving the efficiency of many production industrial plants to minimize downtimes, as well as to reduce operational costs for interventions. Solutions reviewed in literature are increasingly based on machine learning and deep learning methods for prediction of fault proneness with respect to normal working conditions. Many state-of-the art solutions are not actually applied in real scenarios, and have restrictions to be executed in real-time in the production environment. In this paper, a framework for predictive maintenance is presented. It has been built upon a deep learning model based on Long-Short Term Memory Neural Networks, LSTM and Convolutional LSTM. The proposed model provides a one-hour prediction of the plant status and indications on the areas in which the intervention should be performed by using explainable LSTM technique. The solution has been validated against real data of ALTAIR chemical plant, demonstrating an high accuracy with the capability of being executed in real-time in a production operative scenario. The paper also introduced business intelligence tools on maintenance data and the architectural infrastructure for the integration of predictive maintenance approach.", "title": "A Deep Learning Approach for Short Term Prediction of Industrial Plant Working Status", "claims": null}, {"metadata": {"year": 2020}, "authors": ["D. Karimanzira", "T. Rauschenbach"], "summary": "Reverse Osmosis (RO) desalination plants are highly nonlinear multi-input-multioutput systems that are affected by uncertainties, constraints and some physical phenomena such as membrane fouling that are mathematically difficult to describe. Such systems require effective control strategies that take these effects into account. Such a control strategy is the nonlinear model predictive (NMPC) controller. However, an NMPC depends very much on the accuracy of the internal model used for prediction in order to maintain feasible operating conditions of the RO desalination plant. Recurrent Neural Networks (RNNs), especially the Long-Short-Term Memory (LSTM) can capture complex nonlinear dynamic behavior and provide long-range predictions even in the presence of disturbances. Therefore, in this paper an NMPC for a RO desalination plant that utilizes an LSTM as the predictive model will be presented. It will be tested to maintain a given permeate flow rate and keep the permeate concentration under a certain limit by manipulating the feed pressure. Results show a good performance of the system.", "title": "Deep Learning Based Model Predictive Control for a Reverse Osmosis Desalination Plant", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Sa\u00fal Langarica", "F. N\u00fa\u00f1ez"], "summary": "Classical model-based control techniques used in process control applications present a tradeoff between performance and computational load, especially when using complex nonlinear methods. Learning-based techniques that allow the controller to learn policies from data represent an appealing alternative with potential to reduce the computational burden of real-time optimization. This article presents an efficient learning-based neural controller, optimized using evolutionary algorithms, designed especially for maintaining diversity of individuals. The search of solutions is conducted in the parameter space of a population of deep neural networks, which are efficiently encoded with a novel compression algorithm. Evaluation against strong baselines demonstrates that the proposed controller achieves better performance in most of the chosen evaluation metrics. Results suggest that learning-based controllers are a promising option for next-generation process control in the context of Industry 4.0.", "title": "Neuroevolutive Control of Industrial Processes Through Mapping Elites", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Martin Cimmino", "Matteo Calabrese", "D. Kapetis", "S. Lygren", "D. Vanzan", "M. Piantanida", "S. Carminati", "G. Esposito", "L. Raimondi", "A. Amendola"], "summary": "\n \n \n Condition-based monitoring is a key element to minimize plant upsets and production losses, guaranteeing at the same time safety and asset integrity, with the final goal of improving operational excellence. A key challenge for this purpose is the capability to anticipate unexpected behaviors, such as undesired trends or spikes in key sensor measurements, such as temperatures or vibrations, which might lead to equipment failures. For this goal, we implemented some innovative Deep Learning algorithms to predict the future trend of sensor variables related to the health condition of important pieces of equipment and subsystems of a big Eni's offshore facility. We showed that the prediction accuracy achieved by Deep Learning algorithms makes them ideal candidates in a real production setting.\n \n \n \n We present a multi-step Deep Learning pipeline consisting of: (a) Multivariate time-series resampling (b) model design and (c) model evaluation. Ten-months of historical sensor data were split into training and test sets (80%-20%) in chronological order. Two Deep Learning (DL) models were implemented: (a) sequence-to-sequence (seq2seq) LSTM (Long-Short Term Memory) encoder-decoder with and without attention-based mechanism (LSTM-EDA) (b) seq2seq temporal convolutional network (TCN). Many-to-one (output value predicted from multiple input values) and many-to-many (multiple outputs predicted from multiple input values) models were implemented using several prediction output windows (w = {4,16,32} time steps) as prediction horizons. Models were compared and evaluated with Mean Absolute Error, Root Mean Square error and R-squared metrics.\n \n \n \n To test whether training on forecast sensor data is beneficial, each model was trained on measured data and also on forecast data, with the forecasting horizon being from 4 up to 32 time steps, i.e. 8 to 48 hours. To evaluate the forecasting performance, we calculated the root mean square error (RMSE) and MAE between the actual values and predicted values. The RMSE and MAE can reflect the discrepancy between the actual values and predicted ones, while the R-squared can represent the trend accuracy of each output window between the actual data and predicted data. Overall, DL models (LSTM-EDA and TCN) performed with high accuracy showing very low MAE errors (0.03 and 0.02), RMSE (0.05 and 0.03) and R-squared (0.94 and 0.85). With reference to the width of the output horizon (w= {4,16, 32}) the model showed that the larger the horizon, the harder the prediction task and model accuracy. The MAE and RMSE of the trained DL models increases with the forecasting horizon because predicted output signals are less accurate for larger forecasting horizons. In addition, we showed that both models work well when predicting signal trends, while sudden signal spikes remain hard to predict with the same accuracy.\n \n \n \n Multivariate time-series forecasting faces a major research challenge to capture and leverage the dynamics dependencies among multiple variables. DL is by far the most promising technology on this area. Recent developments on DL model offer a clear advantage to summarize data with many abstraction layers. By in-depth analysis and empirical evidence, we showed the efficiency of the architecture of LSTM-EDA and TCN models, which indeed successfully captures both short-term and long-term repeating patterns and uses them to effectively forecast the future values of one or more sensor signals.\n", "title": "Predictive Modelling and Monitoring of Industrial Equipment Through Advanced Deep Learning Algorithms", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Gongming Wang", "Q. Jia", "J. Qiao", "J. Bi", "Mengchu Zhou"], "summary": "A continuous stirred-tank reactor (CSTR) system is widely applied in wastewater treatment processes. Its control is a challenging industrial-process-control problem due to great difficulty to achieve accurate system identification. This work proposes a deep learning-based model predictive control (DeepMPC) to model and control the CSTR system. The proposed DeepMPC consists of a growing deep belief network (GDBN) and an optimal controller. First, GDBN can automatically determine its size with transfer learning to achieve high performance in system identification, and it serves just as a predictive model of a controlled system. The model can accurately approximate the dynamics of the controlled system with a uniformly ultimately bounded error. Second, quadratic optimization is conducted to obtain an optimal controller. This work analyzes the convergence and stability of DeepMPC. Finally, the DeepMPC is used to model and control a second-order CSTR system. In the experiments, DeepMPC shows a better performance in modeling, tracking, and antidisturbance than the other state-of-the-art methods.", "title": "Deep Learning-Based Model Predictive Control for Continuous Stirred-Tank Reactor System", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Abdeljalil Hajjoubi"], "summary": "The aim of the thesis is to build a system of data collection and analysis, through the integration of a distributed storage and processing system and machine learning techniques, at a small scale. A dataset of industrial data is available for predictive control analysis. Specifically, the dataset contains historical time series of several parameters regarding a manufacturing process, annotated with breakdowns. Our goal is the develop classification strategies that identifies the conditions that cause a breakdown, exploiting Machine Learning tools. Hadoop and TensorFlow, or analogous softwares could be used.", "title": "A Machine Learning approach to Predictive Control: study on a real industrial application", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Ghadeer I. Maki", "Z. M. Hussain"], "summary": "Classically industrial systems apply a number of techniques to control their components, including the control system, which modify the relationship between input and output signals to configure the system to provide the required response. In most practical systems these signals are continuous, hence it is important to convert them into digital signals to be processed by digital systems. Despite the great development in technology, given the importance of the control system in relation to dynamic systems to achieve optimal performance, but classical control suffers from some important problems. The complexity of the control system represented by the program implementation algorithms and the loss of most information during the process of converting the system to digital and not adapting to external variables or with new updates. In this research classical control is replaced by deep neural networks, which is a thriving field with practical and medical applications and is characterized by its ability to learn and train as it is a branch of machine learning and artificial intelligence. The results proved that the functioning of the neural networks and their performance is similar to classical control systems, with the advantage of simplicity and adaptability.", "title": "Deep Learning for Control of Digital Systems", "claims": null}], "query": "Deep Learning for Predictive Control in Industrial Plants", "summary_abstract": "The integration of deep learning techniques into predictive control for industrial plants has garnered significant attention in recent research, showcasing promising advancements in efficiency and adaptability. Several studies highlight the potential of deep learning models to enhance traditional control systems, particularly in complex environments characterized by nonlinearity and uncertainty.\n\nYuwono and Schwung (2023) present a model predictive controller that leverages deep learning to create a representation of system dynamics, addressing the limitations of low-dimensional state representations in industrial control. Their approach demonstrates the ability to self-optimize production systems, achieving objectives such as minimizing overflow and reducing power consumption.\n\nIn the realm of predictive maintenance, Bellini et al. (2021) emphasize the role of deep learning, specifically Long-Short Term Memory (LSTM) networks, in forecasting plant status and identifying areas requiring intervention. Their framework, validated in a real-world chemical plant, showcases high accuracy and real-time applicability, underscoring the effectiveness of deep learning in operational settings.\n\nKarimanzira and Rauschenbach (2020) explore the use of LSTM networks within a nonlinear model predictive control (NMPC) framework for reverse osmosis desalination plants. Their findings indicate that LSTM can effectively capture complex dynamics and provide reliable long-range predictions, essential for maintaining operational stability.\n\nLangarica and N\u00fa\u00f1ez (2021) introduce a learning-based neural controller optimized through evolutionary algorithms, which demonstrates superior performance compared to classical methods. This research suggests that learning-based approaches can significantly reduce computational burdens while enhancing control performance in industrial applications.\n\nCimmino et al. (2020) implement deep learning algorithms for condition-based monitoring, successfully predicting sensor trends and identifying potential equipment failures. Their multi-step deep learning pipeline illustrates the capability of these models to handle multivariate time-series data, further supporting the case for deep learning in predictive control.\n\nWang et al. (2020) propose a deep learning-based model predictive control (DeepMPC) for continuous stirred-tank reactors, demonstrating its effectiveness in system identification and control. Their results indicate that DeepMPC outperforms traditional methods in terms of modeling accuracy and disturbance handling.\n\nOverall, the collective findings from these studies indicate that deep learning is a transformative force in predictive control for industrial plants, offering enhanced modeling capabilities, real-time performance, and adaptability to complex operational challenges. The integration of these advanced techniques not only improves efficiency but also paves the way for smarter, more autonomous industrial systems.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2005}, "authors": ["Andrea Biasion", "Leandro Bornaz", "F. Rinaudo"], "summary": "The recent upgrading of laser scanning devices has led to a set of new surveying techniques for civil engineering and environmental analysis. The terrestrial laser scanner allows complete and dense 3D digital models of the surface of any object to be reconstructed. This is very useful for natural hazards and risk assessment where morphological investigation is a starting point to evaluate stability properties. Field acquisitions, with laser scanner devices are, in addition, very fast and they are made in safe conditions. However, particular attention must be paid during the acquisition, processing and modelling phases. LIDAR data often include elevated noise (usually gross errors and outliers) that has to be removed with opportune techniques before starting the 3D registration and modelling. In addition, 3D contexts usually have very complex shapes that cannot be recorded with sufficient resolution with a single scan. For this reason, two or more scans must be taken from different points of view of the same object, in order to eliminate shaded areas. To obtain the final 3D model of the object it is therefore necessary to align and geo-reference the single scans using suitable registration techniques. The integration of DDSM geometric data (e.g. the point cloud acquired by using the laser scanner) with image radiometric data allows a new concept of the Solid Image to be obtained. Solid images can be used in many surveying applications to determine, in real time, the position of any point in a 3D reference system, using a normal PC or to carry out correct three-dimensional measurements (lines, areas, volumes, angles, ...), by just selecting some points on the image. In addition, with the Solid Image, even unskilled users can easily plot profiles, sections and plans using simple drawing functions, and can generate stereo models and realistic 3D models.", "title": "Applicazioni ambientali delle tecniche Laser Scanner terrestri", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Shawn  Russell"], "summary": "Light Detection and Ranging (LIDAR) is a surveying tool that utilizes an optical remote sensing technology to measure properties of scattered light to determine range and other information about a target. LIDAR allows highly accurate 3D (x, y and z) measurements to be taken. Collectively, these data points are called cloud points and are among a multitude of data collected using lasers. The lasers are emitted from the LIDAR unit in pulses that contact multiple objects. The amount of time it takes the laser beam to return from the object to the sensor is used to calculate range distance for a point on an object. In addition to lasers, the unit uses the Global Positioning System (GPS) and Inertial Navigation Systems (INS) to give a geospatial component to the data. LIDAR has its roots in the 1960s and 1970s when laser remote sensing instruments were employed for lunar laser ranging, satellite laser ranging and oceanographic atmospheric research. LIDAR was initially ground-based or terrestrial being fixed in one place. In the 1980s, GPS made airborne scanning possible and later gave way to mobile scanning. While LIDAR has been around for more than 40 years, technological advances have made it more commercially available and users everywhere are imagining new uses for this technology. Transportation related industries account for a small portion of the potential uses of LIDAR. However, the potential uses for LIDAR as a whole are monumental. LIDAR can be used to collect more surveying data in an hour than traditional surveying methods could yield over the course of several months. Traditionally, a crew of surveyors would take cross sections every 25 feet for an existing or proposed roadway using at least one person to hold the rod, one to use the instrument and one to document. More crew members might be required if terrain conditions made a clear line of sight difficult. Total stations eventually made taking these shots quicker, but time was required to interpret the data giving it shape in the form of roadway or cross section plans. Mobile LIDAR scans taken from a moving platform can make a single pass of a roadway or waterway collecting data at the rate of 400,000 points per second covering every viewable surface. The amount of data collected by a mobile scan during a one hour period can easily approach a terabyte of computer data sending users scrambling for faster computers and bigger hard drives. Traditional methods are unable to match this amount of data. Airborne and mobile LIDAR units are matched with a Global Navigation Satellite System that connects to the GPS and an Inertial Measurement Unit that is used to cover during the lapse of GPS satellite coverage. These positional systems provide geospatial data that allow point clouds with known horizontal and vertical control points set by a surveyor to be tied down and overlaid with adjacent point clouds to provide more complete coverage of a roadway or waterway. Mobile scans have a 360 degree field of view and collect a swath of data points viewable from the mounted height of the unit on the moving platform for a given roadway or waterway. Recorded data points are limited to what the mobile unit can view. For example, locations behind a median barrier wall will not be viewable on a first pass; therefore, a second pass in the opposite direction may be required to get a complete set of data points for that roadway. In the case of the Interstate 65 bridge over the Watterson Expressway (see above picture) a mobile scan from the Watterson Expressway provides cloud data for the underside of the bridge while a mobile scan from I-65 will provide cloud data for the top of the bridge. Combined point clouds can be utilized by computer software to provide a complete 3D model of that structure. Additionally, the mobile scan from the Watterson Expressway can be combined with an airborne scan. LIDAR can become an essential tool for you. The possible applications for your job are just waiting to be discovered. Use of a living point cloud file (that is regularly updated), similar to KYTC\u2019s photo log viewer, provides highly accurate information that becomes more cost effective every time it is utilized. Explore this interactive 3D model of I-60 in Texas developed from mobile and airborne LIDAR scans: http://www.certainty3d.com/pdf/ models/TxDOTModel.pdf . by Shawn Russell PE, AVS Possible Applications", "title": "Light Detection and Ranging", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Rajnish  Chauhan"], "summary": "Light Detection and Ranging (LIDAR) is a surveying tool that utilizes an optical remote sensing technology to measure properties of scattered light to determine range and other information about a target. LIDAR allows highly accurate 3D (x, y and z) measurements to be taken. Collectively, these data points are called cloud points and are among a multitude of data collected using lasers. The lasers are emitted from the LIDAR unit in pulses that contact multiple objects. The amount of time it takes the laser beam to return from the object to the sensor is used to calculate range distance for a point on an object. In addition to lasers, the unit uses the Global Positioning System (GPS) and Inertial Navigation Systems (INS) to give a geospatial component to the data. LIDAR has its roots in the 1960s and 1970s when laser remote sensing instruments were employed for lunar laser ranging, satellite laser ranging and oceanographic atmospheric research. LIDAR was initially ground-based or terrestrial being fixed in one place. In the 1980s, GPS made airborne scanning possible and later gave way to mobile scanning. While LIDAR has been around for more than 40 years, technological advances have made it more commercially available and users everywhere are imagining new uses for this technology. Transportation related industries account for a small portion of the potential uses of LIDAR. However, the potential uses for LIDAR as a whole are monumental. LIDAR can be used to collect more surveying data in an hour than traditional surveying methods could yield over the course of several months. Traditionally, a crew of surveyors would take cross sections every 25 feet for an existing or proposed roadway using at least one person to hold the rod, one to use the instrument and one to document. More crew members might be required if terrain conditions made a clear line of sight difficult. Total stations eventually made taking these shots quicker, but time was required to interpret the data giving it shape in the form of roadway or cross section plans. Mobile LIDAR scans taken from a moving platform can make a single pass of a roadway or waterway collecting data at the rate of 400,000 points per second covering every viewable surface. The amount of data collected by a mobile scan during a one hour period can easily approach a terabyte of computer data sending users scrambling for faster computers and bigger hard drives. Traditional methods are unable to match this amount of data. Airborne and mobile LIDAR units are matched with a Global Navigation Satellite System that connects to the GPS and an Inertial Measurement Unit that is used to cover during the lapse of GPS satellite coverage. These positional systems provide geospatial data that allow point clouds with known horizontal and vertical control points set by a surveyor to be tied down and overlaid with adjacent point clouds to provide more complete coverage of a roadway or waterway. Mobile scans have a 360 degree field of view and collect a swath of data points viewable from the mounted height of the unit on the moving platform for a given roadway or waterway. Recorded data points are limited to what the mobile unit can view. For example, locations behind a median barrier wall will not be viewable on a first pass; therefore, a second pass in the opposite direction may be required to get a complete set of data points for that roadway. In the case of the Interstate 65 bridge over the Watterson Expressway (see above picture) a mobile scan from the Watterson Expressway provides cloud data for the underside of the bridge while a mobile scan from I-65 will provide cloud data for the top of the bridge. Combined point clouds can be utilized by computer software to provide a complete 3D model of that structure. Additionally, the mobile scan from the Watterson Expressway can be combined with an airborne scan. LIDAR can become an essential tool for you. The possible applications for your job are just waiting to be discovered. Use of a living point cloud file (that is regularly updated), similar to KYTC\u2019s photo log viewer, provides highly accurate information that becomes more cost effective every time it is utilized. Explore this interactive 3D model of I-60 in Texas developed from mobile and airborne LIDAR scans: http://www.certainty3d.com/pdf/ models/TxDOTModel.pdf . by Shawn Russell PE, AVS Possible Applications", "title": "Light Detection and Ranging", "claims": null}, {"metadata": {"year": 2005}, "authors": ["M. Abrams", "D. Tratt"], "summary": "Progress in laser sources for surface and atmospheric remote sensing is placed in the context of 3D mapping and imaging laser radar systems and ambitions for future high resolution mapping systems.", "title": "Progress in laser sources for lidar applications: laser sources for 3D-imaging remote sensing", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Aradhana Haldar", "A. Deshpande", "Pankaj Raibagkar"], "summary": "---------------------------------------------------------------------***--------------------------------------------------------------------Abstract \u2013 LIDAR (Light Detection and Ranging) is an optical remote detecting technology that measures properties of scattered light to discover extent and/or other data of a distant object. The pervasive methodology to focus separation to an item or surface is to utilize laser pulses. Like the radar technology, which uses radio waves, the range to an object is controlled by measuring the time defer between transmission of a pulse and acknowledgment of the reflected sign. LIDAR has applications in topography, geography, Geomantic, paleohistory, geomorphology, seismology, ranger service, remote detecting and air physical science. [1] Uses of LIDAR incorporate ALSM (Airborne Laser Swath Mapping), LIDAR Contour Mapping. The acronym LADAR (Laser Detection and Ranging) is regularly utilized as a part of military settings. The expression \"laser radar\" is likewise being utilized despite the fact that LIDAR does not utilize microwaves or radio waves, which is definitional to radar. Additionally, new innovations, for example, digital image processing and Global Positioning System (GPS) providing positional data inside of cm accuracy and Geographic Information Systems (GIS), and so on can be utilized to incorporate and procedure spatial information for applications running from basic mapping to supporting resource exploration and development; from environmental administration to the arranging and organization of transportation system, urban advancement, telecommunication system, and utility infrastructures. These advancements can successfully be utilized to handle the present day complex urban issues.", "title": "Tracking airborne objects using LiDAR", "claims": null}, {"metadata": {"year": 2018}, "authors": ["G. Bell", "Jeremy A. Sharp", "T. McAlpin", "A. R. Jackson", "G. Herring"], "summary": "INTRODUCTION: Lidar scanning is a process that collects high-resolution geometric, bathymetric, and topographic data. Lidar uses light pulses to conduct remote sensing and to collect high resolution data (typically referred to as a \u201cpoint cloud\u201d). Most scanners mount on a survey tripod or other fixed support structure and can rotate 360\u00b0 (referred to as a panoramic scan). The key component in the scanner is the polygon-shaped mirror that has flat reflective surfaces (mirror facets) arranged around a wheel perimeter to allow rotating or oscillating movement of the polygon mirror wheel (Figure 1). This allows for the measurement laser beam to be deflected, thus scanning multiple locations at once (RIEGL 2017). A brief review of lidar can be found in Jaboyedoff et al. 2010.", "title": "Laboratory techniques and procedures using lidar", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Bertrand Gaschot"], "summary": "Application of Lidar to 3D Structural Mapping", "title": "Application of Lidar to 3D Structural Mapping", "claims": null}, {"metadata": {"year": 2009}, "authors": ["\uc815\uba85\ud76c"], "summary": "Airbone laser altimeters have been utilized for 3D topographic mapping with high resolution and accuracy, which is a rapidly growing remote sensing technique that measures the round-trip time emitted laser pulse to determine the topography. The waveform processing algorithms to parameterize and model the return signal resulting from the interaction of the transmitted laser pulse with the surface is proposed in this paper, The method to extract 3D information from Lidar data such as canopy and buildings is also described for 3D visualization.", "title": "Lidar \uc790\ub8cc\uc758 \ubd84\uc11d\uacfc 3\ucc28\uc6d0 \uc815\ubcf4 \ucd94\ucd9c", "claims": null}], "query": "usage of 2d and 3d reflective markers/targets in civil and industrial lidar applications", "summary_abstract": "The usage of 2D and 3D reflective markers and targets in civil and industrial LIDAR applications is a significant area of focus in contemporary surveying and mapping technologies. The integration of LIDAR (Light Detection and Ranging) systems has revolutionized the way data is collected and processed, allowing for highly accurate 3D measurements and the creation of detailed digital models.\n\nBiasion et al. (2005) highlight the importance of using multiple scans from different perspectives to capture complex shapes and eliminate shaded areas, which is crucial for accurate 3D modeling. This process often involves aligning and geo-referencing individual scans to create a cohesive model. The authors also discuss the concept of \"Solid Images,\" which combine geometric and radiometric data, enabling users to perform real-time measurements and generate 3D models with ease.\n\nRussell (2008) emphasizes the efficiency of LIDAR technology, noting that mobile LIDAR systems can collect vast amounts of data\u2014up to 400,000 points per second\u2014far surpassing traditional surveying methods. This capability is particularly beneficial in civil engineering, where comprehensive data collection is essential for infrastructure projects. The integration of GPS and Inertial Navigation Systems (INS) with LIDAR enhances the geospatial accuracy of the collected data, allowing for the creation of detailed point clouds that can be combined to form complete 3D models of structures.\n\nChauhan (2017) reiterates the versatility of LIDAR, which can be applied in various fields, including topography and urban planning. The technology's ability to provide high-resolution data makes it invaluable for addressing complex urban issues and supporting resource management.\n\nGaschot (2019) and other authors also point out that LIDAR's application in 3D structural mapping is growing, with advancements in laser technology enabling more precise and efficient data collection. The use of reflective markers and targets can enhance the accuracy of these measurements, particularly in challenging environments.\n\nIn summary, the collective findings from these papers underscore the transformative impact of LIDAR technology in civil and industrial applications. The use of 2D and 3D reflective markers is integral to maximizing the accuracy and efficiency of data collection, ultimately leading to better-informed decision-making in various fields, from infrastructure development to environmental management.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2003}, "authors": ["Oliver Schneider", "N. Braun", "G. Habinger"], "summary": "$%675$&7 We describe an approach to a new authoring method for interactive storytelling. After positioning digital storytelling in a theoretical context to literature, we consider the author\u2019s view of the tools to be used and introduce a coherent environment that does not restrict the creative process and lets the author feel comfortable, leading him to create well-narrated, interactive non-linear stories. We describe the implementation of the story engine authoring module, which is followed by a project description.", "title": "Storylining Suspense - An Authoring Environment for Structuring Non-linear Interactive Narratives", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Llu\u00eds Any\u00f3 Sayol", "\u00c1ngel Colom Pons"], "summary": "We put forward a suitable analytical model for studying suspense in video games. This model is based on the analysis of perspective, focalization and the figure of the implied player, which is fundamental to understand the relationship between game, player and emotional effects. We critically review the previous research on point of view in films and video games with the aim of achieving a better understanding of audiovisual narration. The resulting model is a proposal for, in the first place, systematising the relationship between the player -considered here as a theoretical concept: the implied player- and the game. Once this is done, it allows us to study suspense in video games from a narrative perspective and leads us to the conclusion that in video games suspense is not related to a waiting situation \u2013like in films- but to the effort of overcoming difficulties that we know from a previous play.", "title": "Suspense mechanics in narrative video games", "claims": null}, {"metadata": {"year": 2015}, "authors": ["R. Doust"], "summary": "Many computational models of narrative have focussed on the structure of the narrative world. Such models have been implemented in a wide variety of systems, often linked to characters\u2019 goals and plans, where the goal of creating suspenseful stories is baked into the structure of each system. There is no portable, independently motivated idea of what makes a suspenseful story. \n \nOur approach is instead to take the phenomenon of suspense as the starting point. We extend an existing psychological model of narrative by Brewer and Lichtenstein (1982) which postulates suspense, curiosity and surprise as the fundamental elements of entertaining stories. We build a formal model of these phenomena using structures we call narrative threads. \n \nNarrative threads are a formal description of a reader\u2019s expectations about what might happen next in a given story. Our model uses a measure for the imminence of the predicted conflict between narrative threads to create a suspense profile for a given story. We also identify two types of suspense: conflict-based and revelatory suspense. \n \nWe tested the validity of our model by asking participants to give step- by-step self-reported suspense levels on reading online story variants. The results show that the normalised average scores of participants (N = 46) are in agreement with the values predicted by our model to a high level of statistical significance. \n \nOur model\u2019s interface with storyworld knowledge is compatible with recent developments in automatic harvesting of world knowledge in the form of event chains such as Chambers and Jurafsky (2008). This means that it is in principle scalable. By disentangling suspense from specific narrative content and planning strategies, we arrive at a domain-independent model that can be reused within different narrative generation systems. We see our work as a signpost to encourage the further development of narrative models based on what we see as its fundamental ingredients.", "title": "A domain-independent model of suspense in narrative", "claims": null}, {"metadata": {"year": 2022}, "authors": ["N. Horbach", "Sophi\u0430 Mishchenko"], "summary": "and interest that is created particularly with a forced delay of an expected solution. O. Bozko\u2019s theoretical theses were used in following research. According to them, the narrative tension is created with a strategical incompleteness which increases the interest", "title": "Suspense as a cinema-poetic element in V. Gabor\u2019s novella \u00abThe telegram\u00bb", "claims": null}, {"metadata": {"year": 2013}, "authors": ["H. Wulff", "Mike Friedrichsen"], "summary": "Suspense, \"a curious mixture of pain and pleasure\" (Barnet et al. 1971: 107), is viewed as a two-fold phenomenon having extratextual (experiential) and intratextual (narrative) dimensions (Vorderer et al. 1996: vii). Traditionally, narrative suspense has been regarded as resulting from such unfolding of the plot when the progress of events acquires a conflicting and alternative pattern. The latter triggers the readers\u2019 effort to navigate a challenging problem space for an important target outcome or to restore what is behind instances of narrative ellipsis. The emergent pattern makes the possibility of various outcomes salient while leaving it delayed (Gerrig 1993: 77-90). Accounting for the effect suspense entails has remained so far within the range of psychological and narratological quests. Probing into intricacies of literary narrative and verbal texture alone does not provide a sufficient ground for revealing the underlying mechanisms of suspense as a textual challenge. Given the basic assumptions of cognitive poetics (see Freeman 2000) and possible worlds theory in its narrative version (Pavel 1986; Ryan 1991; Dolezel 1998; Werth 1999), literary suspense can be viewed as an emotively charged emergent conceptual structure that results from (i) the conflict of mental spaces (cf. Fauconnier 1994) differing in their modality and/or (ii) the clash of alternative narrative subworlds involving the protagonist as a focalizer. The reconstructed conceptual integration network does not only account for the textual concept inferred but gives access to the mechanism of empathetic distress (Zillman 1996: 217), \"a sweet pain of anxiety\" (Brewer 1996: 108) experienced by the reader.", "title": "Conceptual Blending in Narrative Suspense: Making Pain of Anxiety Sweet", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Elaine Indrusiak"], "summary": "This article explores the construction of narrative suspense in Edgar Allan Poe\u2019s and Alfred Hitchcock\u2019s works. Central to their creations is a dual narrative structure that builds tension by articulating two stories in one. Narratological analyses of Poe\u2019s tales and of Hitchcock\u2019s thrillers indicate that their suspense does not stem from gothic topoi, but from plots skilfully wrought to manipulate the readers/viewers\u2019 attention. The first part of the article introduces the concept of narrative suspense. Next, structural principles defended by Poe and Hitchcock are presented. The last two parts discuss narrative techniques in the works by both authors. Given the authors\u2019 influence within their respective media, the analysis has implications for studies of the short story and of the suspense thriller as a narrative genre.", "title": "Narrative Suspense in Edgar Allan Poe and Alfred Hitchcock", "claims": null}, {"metadata": {"year": 2020}, "authors": ["David Wilmot", "Frank Keller"], "summary": "Suspense is a crucial ingredient of narrative fiction, engaging readers and making stories compelling. While there is a vast theoretical literature on suspense, it is computationally not well understood. We compare two ways for modelling suspense: surprise, a backward-looking measure of how unexpected the current state is given the story so far; and uncertainty reduction, a forward-looking measure of how unexpected the continuation of the story is. Both can be computed either directly over story representations or over their probability distributions. We propose a hierarchical language model that encodes stories and computes surprise and uncertainty reduction. Evaluating against short stories annotated with human suspense judgements, we find that uncertainty reduction over representations is the best predictor, resulting in near human accuracy. We also show that uncertainty reduction can be used to predict suspenseful events in movie synopses.", "title": "Modelling Suspense in Short Stories as Uncertainty Reduction over Neural Representation", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Pablo Delatorre", "C. Le\u00f3n", "Pablo Gerv\u00e1s", "M. Palomo-duarte"], "summary": "ABSTRACT Suspense is a key narrative issue in terms of emotional gratification, influencing the way in which the audience experiences a story. Virtually all narrative media uses suspense as a strategy for reader engagement regardless of the technology or genre. Being such an important narrative component, computational creativity has tackled suspense in a number of automatic storytelling. These systems are mainly based on narrative theories, and in general lack a cognitive approach involving the study of empathy or emotional effect of the environment impact. With this idea in mind, this paper reports on a computational model of the influence of decorative elements on suspense. It has been developed as part of a more general proposal for plot generation based on cognitive aspects. In order to test and parameterise the model, an evaluation based on textual stories and an evaluation based on a 3D virtual environment was run. In both cases, results suggest a direct influence of emotional perception of decorative objects in the suspense of a scene.", "title": "A computational model of the cognitive impact of decorative elements on the perception of suspense", "claims": null}], "query": "narrative suspense theory", "summary_abstract": "The research on narrative suspense theory reveals a multifaceted understanding of how suspense operates within storytelling across various media, including literature, film, and video games. A central theme is the distinction between different types of suspense and the mechanisms that generate it.\n\nDoust (2015) proposes a formal model of suspense that identifies two primary types: conflict-based and revelatory suspense. This model is grounded in psychological principles and emphasizes the importance of narrative threads\u2014expectations about future events in a story. The study found that suspense is closely linked to the reader's anticipation of conflict, which can be quantitatively measured, thus providing a scalable framework for narrative generation systems.\n\nIn the realm of video games, Any\u00f3 Sayol and Colom Pons (2017) argue that suspense arises not from mere waiting, as in traditional films, but from the player's active engagement in overcoming challenges. This perspective highlights the interactive nature of suspense in gaming, where the player's efforts directly influence their emotional experience.\n\nWulff and Friedrichsen (2013) delve into the cognitive aspects of suspense, suggesting that it emerges from the conflict of mental spaces and alternative narrative possibilities. They assert that suspense is not just a textual phenomenon but also involves the reader's emotional engagement, creating a complex interplay between narrative structure and reader experience.\n\nIndrusiak (2018) examines the works of Edgar Allan Poe and Alfred Hitchcock, noting that their suspenseful narratives are constructed through dual storylines that manipulate audience attention. This structural approach to suspense emphasizes the importance of narrative techniques in crafting tension.\n\nDelatorre et al. (2017) further explore the emotional dimensions of suspense, suggesting that environmental factors, such as decorative elements in a scene, can significantly impact the audience's experience of suspense. Their findings indicate that emotional perception plays a crucial role in how suspense is constructed and experienced.\n\nOverall, the literature collectively underscores that narrative suspense is a complex interplay of structural, cognitive, and emotional elements, with implications for both traditional storytelling and interactive media. The models and theories presented provide valuable insights into how suspense can be effectively generated and manipulated to enhance audience engagement across various narrative forms.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2015}, "authors": ["Ramlan Ramlan", "Nahrowi Nahrowi"], "summary": "Hal\u00e2l Certification as an Application of Islamic Business Ethics in Muslim Consumer Protection Hal\u00e2l certification in food is needed by Muslim consumers to provide assurances that they are not consuming or using product or service that are not hal\u00e2l so that no sin is commited. It is the duty of government to provide assurances to Muslim consumers that consumer goods or services that are circulating amongst society are really hal\u00e2l. The hal\u00e2l label itself provides benefits to businesses such as improving consumer confidence to consume, reaching the global hal\u00e2l food market, increasing the marketability of products in market and low cost investment. Muslim consumers also have to be smart before purchasing product or services for security and safety. DOI:\u00a0 10.15408/ajis.v14i1.1251", "title": "SERTIFIKASI HALAL SEBAGAI PENERAPAN ETIKA BISNIS ISLAMI DALAM UPAYA PERLINDUNGAN BAGI KONSUMEN MUSLIM", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Muhammad Tariq Khan", "T. Khan", "Sheraz Ahmed"], "summary": "Halal refers to methods of producing goods & services in the manner allowed by Shariah or Islamic law, which invest besides food preparation and food products but also encompasses the production of pharmaceutical, Cosmetics, Garments & fashion, Tourism Practices and also services like tourism, finance & banking and many more. Halal not only a religious observance and obligation for Muslims but also has emerged as a powerful market force alike both for Muslims and non-Muslims, so gained an increasing acceptance among Muslims along with non-Muslim consumers. Halal is becoming a global symbol of a lifestyle choice and quality assurance choice has promoted values as economic and social justice, earth stewardship, social responsibility, and animal welfare have gained interest which is beyond religious compliance. The global halal market is 1.7 billion consumers and worth almost US$ 2.3 trillion. This paper has focused on items Muslims are using other than food.", "title": "Halal Products: Not Restricted to Food and its Marketing Opportunity in the Muslim World", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Dr. Muhammad Tariq Khan", "Dr. Tariq Iqbal Khan", "Mr. Sheraz Ahmed"], "summary": "Halal refers to methods of producing goods & services in the manner allowed by Shariah or Islamic law, which invest besides food preparation and food products but also encompasses the production of pharmaceutical, Cosmetics, Garments & fashion, Tourism Practices and also services like tourism, finance & banking and many more. Halal not only a religious observance and obligation for Muslims but also has emerged as a powerful market force alike both for Muslims and non-Muslims, so gained an increasing acceptance among Muslims along with non-Muslim consumers. Halal is becoming a global symbol of a lifestyle choice and quality assurance choice has promoted values as economic and social justice, earth stewardship, social responsibility, and animal welfare have gained interest which is beyond religious compliance. The global halal market is 1.7 billion consumers and worth almost US$ 2.3 trillion. This paper has focused on items Muslims are using other than food.", "title": "Halal Products: Not Restricted to Food and its Marketing Opportunity in the Muslim World", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Saeed Abdalla Ahmed Almelaih Alfzari", "Siti Zaleha Omain"], "summary": "Halal is a dietary guideline practised by Muslims. Halal is viewed as a seal of approval for food quality that cannot be easily verified by consumers prior to, during, or after eating. As a result, Halal certification serves as a useful tool for consumers to make educated purchasing decisions. Halal food and beverage supply chains relate to the process of managing halal food and beverage items from manufacturing to consumption, involving multiple partners in the SC networks. This study examines the significance of Halal Food Supply Chain features in the UAE market, as well as consumer trust and satisfaction. This study examined the supply chain in the United Arab Emirates, with an eye toward the future impact of HFC systems on customer perceptions of halal food practices.", "title": "Halal food supply chain practices", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Sharifah Zannierah Syed Marzuki", "C. Hall", "P. Ballantine"], "summary": "Halal certification at restaurants ensures that foods are served according to Islamic dietary laws. Halal means permitted or lawful or fit for consumption. The purpose of this study is to explore the attributes of halal certification among Malaysian restaurant managers. A national mail survey was conducted on 2,080 respondents along with 33 interviews in 4 selected states and in the Federal Territory of Kuala Lumpur. The results indicate that restaurant managers have high expectations toward halal certification as it signifies attributes such as trust, safety, hygiene, and also perceived by participants as an important aspect in the food service industry.", "title": "Restaurant Manager and Halal Certification in Malaysia", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Ida Madieha Abdul Ghani Azmi", "Noriah Ramly", "Majdah Zawawi"], "summary": "Large-scale production of halal food opened the path to a trade industry that has now successfully expanded into other areas of the market, which led to the promotion of other halal products such as Islamic banking and finance, health care, cosmetics, lifestyle, hospitality and tourism. In the context of regulatory and governance, the fact that the global halal economy is expanding by leaps and bounds provides significant challenges to the policy makers and regulators in framing an eco-system that not only support but also nurtures and sustains the growth of the industry. [...]this legal framework, standards and governance, that defines \u201chalal-ness\u201d in its attributes, or \u201cconformity with Islamic practices\u201d must be world best, second to none. The third paper by Mohd Imran Khan, investigates the various risks in halal food supply chain using the multi criteria decision-making (MCDM) approach to identify the various risk elements in halal supply chain. The factors measured in his paper, among others, include raw material cost risks, supplier fairness, raw material integrity issue, halal market and the rest.", "title": "Guest editorial", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Amany Lubis"], "summary": "Halal food, which is characterized by lawful and good, has been measured by the standards of what can be eaten and cannot be eaten according to Islamic jurisprudence. The are condition and criteria of halalfood. Supervisory Board of the food products of the Council of Indonesian Ulama sends its inspectors to food factories in order to monitor the stages of production lines, and make sure to the safety of the food. In this article,halal food for Muslim will be discussed, including to unify the standards of halal products in Muslim countries, and implementing the Islamic way of undergoing contracts and transactions. DOI: 10.15408/ajis.v17i1.6222", "title": "\u0627\u0644\u0634\u0631\u0648\u0637 \u0648\u0627\u0644\u0636\u0648\u0627\u0628\u0637 \u0627\u0644\u0634\u0631\u0639\u064a\u0629 \u0644\u0644\u063a\u0630\u0627\u0621 \u0627\u0644\u062d\u0644\u0627\u0644: \u062e\u0628\u0631\u0629 \u0625\u0646\u062f\u0648\u0646\u064a\u0633\u064a\u0627", "claims": null}, {"metadata": {"year": 2019}, "authors": ["M. Tieman"], "summary": "\nTraditionally, halal status has been assumed for all non-pork food products\u00a0and non-alcoholic beverages available in Muslim countries. Halal certification bodies changed this precept, however, by issuing halal certificates for processed\u00a0food, beverages, and non-food products based on an assessment of ingredients used and production processes. This allowed brand owners to place a halal logo\u00a0on their products or outlets. The result has been that halal-certified products are available in the market place but, at the same time, there is a greater diversity of halal logos due to the presence of imported products sold by retailers. \n", "title": "Branding Halal: A Delicate Balance", "claims": null}], "query": "halal executive", "summary_abstract": "The concept of halal extends beyond mere dietary restrictions, evolving into a significant market force that encompasses various industries and services, including pharmaceuticals, cosmetics, tourism, and finance (Khan et al., 2020). Halal certification plays a crucial role in ensuring that products and services comply with Islamic law, thereby providing Muslim consumers with the assurance that they are engaging with halal-compliant goods (Ramlan & Nahrowi, 2015; Alfzari & Omain, 2022). \n\nThe halal certification process is not only vital for food products but also for other sectors, as it enhances consumer trust and satisfaction (Alfzari & Omain, 2022). For instance, in the restaurant industry, halal certification is perceived as a marker of trust, safety, and hygiene, which are essential attributes for food service providers (Syed Marzuki et al., 2012). Furthermore, the halal market is substantial, with an estimated 1.7 billion consumers and a value of approximately US$ 2.3 trillion, indicating its growing acceptance among both Muslim and non-Muslim consumers (Khan et al., 2020).\n\nThe expansion of the halal economy presents challenges for policymakers and regulators, who must create a robust legal framework to support and sustain the industry's growth (Azmi et al., 2022). This includes establishing clear standards and governance to define what constitutes \"halal-ness\" in various products and services (Azmi et al., 2022). Additionally, the halal food supply chain is complex, involving multiple stakeholders, and requires careful management to ensure compliance with halal standards throughout the production process (Alfzari & Omain, 2022).\n\nIn summary, halal certification is a critical component of the halal executive, serving as a guarantee of compliance with Islamic principles across various sectors, thereby fostering consumer confidence and contributing to the growth of a diverse halal market (Ramlan & Nahrowi, 2015; Khan et al., 2020; Azmi et al., 2022).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2001}, "authors": ["R. Tucker"], "summary": "Robert B. Tucker DEJECTED LEADERS OF BANKRUPT DOT-COM FIRMS ARE OFTEN quoted as saying, \"we just couldn't seem to get our business model worked out, and we finally ran out of cash.\" Not surprising. Even the best-known dot-coms must be considered business model experiments, since their \"path to profitabil\u00ad ity\" is still before them. Yet while dot-com firms are under extreme pressure to prove the efficacy of their business models, existing firms face a challenge that is no less daunting: how to sustain the efficacy of their business models in the face of unrelenting change and competition. The fact is, no matter how bulletproof your firm's current business model, it will be challenged by new business mod\u00ad els. Over time, it will also be imitated, and thereby diluted and commoditized. Upstarts may or may not have staying power, yet collectively they can render today's method of cre\u00ad ating value for customers passe, or worse. The new reality is that business models have shelf lives, like loaves of bread at the supermarket. You must constantly attempt to discover new business models if your company hopes to survive and grow. Hatching new business models can conjure images of well-financed whiz kids inside business incubators hover\u00ad ing around whiteboards, excitedly brainstorming billiondollar breakthroughs. Yet if the dot-com crash has lessons, those lessons may be that native genius is less than what is called for and piles of venture capital can't insure market acceptance.", "title": "STRATEGY INNOVATION TAKES IMAGINATION", "claims": null}, {"metadata": {"year": 2002}, "authors": ["Joseph A. DiVanna"], "summary": "When is a bank a bank? Is a store still a store or is it a retail portal? If doing business on the Internet is called cyberspace, should traditional brick-and-mortar channels be called terraspace? If there is only one lesson to be learned from the dot-com phenomenon, it is a reacquaintance with the language of business. The common belief is that technology is both a catalyst for business and the driving force behind society, the results of which have ushered in a new language for the eEconomy, often blurring traditional definitions.", "title": "Dot-com Lessons learned: To Be or Not To Be, Value is the Question", "claims": null}, {"metadata": {"year": 2004}, "authors": ["D. Barnes", "Matthew Hinton", "S. Mieczkowska"], "summary": "This paper reports on research that investigates how three start\u2010up dotcoms, founded at the height of the Internet boom avoided joining the many dotbombs of the subsequent crash of the Internet economy. The paper describes how the companies made use of Internet\u2010based information and communication technologies (ICTs) in their business operations. It draws lessons from their experiences that might be applied more generally to other dotcom start\u2010ups. These centre on the need to use the technology to manage business processes to meet the clearly definable needs of a targeted niche of customers. The desirability of suitable in\u2010house ICT expertise is stressed.", "title": "Avoiding the fate of the dotbombs: lessons from three surviving dotcom start-ups", "claims": null}, {"metadata": {"year": 2006}, "authors": ["H. Hwang", "C. M. Stewart"], "summary": "The dot-com industry began in the early 1990s as a collection of startup companies using the Internet as their primary means to conduct business. These companies typically used the \u201c.com\u201d suffix in their company names, such as Amazon.com, and proliferated in the late 90\u2019s with the massive investments in Internet-related stocks and enterprises. But with the failure and consolidation of many of these companies their numbers have since dwindled. The catastrophic collapse of the dot-coms that shook the U.S. economy started in May 2000. More than 210 dotcom companies failed in 2000 (Hirakubo & Friedman, 2002) and a total of 762 dot-coms closed for the period January 2000 to December 2001 (Pather, Erwin, & Remenyi, 2003). Since many of these dot-coms began to lay off their staff, the unemployment rate also increased from 3.9% to 6% by 2002 (Callahan & Garrison, 2003; Howard, 2001). The dot-com bubble burst because the boom was based on the false premise that new technology would eliminate the need for brick-and-mortar stores as this new business model would supplant the old one, thereby converting the \u201cOld Economy,\u201d which is based on the production of physical goods into a \u201cNew Economy,\u201d which is based on heavy use of information and communication technology (Rauch, 2001). Although a great deal can be learned from examining the dot-com successes, it is equally important to study reasons for the failures. Examining the mistakes made by the dot-coms can provide insight into the evolution of e-commerce as a means of conducting business and furthermore help to form the basis on which new strategies can be developed for the future e-commerce environment.", "title": "Lessons from Dot-Com Boom and Bust", "claims": null}, {"metadata": {"year": 2004}, "authors": ["Peter G. W. Keen"], "summary": "The crash that hit many dot corn firms and resulted in an abrupt end of enthusiastic forecasts of e-commerce development should be seen as an opportunity to learn from the mistakes that have been made and to move on to more viable and sustainable business models. Using \u2018old generation\u2019 e-commerce experience as a huge laboratory directs attention towards general patterns and trends of a long-lasting development path in e-commerce. This approach proves to be more useful than concentrating on single exceptional cases and their dramatic failure in the crash. It will be argued that e-commerce is just about business, and its evolution, thus, follows the well-known rules of investment, financing and market reactions.", "title": "Building new generation e-business: exploiting the opportunities of today", "claims": null}, {"metadata": {"year": 2004}, "authors": ["K. Soufani", "Terence Tse"], "summary": "A theoretical framework is developed that illustrates the conceptual role of information on the World Wide Web in enhancing the economic efficiency of dot.com-centered activities.Also developed is an efficiency-chain model that theoretically evaluates the effects on each stage in the efficiency chain when more information is derived from or passed through the Internet. Integral to this model are \"single stage efficiency gains\" (the overall improvements that may result from the efficiency gains in each agent, isolated and independent of the others) and the \"ripple effect\" (spillover effects in multiple stages and multiple enterprises).As dot.com enterprises respond to the needs of information-aware customers by pursuing information-intensive strategies designed to enhance economic efficiency, they can better satisfy the needs of other customers as well. Given the abundance of information and the low cost of searching the Internet, competition is likely to become intense. As a result, firms must strive to mine information and increase their information intensity in order to win customers. Enterprises that fail to use the newly emerging information-intensive networks on the World Wide Web to gain efficiency that matches the efficiency gains of others are doomed to diminish with time. (SAA)", "title": "Economic Efficiency in Traditional and Dot.Com Firms: A Theoretical Approach", "claims": null}, {"metadata": {"year": 2007}, "authors": ["Hokey Min", "J. Caltagirone", "Adrea Serpico"], "summary": "As advancements in internet technology continues to revolutionise today's business practices, it is increasingly more apparent that the internet has become an integral part of the daily routine. The internet generates a high level of interest from both retailers and their customers, because of its ability to expedite the purchase ordering process, simplify purchase payment procedures, reduce paperwork, eliminate order errors, and effectively utilise working capital. Leveraging such advantages, a large number of entrepreneurs explored online sales outlets by establishing dot-com ventures. Following this trend, many established brick-and-mortar retailers also jumped onto the dot-com bandwagon. Unfortunately, many dot-com ventures did not survive. We would like to summarise what lessons we learned from dot-com failures and find a way to apply these lessons to future online retail business opportunities.", "title": "Life after a dot-com bubble", "claims": null}, {"metadata": {"year": 2001}, "authors": ["B. Mahadevan"], "summary": "T he growth of Internet-based businesses is truly meteoric. It has dwarfed the historical growth patterns of other sectors. Over the years, several organizations doing business through the Internet have come out with their own set of unique propositions to succeed in the business. For instance Amazon.com demonstrated how it is possible to \u201cdis-intermediate\u201d the supply chain and create new value out of it. Companies such as Hotmail and Netscape made business sense out of providing free products and services. On the other hand, companies such as AOL and Yahoo identified new revenue streams for their businesses. Similarly companies such as Vertical Net engaged in building on-line communities. It is increasingly becoming clearer that the propositions that these organizations employed in their businesses could collectively form the building blocks of a business model for an Internet-based business. Several variations of these early initiatives as well as some new ones being innovated by recent Internet ventures have underscored the need for some theory-building in this area.", "title": "Mahadevan CMR su00", "claims": null}], "query": "What can existing businesses learn from the business approaches of the dot-com organizations?", "summary_abstract": "The exploration of business approaches from dot-com organizations offers valuable insights for existing businesses seeking to adapt and thrive in a rapidly changing market. A key takeaway is the necessity for continuous innovation in business models. Tucker (2001) emphasizes that even established firms must regularly reassess and evolve their business models to avoid obsolescence, as new competitors can quickly render existing methods ineffective. This notion is echoed by Keen (2004), who suggests that the dot-com crash serves as a learning opportunity, highlighting the importance of developing sustainable business models rather than relying on fleeting trends.\n\nMoreover, the experiences of successful dot-coms reveal the significance of understanding customer needs and leveraging technology effectively. Barnes et al. (2004) note that startups that thrived during the dot-com boom utilized Internet-based information and communication technologies to meet the specific demands of niche markets. This aligns with the findings of Hwang and Stewart (2006), who argue that the dot-com era's failures stemmed from a misunderstanding of the market dynamics, where the assumption that technology would completely replace traditional business models proved misguided.\n\nAdditionally, the concept of information intensity is crucial. Soufani and Tse (2004) highlight that businesses must harness the vast amounts of information available online to enhance their operational efficiency and customer satisfaction. This is particularly relevant in today's competitive landscape, where the ability to adapt and respond to information-driven market changes can determine a company's success.\n\nIn summary, existing businesses can learn from the dot-com era by embracing continuous innovation, understanding customer needs, effectively utilizing technology, and leveraging information to enhance efficiency. These lessons underscore the importance of adaptability and strategic foresight in navigating the complexities of modern commerce.", "summary_extract": null}, {"papers": [{"metadata": {"year": 1995}, "authors": ["A. Ayton", "J. Price"], "summary": "Introduction: the military revolution from a medieval perspective, Andrew Ayton, J.L. Price. The conqueror's footprints in Domesday Book, J.J.N. Palmer towns, mottes and ring-works of the conquest, Barbara English alms for the Holy land - the English Templars and their patrons, John Walker knights, esquires and military service - the evidence of the Armorial cases before the Court of Chivalry, Andrew Ayton town defences in medieval England and Wales, D.M. Palliser war and peace in the Works of Erasmus - a medieval perspective, Peter Heath Josse Clichtove and the Just War, Howell A. Lloyd \"wise and experimented\" - Sir William Pelham, Elizabethan soldier and landlord, c.1560-87, R.W. Ambler a state dedicated to war? the Dutch Republic in the 17th century, J.L. Price.", "title": "The Medieval military revolution : state, society and military change in medieval and early modern Europe", "claims": null}, {"metadata": {"year": 2021}, "authors": ["V. Shagaev", "L. Alyaeva"], "summary": "The article examines the sources of legal regulation of military proceedings in Western European countries, various approaches to the organization of the system of military courts and the definition of participants in military criminal proceedings, as well as some issues of making a court decision, its verification and revision.", "title": "The system of military courts of the armies of Wester Europe in\u00a0the XIX century", "claims": null}, {"metadata": {"year": 2003}, "authors": [], "summary": "Standing peacetime armies of any substantial strength were virtually unknown. They were too expensive, logistically unfeasible, and regarded as dangerous by populace and parliament alike. Much of the nominal military might of a country during peace was often in the form of sections of the nobility, whose traditional role it was to command and fight during war (e.g. France\u2019s compagnies d\u2019ordonnance, heavy cavalry companies). While feudalism as a military system was much diminished, feudal obligations in war could remain in some form. English nobility, for example, were required to provide strictly defined military materiel proscribed by title, while the kings of France could call the ban and arri\u00e9re-ban, requiring military service (or a proxy) of all those holding land directly or indirectly.", "title": "MILITARY SCIENCE IN WESTERN EUROPE IN THE SIXTEENTH CENTURY PROLOGUE:THE NATURE OF ARMIES IN THE 16 CENTURY", "claims": null}, {"metadata": {"year": 1996}, "authors": ["C. Storrs", "H. Scott"], "summary": "Warfare was a permanent and ubiquitous feature of early modern Europe, stimulating new developments in military technology and organization. Firearms became more important and, eventually, dominant, and armies more specialized and complex. By 1600 the proportions of infantry, artillery, and cavalry were roughly equal. Thereafter, the proportion of cavalry declined even more rapidly, while its nature also underwent fundamental change. The heavily armoured knights, the backbone of medieval warfare, came to be replaced by lighter horsemen, and the rn\u00ea&oelig;e by the more disciplined though less effective caracole. New artillery made traditional fortifications redundant and necessitated much reconstruction along more modern lines. Armies were broken down into smaller units, while training and discipline became more important. Above all, the military forces of most states grew rapidly in size, reaching a peak in the first decade of the eighteenth century, and", "title": "The Military Revolution and the European Nobility, c. 1600-1800", "claims": null}, {"metadata": {"year": 2020}, "authors": ["L. Taylor"], "summary": "From around 1130 to 1240, Norway was troubled by a series of internal conflicts and succession disputes. In this article, I explore one element of Norwegian prelates\u2019 conduct during this period which has received little attention: their involvement in military activities. By comparing how prelates are shown to behave during periods of military conflict in the kings\u2019 sagas with the ideal mode of behaviour set out in canon law, instructional treatises, and papal letters, I explore the extent to which clerical involvement in military activities was considered acceptable in Norway across the high medieval period. I conclude that although it was considered ideal for clerics to eschew involvement in violent conflict, some direct participation in military activities was tacitly accepted within high medieval Norway. This role was limited, however, by the belief that the clergy should not shed blood or bear arms. Although there is some suggestion in sources for mid-13th-century Norway that prelates became less involved in military affairs across this period, there is little evidence that attitudes within Norway towards clerical participation in war had significantly shifted by the conclusion of the so-called \u2018civil war\u2019 period.", "title": "Bishops, war, and canon law", "claims": null}, {"metadata": {"year": 2015}, "authors": ["G. Halfond"], "summary": "Contents: Introduction Bibliography of publications by Bernard S. Bachrach. Part I: The History and Historiography of War in Late Antique and Early Medieval Europe: The Roman frontier along the Upper Danube in Late Antiquity, Andreas Schwarcz War and peace in the Acta of the Merovingian church councils, Gregory I. Halfond Reflections on Alfred the Great as a military leader, Richard Abels Conrad II (990-1039), the first medieval emperor of three kingdoms, Herwig Wolfram Maps of French history before the atlas of Auguste Longnon, Walter Goffart. Part II Warfare in the East in the Crusading Era: A medieval siege of Troy: the fight to the death at Acre, 1189-1191 or the tears of a' alAua,[yen] al-DA\"n, John H. Pryor King Richard's plan for the Battle of ArsA\"f/Arsur, 1191, Benjamin Z. Kedar Crusading warfare, chivalry and the enslavement of women and children, John Gillingham The Catalan company in the East: evolution of a wandering army (1303-1311), David Jacoby. Part III European Warfare in the Central and Later Middle Ages: A decisive century in the struggle against Islam in Iberia, ca. 1031-1157: grand strategic perspectives, Manuel Rojas Gabriel Contrary winds: theories of history and the limits of Sachkritik, Stephen Morillo Women in the context of romanesque combat scenes in Spain and France: virtue, judgment and rape, James F. Powers and Lorraine C. Attreed The Battle of Bouvines 27 July 1214, John France King Edward I's military bureaucracy: the case of Peter of Dunwich, David Bachrach The trebuchets of the Tower, Michael Prestwich The military obligations of the Danish church in the High Middle Ages, Niels Lund The implications of the Anonimo Romano account of the Battle of Crecy, Kelly DeVries. Index.", "title": "The Medieval Way of War: Studies in Medieval Military History in Honor of Bernard S. Bachrach", "claims": null}, {"metadata": {"year": 2001}, "authors": ["J. Glete"], "summary": "The sixteenth and seventeenth centuries saw many ambitious European rulers develop permanent armies and navies. War and the State in Early Modern Europe examines this military change as a central part of the political, social and economic transformation of early modern Europe. This important study exposes the economic structures necessary for supporting permanent military organisations across Europe. Large armed forces could not develop successfully without various interest groups who needed protection and were willing to pay for it. Arguing that early fiscal-military states were in fact protection-selling enterprises, the author focuses on: * Spain, the Dutch Republic and Sweden * the role of local elites * the political and organisational aspects of this new military development", "title": "War and the State in Early Modern Europe: Spain, the Dutch Republic and Sweden as Fiscal-Military States", "claims": null}, {"metadata": {"year": 1983}, "authors": ["Nico  J\u00f6rg"], "summary": "The beginnings of modem military penal and disciplinary law coincide with the so-called military revolution (1560-1660) [1]. During this period the army was created, more or less as it still exists to this day: an organisation of strict discipline, hierarchical structure and subject to the State. In 1590 the \"Artyculbrief ofte Ordonnantie op de Discipline Militaire\" was drafted in the Netherlands for the mercenaries of Prince Maurice. The drill of the mercenaries allowed them to function as cogs in a machine, as it were. This military penal code was adopted by the States-General. It defined offences, punishments and the jurisdiction of the court martial. The \"Artyculbrief\" was replaced in 1799 by the \"Regulations of Military Discipline or Criminal Code for the State Militia.\" In 1814/1815, new laws in the field of military penal and disciplinary law and procedure were created, though not to everyone's satisfaction. In 1923, a new military penal code was introduced which put an end to analogical interpretation that had been possible ever since the \"Artyculbrief\". In 1963, the military law of criminal procedure was partly amended. Since 1965, a parliamentary procedure for the complete revision of military criminal procedural law has been pending, while much work has been done on military disciplinary law since 1967 this latter circumstance being of some consequence for military penal law too. The Netherlands is not the only country where fundamental rights would appear to lead a laborious existence in the armed forces. Fundamental rights vary in meaning, just as the societies in which they have developed vary. While fundamental rights in America were of a pre-state nature, were pre-political, self-evident rights, on the European continent they were political achievements. They gave rise to the New Order after the fall of the Ancien Regime, an order in which the separation of powers [2] was to prevent tyranny. Specifically, the separation of powers in Europe can also be seen as the rational division of the three state tasks: legislation, administration and judicature, the result being reliable and calculable law [3]. (Whether this is still", "title": "Organisational change and fundamental rights in the Dutch army: 1966\u20131976", "claims": null}], "query": "military law in medieval Europe", "summary_abstract": "The exploration of military law in medieval Europe reveals a complex interplay between legal frameworks, military organization, and societal norms. Ayton and Price (1995) provide a foundational understanding of the military revolution from a medieval perspective, highlighting the evolution of military obligations and the role of nobility in warfare. They note that while standing armies were rare, the feudal system imposed specific military duties on the nobility, which were often codified in legal terms.\n\nShagaev and Alyaeva (2021) delve into the legal regulation of military proceedings in Western Europe, discussing the organization of military courts and the participants involved in military criminal proceedings. Their work underscores the formalization of military law during this period, which was essential for maintaining order within the ranks and addressing military offenses.\n\nThe transition from feudal military obligations to more structured military organizations is further examined by Glete (2001), who discusses the emergence of permanent armies in the early modern period. This shift necessitated the development of military penal and disciplinary laws, as highlighted by J\u00f6rg (1983), who traces the origins of modern military law to the military revolution of the 16th and 17th centuries. He notes that the establishment of a hierarchical military structure required clear definitions of offenses and punishments, leading to the creation of military codes that governed conduct.\n\nTaylor (2020) provides a unique perspective on the involvement of clerics in military activities during the high medieval period in Norway, revealing that while canon law ideally discouraged clerical participation in warfare, some involvement was accepted. This reflects the broader societal attitudes towards military service and the legal implications of such participation.\n\nIn summary, the collective findings from these papers illustrate that military law in medieval Europe was characterized by a gradual evolution from feudal obligations to more formalized legal structures governing military conduct. The interplay between military organization, societal norms, and legal frameworks shaped the development of military law, which became increasingly important as states transitioned to more permanent military forces.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2018}, "authors": ["L.  Lu", "Y. H. Zheng"], "summary": "In this work, we describe the fabrication of an electrochemical sensor for the detection of Hg2+in various water samples. The electrochemical sensor is fabricated on an indium tin oxide (ITO) modified with multi\u2013walled carbon nanotubes (MWCNT) and reduced graphene oxide (RGO) hybrid film. The MWCNT was firstly dispersed using graphene oxide (GO) as dispersant. After coating on the ITO, the GO was then electrochemically reduced to RGO. The obtained thin film was characterized by scanning electron microscope (SEM), FTIR, Raman spectroscopy and 3D optical surface profiler. Cyclic voltammetry and differential pulse voltammetry were employed to investigate the electrocatalytic performance towards the Hg2+ oxidation. Under optimum conditions, the proposed sensor showed a wider linear range at Hg(II) concentrations of 0.05\u2013150 nM. The limit of detection was calculated to be 0.05 nM.", "title": "Electrochemical Detection of Hg(II) in Environmental Water Samples Based on Multiwalled Carbon Nanotube\u2013Reduced Graphene Oxide Hybrid Film", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Jin Zou", "Jiawei Liu", "Guanwei Peng", "Haiyan Huang", "Linyu Wang", "Limin Lu", "Yansha Gao", "D. Hu", "Shangxing Chen"], "summary": "Mercuric ion (Hg2+) in aqueous media is extremely toxic to the environment and organisms. Therefore, the ultra-trace electrochemical determination of Hg2+ in the environment is of critical importance. In this work, a new electrochemical Hg2+ sensing platform based on porous activated carbon (BC/Cu2O) modified with cuprous oxide was developed using a simple impregnation pyrolysis method. Differential pulse anodic stripping voltammetry (DPASV) was used to investigate the sensing capability of the BC/Cu2O electrode towards Hg2+. Due to the excellent conductivity and large specific surface area of BC, and the excellent catalytic activity of Cu2O nanoparticles, the prepared BC/Cu2O electrode exhibited excellent electrochemical activity. The high sensitivity of the proposed system resulted in a low detection limit of 0.3 ng\u00b7L\u22121 and a wide linear response in the ranges from 1.0 ng\u00b7L\u22121 to 1.0 mg\u00b7L\u22121. In addition, this sensor was found to have good accuracy, acceptable precision, and reproducibility. All of these results show that the BC/Cu2O composite is a promising material for Hg2+ electrochemical detection.", "title": "An Electrochemical Sensor Based on a Porous Biochar/Cuprous Oxide (BC/Cu2O) Composite for the Determination of Hg(II)", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Aiping  Huang", "Hui  Li", "Danke  Xu"], "summary": "Abstract In this paper, an on-chip electrochemical sensor was designed by integrating an indium tin oxide (ITO) three-electrode with a low-volume cell, and trace Hg2+ was detected by differential pulse anodic stripping voltammetry (DPASV) after online flow accumulation in the sensor. The working electrode of the ITO three-electrode was composed of in-situ electrodeposited flower-cluster gold nanoparticles (AuNPs) microarray on the surface of the ITO, the reference electrode was Ag/AgCl, and the auxiliary electrode was ITO. The in-situ flow accumulation in the on-chip electrochemical sensor was used to detect the linear range of Hg2+ from 0.63 to 80\u00a0ppb with a detection limit of 0.11\u00a0ppb (S/N\u00a0=\u00a03). The sensors had good selectivity towards Hg2+ in the interferences of eight common cations. The recovery rates of 1\u00a0ppb Hg2+ were 101% in the tap water and 106% in the lake water. Compared with the traditional detection method, the on-chip electrochemical sensor by the in-situ flow accumulation of Hg2+ required low-volume sample and got a more sensitive and stable signal, so it had excellent potential for on-site and on-line detection of mercuric pollutant.", "title": "An on-chip electrochemical sensor by integrating ITO three-electrode with low-volume cell for on-line determination of trace Hg(II)", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Yuan  Liao", "Qian  Li", "Na  Wang", "Shijun  Shao"], "summary": "Abstract A new BIM/MCNF/Nafion-modified glassy carbon electrode, combining the specific Hg 2+ -complexing ability of bis(indolyl)methane (BIM) with the unique properties of mesoporous carbon nanofiber (MCNF), was fabricated for anodic stripping analysis of Hg 2+ . BIM was synthesized and served as an ionophore for highly selective adsorption heavy metal Hg 2+ from the sample solution. MCNF displayed extraordinary electrical conductivity and great adsorption ability to enhance the sensitivity of the modified electrode. Nafion, a sulfonated cation-exchange polymer, supplied the good stability of the modifying layer. The as-prepared electrochemical sensor significantly improved selectivity and sensitivity towards Hg 2+ determination. The stripping current increased linearly with Hg 2+ concentration at the range of 5\u2013500\u00a0nM under 15\u00a0min accumulation at open-circuit potential, with a detection limit of 0.3\u00a0nM. Meanwhile, the proposed electrode could avoid some interference of other heavy metal ions coexisting with Hg 2+ , such as Cd 2+ , Pb 2+ and Cu 2+ . This new method has been successfully applied for the detection of Hg 2+ in Yellow River with satisfactory results.", "title": "Development of a new electrochemical sensor for determination of Hg(II) based on Bis(indolyl)methane/Mesoporous carbon nanofiber/Nafion/glassy carbon electrode", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Ziping  Zhang", "Jungang  Yin", "Zhaoyang  Wu", "Ruqin  Yu"], "summary": "Engineered nucleic acid probes containing recognition and signaling functions find growing interest in biosensor design. In this paper, we developed a novel electrochemical biosensor for sensitive and selective detecting of Hg(2+) based on a bifunctional oligonucleotide signal probe combining a mercury-specific sequence and a G-quadruplex (G4) sequence. For constructing the electrochemical Hg(2+) biosensor, a thiolated, mercury-specific oligonucleotide capture probe was first immobilized on gold electrode surface. In the presence of Hg(2+), a bifunctional oligonucleotide signal probe was hybridized with the immobilized capture probe through thymine-mercury(II)-thymine interaction-mediated surface hybridization. The further interaction between G4 sequence of the signal probe and hemin generated a G4-hemin complex, which catalyzed the electrochemical reduction of hydrogen peroxide, producing amplified readout signals for Hg(2+) interaction events. This electrochemical Hg(2+) biosensor was highly sensitive and selective to Hg(2+) in the concentration of 1.0 nM to 1 \u03bcM with a detection limit of 0.5 nM. The new design of bifunctional oligonucleotide signal probes also provides a potential alternative for developing simple and effective electrochemical biosensors capable of detecting other metal ions specific to natural or artificial bases.", "title": "Electrocatalytic assay of mercury(II) ions using a bifunctional oligonucleotide signal probe.", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Yuanyuan  Lin", "Yang  Peng", "Junwei  Di"], "summary": "Abstract Nanoporous gold nanoparticles (np-Au NPs), dispersed on the indium tin oxide film coated glass, were firstly employed to fabricate Hg(II) sensing platform in differential pulse anodic stripping voltammetry (DPASV). The np-Au NPs integrated the features of highly porous nanoparticles and the high affinity of gold nanomaterials for mercury, which endowed the sensing platform with high sensitivity and good reproducibility. Following optimization of the method, a calibration curve from 0.1 to 10\u00a0\u03bcg/L Hg(II) was obtained in HCl solution. The detection limit was 0.03\u00a0\u03bcg/L (0.15\u00a0nM) at a signal-to noise ratio of 3. The interference from other heavy metal ions associated with Hg(II) detection could be effectively avoided. The performance of the proposed electrode was also applied in the detection of trace Hg(II) in different real samples with satisfactory results.", "title": "Electrochemical detection of Hg(II) ions based on nanoporous gold nanoparticles modified indium tin oxide electrode", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Erhu Xiong", "Xiaohua Zhang", "Yunqing Liu", "Jiawan Zhou", "Peng Yu", "Jinhua Chen"], "summary": "In this work, a selective electrochemical biosensor for the sensitive detection of mercury ions (Hg2+) has been developed based on the exonuclease III (Exo III)-assisted target recycling and hybridization chain reaction (HCR) amplification strategies. A thymine (T)-rich capture probe (CP) was first self-assembled on the gold electrode surface through Au\u2013S bonding and then blocked with 6-mercaptohexanol (MCH). Subsequently, the auxiliary probe (AP) hybridized with the CP to form a double-stranded DNA (ds-DNA) structure. The presence of Hg2+ led the CP to form a T\u2013Hg2+\u2013T hairpin structure with recessed 3\u2032-termini, which triggered the Exo III cleavage process accompanied by the release of Hg2+ for analyte recycling, and the remaining DNA fragments of the CP on the electrode hybridized with hairpin probe 1 (HP1) and hairpin probe 2 (HP2) to implement the HCR process. The long ds-DNA structures provided abundant adsorption sites for the electroactive complex (Ru[(NH3)6]3+), leading to a significant differential pulse voltammetry (DPV) response. The developed electrochemical biosensor afforded highly sensitive and selective detection of Hg2+, and the analytical method would have great potential applications in the monitoring of Hg2+ in real environmental samples.", "title": "An electrochemical biosensor for sensitive detection of Hg2+ based on exonuclease III-assisted target recycling and hybridization chain reaction amplification strategies", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Jie Zhou", "Dan Wei", "Changchun Hu", "Shuo Li", "Liang Ma", "Zhu Chen", "Yan Deng"], "summary": "In this work, a simple label-free aptamer-based electrochemical sensor was developed for determination of mercury ions (Hg2+) by polylysine (PLL) modified black phosphorus nanosheets (BP) and gold nanoparticles (AuNPs) as signal amplification strategy. The PLL-BP composite\n materials greatly improved electron transfer speed and electrochemical behavior of electrode in the detection of analytes. AuNPs can amplify the electrochemical signal and facilitate the binding of aptamers to the electrode. The selected aptamer (Apt) can bind specifically to Hg2+\n to form a T-Hg2+-T structure, which we used to detect Hg2+. Experimental results showed that the prepared electrochemical aptamer sensor exhibited wide linear range from 1.0\u2013500 nM, along with a low detection limit of 0.14 nM. Moreover, the electrochemical aptamer\n sensor has good specificity, stability, and repeatability. Therefore, this proposed electrochemical aptamer sensor provide a simple and sensitive method for the detection of Hg2+, which is expected to be used for the detection of trace amounts of Hg2+ in actual samples.", "title": "A simple label-free electrochemical aptamer sensor for detection of Hg2+ based on black phosphorus", "claims": null}], "query": "electrochemical apatsensor for Hg(II) detection", "summary_abstract": "The research on electrochemical aptasensors for the detection of Hg(II) reveals a variety of innovative approaches and materials that enhance sensitivity and selectivity. Several studies highlight the development of electrochemical sensors utilizing different substrates and methodologies to achieve low detection limits and wide linear ranges for Hg(II) detection.\n\nLu and Zheng (2018) fabricated an electrochemical sensor on an indium tin oxide (ITO) substrate modified with multi-walled carbon nanotubes and reduced graphene oxide, achieving a detection limit of 0.05 nM and a linear range of 0.05\u2013150 nM for Hg(II). Similarly, Zou et al. (2023) introduced a porous activated carbon modified with cuprous oxide, which demonstrated a remarkable detection limit of 0.3 ng\u00b7L\u22121 and a linear response from 1.0 ng\u00b7L\u22121 to 1.0 mg\u00b7L\u22121, showcasing the effectiveness of this composite material.\n\nHuang et al. (2019) designed an on-chip electrochemical sensor that utilized a three-electrode system with gold nanoparticles, achieving a detection limit of 0.11 ppb and a linear range of 0.63 to 80 ppb. This design emphasized the sensor's selectivity against common cation interferences. In another approach, Liao et al. (2015) developed a sensor using bis(indolyl)methane and mesoporous carbon nanofibers, which provided a detection limit of 0.3 nM and demonstrated good selectivity against other heavy metals.\n\nZhang et al. (2013) presented a biosensor based on engineered nucleic acid probes, achieving a detection limit of 0.5 nM for Hg(II) through a unique G-quadruplex interaction. Xiong et al. (2016) further advanced the field with a biosensor employing exonuclease-assisted recycling and hybridization chain reaction, which significantly amplified the electrochemical signal for Hg(II) detection.\n\nLastly, Zhou et al. (2022) developed a label-free aptamer-based electrochemical sensor using black phosphorus nanosheets and gold nanoparticles, achieving a low detection limit of 0.14 nM and a linear range from 1.0 to 500 nM. This sensor demonstrated excellent specificity and stability, indicating its potential for real-world applications.\n\nCollectively, these studies illustrate the diverse strategies employed in the development of electrochemical sensors for Hg(II) detection, emphasizing advancements in materials and methodologies that enhance sensitivity, selectivity, and applicability in environmental monitoring.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2007}, "authors": ["Pelrologist C. OsnonNB HurroN", "Geological Suraey", "F. T. SBBlvr"], "summary": "During an investigation of the distribution and quantity of greensands of the Upper Cretaceous and Tertiary formations in New Zealand, a considerable number of samples was collected with a view to a mineralogical study of glauconite. Of the specimens collected, seven have been used in the present study. The samples were crushed and passed through a 60-mesh sieve, and then concentrated to some extent by shaking on a 100-mesh; material having a grain-size greater than 60-mesh or less than 100-mesh was rejected as too impure. The glauconite in the sand of this grade was then further concentrated by electro-magnetic treatment and subsequently purified by flotation in bromoform or bromoform-benzene mixtures. Clerici solution was not used in preparing any samples of glauconite for analysis, for Gruner (1935, p. 704) has found experimentally that in the case of a sample of glauconite boiled in thallium nitrate solution for four hours, about 25/6 of the K ions were replaced by thallium ions, with resultant increase in the specific gravity from 2.81 to 3.02. The behavior of glauconite in Clerici solution is similar to that of the stilpnomelane minerals, for Holzner (1933) found that thallium v/as absorbed into the structure to such an extent that analysis showed 9.83Vo TlzO, while KrO was reduced lrom 2 .2O to 0.29/e. Centrif uging was necessary in two cases, while one specimen of glauconitic greensand, rich in coarse biotite, was separated from the latter constituent by the simple procedure of vibration on cardboard, the mica remaining on the board and the glauconite rolling off. Microscopic inspection was carried out before chemical analysis was undertaken. The glauconite content of the samples ranges between wide limits, from about 10/6 in P. 4640,* a glauconitic limestone from Milburn * Numbers P.4&0 etc. refer to specimens in the mineral and rock collections oI the Nerv Zealand Geological Survey, Wellington.", "title": "COMPOSITION AND PROPERTIES OF SOME NEW ZEALAND GLAUCONITES", "claims": null}, {"metadata": {"year": 2006}, "authors": ["Subbetic  Zone", "Betic  Cordilleras", "SE  Spain"], "summary": "Glauconite and Ca phosphate peloids occur in Jurassic and Cretaceous bioclastic carbonate rocks from pelagic swell sequences of the Algayat-Crevillente Unit (Subbetic Zone). The size and morphology of the peloids are controlled by the bioclasts. The glauconite in both stratigraphic positions is K rich (>0.69 atoms p.f.u.) and shows well-defined 10 A. lattice fringes. Poorly crystalline areas with a composition of Fe-smectite are found within the peloids, indicating the nature of the glauconitic precursor. This precursor would be formed in the shielded microenvironments of the bioclast and later transformed to glauconite by equilibration of peloids with sea water that culminated with the crystallization of a phosphatic phase. The greater presence of smectite areas in the Jurassic peloids and the lower K contents (0.69-0.81) of these glauconites, compared with the Cretaceous glauconites (0.81-0.89) can be explained by the calcitic early diagenetic cementation which stopped the process of glauconitization. The green nodular concretions formed in a marine environment and composed mainly of phyllosilicates have generally been called glauconitic peloids. The term glauconite is commonly reserved for the Feand K-rich mica structure mineral, (McRae, 1972; Odin & Matter, 1981), while the Kpoor terms are commonly known as glaucony (Odin & Letolle, 1980; Odin & Matter, 1981; Stille & Clauer, 1994) or glauconite smectites (Dasgupta et al., 1990; Ostwald & Bolton, 1992), which correspond to the term nontronite according to Gfiven (1988). The A I P E A N o m e n c l a t u r e Committee defines glauconite as an Fe-rich dioctahedral mica with tetrahedral AI (or Fe 3+) usually >0.2 atoms p.f.u, and octahedral R 3+ >1.2 atoms (Bailey, 1980). The octahedral sheet consists mainly of Fe 3+, which distinguishes glauconite from muscovite and phengite micas. Thus, materials containing an Fe-rich mica as a major component can be called glauconitic micas. Specimens with expandable layers intermixed can be described as randomly interstratified glauconite-smectite (Odom, 1984). Glauconitization typically occurs in submarine conditions in areas with low sedimentation rate whose depths oscillate between 100 and 300 m. Two hypotheses have been proposed traditionally for the genesis of glauconite: (a) progressive Fe incorporation into degraded illite structure (Burst, 1958a,b; Hower, 1961); and (b) crystallization of glauconitic smectite in the pores of various framework types (quartz, K-feldspar, biotite, calcitic bioclasts and faecal pellets) concomitant with its progressive dissolution in a confined environment (Odin & Matter, 1981; Odin & Fullagar, 1988; Stille & Clauer, 1994). Later, these particles could @ 1998 The Mineralogical Society 548 J. Jimknez-Millim et al. be modified by K addition to fl~e smectitic material and the crystallization of phosphatic phases equilibrated with the sea water (Stille & Clauer, 1994). The glauconitic and phosphatic facies have often been found together in marine formations of very different ages (Odin & Letolle, 1980; Carson & Crowley, 1993). Dasgupta et al. (1990) distinguished the processes of glauconitization that imply K-rich frameworks and those which do not. In the Betic Cordilleras, glauconite has been found in materials related to Mesozoic discontinuities of different ages and in some cases related to phosphatic facies (Debrabant & Paquet, 1975; Bocchi & Luccbini, 199I; Martln-Algarra & Vera, 1994; Vera & Martin-Algarra, 1994). Martinez-Ruiz (1993) recognized mica of glauconitic composition in the fine fraction of Betic marls, using analytical electron microscopy (AEM). In this study, we have carried out the compositional and textural characterization of glauconitic peloids in materials associated with two discontinuities from the Algayat-Crevillente Unit (Subbetic Zone, Betic Cordillera), principally using scanning electron microscopy with an energy dispersive system (SEM-EDS) and transmission electron microscopy combined with AEM (TEM-AEM) methods, with the objective to contribute to a better understanding of the processes implied in their genesis. G E O L O G I C A L A N D S T R A T I G R A P H I C S E T T I N G Samples of glauconite-bearing rocks were taken from Jurassic and Cretaceous materials of the Algayat-Crevillente Unit, which is located in the eastern part of the Subbetic Zone (province of Alieante, SE Spain) (Fig. 1A). In this unit, the following formations (Fm) were distinguished (Nieto, 1996): (a) Gavilfin Fm dolostones and limestones with a stratigraphic break at the top (DI); (b) Zegri Fm --rhythmite of limestones and marls with a stratigraphic break (D2) at the top; (c) Veleta Fm limestones with chert; (d) Upper Ammonitico Rosso Fm comprising two members, separated by a stratigraphic break (D3), in which the dominant facies are nodular limestones. The Jurassic-Cretaceous stratigraphic break (D4) is located at the top; (e) Carretero Fm marly limestones alternating with marls; (f) Represa Fm dark (grey or black) marls, limestones and marly IimestQnes with a hardground which result in a stratigraphic break at the top; (h) 'Capas Rojas' Fm marly limestones alternating with white and pink marls. The sedimentary rocks with glauconite are located at two stratigraphic levels (Fig. 1B): (1) in a 2-30 cm thick stratum of green sandy limestone located at the base of the Zegri Fm and directly on the discontinuity D1 surface. The better outcrops of this bed coincide with palaeokarstic depressions developed at the top of the Gavil/m Fm (Sierra of Crevillente). (2) At the top of the Represa Fm, which is observed to the South of the Sierra Algayat (La Canalosa outcrop), in a 50 cm thick stratum which, in its upper part, has a bioturbated hardground covered by a fine plate of Fe oxides and limonite.", "title": "Glauconite and phosphate peloids in Mesozoic carbonate sediments ( Eastern Subbetic Zone , Betic Cordilleras , SE Spain )", "claims": null}, {"metadata": {"year": 2003}, "authors": ["E. Diaz", "M. Prasad", "G. Mavko", "J. Dvorkin"], "summary": "Glauconite is an iron rich variety of clay that can be found as individual pellets, composite grains, and intergranular cement. Its density ranges between 2.4 g/cm3 and 2.95 g/cm3, averaging 2.67 g/cm3. It has a Moh's scale hardness of 2. Authigenic glauconite is formed under a limited range of geologic and geochemical conditions; it typically develops on the outer margins of continental shelves, in areas of low sediment input (Odin, 1980), and its presence is valuable as an indicator of transgressive sequences.\n\nIdentifying glauconite in the subsurface is important for depositional environment interpretation, stratigraphic correlation, dating, tracing of unconformities, and geochemical exploration for source and reservoir rocks (Srivastava, 1986). A number of commercial hydrocarbon reservoirs are glauconitic sandstones\u2014for example in Colombia, Ecuador, Peru, Venezuela, Australia, Eastern China, North Sea, United States, Canada, Saudi Arabia, and Ireland.\n\nAlthough glauconite tends to exist as grains and as such is part of the rock framework, under moderate overburden pressure, these grains are easily compacted (Figure 1) and may form a pseudomatrix that occludes the original primary porosity. This behavior is in contrast to that observed in clay minerals. This problem, and the fact that there are no published studies about the elastic properties of glauconite and glauconitic sandstones, motivated this research to understand their rock physics properties. We present analyses of data from five lithologies containing varying amounts of glauconite and identify the best seismic attributes to evaluate its presence and the reservoir quality. Figure 1. \nOptical image of a glauconitic sandstone (made at 20X magnification) showing formation of a pseudomatrix that occludes the original primary porosity. Glauconite=green, Quartz=white. \n\n\n\nThe samples in this study come from Caballos Formation in Putumayo and Upper Magdalena Basins, Colombia (Figure 2), which is described as a marine transgressive blanket sandstone deposited in a shallow \u2026", "title": "Effect of glauconite on the elastic properties, porosity, and permeability of reservoir rocks", "claims": null}, {"metadata": {"year": 1993}, "authors": ["G. Carson", "S. Crowley"], "summary": "Abstract Glauconite and phosphate from the Cenomanian of Devon, southwest England, may be classified into different groups on the basis of petrography. Chemical analysis of these different fabrics reveals a strong association between glauconite and phosphate occurring on a sub-microscopic scale. Despite this intimate association, certain deductions may be made about the mineralogy of the glauconite (notably the K concentration) and its relationship to its morphology. Glauconite and phosphate formation took place in the transitional zone between oxic and anoxic environments (suboxic zone). Iron oxyhydroxides were a potential source of phosphorus and iron, while 'sulphide fluxing' maintained pH levels appropriate for carbonate dissolution and subsequent mineralisation.", "title": "The glauconite\u2212phosphate association in hardgrounds: examples from the Cenomanian of Devon, southwest England", "claims": null}, {"metadata": {"year": 1982}, "authors": ["L. Evans"], "summary": "Humo-Ferric Podzols (Typic Cryorthods) developed on glacial till or glaciolacustrine deposits were sampled in the Chapleau-Foleyet area of northeastern Ontario. pHs of Ae horizons ranged from 3.30 to 4.20 and of C horizons from 4.40 to 4.85. Cation exchange capacities (CEC) of mineral soil horizons averaged 1.7\u2002meq/100\u2002g, whilst those of the forest humus (fibrimors) averaged 166\u2002meq/100\u2002g. The CECs of the Bf and BC horizons were highly pH-dependent, averaging 3.2\u2002meq/100\u2002g at pH 4 and 16.8\u2002meq/100\u2002g at pH 8. The pH-dependency of CEC in these horizons was believed to be due to Al-complexes with organic matter. Calculated elemental loss using Zr as an internal standard showed significant losses of Mg, Fe, Ca and Al from Ae (albic) horizons and gains of Fe and, to a lesser extent, gains of Al in Bf (spodic) horizons. The clay mineralogy of Ae horizons was dominated by a high-charge smectite and that of Bf and BC horizons by chlorite.", "title": "CHARACTERISTICS OF SOME LOAMY-TEXTURED PODZOLS IN NORTHEASTERN ONTARIO", "claims": null}, {"metadata": {"year": 1975}, "authors": ["S. G. McRae"], "summary": "SUMMARY Twelve stagnogley soils of the Denchworth series from East Kent were studied to determine the effect of indigenous glauconite on their fertility. The presence or absence of glauconite was not the major factor controlling the potassium or magnesium status which was largely determined by the nature of the < 2 \u03bcm clay. Glauconitic soils had much higher levels of available phosphate than corresponding non-glauconitic soils due to calcium phosphate associated with the glauconite in the parent material. Weathering has partially converted this to aluminium and iron phosphates which were the main forms of phosphate present, especially in the A (0\u201315 cm) horizons.", "title": "The presence of indigenous glauconite in soils and its effect on soil fertility:II. Soils developed on Gault Clay (Denchworth series)", "claims": null}, {"metadata": {"year": 1941}, "authors": ["S. Hendricks", "C. S. Ross"], "summary": "A method of study found applicable to the clay minerals clarifies the chemical relations of glauconite and celadonite, and gives significant information about their genesis. The representative formula of glauconite is: (K, Ca 1/2 , Na) . 84 (Al . 47 Fe... . 97 Fe.. . 19 Mg .40 ) (Si 3.65 Al . 35 )O 10 (OH) 2 and of celadonite is: (K, Ca 1/2 , Na) .84 (Al . 34 Fe... .76 Fe.. . 24 Mg .76 ) (Si 3.89 Al . 11 )O 10 (OH) 2 , which represent very closely compositions of a large number of mineral specimens. These formulas show that both minerals belong to the heptaphyllite group of micas. A supply of ferrous iron (necessitating a reducing environment), magnesium, and potassium, is a controlling condition for the formation of both minerals. Glauconite, which forms only in a marine environment, maintained in a reducing condition by bacterial action, derives its magnesium and potassium from the sea water, and its other constituents from mud. Celadonite, which commonly forms in vesicular basalts, derives its essential magnesium, iron, and silica from olivine, and its other constituents from deuteric solutions.", "title": "Chemical composition and genesis of glauconite and celadonite", "claims": null}, {"metadata": {"year": 1989}, "authors": ["Yu  Rongbing"], "summary": "Glauconite is widely distributed in space and time. In China glauconite \u2014bearing strata are of extensive occurrence ranging from Late Cretaceous to Middle Proterozoic in age. X-ray powder analysis, X-ray diffraction analysis and differential thermal analysis revealed that glauconite has a mica \u2014type structure between dioctahedron and trioctahedron. Its chemical composition is: Al2O3 10.67%, SiO2 49.23%, MgO 3.24%, Fe2O3 17.40%, FeO 1.8%, and K2O 7.92%. Its crystal form and thermal properties are very similar to those of mica\u2014like clay minerals. According to isotopic age determinations of glauconite in modern oceans (including the East China Sea, the South China Sea and the Yellow Sea), we consider that in the extreme case glauconite is an authigenic mineral. It is generally formed in sedimentary rocks. It has further proved that glauconite can be used to determine the ages of sedimentary rocks.", "title": "Application of glauconite in sedimentary geochronology", "claims": null}], "query": "What is the common pH of glauconite in soils?", "summary_abstract": "The research question regarding the common pH of glauconite in soils is addressed through various studies that explore the mineral's characteristics and its environmental conditions. \n\nFrom the findings, it is evident that glauconite typically forms in marine environments under specific geochemical conditions. For instance, Diaz et al. (2003) highlight that glauconite is an iron-rich clay that develops on continental shelf margins, where low sediment input prevails. This suggests that the pH of the surrounding environment during glauconite formation is likely to be influenced by the marine conditions, which can vary but often lean towards neutral to slightly alkaline due to the presence of seawater.\n\nIn a more specific context, Carson and Crowley (1993) discuss the formation of glauconite in transitional zones between oxic and anoxic environments, indicating that the pH levels in these zones can be conducive to glauconite and phosphate mineralization. This implies that the pH may not be strictly acidic or basic but rather fluctuates based on the local sedimentary and biological processes.\n\nMoreover, the studies by OsnonNB HurroN et al. (2007) and Jim\u00e9nez-Mill\u00e1n et al. (2006) provide insights into the mineralogical composition of glauconite, noting that its formation is often associated with specific sedimentary conditions that could influence pH. For example, the presence of iron oxyhydroxides and the conditions of the sedimentary environment can affect the pH levels during glauconitization.\n\nOverall, while specific pH values for glauconite in soils are not explicitly stated in the abstracts, the collective evidence suggests that glauconite typically forms in environments where pH is likely neutral to slightly alkaline, influenced by marine conditions and sedimentary processes. Further research would be necessary to quantify the exact pH levels associated with glauconite in various soil contexts.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2010}, "authors": ["Abhijeet Ghadge", "S. Dani"], "summary": "This paper considers a systems perspective towards managing these risks. It presents variables that may affect next generation supply chains and applies a causal loop model towards depicting the causal linkages of these variables with future supply disruptions. The causal linkages between the variables are then highlighted with regards to the supply chain process and the nodes, and the causes of future risks are identified. From the causal loop diagram the risk propagation is derived in a form of risk framework.", "title": "Supply Chains Risks: A Systems Thinking Perspective", "claims": null}, {"metadata": {"year": 2006}, "authors": ["ChenYing-Ju", "SeshadriSridhar"], "summary": "V. Agrawal and S. Seshadri (2000) [Risk intermediation in supply chains. IIE Transactions, 32, 819-831] considered a problem in which a single risk neutral distributor supplies a short-lifecycle, l...", "title": "Supply chain structure and demand risk", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Li Yi-song"], "summary": "This paper focuses on the research findings in the supply chain risk management,particularly on identifying and mitigating supply side risks related to price and supply chain disruptions,and managing demand side risks through the judicious use of efficient and responsive chains,which would help companies develop effective,flexible and robust supply chains.", "title": "Analysis on Supply and Demand Risk Management of Supply Chain", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Santanu Mandal"], "summary": "With growing competition and dynamicity in the tastes and preferences of customers, there is a recent growth in the number of supply chain disruptions as well; for e.g. the recent flood in Thailand affecting the global operations of hard-disk manufacturers like Seagate, Western Digital to car makers Honda and Toyota. Thus an increased need is felt by these firms to adopt effective risk mitigation strategies which have further increased the necessity of studying several types of risk inherent in the supply chains and corresponding significant variables affecting each of them. Till date, there is no study conceptualizing a framework for measuring exclusively each of the risk types inherent in the supply chain. The present study attempts to formulate a framework for managing demand side risk from a supply chain perspective for a focal firm through structured literature review and have contributed by adding important predictors like trust towards customer, trust towards suppliers, and relationship quality with suppliers, relationship quality with customers and information sharing with suppliers with the existing ones like supplier dependence, customer dependence, supplier concentration, single sourcing and global sourcing.", "title": "Conceptualizing a Model for Demand -Side Risk in Supply Chain", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Amulya Gurtu", "Jestin Johny"], "summary": "The risks associated with global supply chain management has created a discourse among practitioners and academics. This is evident by the business uncertainties growing in supply chain management, which pose threats to the entire network flow and economy. This paper aims to review the existing literature on risk factors in supply chain management in an uncertain and competitive business environment. Papers that contained the word \u201crisk\u201d in their titles, keywords, or abstracts were selected for conducting the theoretical analyses. Supply chain risk management is an integral function of the supply network. It faces unpredictable challenges due to nations\u2019 economic policies and globalization, which have raised uncertainty and challenges for supply chain organizations. These significantly affect the financial performance of the organizations and the economy of a nation. Debate on supply chain risk management may promote competitiveness in business. Risk mitigation strategies will reduce the impact caused due to natural and human-made disasters.", "title": "Supply Chain Risk Management: Literature Review", "claims": null}, {"metadata": {"year": 2012}, "authors": ["A. Hafezolkotob", "A. Makui"], "summary": "Article history: Received 30 June 2011 Received in revised form September, 01, 2011 Accepted 06 September 2011 Available online 17 September 2011 Demand uncertainty obliges all participants through a supply chain to make decisions under uncertainty. These decisions extend across price, investment, production, and inventory quantities. We take account of competition between two supply chains under demand uncertainty. These chains internally are involved in vertical pricing competition; however, they externally participate in horizontal pricing and service level competitions by offering a singletype product to the market. Since firms may have various attitudes against demand uncertainty and its related risks, different risk structures for competitive supply chains are considered. We assume that risk-averse firms are able to decrease demand uncertainty by information gathered from market research. For risk-averse participants in a chain, market research investment is an appropriate ground for vertical coordination, which diminishes risk through a supply chain. Optimal strategies based on game theory are obtained for different risk structures; furthermore, for each structure the effects of risk sensitivity as well as market research efficiency on these optimal strategies are investigated. Finally, we propose two scenarios for information sharing between risk-averse participants. \u00a9 2012 Growing Science Ltd. All rights reserved", "title": "Competition of two supply chains with different risk structures: applying market research option", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Ulf Paulsson"], "summary": "A number of trends like globalisation, outsourcing, single sourcing, and leanness have together created a more vulnerable supply chain. This increased vulnerability has made supply chain risk issues a hot topic in academia as well as in industry. There has, however, been a lack of suitable models. The objective of this paper is to develop one such model \u2013 a conceptual model with the help of which risks, seen from the perspective of a single focal company in the chain, related to disturbances and interruptions in the physical flow in the supply chain, could be structured and described on a general level. Only economic risks are considered. The paper is based on a literature review and some case studies. The literature review was focused on articles and reports/books on supply chain risk management and related areas. There are one main case study (Beta) and two smaller ones (Alfa and Gamma). The author has spent five weeks on the floor in two of the factories of Beta. One basic assumption in the model is that the supply chain can be divided into three \"parts\" named: Demand side, Production and Supply side. Another assumption is that there exists a need to analyse the supply chain flow-related risks both from the perspective of the single company in the chain and from the perspective of the supply chain of which it is a part. Three existing cases illustrate the use of the model. Finally, weaknesses of the model are discussed and some possible directions for further elaboration are presented. (Less)", "title": "Developing a Supply Chain Risk Model", "claims": null}, {"metadata": {"year": 2022}, "authors": ["A. Duong", "V. Vo", "Maria Do Sameiro Carvalho", "P. Sampaio", "H. Truong"], "summary": "PurposeThis article aims to examine the simultaneous effect of risks on physical and intangible dimensions of supply chain performance under the globalization and Covid-19 perspectives.Design/methodology/approachThe manipulation of literature reviews together with the combination of Q-sort and empirical data in the construction industry to identify and assess risks and supply chain performance, is a novel approach in the supply chain risk management area. The analysis of Structural Equation\u00a0Modeling that is able to calculate the simultaneous impact of various risks on supply chain performance, is used to validate this relationship.FindingsGlobal supply chains are currently facing interruptions caused by several sources of inherent uncertainties, e.g. natural disasters, war and terrorism, external legal issues, economic and political instability, social and cultural grievances, and diseases. The weaknesses of the current global supply chain have been revealed, resulting in delays, supply unfulfillment, labor shortages and demand fluctuation. These supply chain risks have a great on supply chain performance indicators, and the magnitude of their impact tends to increasingly impact in the context of globalization and the Covid-19 pandemic. Findings showed that the proposed risk models can be explained with Variance of supplier performance (25.5%), Innovation and learning (21.2%), Internal business (61.9%), Customer service (39.4%) and Finance (39.7%).Research limitations/implicationsSupply chain managers should keep in mind acceptable cost/benefit trade-offs in corporate risk mitigation efforts associated with major contingency risks. In doing so, the proposed hypothesized model can be \u201ca road map\u201d to achieve this purpose. Our research favors the adoption of supply chain management strategies, e.g. postponement, speculation and avoidance.Originality/valueThe trend toward globalization and the emergence of the Covid-19 pandemic increasing supply chain complexity are regarded as key drivers of supply chain risk and therefore enhance vulnerability to supply chain.", "title": "Risks and supply chain performance: globalization and COVID-19 perspectives", "claims": null}], "query": "\"supply chain demand risk\" theory", "summary_abstract": "The research question regarding \"supply chain demand risk\" theory is addressed through various perspectives in the provided abstracts. Collectively, these papers highlight the complexities and challenges associated with managing demand risks within supply chains, particularly in the context of increasing globalization and market uncertainties.\n\nGhadge and Dani (2010) emphasize a systems approach to understanding supply chain risks, proposing a causal loop model that identifies variables affecting supply disruptions. This model serves as a framework for understanding how demand risks propagate through the supply chain. Similarly, Li (2010) focuses on mitigating demand-side risks by advocating for efficient and responsive supply chains, which can help organizations navigate price fluctuations and disruptions effectively.\n\nMandal (2012) contributes to the discourse by proposing a framework specifically for managing demand-side risks, identifying key predictors such as trust and relationship quality with suppliers and customers. This highlights the importance of interpersonal dynamics in mitigating risks associated with demand fluctuations. Furthermore, Hafezolkotob and Makui (2012) explore the competitive aspects of demand uncertainty, suggesting that risk-averse firms can reduce demand uncertainty through market research, which can enhance vertical coordination within supply chains.\n\nThe broader implications of these findings are underscored by Gurtu and Johny (2021), who discuss the increasing uncertainties in supply chain management due to global economic policies and natural disasters. They argue that effective risk mitigation strategies are essential for maintaining competitiveness in a volatile environment. Paulsson (2005) also notes that trends such as globalization and outsourcing have heightened supply chain vulnerabilities, necessitating a structured approach to risk management that considers both demand and supply-side factors.\n\nLastly, Duong et al. (2022) examine the simultaneous effects of various risks on supply chain performance, particularly in light of the disruptions caused by the COVID-19 pandemic. Their findings suggest that understanding and managing demand risks is crucial for maintaining supply chain performance amidst global uncertainties.\n\nIn summary, the literature collectively underscores the multifaceted nature of supply chain demand risk, advocating for comprehensive frameworks that incorporate relational dynamics, competitive strategies, and the impact of external uncertainties to effectively manage these risks.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2019}, "authors": ["Maksym Obrizan"], "summary": "This paper investigates the effect of violence on happiness in 14 countries affected by recent conflicts, using data from the \u00e2\u20ac\u02dcLife in Transition' III survey conducted in 2016 by the European Bank for Reconstruction and Development and the World Bank in all transition countries from a former Eastern Bloc (except for Turkmenistan). Out of 19,081 observations with non-missing responses, 2,605 (or 13.7%) households have been affected by some form of violence. When each of the three violence indicators \u00e2\u20ac\u201c physical injury, having a household member killed or living in a household displaced due to a conflict \u00e2\u20ac\u201c is entered separately, the probability of being happy reduces by 6.3-8.2% points depending on the type of violence. When three violence indicators are added at once, and also in some robustness checks, it is only forced displacement that has a statistically significant negative effect on happiness. These results indicate that violence has a prolonged and substantial negative effect on life satisfaction, given that many of the conflicts started 20 years ago or more.", "title": "Violent conflict and unhappiness: evidence from the 2016 \u00e2\u20ac\u02dcLife in Transition' III surve", "claims": null}, {"metadata": {"year": 2010}, "authors": ["C. Arslan", "Erdal Hamarta", "M. Uslu"], "summary": "The current study used a survey model to analyze 306 university students to investigate relationship between life satisfaction, self-esteem and conflict communication. Data were collected from the Rosenberg Self-esteem Scale,\u00a0the Satisfaction with Life Scale\u00a0and Conflict Communication Scale.\u00a0Pearson product-moment correlation coefficients were determined.\u00a0The results of the study show that self-esteem is positively correlated with confrontation, emotional expression, self-disclosure and life satisfaction. The results also show that life satisfaction is positively correlated with confrontation, emotional expression and self-disclosure. \n \n \u00a0 \n \n Key words:\u00a0Conflict communication, self-esteem, and life satisfaction", "title": "The relationship between conflict communication, self- esteem and life satisfaction in university students", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Yew-Kwang  Ng"], "summary": "Life satisfaction is likely to be more (than happiness) liable to be affected by shifts in the aspiration level, reducing the comparability of the resulting indices. Life satisfaction and/or preference may differ from happiness due to a positive valuation on the contribution to or a concern for the happiness of others. In the presence of such a divergence, levels of life satisfaction may be misleading.", "title": "Happiness or Life Satisfaction?", "claims": null}, {"metadata": {"year": 2018}, "authors": ["A. K. Singh", "G. Shankar", "Aman Sachdeva"], "summary": "Work life balance is a function of the perceived work to family and family to work conflict. The current research in the work life domain has moved from the scarcity hypothesis to the enhancement hypothesis with the emphasis on the positive spillover between the two domains. Spirituality also affects the perception of balance and conflict. This study explores the link between work life balance, conflict, enrichment, spirituality, and life satisfaction amongst respondents in Education, Banking/ other Professional Services, and Healthcare. The findings reveal that the three sectors are significantly different with Educationists having higher mean scores on balance, enrichment, organizational citizenship behaviors, spirituality, and life satisfaction. Work family conflict, enrichment, and spirituality significantly influence balance and life satisfaction of the respondents.", "title": "Exploring the Link Between Work Life Balance, Conflict, Enrichment, Spirituality and Life Satisfaction: An Empirical Study", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Md. Muztoba Ali", "Md. Rafat Tahsin", "Nurunnahar Sompa"], "summary": "This research focuses on the associations between cognitive emotion regulation processes, conflict resolution strategies and life satisfaction of persons having different types of love and relationship experiences. Cognitive emotion regulation indicates cognitive reactions, related to the emotional involvement, of our brain to an event that attempts to influence the elicited emotions or that event itself. The study is a cross-sectional one with 310 Bangladeshi respondents in the age range 20-34. Several groups and sub-groups were formulated. For the overall sample, satisfaction with life had a statistically significant positive correlation with adaptive cognitive emotion regulation strategies and conflict resolution styles, while it had a statistically significant negative correlation with less-adaptive cognitive emotion regulation strategies (p < 0.01). ANOVA and t-test analysis showed some crucial findings by comparing the primary groups and sub-groups of the study, regarding differences in life satisfaction and the differences in the uses of specific cognitive emotion regulation strategies for various love experiences. Standard regression analysis of data also revealed that the model explained a large portion of variance (50.40%) in life satisfaction (p < .001).", "title": "Exploring the Relationship between Life Satisfaction and Love-relationship Experiences under the Light of Cognitive Emotion Regulation and Conflict Resolution: The Consequences of their Interplays", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Juan Camilo  Chaparro", "Eduardo A.  Lora"], "summary": "This paper makes use of the 2006 Gallup World Survey, which includes opinions on satisfaction with various aspects of life in 130 countries. Although a very solid relationship is found between satisfaction and income (both across and within countries), raising doubts regarding the well-known Easterlin Paradox, a new paradox arises: \"unhappy growth,\" where faster growth rates are accompanied by lower levels of satisfaction. The losses of satisfaction associated with growth are more pronounced in the material domains of life and are greater in richer and more urban societies. At the individual level, although higher incomes tend to be reflected in greater satisfaction, an increase in the income of the social group to which an individual belongs has the opposite effect. The conflictive relationship between satisfaction and income has implications for political economy. In particular, it suggests a simple mechanism for explaining various characteristic traits of economic and social populism.", "title": "The Conflictive Relationship between Satisfaction and Income", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Eduardo Lora", "J. Chaparro"], "summary": "This paper makes use of the 2006 Gallup World Survey, which includes opinions on satisfaction with various aspects of life in 130 countries. Although a very solid relationship is found between satisfaction and income (both across and within countries), raising doubts regarding the well-known Easterlin Paradox, a new paradox arises: \u201cunhappy growth,\u201d where faster growth rates are accompanied by lower levels of satisfaction. The losses of satisfaction associated with growth are more pronounced in the material domains of life and are greater in richer and more urban societies. At the individual level, although higher incomes tend to be reflected in greater satisfaction, an increase in the income of the social group to which an individual belongs has the opposite effect. The conflictive relationship between satisfaction and income has implications for political economy. In particular, it suggests a simple mechanism for explaining various characteristic traits of economic and social populism.", "title": "The Conflictive Relationship between Satisfaction and Income", "claims": null}, {"metadata": {"year": 2011}, "authors": ["M. Moland"], "summary": "Past research regarding the link between conflict resolution styles and marital satisfaction have been consistent; each partner's level of relationship satisfaction is positively related to the frequency with which both partners use constructive strategies to resolve conflict (such as agreement, compromise, and humor) and negatively related to the frequency with which each partner uses destructive strategies to resolve conflict (such as conflict engagement, withdrawal, and defensiveness) (Kurdek, 1995). The purpose of this research was to examine the links between conflict and satisfaction in romantic relationships that are one or more years in duration. Six self-report indices (the Relationship Assessment Scale - RAS, Conflict Resolution Styles Inventory - CRSI, Ineffective Arguing Inventory - IAI, Assessing Relationship Conflicts Questionnaire - ARCQ, Relationship Styles Questionnaire - RSQ, and Last Argument Questionnaire - LAQ) were administered to 235 participants. Several significant findings were obtained. Fifty six percent of the variance in relationship satisfaction was explained by all the conflict measures (CRSI, IAI, and RSQ). Females scored significantly higher than males on conflict engagement. For couples with more severe perpetual problems, Satisfaction was lower on the Relationship Assessment Scale. Married participants scored significantly higher than participants who were in a dating relationship on both conflict engagement and withdrawal. Post-graduates (those earning a Master's degree and above) scored significantly higher than those who either attended and/or graduated from college on relationship satisfaction and positive problem solving, and lower on ineffective arguing. Participants who reported a medium income level scored significantly higher on conflict withdrawal than those who reported they earned a low income.", "title": "Conflict And Satisfaction In Romantic Relationships", "claims": null}], "query": "Conflict and happiness or life satisfaction", "summary_abstract": "The relationship between conflict and happiness or life satisfaction is explored across several studies, revealing a complex interplay between various forms of conflict and individual well-being.\n\nObrizan (2019) highlights the detrimental impact of violence on happiness, noting that individuals affected by violence\u2014particularly those who have experienced forced displacement\u2014exhibit a significant reduction in life satisfaction. This suggests that exposure to conflict can have long-lasting negative effects on emotional well-being.\n\nIn a different context, Arslan et al. (2010) investigate the role of conflict communication among university students, finding that effective communication strategies, such as confrontation and emotional expression, are positively correlated with life satisfaction. This indicates that how individuals manage conflict can influence their overall happiness.\n\nAli et al. (2022) further elaborate on the relationship between conflict resolution strategies and life satisfaction, demonstrating that adaptive cognitive emotion regulation strategies are associated with higher life satisfaction, while maladaptive strategies correlate negatively. This underscores the importance of effective conflict resolution in fostering emotional well-being.\n\nMoland (2011) examines conflict resolution styles in romantic relationships, revealing that constructive conflict resolution strategies are linked to higher relationship satisfaction. This suggests that the manner in which couples handle conflicts can significantly affect their overall happiness within the relationship.\n\nOverall, these studies collectively suggest that while exposure to conflict\u2014especially violent conflict\u2014can severely diminish happiness and life satisfaction (Obrizan, 2019), effective conflict management and resolution strategies can enhance well-being (Arslan et al., 2010; Ali et al., 2022; Moland, 2011). Thus, the impact of conflict on happiness is not solely negative; it is also shaped by the ways individuals navigate and resolve conflicts in their lives.", "summary_extract": null}, {"papers": [{"metadata": {"year": 1991}, "authors": ["M. Shizawa", "K. Mase"], "summary": "The principle of superposition is applied to various motion estimation problems. It can potentially resolve the difficulty of analyzing multiple motion, transparent motion and motion boundaries by using a common mathematical structure. The authors demonstrate that, by applying the principle, the techniques of optical flow, 3D motion and structure from flow fields, direct method for 3D motion and structure recovery, motion and structure from correspondences in two frames can be extended coherently to deal with multiple motion. The theory not only produces multiple-motion versions of the existing algorithms, but also provides tools for the theoretical analysis of multiple motion. Since the approach is not at the algorithm level as are conventional segmentation paradigms, but at the level of computational theory, i.e. of constraints, theoretical results derived also contribute to psychophysical and physiological studies on the preattentive stages of biological motion vision systems. The paper emphasizes the universality of the principle.<<ETX>>", "title": "Principle of superposition: a common computational framework for analysis of multiple motion", "claims": null}, {"metadata": {"year": 1964}, "authors": ["G. Duff"], "summary": "The principle of superposition of states requires that the states of a dynamical system form a linear vector space. This hypothesis of linearity has usually been regarded as a fundamental postulate in quantum mechanics, of a kind that cannot be explained by classical concepts. Indeed, Dirac [2, p. 14] comments that \"the superposition that occurs in quantum mechanics is of an essentially different nature from any occurring in the classical theory, as is shown by the fact that the quantum superposition principle demands indeterminacy in the results of observations in order to be capable of a sensible physical interpretation.\"", "title": "On the Principle of Superposition in Quantum Mechanics", "claims": null}, {"metadata": {"year": 1975}, "authors": ["Richard  Schlegel"], "summary": "With the interaction interpretation, the Lorentz transformation of a system arises with selection from a superposition of its states in an observation-interaction. Integration of momentum states of a mass over all possible velocities gives the rest-mass energy. Static electrical and magnetic fields are not found to form such a superposition and are to be taken as irreducible elements. The external superposition consists of those states that are reached only by change of state of motion, whereas the internal superposition contains all the states available to an observer in a single inertial coordinate system. The conjecture is advanced that states of superposition may only be those related by space-time transformations (Lorentz transformations plus space inversion and charge conjugation). The continuum of external and internal superpositions is examined for various masses, and an argument for the unity of the super-positions is presented.", "title": "Superposition in quantum and relativity physics\u2014An interaction interpretation of special relativity theory: Part III", "claims": null}, {"metadata": {"year": 2007}, "authors": ["Shi Tian-zhi"], "summary": "Solving the null results of Michelson-Morley experiments that have puzzled physicists for more than a century,the article sets out one of the universal fundamental principles in physics\u2014\u2014the principle of motion superposition.It can be stated as: the total motion of a particle is the superposition of its independent motions simultaneously,motions at different times or at different places or of different objects can never superpose.The principle of Michelson-Morley experiments is totally wrong because it miscomputes the optical path difference,which is a mere function of distances and refraction indexes of mediums and is independent of time.The Michelson-Morley experiments never disprove the existence of ether.", "title": "ON THE PRINCIPLE OF MICHELSON-MORLEY EXPERIMENTS", "claims": null}, {"metadata": {"year": 2018}, "authors": ["M. Zych", "F. Costa", "T. Ralph"], "summary": "In modern physics only relative quantities are considered to have physical significance. For example, position assigned to a system depends on the choice of coordinates, and only relative distances between different systems have physical relevance. On the other hand, in quantum theory the scenario where one system, A, is localised around certain position while another system B is in a spatial superposition is considered to be physically different from the scenario where A is in a superposition, while B is localised. Different physical effects are anticipated in the two scenarios especially when the two systems have widely different masses. Here we show that for any superposed amplitudes that are related by a symmetry transformation, the above scenarios are physically equivalent: probabilities for any measurement are the same whether we assign the superposition state to system A or to system B. More generally, linearity of quantum theory guarantees that if a theory is invariant under some symmetry transformations it is necessarily also invariant under their arbitrary `superpositions'. Thus the notion of a superposition turns out to be relative to the choice of coordinates, once it is realised that relations between coordinates do not need to be classical.", "title": "Relativity of quantum superpositions", "claims": null}, {"metadata": {"year": 2007}, "authors": ["H. Feng"], "summary": "The view-point that \"electron goesthrough two slils at the sametime\",put forward by P.A.M.Dirac in his master piece The Principle of Quantum Mechanics is always argued and the question \"can the electron be split?\" will be considered naturally,arousing many diffrent meanings or paradox.Usually,such issue is not analyzed intensively,leading to many people's suspicion on the base of quantum theory.To understand above issue is related with \"the pinciple of of states\" in quantum mechanics.Compering with macroscopic phenomenon,there are many diffrent,new features in quantum phnomenon.We could get a integrated formulation of microscopic phenomenon only when we had recognized these features in quantum phenomenon and \"translated\" with classicalanguage to illustrate.", "title": "The Principle of Superposition of States in Quantum Mechanics", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Angelo  Bassi", "Tejinder P. Singh", "Hendrik  Ulbricht"], "summary": "The principle of linear superposition is a hallmark of quantum theory. It has been confirmed experimentally for photons, electrons, neutrons, atoms, for molecules having masses up to ten thousand amu, and also in collective states such as SQUIDs and Bose-Einstein condensates. However, the principle does not seem to hold for positions of large objects! Why for instance, a table is never found to be in two places at the same time? One possible explanation for the absence of macroscopic superpositions is that quantum theory is an approximation to a stochastic nonlinear theory. This hypothesis may have its fundamental origins in gravitational physics, and is being put to test by modern ongoing experiments on matter wave interferometry.", "title": "Is Quantum Linear Superposition an Exact Principle of Nature", "claims": null}, {"metadata": {"year": 2007}, "authors": ["Walter  Blum"], "summary": "We apply the principle of superposition to two quantum mechanical states of which one has a short life time 1/\u0393. They must be identical, but may have a small difference in energy-momentum comparable to \u0393. This produces new effects resulting from the linear relationship that appears between the S-matrix element and the production cross section of the decay products of the short lived state. An optical theorem for diffraction dissociation processes is proposed.This mechanism also provides a measure of the non-orthogonality between unstable particles that are eigenstates of a non-Hermitian Hamiltonian.", "title": "Extension of the Quantum Mechanical Principle of Superposition to Non-Identical States with Short Life Times", "claims": null}], "query": "How does one consolidate the superposition principle\u2014where a mere electron is capable of a simultaneous multi-spatial existence\u2014with the linearity of motion associated with speed comprehended as a function of space over time?", "summary_abstract": "The research question regarding the consolidation of the superposition principle with the linearity of motion, particularly in the context of an electron's multi-spatial existence and its speed as a function of space over time, is addressed through various perspectives in the provided abstracts.\n\nThe principle of superposition is a foundational concept in quantum mechanics, asserting that a system can exist in multiple states simultaneously. Duff (1964) emphasizes that this principle necessitates a linear vector space for the states of a dynamical system, which is a departure from classical interpretations. This linearity is crucial for understanding how quantum systems, such as electrons, can exhibit behaviors that seem paradoxical when viewed through a classical lens.\n\nShizawa and Mase (1991) extend the superposition principle to motion estimation problems, suggesting that it can coherently address complex motion scenarios. Their work implies that the mathematical structure underlying superposition can be applied to analyze motion in a way that aligns with the linearity of motion, thereby providing a framework for understanding how multiple motions can coexist and be analyzed simultaneously.\n\nFeng (2007) discusses the implications of the superposition principle in quantum mechanics, particularly in relation to the behavior of electrons. He notes that the notion of an electron \"going through two slits at the same time\" raises questions about the nature of quantum states and their interpretation. This highlights the challenge of reconciling quantum phenomena with classical concepts of motion and existence.\n\nZych et al. (2018) further explore the relative nature of superposition in quantum systems, indicating that the physical significance of superposition is contingent upon the choice of coordinates. This suggests that the linearity of quantum theory allows for a flexible interpretation of superposition, which could be relevant in understanding how an electron's speed and position can be perceived in a multi-spatial context.\n\nBassi et al. (2015) address the limitations of superposition in macroscopic objects, proposing that while quantum superposition is well-established for small particles, it does not manifest in larger systems. This distinction is crucial for understanding the boundaries of the superposition principle and its implications for motion and speed.\n\nIn summary, the collective insights from these papers suggest that the superposition principle is integral to understanding the behavior of quantum systems, including electrons, and that its linearity can be applied to analyze motion in a multi-dimensional context. However, the challenge remains in reconciling these quantum behaviors with classical notions of motion and existence, particularly as the scale of the system increases.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2011}, "authors": ["Teng Hao-hua"], "summary": "Purpose To study the clinical,histopathological and prognostic characteristics of pituitary adenoma.Methods A retrospective analysis were performed by review of clinical,radiological and prognostic data and hematoxylin-eosin and immunohistochemical stains in 108 cases of pituitary adenoma from May 2005 through December 2007.Results Of the 108 cases in our series,45 were males and 63 females.The mean age at presentation was 48.0 years(range 17~89 years).By magnetic resonance,18 cases were microadenomas,87 were macroadenomas and the remained 3 were giant adenomas.Immunohistochemically,50 cases were growth hormone producing adenoma,28 prolactin producing adenoma,11 adenocorticotropic hormone producing adenoma,10 gonadotropin producing adenoma,4 null cell adenomas and 5 other mixed hormones-secreting adenoma.The follow-up duration ranged from 30 months to 60 months(median of 43.9 months),and 36 cases of them recurred.Conclusions The recurrence of pituitary adenoma is significantly related to the size of the tumor and whether it is completely resected(P0.05),however,not related to the patient's age,sex,immunophenotype and invasiveness(P0.05).The close correlation of immunohistochemical results with clinical manifestations suggests that immunophenotyping might be useful in the classification,further studies and treatment of pituitary adenoma.", "title": "Pituitary adenoma: clinicopathologic analyses of 108 cases", "claims": null}, {"metadata": {"year": 1980}, "authors": ["C. Muhr", "J. Bergstrom", "P. Enoksson", "R. Hugosson", "P. Lundberg"], "summary": "This study was based on 24 consecutive patients operated on for pituitary adenoma with suprasellar extension, and treated postoperatively with radiotherapy. The follow-up period was 5 to 10 years. A clinical examination, endocrine evaluation, perimetry, computerized tomography (CT) and sellar roentgenography were performed in 19 of 20 surviving patients. The CT scans revealed four recurrences, while perimetry only showed an increased visual field defect in one patient. Plain sellar films were of no value for diagnosis of recurrence. Prolactin determinations seemed to have a limited value in recurrence diagnosis, but very high levels spoke in favor of a recurrence. Thus, CT was found to be the superior method of diagnosing recurrences. It is proposed that CT be used regularly in the follow-up study of patients operated on for a pituitary adenoma.", "title": "FOLLOW\u2010UP STUDY WITH COMPUTERIZED TOMOGRAPHY AND CLINICAL EVALUATION 5 TO 10 YEARS AFTER SURGERY FOR PITUITARY ADENOMA", "claims": null}, {"metadata": {"year": 1986}, "authors": ["J. Trouillas", "C. Girod", "G. Sassolas", "B. Claustrat"], "summary": "Twenty-six out of 400 surgically removed pituitary tumors were identified as gonadotropic adenomas (frequency 6.25%). Morphologic, immunocytochemical, and hormonal characteristics of the gonadotropic adenoma are described. The following morphologic characteristics may suggest the diagnosis: arrangement in cords of cells showing signs of cellular activity, secretory granules, which vary in electron density, form, and size (mean diameter 150 nm) and which are numerous in the extensions near the capillaries, and a rough endoplasmic reticulum arranged in short cisternae. However, owing to the morphofunctional variations from one adenoma to another, we consider that only immunoreactivity with gonadotropin antisera proves the diagnosis. Because the material used to generate the gonadotropin antisera was purified but not pure, for precise characterization of the immunoreactivity, absorption tests with various antigens have to be performed. FSH-LH adenomas (n = 14), FSH adenomas (n = 7), and alpha-subunit adenomas (n = 5) but no LH adenomas were identified in our series. No evident morphologic difference related to the type of immunoreactivity and to the sex was found. Almost all the tumors were large with visual signs caused by suprasellar extension. A recurrence following adenomectomy was noted with a frequency of 12%. The diagnosis of gonadotropic adenoma was considered preoperatively in six out of 26 patients only, on the basis of increased serum gonadotropin levels. Correlations between hormonal levels and pathologic data were established in 17 patients. Seven tumors were associated with high serum gonadotropin levels (FSH and LH: three patients and FSH alone: four patients). In ten cases, the serum gonadotropin levels were normal but serum alpha-subunit assay had not been performed. For the pathologist systematically testing the adenomas with many antisera, gonadotropic adenomas are not as rare as for the clinician. Immunocytochemical studies with gonadotropin antisera and serum gonadotropic determination must be performed in all tumors, especially in those pituitary adenomas that appear nonfunctional.", "title": "The human gonadotropic adenoma: pathologic diagnosis and hormonal correlations in 26 tumors.", "claims": null}, {"metadata": {"year": 1998}, "authors": ["Hideko  Endo", "Toshiaki  Sano", "Shahnaz  Begum", "Shozo  Yamada"], "summary": "Clinically nonfunctioning pituitary adenomas are generally seen in middle-aged and older patients, and most of them may be gonadotropin-immunoreactive adenomas, that is, gonadotroph adenomas. Our aim was to clarify the relationships between the gonadotropin immunoreactivity, patient age, sex, and microscopic features in 68 gonadotroph adenomas with special reference to either gonadotropin-immunonegative or intensively immunopositive adenomas.There were 68 patients with gonadotroph adenomas (mean age 54.7 yr) in the study, including 39 men (mean age, 52.8 yr) and 29 women (mean age, 57.4 yr). The adenomas were diagnosed on the basis of immunoreactivity for gonadotropins (\u03b2-subunit of follicle-stimulating hormone: \u03b2-FSH; \u03b2-subunit of luteinizing hormone: \u03b2-LH; and the \u03b1-subunit of the pituitary glycoprotein hormone: \u03b1-SU) by the avidin-biotin peroxidase complex (ABC) method or by the characteristic histological feature of a perivascular or pseudorosette pattern, that is, the cells aligned polarity directed toward the capillaries.Fifty-four adenomas (79%) were positive for one or more gonadotropin subunits and \u03b2-FSH was the most common subunit encountered (47/68, 69%). In men \u03b2-FSH immunoreactivity was similar among all age groups, whereas in women, it was significantly less frequent in patients who were 50 yr or older, compared to younger patients. Gonadotropin-immunonegative adenomas were seen in 4 men (mean age, 46.8 yr) and 10 women (mean age, 61.5 yr). Among the 22 women aged 50 or over, \u03b2-FSH was negative in 12 tumors (55%), whereas in men of the same age group, it was negative in 3 of 26 tumors (12%). The reason for this reduced frequency is not clear, but the postmenopausal state and associated changes in the systemic endocrine state may play a role.Adenomas that were intensively positive for \u03b2-FSH showed an unusual morphology other than the characteristic perivascular pattern, regardless of the patients' age and sex; the tumor cells had abundant vacuolated cytoplasms and were arranged in a sheet-like pattern. Electron microscopically, these cells with abundant cytoplasm had well-developed Golgi complexes, suggesting an enhanced activity of gonadotropin synthesis, and these adenomas seem to be endocrinologically, if not clinically, functioning.The results indicate that gonadotroph adenomas may vary from functioning adenomas with intense immunoreactivity and unusual histology to immunonegative and less functioning adenomas, which are more frequent in women 50 yr or older.", "title": "Human pituitary gonadotroph adenomas: Relationship between gonadotropin immunoreactivity and clinicopathologic findings", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Kiyoshi  Mukai", "Edward L Seljeskog", "Louis P Dehner"], "summary": "The clinical manifestations, pathological features and follow-up data on 12 cases of pituitary adenoma in patient less than 20 years old were evaluated. This group represented 8.5% of our 142 cases of pituitary adenoma from all age groups during the period of study. There were five males and seven females whose ages ranged from 13 to 19 years at diagnosis. Immunocytochemical staining demonstrated the presence of prolactin in 10 tumors, ACTH in one tumor and the remaining neoplasm was negative for the five major pituitary hormones (prolactin, hGH, ACTH, TSH, gonadotrophin). The results of the immunocytochemistry correlated appropriately with the clinical manifestations. Extension beyond the sella turcica at presentation was a common feature as evidenced by the high incidence of visual defects (75%). A complete excision was accomplished in only two patients. The aggressive behavior of these tumors was demonstrated by a recurrence rate of 50% and only a single long-term cure. Early detection and therapy, if possible, are essential for the successful management of pituitary adenomas in younger patients as implied by our study and other reports in the literature.", "title": "Pituitary adenomas in patients under 20 years old. A clinicopathological study of 12 cases", "claims": null}, {"metadata": {"year": 2011}, "authors": ["A. Pal", "C. Capatina", "A. P. Tenreiro", "P. D. Guardiola", "J. Byrne", "S. Cudlip", "N. Karavitaki", "J. Wass"], "summary": "Objectives\u2002 The frequency of pituitary tumour regrowth after an episode of classical pituitary apoplexy is unknown. It is thus unclear whether regrowth, if it occurs, does so less frequently than with non\u2010apoplectic non\u2010functioning pituitary macroadenomas that have undergone surgery without postoperative irradiation. This has important repercussions on follow up protocols for these patients.", "title": "Pituitary apoplexy in non\u2010functioning pituitary adenomas: long term follow up is important because of significant numbers of tumour recurrences", "claims": null}, {"metadata": {"year": 1981}, "authors": ["C. Muhr", "K. Bergstr\u00f6m", "R. Hugosson", "P. Lundberg"], "summary": "A study of the recurrence rate in 41 patients operated and postoperatively treated with irradiation for a pituitary adenoma was performed 10-15 years after operation. 15 of the 41 patients are now deceased among whom 1 had a known recurrence. 25 out of the now 26 living patients were investigated with a clinical examination, perimetry, computed tomography (CT), roentgen examination of the sella turcica and an endocrine evaluation. The outstanding capability of the CT to visualize pituitary adenoma recurrences was clearly demonstrated as this technique revealed recurrences in 6 patients, only 2 of which had progressive visual field defects and none progressive changes on plain sellar films.", "title": "Pituitary adenomas: computed tomography and clinical evaluation in a follow-up after surgical treatment.", "claims": null}, {"metadata": {"year": 1980}, "authors": ["C  Muhr", "K  Bergstr\u00f6m", "R  Hugosson", "P O Lundberg"], "summary": "A study of the recurrence rate in 41 patients operated and postoperatively treated with irradiation for a pituitary adenoma was performed 10-15 years after operation. 15 of the 41 patients are now deceased among whom 1 had a known recurrence. 25 out of the now 26 living patients were investigated with a clinical examination, perimetry, computed tomography (CT), roentgen examination of the sella turcica and an endocrine evaluation. The outstanding capability of the CT to visualize pituitary adenoma recurrences was clearly demonstrated as this technique revealed recurrences in 6 patients, only 2 of which had progressive visual field defects and none progressive changes on plain sellar films.", "title": "Pituitary adenomas: computed tomography and clinical evaluation in a follow-up after surgical treatment.", "claims": null}], "query": "Follow-up frequency and duration for patients with gonadotroph pituitary adenomas", "summary_abstract": "The research question regarding the follow-up frequency and duration for patients with gonadotroph pituitary adenomas is addressed through various studies that highlight the clinical characteristics, recurrence rates, and diagnostic methods for these tumors.\n\nTeng (2011) conducted a retrospective analysis of 108 pituitary adenoma cases, noting that the follow-up duration ranged from 30 to 60 months, with a median of 43.9 months. The study found that tumor size and complete resection significantly influenced recurrence rates, suggesting that follow-up should be closely tied to these factors.\n\nMuhr et al. (1980) emphasized the importance of imaging techniques, particularly computed tomography (CT), in monitoring patients post-surgery. Their study of 24 patients indicated that CT was superior for detecting recurrences, which occurred in four cases during a follow-up period of 5 to 10 years. This suggests that regular CT scans could be a critical component of follow-up protocols.\n\nTrouillas et al. (1986) reported a recurrence rate of 12% in their study of gonadotropic adenomas, reinforcing the need for vigilant follow-up, especially given that many of these tumors were large and symptomatic. They highlighted that preoperative identification based on serum gonadotropin levels was limited, indicating that ongoing monitoring of hormonal levels may be necessary.\n\nEndo et al. (1998) further explored the characteristics of gonadotroph adenomas, noting that these tumors often present in middle-aged patients and can vary in their immunoreactivity. Their findings suggest that the follow-up approach may need to consider the specific immunohistochemical profile of the adenoma, as this could influence recurrence risk.\n\nIn summary, the collective findings from these studies suggest that follow-up for patients with gonadotroph pituitary adenomas should be tailored based on tumor size, surgical outcomes, and imaging results, with CT scans being particularly valuable for detecting recurrences. Regular monitoring of hormonal levels may also play a role in follow-up strategies, especially given the variability in tumor characteristics (Teng, 2011; Muhr et al., 1980; Trouillas et al., 1986; Endo et al., 1998).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2018}, "authors": ["M. Guerini", "F. Lamperti", "Andrea Mazzocchetti"], "summary": "In this paper we discuss some of the monetary policy issues that have involved major central banks worldwide since the 2008 financial crisis, and which remain open. We provide an excursus of the unconventional monetary policies adopted by central banks in the last decade, focusing on the European Central Bank and the Federal Reserve, and we discuss the rationale and efficacy of some unconventional monetary instruments, of inflation targeting and of central bank independence. We also provide a perspective on possible future developments of monetary policy. We argue that while unconventional monetary policy was useful, there is still ample space for improvement: in the recessions to come, unconventional monetary policy will need to be better coordinated with fiscal, micro- and macro-prudential policies to provide more inclusive results that might positively affect the real economy beyond the financial system.", "title": "Unconventional Monetary Policy: Between the Past and Future of Monetary Economics", "claims": null}, {"metadata": {"year": 2018}, "authors": ["M. Guerini", "F. Lamperti", "Andrea Mazzocchetti"], "summary": "In this paper we discuss some of the monetary policy issues that have involved major central banks worldwide since the 2008 financial crisis, and which remain open. We provide an ex-cursus of the unconventional monetary policies adopted by central banks in the last decade, focusing on the European Central Bank and the Federal Reserve, and we discuss the rationale and efficacy of some unconventional monetary instruments, of inflation targeting and of central bank independence. We also provide a perspective on possible future developments of monetary policy. We argue that while unconventional monetary policy was useful, there is still ample space for improvement: in the recessions to come, unconventional monetary policy will need to be better coordinated with fiscal, micro- and macro-prudential policies to provide more inclusive results that might positively affect the real economy beyond the financial system.", "title": "Unconventional monetary policy: between the past and future of monetary economics*", "claims": null}, {"metadata": {"year": 2011}, "authors": ["John C. Williams"], "summary": "Presentation to the Swiss National Bank Research Conference, Zurich, Switzerland, September 23, 2011", "title": "Unconventional monetary policy: lessons from the past three years", "claims": null}, {"metadata": {"year": 2013}, "authors": ["John C. Williams"], "summary": "Good afternoon and thank you. It's a pleasure to be on this panel. The main theme of my remarks is the critical importance of research in designing and implementing unconventional monetary policies since the financial crisis. I hope this isn't a controversial thesis. Nonetheless, I should note that, as always, my remarks represent my own views and do not necessarily reflect the views of others in the Federal Reserve System. In response to the financial crisis, the Federal Open Market Committee (FOMC) lowered the federal funds rate to essentially zero in December 2008 and has kept it there ever since. The FOMC then turned to several \" unconventional \" monetary policies, such as large-scale asset purchases (LSAPs) and forward guidance. In my remarks today, I'd like to focus on the lessons we've learned about these unconventional policies and their prospective role going forward. Before the financial crisis, almost everything we knew about unconventional monetary policy came from studies of the Japanese \" lost decade \" and a few scattered episodes in the U.S., such as Operation Twist in the 1960s. The conventional wisdom at the time seemed to be that longer-term bond purchases didn't have much effect on longer-term yields. As the financial crisis unfolded, however, central banks became willing to give large-scale bond purchases a try. In late 2008, the Federal Reserve announced it would purchase a large quantity of agency debt and mortgage-backed securities, and the announcement of this program had a striking effect on longer-term yields and mortgage rates. In January 2009, the Bank of England followed suit and announced its own asset purchase program, which similarly had sizable effects.", "title": "Lessons from the financial crisis for unconventional monetary policy", "claims": null}, {"metadata": {"year": 2015}, "authors": ["R. Vergara"], "summary": "Monetary policy has mostly been about setting the appropriate interest rates to stir the economy towards a path coherent with its fundamentals. In this sense, the developments in most central banks in the last 6 years have not been out of the box, so to speak. But we can probably say, with little risk of overstating reality, that the ways of achieving changes in the cost of credit through monetary policy has changed significantly since the global financial crisis.", "title": "Rodrigo Vergara: Monetary policy through asset markets - lessons from unconventional measures and implications for an integrated world", "claims": null}, {"metadata": {"year": 2013}, "authors": ["T. White", "Cetin Ciner", "Peter W. Schuhmann", "Joseph A. Farinella"], "summary": "For several decades, central banks in advanced economies typically used a policy interest rate as their tool for conducting monetary policy. In response to the global financial crisis (GFC) of 2007\u20132009 and the deep recession it caused in parts of the world, central banks in many advanced economies lowered their policy interest rates to near-zero levels. As economic growth remained weak, interest rates persisted at near-zero levels and some central banks used \u2018unconventional\u2019 monetary policy to stimulate economic activity. (See Explainer: The Global Financial Crisis and the Deputy Governor\u2019s speech Lessons and Questions from the GFC for an explanation of the causes and consequences of the GFC.) These unconventional measures have again become prominent as central banks around the world respond to the severe economic consequences of the coronavirus (COVID-19) global pandemic.", "title": "Unconventional Monetary Policy", "claims": null}, {"metadata": {"year": 2020}, "authors": ["C. Pfister", "Jean-Guillaume Sahuc"], "summary": "This paper takes stock of the literature on the unconventional monetary policies, from their implementation to their effects on the economy. In particular, we discuss in detail the two main measures implemented in most developed economies, namely forward guidance and large-scale asset purchases. Overall, there is near consensus that these measures have been useful, although there are a few dissenting views. Because unconventional monetary policies have left their mark on economies and on the balance sheets of central banks, we offer insights into their legacy and ask whether they have led to a change in \"the rules of the game\" for setting interest rates and choosing the size and composition of central banks\u2019 balance sheets. Finally, we discuss whether to modify the objectives and the instruments of monetary policy in the future, in comparison with the pre-crisis situation.", "title": "Unconventional Monetary Policies: A Stock-Taking Exercise1", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Anil Perera"], "summary": "The global financial and economic crisis has warranted authorities to pursue extraordinary policy measures including fiscal stimulus and excessive monetary accommodation. Particularly, central banks in many countries resorted to conventional monetary policies and hence, exhausted the entire monetary arsenal and also adopted unconventional monetary policies including quantitative easing. These policies have resulted in positive impacts by way of restoring financial markets, ensuring stability in financial systems, mitigating the adverse impact of economic recession and also supporting the recovery process. However, the undue expansion in balance sheets of central banks and resultant increases in monetary bases due to the adoption of extraordinary policy measures pose several risks in different dimensions. Particularly, the continuation of such policies pose a pressure on achieving the key mandate of central banks, i.e. maintaining price stability over the medium to longer horizon. This study points to possible pressures on price levels during the approaching period due to the adoption and also the continuation of unconventional monetary policies by central banks, particularly in advanced countries. In addition, this study examines possible break down in the key channel of monetary transmission mechanism, i.e. interest rate channel during the crisis period. In such context, this study suggests unwinding balance sheet expansions without further delays as economies have revived, credit and broad money aggregates recorded positive growth rates and price levels have commenced to pick-up. However, exit strategies need to follow a timely and gradual process. Hence, this study draws important implications for central bankers and financial market players, both in advanced and emerging countries.", "title": "Monetary Policy in Turbulent Times: Impact of Unconventional Monetary Policies", "claims": null}], "query": "Policy Lessons and the Future of Unconventional Monetary Policy", "summary_abstract": "The collection of papers provides a comprehensive overview of the evolution, effectiveness, and future considerations of unconventional monetary policy, particularly in the context of the financial crises that have shaped recent economic landscapes.\n\nGuerini, Lamperti, and Mazzocchetti (2018) highlight the unconventional monetary policies adopted by major central banks, such as the European Central Bank and the Federal Reserve, since the 2008 financial crisis. They argue that while these policies have been beneficial, there is significant room for improvement. The authors emphasize the need for better coordination between unconventional monetary policy and fiscal, micro-, and macro-prudential policies to achieve more inclusive economic outcomes.\n\nWilliams (2013) underscores the critical role of research in shaping unconventional monetary policies post-crisis. He notes that the Federal Reserve's shift to large-scale asset purchases and forward guidance was a significant departure from previous practices, which were largely informed by historical precedents like Japan's \"lost decade.\" This shift has had a notable impact on long-term yields and mortgage rates, suggesting that unconventional measures can be effective when traditional tools are exhausted.\n\nVergara (2015) and White et al. (2013) both discuss the transformation of monetary policy tools in response to the global financial crisis. They note that central banks have moved away from solely relying on interest rates to employing unconventional measures to stimulate economic activity, particularly as interest rates approached zero and economic growth remained sluggish.\n\nPfister and Sahuc (2020) provide a synthesis of the literature on unconventional monetary policies, particularly focusing on forward guidance and large-scale asset purchases. They conclude that these measures have been largely effective, but they also raise questions about the long-term implications for monetary policy frameworks and the potential need for adjustments in objectives and instruments moving forward.\n\nPerera (2010) addresses the risks associated with the extensive use of unconventional monetary policies, particularly the pressures they place on central banks' mandates for price stability. He advocates for a gradual unwinding of balance sheet expansions as economies recover, emphasizing the importance of a well-timed exit strategy to mitigate potential inflationary pressures.\n\nIn summary, the papers collectively suggest that while unconventional monetary policies have played a crucial role in stabilizing economies post-crisis, there is a consensus on the need for improved coordination with other policy measures and a careful approach to unwinding these policies to ensure long-term economic stability. The lessons learned from these experiences will be vital in shaping the future of monetary policy.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2007}, "authors": ["Judith A. Kolb", "Louise E. Sandmeyer"], "summary": "In the university initiative described in this article, a series of project teams were funded to work on a variety of collaborative projects. The focus of this piece is on the framework that was developed and executed to select, support, and evaluate these teams. The framework is explained and described using data gathered throughout the study and discussed using Kolb\u2019s components of organizational support for teams.", "title": "Building a Framework that Supports Project Teams: An Example from a University/Community Collaborative Initiative", "claims": null}, {"metadata": {"year": 2007}, "authors": ["Wang Jing-xiu"], "summary": "Project management is to combine various systems,project schemes,personnel and resources together and to complete every job of the project on specified time,in budget and within the scope of quality targets.In the course of project management,which is favorable to the flexibility of business administration to make team building easy,to encourage staff's enthusiasm and making,to improve the performances of enterprises,to strengthen and coordinate ability inside enterprises,and help the staff's instinct to work.The paper describes haw to build a project team,thus making all members get to a common view and work in full cooperation and with unity of purpose for realizing the final goal of project.", "title": "Brief Discussion on Team Building in Project Management", "claims": null}, {"metadata": {"year": 2002}, "authors": ["J. Sullivan", "D. W. Knight", "L. Carlson"], "summary": "Teamwork in lower division projects courses is increasingly common. To ensure student effectiveness in these teams, instructors must assume a facilitator role to nurture team process and progress. This paper describes team-building activities in two introductory engineering design/build projects courses at the University of Colorado at Boulder. The discussion is organized around developmental stages that teams typically pass through-forming, storming, norming and performing-with stage-appropriate reference to team needs, assessment and activities designed to build team skills. An evaluation of this team-building program is provided, based on skills self-assessment data gathered from students during one semester. Implications and suggestions for curriculum improvement are discussed.", "title": "Team building in lower division projects courses", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Liao Yuan-hong"], "summary": "A project is a temporary endeavor undertaken to create a unique product or service.A group of people from different functional departments,working together and exchanging information,is called a project team.To deal with the underlying complexities of the project teams formation process,a methodology for the project teams formation is developed,which is based on fuzzy clustering and grey decision theory.We can firstly get the construction of the project team by fuzzy cluster,through analyzing customer demands and characteristic of the projects.Secondly,the team members are chose by grey decision method based on team construction and the human resource of the project.After the two steps above-mentioned,we can get the effective project team lastly.In this team one can replace or share the work for the other,which contributes to applying human resource more effectively and reducing the cost of human resource in the conditions of uncertainty.", "title": "Project Team Formation based on the Methods of Fuzzy Clustering and Grey Decision Making", "claims": null}, {"metadata": {"year": 2007}, "authors": ["Liu Xuelon"], "summary": "Project team has been widely used various construction project as a main organizational model for construction enterprises.The paper describes the characteristics of project team,discusses how to build project team and team spirit,and provides some principles and suggestions for project team building.", "title": "Establishment of Project Team", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Chen Ming"], "summary": "The author points out that the development of project management is actually the result of practical activities of project management.In other words,project management is to combine various systems,project schemes,personnel and resources together and to complete every job of the project on specified time,in budget and within the scope of quality targets.Thereof,the project team and personnel quality is the key to the success and failure of any project.The paper describes how to build a project team,thus making all members get to a common view and work in full cooperation and with unity of purpose for realizing the final goal of project.", "title": "Brief Discussion on Team Building in Project Management", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Robert Buttrick"], "summary": "Abstract", "title": "The project framework", "claims": null}, {"metadata": {"year": 2010}, "authors": ["P. D. Meo", "Diego Plutino", "G. Quattrone", "D. Ursino"], "summary": "In this paper, we propose a system for the management of team building and team update activities particularly suited in a scenario where projectised organisations operate. Our system is characterised by three important features that appear particularly relevant in this scenario; specifically: 1) it defines and uses a model to uniformly represent and handle expert skills; 2) it is highly distributed and, therefore, is tailored to the typical organisation of the current job market where projectised organisations are intermediating most job positions; 3) it considers not only experts' technical skills, but also their social and organisational capabilities, as well as the affinity degree possibly shown by them when they worked together in the past. In this paper, we first describe our system in all details; then, we present the results of some experiments devoted to evaluate its performance; finally, we compare it with some related systems already proposed in the literature.", "title": "A team building and team update system in a projectised organisation scenario", "claims": null}], "query": "PROJECT TEAM BUILDING FRAMEWORK", "summary_abstract": "The collection of papers provides a comprehensive overview of frameworks and methodologies for effective project team building. Kolb and Sandmeyer (2007) present a structured framework developed for selecting, supporting, and evaluating project teams within a university initiative, emphasizing the importance of organizational support in fostering team success. Similarly, Wang (2007) highlights the significance of aligning team members towards a common goal, advocating for a collaborative environment that enhances enthusiasm and performance.\n\nSullivan et al. (2002) delve into the dynamics of teamwork in educational settings, outlining the developmental stages of team formation\u2014forming, storming, norming, and performing\u2014and the corresponding activities that can nurture team skills. This approach underscores the necessity of facilitator roles in guiding teams through these stages to ensure effectiveness.\n\nLiao (2010) introduces a methodology for project team formation that utilizes fuzzy clustering and grey decision theory, allowing for a systematic approach to assembling teams based on project characteristics and human resources. This method aims to optimize team composition and resource allocation, particularly in uncertain conditions.\n\nLiu (2007) and Chen (2005) both emphasize the foundational principles of project team building, advocating for the cultivation of team spirit and unity of purpose among members to achieve project objectives. Their insights reinforce the idea that a well-constructed team is crucial for project success.\n\nLastly, Meo et al. (2010) propose a system for managing team building activities in projectized organizations, focusing on the integration of technical and social skills among team members. Their system aims to enhance team dynamics by considering past collaborations and affinities, which can significantly impact team performance.\n\nCollectively, these papers illustrate that effective project team building requires a multifaceted approach, incorporating structured frameworks, developmental understanding, and strategic methodologies to foster collaboration and achieve project goals.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2019}, "authors": ["J. Canino", "K. Teichert"], "summary": "In 2018, people all over the world were reading Frankenstein in celebration of its 200 anniversary. There is no doubt that the ideas explored in Frankenstein are still timely and important to today\u2019s engineers. However, Frankenstein is rarely discussed in technical classes. This paper discusses a design project for first-year Mechanical Engineering students that asked students to select and explore themes from Frankenstein as a guide for the design of an autonomous robot. In essence, the students were required to develop a target customer that would benefit from the theme they selected. The use of the novel to generate concepts for the robots was supported by using a form of double column notes. This processes required the students to select several quotes from the book as inspiration for concept generation. Students sketched the robot ideas and explained the connection between the quote from the book and the concept. The primary contribution of this work is the lessons learned through the process of implementing this project. These lessons learned are illustrated using student work and an analysis of a survey to determine students\u2019 experiences with the project. Among the findings reported is the observation that many students developed merely a superficial, or surface level, connection to the literature. However, other students were able to delve deeper, presenting ideas such as social rejection. Telling results are presented from a student survey about the experience indicated that only 11% of the students read the entire book whereas 22% said they read 20% or less of the book, and only 15% of students reported enjoying reading the book. Perhaps most encouraging, 67% of students reported that there are important concepts in Frankenstein for today\u2019s engineers, and 63% said it was moderately to highly 2 important to incorporate humanities into science and engineering. These findings suggest an acknowledgment of the importance of the humanities in STEM from the students, and provide valuable insight for future implementation.", "title": "A Frankenstein-inspired Engineering Design Project", "claims": null}, {"metadata": {"year": 2019}, "authors": ["C. Parker"], "summary": "A guide to assist students with their English study of Mary Shelley's 1818 novel 'Frankenstein'", "title": "MCC Library: ENGLISH Year 12 Frankenstein : Themes and Ideas", "claims": null}, {"metadata": {"year": 2003}, "authors": ["D. Bice", "Tamra J. Courey"], "summary": "Abstract \"Frankenstein Meets Maslow\" details the academic exchange between colleagues (an English and a Nursing professor) when an English student writes a cross-disciplinary analysis of Mary Shelley's Frankenstein, within the framework of Maslow's Hierarchy of Needs. Once the dialogue begins, academic boundaries blur, pedagogy blends, and innovative insightful discourse ensues. ********** Those of us who teach literature-based developmental English often read our student essays in an almost semi-conscious, if not an auto-pilot state, fully expecting the traditional approach and 'freshman' interpretation of the steady, well worn classics we assign. Therefore, in literature survey courses or undergraduate baseline genre courses, we expect the pedantic evaluations that border on plot review with the insipid, stock, biographical references that may or may not be relevant. We have also come to expect the obvious interpretations that are conducive to the respective text, ones that may be absolutely accurate, but quite predictable, those repetitions of character, theme, landscape, or treatment that offer no thought or imagination but present merely what 'they' think we require. But once in awhile, there is that innovative essay that offers more than the obvious, one that transcends not only the assignment and its genre but the designated course of study, one that takes the student essay to a new level of thought, and one that encourages academic exchange between colleagues. Once in awhile there is that one student essay that inspires, illuminates, and yes, teaches the well-worn teacher. In my Summer 2002 English course (sequential composition that incorporates thematic literature), freshman students were asked to submit an in depth interpretation of Mary Shelley's Frankenstein. Predictably, most students rushed to examine: 1. The 'mad' scientist--Dr. Frankenstein who creates a being from body parts of assorted dead beings and who is so engrossed in his project that he withdraws from society and systematically deteriorates physically as well as emotionally. This perspective may also include an ethical look at religion vs. science, the scientific ego, and the God-complex. 2. Isolation--clearly in one level of the novel, the protagonist, Dr. Frankenstein is isolated within his realm of creation; also isolated are the characters left at home: Dr. Frankenstein's family, friends, and betrothed, Elizabeth. In another level of the novel, once the monster is 'born,' he is left to fend for himself; even those rustic characters he meets in his journey are isolated in one way or another. And finally, in the 'outer' level of the triad narrative, Robert Walden is isolated at sea, apart from his sister, who is left formulaically at home. 3. Abandonment--Victor immediately abandons his creation; this action culminates in the victimization of Victor's family and friend; 'sane' life as we know it is lost as we observe the monster abandoning his civility, with creator and creation ultimately doomed. 4. Revenge--the student will often gravitate to this 'simple' topic as it is recognizable and an often justifiable emotion. The respective arguments that detail who gets revenge and for what reason often proves interesting, but this topic is trite and tired. Of course, I am generalizing, and there are more treatments and many variations; but for the most part, essays from class to class and semester to semester are hardly earth shattering. Yet, that summer, one dynamic essay dared to link the fundamentals of a humanistic psychological theory to traditional literary content. And it worked; in fact, it 'rippled.' The essay explores Dr. Frankenstein's monster in relation to Abraham Maslow's Hierarchy of Needs. Maslow argues that there is a deliberate, methodical route individuals are motivated to travel in order to attain self-actualization, that is, the highest and most healthy state of being. \u2026", "title": "Frankenstein Meets Maslow", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Melissa Bloom Bissonette"], "summary": "Student readers often switch, with a taste for the unambiguous, from presuming Mary Shelley's monster to be wholly bad, to exonerating him completely, and sometimes swinging back to \"evil.\" Both sympathy and condemnation\u2014the \"whose fault is it\" debate\u2014curtail the kind of critical thinking and application that is the goal of most of the courses in which Frankenstein is taught. The students' imagination of the heart and the professor's conceptual framework, sympathy and analysis, run parallel to and often counteract or resist each other. This essay proposes the importance of emphasizing the monster himself, returning to the monster as a monster, refusing to allow him to melt into a symbol in the classroom; it further suggests a practice of \"teaching the monster\" as a pedagogy of alienation that can be applied beyond Frankenstein.", "title": "Teaching the Monster: Frankenstein and Critical Thinking", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Jihad Jaafar Waham"], "summary": "This article examines Mary Shelley's Frankenstein as an example of Gothic literature. The author analyzes the novel's themes, characters, and literary devices to explore how Shelley uses Gothic elements to create a complex and emotionally resonant work. The article also delves into the historical and cultural context in which the novel was written, highlighting the influence of Romanticism and Enlightenment philosophy. Ultimately, the article argues that Frankenstein is a masterpiece of Gothic literature that continues to captivate readers and inspire new interpretations. In \"The Art of Gothic Literature: An Analysis of Mary Shelley's Frankenstein,\" the author examines Shelley's famous novel and its contribution to the Gothic literary tradition. The article explores the novel's themes, including the dangers of scientific progress, the limits of human knowledge, and the consequences of playing god. The author also analyzes the novel's structure, characterization, and use of symbolism, highlighting the ways in which Shelley draws upon Gothic conventions while also subverting them. Ultimately, the article argues that Frankenstein remains a powerful and influential work of Gothic literature that continues to captivate readers more than two centuries after its publication. This article analyzes Mary Shelley's novel \"Frankenstein\" through the lens of gothic literature. The author explores how Shelley incorporates various gothic elements such as supernatural occurrences, grotesque imagery, and emotional intensity to create a dark and unsettling atmosphere. The article also delves into the themes of the novel, including the dangers of playing god and the isolation and alienation experienced by the creature. Through a close reading of the text, the author highlights the literary techniques that Shelley employs to convey these themes and to create a timeless work of gothic literature. Ultimately, the article argues that \"Frankenstein\" remains a relevant and powerful example of the gothic genre due to its ability to evoke fear, explore complex themes, and showcase the artistry of its author.", "title": "The Art of Gothic Literature: An Analysis of Mary Shelley's Frankenstein", "claims": null}, {"metadata": {"year": 2020}, "authors": ["K. A. Ozherelyev"], "summary": "The paper analyzes the key philosophical contexts and subtexts of M. Shelley\u2019s most famous work \u201cFrankenstein\u201d. According to the author of the article, the philosophical layer of this Gothic novel consists of ideas and maxims that directly inherit the concepts of the worldview platforms of Plato, J.-J. Russo, G. W. F. Hegel, K. F. Volney, W. Godwin, M. Wollstonecraft, as well as the philosophy of the New Age and romanticism. An assumption is made, on the one hand, about the proximity of some worldview attitudes of these philosophers and the author of \u201cFrankenstein\u201d and, on the other hand, about the deliberate introduction of philosophical passages into the fabric of the novel, which play the role of retardation elements.", "title": "Philosophical contexts in Mary Shelley\u2019s novel \u00abFrankenstein\u00bb", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Markus  Oppolzer"], "summary": "In the early 1970 s, a number of influential science fiction scholars reached a consensus that their genre of interest developed out of Gothic fiction and that Mary Shelley\u2019s Frankenstein (1818) marked a turning point in an ongoing reorientation towards a more secular and materialist understanding of human life and progress. The protagonist\u2019s scientific career, which advances from a fascination with the occult to academic studies of chemistry and anatomy, seemed to mirror this paradigm shift. The new status of Frankenstein as a foundational text of the modern science fiction novel has frequently led to a narrow focus on its science, the uncanny anticipation of recent bioethical debates and Victor Frankenstein\u2019s role as the archetypal (mad) scientist. However, the generic tension between Gothic themes and science fiction elements is instrumental to the novel\u2019s design. Frankenstein, for example, is a far more problematic character than some science fiction scholars are willing to acknowledge. Using autodidacticism, blindness, and manipulativity as my three analytical categories I shall demonstrate that he has more in common with the perpetrators of Gothic fiction than with modern scientists.", "title": "Gothic Science: The Failure of Education in Mary Shelley\u2019s Frankenstein", "claims": null}, {"metadata": {"year": 2003}, "authors": ["Gayatri Chakravorty Spivak"], "summary": "Mary Shelley\u2019s Frankenstein emerges out of a particular conjuncture of British class history. A text of nascent feminism, it remains cryptic, I think, simply because it does not speak the language of feminist individualism that we have come to hail as the language of high feminism within English literature. Barbara Johnson\u2019s brief study tries to rescue this recalcitrant text for the service of feminist autobiography.1 Alternatively, George Levine reads Frankenstein in the context of the creative imagination and the nature of the hero. He sees the novel as a book about its own writing and about writing itself, a Romantic allegory of reading.2", "title": "Frankenstein and Devi\u2019s Pterodactyl", "claims": null}], "query": "What are the important themes and ideas in the text \"Frankenstein\", that students pursuing M.A. in English Literature find it difficult to understand and imagine?", "summary_abstract": "The exploration of Mary Shelley's \"Frankenstein\" reveals several themes and ideas that students pursuing an M.A. in English Literature often grapple with, leading to varying levels of understanding and interpretation. A common challenge noted across the literature is the superficial engagement many students have with the text. For instance, a study by Canino and Teichert (2019) found that a significant portion of students did not read the entire novel, with only 11% completing it, which likely contributes to a lack of depth in their analyses. This superficiality often results in predictable interpretations focused on familiar themes such as isolation, abandonment, and revenge, as highlighted by Bice and Courey (2003).\n\nMoreover, the complexity of the monster's character poses a significant challenge for students. Bissonette (2010) points out that students frequently oscillate between viewing the creature as wholly evil or completely sympathetic, which complicates critical discussions and hinders deeper analysis. This dichotomy reflects a broader struggle to reconcile the monster's role within the narrative and its implications for themes of alienation and societal rejection.\n\nThe philosophical underpinnings of \"Frankenstein\" also present difficulties. Ozherelyev (2020) discusses how the novel is steeped in philosophical contexts that may be challenging for students to navigate, particularly those related to Romanticism and Enlightenment thought. This complexity is echoed in Waham's (2023) analysis of the Gothic elements in the novel, which intertwine with its themes of scientific ambition and the consequences of playing god, further complicating students' interpretations.\n\nAdditionally, the historical and cultural contexts surrounding the novel can be daunting. Spivak (2003) notes that \"Frankenstein\" emerges from a specific class history and feminist discourse that may not align with contemporary understandings of feminism, adding another layer of complexity for students attempting to engage with the text.\n\nIn summary, students pursuing an M.A. in English Literature often find \"Frankenstein\" challenging due to its multifaceted themes, complex characterizations, philosophical depth, and historical context. These elements can lead to superficial readings or polarized interpretations, highlighting the need for more nuanced approaches to teaching and understanding this seminal work.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2013}, "authors": ["J. McDonald"], "summary": "This study examined the relationship between music preference and extraversion on complex task performance in a sample of 34 college students from a small, Christian, liberal arts university. Separated into two groups of high and low extraversion, these 34 participants were invited to participate in the experimental phase of the study. For the experimental phase, each participant experienced three different music conditions (Preferred, Preset, and Silence) while performing a complex reading comprehension task. The results revealed a significant interaction effect between level of extraversion and music condition. Individuals with higher levels of extraversion performed marginally significantly better listening to preferred music during the complex task compared to silence and a preset music selection; all other sound conditions were not significant. The Effect of Music Preference on Complex Task Performance Music, like all other art forms, is unique to human beings. It expresses emotion shared by all cultures, closes the gap between eras of time, and reflects attitudes and progression. The power of music holds a strong influence over many human functions and recent technological advances of the 20 and 21 centuries have made music highly accessible. In addition, the ubiquitous nature of music accessibility in modern society raises important questions about the pervasive presence of music in work and educational settings. Studies searching for the distracting or facilitating effect of music on task performance have generally been inconsistent. Despite inconclusive results, public acceptance of claims that support the facilitative function of music in task performance is common. The highly publicized study by Rauscher, Shaw, and Ky conducted in 1994 for example, found significant results \u201cproving\u201d Mozart music increased spatial reasoning performance in college students. McKelvie & Low (2002) conducted a 1 Mcdonald: The Effect of Music Preference on Complex Task Performance Published by Pepperdine Digital Commons, 2013", "title": "The Effect of Music Preference on Complex Task Performance", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Brayan Rodr\u00edguez"], "summary": "The authors describe the importance of music design for background instrumental music and the effect on task performance. Three instrumental music conditions that differ in tempo, articulation, mode, and musical meter were tested using a complex task scenario. The task was performed using a complicated web-interface that required users to focus their attention and perform several specific interactions for successfully finishing the task. All the interactions with the interface were recorded. Moreover, a mixed assessment of the emotional state, perceived task performance, and music perception was asked to participants upon task completion. Experimental results revealed that music design has complex effects on task performance and emotion. Also, the results revealed important trends that can help design music environments to control frustration when confronted to complex and cognitively demanding tasks. Instrumental Music Design: Influence on Task Performance", "title": "Influence on Task Performance", "claims": null}, {"metadata": {"year": 2021}, "authors": ["A. Arboleda", "Christian Arroyo", "Brayan Rodr\u00edguez", "Carlos A. Arce-Lopera"], "summary": "Studies on the effect of music on task performance are contradictory about this relationship\u2019s direction and valence. Task characteristics may be accounting for these inconclusive findings. Thus, this study employs effort to mediate music\u2019s effect on task performance (objective and perceived) under a stressful decision-making process. This is a between-group experiment with three conditions: slow-tempo music, fast-tempo music, or no music. We designed a computer web interface, where participants did a stressful task. Results demonstrated that participants made a strong effort under the conditions with music. Hence, turning the music off under stressful activities is favorable in terms of performance. The article contributes to understanding the interaction between music and task performance, expanding the discussion within a stressful task.", "title": "A stressful task is when to turn off the music: Effect of music on task performance mediated by cognitive effort", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Manuel F. Gonzalez", "J. R. Aiello"], "summary": "Researchers have documented various (sometimes conflicting) effects of music on cognitive task performance, and have highlighted several mechanisms through which these effects may occur (e.g., arousal, mood, attention). To further understand these effects, we consider interactions between music-based, task-based, and performer-based characteristics. Specifically, we drew from the distraction-conflict theory of social facilitation and research on boredom proneness to hypothesize that music-along with its complexity and volume-facilitates simple task performance and impairs complex task performance, and that one's preference for external stimulation (a dimension of boredom proneness) moderates these effects. We tested our hypotheses in a laboratory experiment, in which participants completed cognitive tasks either in silence or with music of varying complexity and volume. We found that (a) music generally impaired complex task performance, (b) complex music facilitated simple task performance, and (c) preference for external stimulation moderated these effects. Therefore, the data suggest that music's effects on task performance depend on the music, the task, and the performer. (PsycINFO Database Record (c) 2019 APA, all rights reserved).", "title": "More than meets the ear: Investigating how music affects cognitive task performance.", "claims": null}, {"metadata": {"year": 2016}, "authors": ["B. Geethanjali", "K. Adalarasu", "M. Jagannath", "R. Rajasekaran"], "summary": "Our study demonstrates the enhancement of task performance aided by music. Task performance and associated physiological changes were studied in participants who listened to music (Indian classical instrument or Indo jazz). Their mood and their perceived emotions were evaluated using positive and negative affect schedule and self assessment manikin scales, and their pulse rate was measured. Visual Go and No-go trials were given as task stimuli and task performance was measured. Changes in the mean pulse rate were significantly low (P < 0.05) during Indo jazz. We conclude that Indian classical instrument and Indo jazz induced positive effects and enhanced task performance.", "title": "Enhancement of Task Performance Aided by Music", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Ying  Fang", "Ruiqian  An", "Junxia  Wang", "Zhanxun  Dong"], "summary": "Listening to music at work seems to be the habit of many designers. As they believe that music can stimulate creativity and help them concentrate. Design tasks mainly fall into two categories: repetitive tasks such as image matting and creative tasks such as brainstorming. In order to study how different types of music actually impact work performance, participants were asked to perform image-matting and brainstorming under three different experimental background conditions (no music, light music, and hard music). Physiological indexes such as heart rate and GSR (Galvanic Skin Response) were recorded to measure the influence of music on mood. Task results were carefully examined to indicate how music impacts task performance. Our analysis reveals that listening to music would lower task performance for repetitive task as well as creative task, and hard music is even worse than light music. However, listening to music doesn\u2019t seem to impact anxiety, arousal or mental effort. Plus, arousal improves when competing creative design tasks compared with repetitive design tasks.", "title": "Handling Design Tasks: Effects of Music on Mood and Task Performance", "claims": null}, {"metadata": {"year": 2013}, "authors": ["S. K. Balogun", "Nicole M. Monteiro", "Tshephiso Teseletso"], "summary": "Equivocality on the influence of music on task performance led to the present study investigating the effects of music genre and music language on task performance. Using 60 students who were randomly assigned to a 2 X 3 ANOVA design under two conditions of music genre (Pop, Gospel) and three music language conditions (English, French, Setswana), the students were asked to perform a cognitive/perceptual task. It was revealed that performance was generally poor among the students but worse of under French language, whether Pop or Gospel, followed by Setswana language; while performance was better with English Pop music. It was concluded that the genre and language selection of music that students use to study may significantly impact task performance. As students listen to their music devices, they may be advised to choose their songs wisely to facilitate optimal arousal, attention and mood for better performance.", "title": "Effects of Music Genre and Music Language on Task Performance Among University of Botswana Students", "claims": null}, {"metadata": {"year": 1968}, "authors": ["William Wokoun"], "summary": "Abstract : Sixty-three subjects worked at a vigilance task for an hour while listening to one of three musical programs. While all three programs included the same 23 selections, the Ascending Program grew steadily more lively, and the Descending Program grew steadily less lively, while the Increasingly Variable Program progressively increased the contrast between adjacent selections. Results showed that changing the sequence of the 23 selections profoundly affected reaction times and variabilities, as well as individual consistency. Subjects performed the vigilance task better with the Ascending Program than with the other two programs. (Author)", "title": "EFFECTS OF MUSIC ON WORK PERFORMANCE.", "claims": null}], "query": "What is the relationship between music and task performance+", "summary_abstract": "The relationship between music and task performance is complex and varies based on several factors, including the type of task, the characteristics of the music, and individual differences among performers. \n\nMcDonald (2013) found that extraversion levels influenced how individuals performed complex tasks while listening to music. Participants with higher extraversion performed better when listening to their preferred music compared to silence or preset music. This suggests that personal music preference can enhance task performance, particularly for those who are more extroverted.\n\nRodr\u00edguez et al. (2020) explored how different instrumental music designs affect task performance during complex tasks. Their findings indicated that music can have both positive and negative effects on performance, depending on its design, which can help manage frustration during cognitively demanding tasks.\n\nIn a study by Arboleda et al. (2021), the impact of music on performance during stressful decision-making tasks was examined. They found that music generally encouraged greater effort, but turning off the music during stressful activities led to better performance outcomes. This highlights the potential for music to either facilitate or hinder performance based on the task's nature and the stress involved.\n\nGonzalez and Aiello (2019) provided further insight by demonstrating that music can impair complex task performance while facilitating simpler tasks. Their research indicated that the complexity and volume of music play significant roles in determining its effects on performance, suggesting that the interaction between music, task type, and individual preferences is crucial.\n\nGeethanjali et al. (2016) reported that listening to specific types of music, such as Indian classical or Indo jazz, positively influenced task performance and physiological responses, indicating that certain genres can enhance cognitive functioning.\n\nFang et al. (2020) found that music negatively impacted performance on both repetitive and creative tasks, with hard music being particularly detrimental. This suggests that while music is often used to stimulate creativity, it may not always be beneficial for task performance.\n\nBalogun et al. (2013) highlighted the importance of music genre and language, revealing that performance was generally poorer when listening to music in a non-native language, emphasizing the need for careful selection of music to optimize task performance.\n\nLastly, Wokoun (1968) demonstrated that the sequencing of music could significantly affect performance on vigilance tasks, with more lively music leading to better outcomes.\n\nIn summary, the relationship between music and task performance is influenced by individual differences, task complexity, music characteristics, and contextual factors. While music can enhance performance in some scenarios, it can also be distracting or detrimental in others, underscoring the need for a nuanced understanding of this relationship.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2016}, "authors": ["Omuta Ged"], "summary": "Background: One of the spin-off effects of the urban-based medical services established by the colonial administration was the total neglect of rural communities. Those that existed lacked infrastructure. Even fifty years after independence, this dichotomy has persisted and become more pronounced. The objective of this study is to examine the state of infrastructure in the primary health care centres in Delta State, Nigeria. Methodology: The study was a survey of the infrastructure of all the PHC centres in nine local government areas; three from each of the three senatorial districts. The facilities covered were sources of water supply, sources of electricity, number of functional beds and type of communication facilities. The field date were cleaned up, processed and analysed using SPSS 10.0. Focus group discussions and key informant interviews were also conducted. In order to make the findings policy-relevant, a project steering committee made of researchers and decision makers and a project management committee made of representatives of decision makers, care providers, care seekers and other stakeholders were se up and integrated into the study. Results: There were varying degrees of infrastructural deficiencies. 34.22 per cent of the PHCs had no access to safe water; 51.33 per cent were not connected to the national electricity grid; and 34.22 per cent of the available beds and 40.89 per cent no means of communication whatsoever. Conclusion: Field data and perspectives of stakeholders revealed that the major cause of infrastructural deficiencies was insufficient funding, lopsided allocation of resources and official corruption. Correspondingly, increased and sustained funding; prioritized allocation of resources and targeted upgrading of facilities, were recommended.", "title": "Infrastructure and Health Care Services in the Niger Delta Region of Nigeria: A Case Study of Physical Amenities in the Primary Health Care System in Delta State", "claims": null}, {"metadata": {"year": 2016}, "authors": ["D. Dabara", "Ankeli Ikpeme Anthony", "J. Guyimu", "E. Oladimeji", "Oyeleke Oyediran"], "summary": "The availability of adequate infrastructure facilities is imperative for the overall economic development of any country. However, developing countries like Nigeria are particularly bedeviled by issues of adequate infrastructure financing and this has great consequences on its citizenry. The aim of this study is to evaluate infrastructure financing strategies and its effect on urban development in Nigeria with a view to providing information that will guide policy makers in the country particularly and similar developing economies in general in making informed decisions on their infrastructure investment strategies. The theoretical research approach was adopted for this study. The study revealed that inadequate investment in basic infrastructure (due to severe budget constraints) and the rapid rate of urbanization in Nigeria is putting considerable strain on the nation's limited infrastructure. Hence, there is a need for large and continuing amounts of investments in almost all areas of infrastructure in Nigeria. Similarly, investment in maintaining existing infrastructure has suffered gross neglect, leaving the country with degraded and inefficient infrastructure services; this is compounded by unprecedented urban growth in major urban centers such as Lagos, Abuja and Port-Harcourt. This has resulted into the explosion of informal settlements (slums) in and around these major cities. This ugly situation is occasioned by mainly lack of long-term funds for infrastructure financing; uncertain political/economic environment; fear of policy reversals by successive governments as typical infrastructure projects span over long periods, etc. In Nigeria, the government has been the sole financier of infrastructural projects and has often taken responsibility for implementation, operations and maintenance as well. There is a need for paradigm shift in this respect as this may not be the best way to execute/finance these projects. It is obvious that the government alone cannot adequately shoulder the responsibility of infrastructure financing, therefore, to ensure urban development in Nigeria; this study recommends the involvement of the private sector in infrastructure financing. Public-Private Partnerships (PPP) offer a promising solution to the financing needs of the country as it could attract foreign investments. It is recommended that infrastructure investments should be made in such a way as to recover the invested resources through a system of user charges. This means that the services of investment projects in the country should no more be continued as a free good.", "title": "Infrastructure Financing and Urban Development in Nigeria", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Remi Oyedeji", "S. Abimbola"], "summary": "Page | 519 Implementing this proposal is not likely to be much of a challenge. Most tertiary hospitals are operated by the federal government, while in most cases PHC facilities are operated by local governments, sometimes with the support of their state governments. Local governments are typically weak (as state governments are in control of their revenue)4 and will likely welcome support from federal government-owned tertiary hospitals. In 1992, the Nigerian government made provision for federal support for PHC through the National PHC Development Agency.8 Federal support for PHC can be extended to include support through tertiary hospitals run by the federal government. Indeed, similar initiatives were implemented by the early generation of teaching hospitals in Nigeria, which operated rural PHC practices in order to expose their undergraduate and postgraduate trainees to real life PHC, and conducted health research in these communities while fulfilling an articulated social responsibility to rural communities.9 We believe these token initiatives can be formalised and scaled up to all tertiary hospitals in Nigeria.", "title": "How tertiary hospitals can strengthen primary health care in Nigeria", "claims": null}, {"metadata": {"year": 2006}, "authors": ["A. Ibrahim", "A. Price"], "summary": "The Nigerian healthcare system is divided into the primary, secondary and tertiary levels of care, which are under the three tiers of government (local, state and federal) with some overlapping of responsibilities. The Federal Ministry of Health (FMOH) develops policies and physical structures, and supervises the operations of the healthcare system. The Primary Health Care (PHC) level has been identified as the appropriate setting to tackle 90 - 95% of major causes of morbidity and mortality because of its proximity to over 80% of the populace. At the PHC level, the services provided include integrated preventive, promotive and community development activities. However, the implementation of PHC in Nigeria has lacked impact and sustainability, thus leading to the research discussed in this paper which is aimed at proposing an innovative procurement system for sustaining PHC facilities in Nigeria. The proposed system is focused at developing and encouraging a new market for investment in primary care and community-based facilities and services. It is a form of Public\u00ad-Private Partnership (PPP) that enjoys national support but with local control. One of the central objectives is the idea of bringing together all the various local stakeholders, interests and users that comprise the local health economy. The arrangement is expected to offer considerable social and economic benefits while satisfying the fundamental philosophy of PHC.", "title": "Public-Private Partnership and sustainable primary healthcare facilities in Nigeria", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Uzoh", "B. A."], "summary": "Good quality infrastructure is a key ingredient for sustainable development. All countries need efficient transport, sanitation, energy and communication systems if they are to prosper and provide a decent standard of living for their population. In Nigeria, poor infrastructure has been responsible for increased poverty, joblessness, insecurity, low investments, low human capacity utilization, poor basic needs, etc., which cannot sustain the economy. This paper examines the importance and effects of infrastructure in Nigeria\u2019s effort towards development. Improved infrastructure accelerates growth and sustains human development, but in Nigeria, the nation cannot boast of any infrastructure that is readily available. The government is also not helping matters, because no meaningful plan or policy has been implemented to show that the remedy is in sight. Therefore, the paper looks at the ways and means of improving infrastructural provision and how it will improve the standards of living, enhance growth, and sustain human and material development. It concludes with the recommendations on how to move Nigeria to the next level of realizing the vision 20:2020 socio-economic objectives.", "title": "Improving Critical Infrastructure for Sustainable Development in Nigeria Towards the Realization of Vision 20 : 2020", "claims": null}, {"metadata": {"year": 2017}, "authors": ["A. Oyebanji", "C. Liyanage"], "summary": "Infrastructure provision is mostly the responsibility of governments using taxpayers\u2019 money through budgetary allocations. However, a growing phenomenon world-wide is the inability of public funds to adequately finance ever-growing infrastructure needs, with serious consequences in developing countries. Although governments in most countries are striving hard towards adequate infrastructure provision, a wide gap still exists in terms of meeting the quantity and quality of infrastructure required for sustainable socio-economic development, principally due to a shortage of public funds (Iboh et al. 2013). Numerous issues persist relating to dwindling resources, variable power supply, inadequate healthcare facilities, poor and insufficient housing, and problems with modes of transport including roads and air, which have become death traps in many developing countries like Nigeria. The appalling state of this infrastructure has given credence to private sector participation in the development of sustainable infrastructure. Other key drivers to the birth of the PPP scheme in the country include the poor economy arising from reduced oil revenue and mismanagement of resources, rising population, and an increased need for infrastructure such as housing, power, transport modes, health and educational facilities.", "title": "Infrastructure provision through public-private partnerships: the experience of Nigeria", "claims": null}, {"metadata": {"year": 2013}, "authors": ["A. Oluchi"], "summary": "Nigeria requires an annual investment of about US $10 billion over the next ten years in order to reduce its infrastructural deficit. This sum is grossly more than the 20% of $1 billion allocated for infrastructural development by the Nigeria Sovereign Investment Act. The current level of infrastructure deficit in Nigeria is a major constraint in the national vision of becoming one of the largest economies by 2020. The foundation upon which sustainable economic progress can be built in Nigeria is, arguably, inextricably linked to the availability and sufficiency of infrastructure. Where infrastructure is lacking or is inadequate, the structural challenges bedeviling the Nigerian economy may not be overcome and achieving sustainable growth and development in order to provide for future Nigerians may remain a mirage. Accordingly, we urge for a sustained focus on infrastructural development evidenced through the allocation of more than 20% funding to the Nigerian Infrastructure Fund.", "title": "Infrastructural Development: The Vision of Nigerian Sovereign Wealth Fund", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Kingsley M. Oserei", "Godwin Uddin"], "summary": "The health sector remains a vital tool for sustainable development of any nation and therefore investment in this sector cannot be overemphasized. The present state of Primary Health Care (PHC) system in Nigeria is alarming, with only about 20% out of the 30,000 PHC facilities relatively distributed throughout the 774 Local Government Areas (LGAs) across Nigeria working partially. This study examines government expenditure on primary health care in Nigeria as well as its relations to real national output within the period 1980 to 2015 using secondary data and the Ordinary Least Square (OLS) econometric technique. The results of the model used revealed government health expenditure to be efficacious for economic growth, and for the well-functioning of primary health care in Nigeria. Nonetheless, such efficacy duly was also understood to be limited in three select aspects: funding/financing strategy, personnel/manpower quality and mobilization, and implementation framework. The paper, in conclusion, attests to the rationale that money spent wisely on capital health expenditure pays off well in both short-run and long-run for individuals, the society and nation at large.", "title": "The Myth and Reality of Government Expenditure on Primary Health Care in Nigeria: Way Forward to Inclusive Growth", "claims": null}], "query": "there is a pressing need for increased Nigerian government investment in the development and enhancement of physical and technological infrastructure for Primary Health Centers (PHCs). This includes ensuring a stable supply of electricity, robust internet infrastructure, access to computers, and cutting-edge software.", "summary_abstract": "The collective findings from the reviewed papers underscore a critical need for increased investment by the Nigerian government in the infrastructure of Primary Health Centers (PHCs). The current state of PHCs in Nigeria is characterized by significant infrastructural deficiencies, including inadequate access to safe water, unreliable electricity supply, and insufficient communication facilities (Omuta Ged, 2016). Specifically, over half of the PHCs lack connection to the national electricity grid, and many do not have basic means of communication, which severely hampers their operational effectiveness.\n\nMoreover, the overarching issue of inadequate infrastructure financing is highlighted as a major barrier to the development of PHCs and the broader healthcare system in Nigeria. Dabara et al. (2016) emphasize that the government's budget constraints and the rapid urbanization are exacerbating the infrastructure deficit, necessitating substantial and sustained investments across all sectors, including health. The authors advocate for a paradigm shift towards Public-Private Partnerships (PPP) to attract private sector investment, which could alleviate the financial burden on the government and enhance infrastructure development (Dabara et al., 2016; Oyebanji & Liyanage, 2017).\n\nThe papers also point out that the current funding levels are grossly inadequate. For instance, Oluchi (2013) notes that Nigeria requires an annual investment of approximately $10 billion to address its infrastructural deficit, far exceeding the current allocations. This lack of investment not only affects the physical infrastructure but also the quality of healthcare services provided at the PHC level, which is crucial for addressing the majority of health issues faced by the population (Kingsley M. Oserei & Godwin Uddin, 2019).\n\nIn conclusion, the evidence presented across these studies strongly supports the assertion that increased government investment in both physical and technological infrastructure for PHCs is essential. This investment should focus on ensuring stable electricity supply, robust internet connectivity, and access to modern technology to enhance the effectiveness and reach of primary healthcare services in Nigeria (Omuta Ged, 2016; Dabara et al., 2016; Kingsley M. Oserei & Godwin Uddin, 2019).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2005}, "authors": ["W. Hiermann"], "summary": "Efficient project planning from a human resource point of view requires projectoriented organization forms to take into account the permanent change of requirements and framework conditions during the duration of a project correctly. Skill management systems are an excellent management tool for this challenge, if in addition to default skill profiles the employees' competence and experience room is defined. This paper describes a best practice skill management model that takes into account compared with other approaches the attributes time and duration and builds up an experience tree of the employees by link technique. Further the factors are represented, which play a role during the conversion of the model in practice.", "title": "Skill Management : Searching Highly Skilled Employees for Teambuilding and Project Management Tasks", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Zeeshan Anwar", "Nazia Bibi", "A. Ahsan"], "summary": "Concept of \u201cSkill Management\u201d is not new in context of human resource management. This concept can also be utilized for project management but unfortunately there is no well defined ** idea or model that software organizations can follow, and make effective use of the human skills for effective project resource allocation. Software professionals possess many skills and most of their skills become worthless because of their single job role\u2020. Expertise based skill management system (EBSMS) is therefore required for proper management of skills for project resource allocation. This research addresses issues related to hard skills of people and corresponding best practices for management of these skills for effective project resource utilization and allocation in Software Industry of Pakistan and proposes EBSM Model.", "title": "Expertise based skill management model for effective project resource allocation under stress in software industry of Pakistan", "claims": null}, {"metadata": {"year": 2018}, "authors": ["M. Akiyoshi", "M. Samejima", "N. Komoda"], "summary": "This paper addresses a project manager skill-up simulator which aims to provide problem-solving learning environment. Project management is inherently human-centric activities, and research work for education has been done by using simulation. The proposed simulator environment is designed to provide well-configured functionalities that make it possible to generate high fidelity scenario on project situation with event rule firing mechanisms, evaluation on a learner\u2019s operations as to the system development progress management phase. The proposed environment realizes effective scenario generation and well-insighted evaluation along with a learner\u2019s interactive operations.", "title": "A Project Manager Skill-up Simulator Towards Problem Solving-based Learning", "claims": null}, {"metadata": {"year": 2014}, "authors": ["S. K. Prasad", "R. Shankar", "Sreejit Roy"], "summary": "It is commonly agreed that project management competence is a critical success factor for project based organizations. However, there is not sufficient research in literature to develop objective analytical methods to identify required project management skill level for a project. In practice, practitioners still need an effective methodology to determine competence level needed to manage a particular project \u2014especially in larger companies where it is difficult for management to subjectively identify right project manager for new and ongoing projects. Even for the projects with assigned project manager, there is value in understanding if the assigned project manager has the right skill level for the project. In this paper, authors address problem of analytically identifying right project management skill level. Specifically, this paper demonstrates how supervised learning can be used to build models for predicting required project management skill level, thus enabling focused and individualized resource management. Further, these techniques are illustrated in the context of real-world project data from a large IT firm.", "title": "Using Predictive Analysis for Improved Project Manager Assignment", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Vl\u0103du\u021b-Severian Iacob"], "summary": "Although the projects are different from each other there are many common things that contribute to their success. Looked overall, the success of a project is the result of a multitude of factors. This person is considered the \"engine\" of the project. The man who makes the action set for the achievement of project objectives to be brought to an end. The project manager must have the technical knowledge and economic diverse. He should be able to choose a team and lead. You must be tenacious, combative, to know how to communicate both within the team and beyond. In a word, the project manager must have an impressive stock of knowledge, skills and abilities and appreciate as Peter Drucker, to \"exist for the organization. To be its servant. Any management who forget this will only cause damage to the organization. \"This study will focus on highlighting the skills of the project manager and their role in managing difficult situations or risk.", "title": "PROJECT MANAGER SKILLS, RISK MANAGEMENT TOOLS", "claims": null}, {"metadata": {"year": 2001}, "authors": ["Reinhard  Lucas", "Thorsten  Weber"], "summary": "Skill management is becoming increasingly important as a form of personalised knowledge management achieved through changes in work organisation. Existing solutions only partly fulfil the necessary criteria. One reason is a lack of requirement analysis embedded in a holistic approach. This paper is a contribution to this type of requirements analysis.", "title": "Skill management: a building block for project management with flexible teams", "claims": null}, {"metadata": {"year": 2007}, "authors": ["F. Tan"], "summary": "Although effective project management is critical to the success of Information Technology (IT) projects, little empirical research has investigated skill requirements for IT Project Managers (PMs). This study addresses this gap by asking nineteen practicing IT PMs to describe the characteristics of both competent and incompetent IT PMs. A semi-structured interview method known as the repertory grid technique is used to elicit these skills. These skills are further sorted into nine skill categories: client management, communication, general management, leadership, personal integrity, planning and control, problem solving, systems development, and team development. Our study complements existing research by providing a richer understanding of several skills which were narrowly defined (e.g., client management, planning and control, and problem solving) and by introducing two new skill categories which had not been previously discussed (e.g., personal integrity and team development). The paper concludes with suggestions for future research.", "title": "Skills of Successful IT Project Managers: A Study Using the Repertory Grid Technique", "claims": null}, {"metadata": {"year": 1993}, "authors": ["B. Shane", "T. Schumacher"], "summary": "SkillSet, a hypertext intelligent system that provides advice and suggests management tools appropriate for solving problems encountered in project management contexts, is described. The suggestions are drawn from a database of 230 managerial tools which have been organized into 60 tool groups. The system helps to diffuse the company's collective experience to project managers throughout the firm. It also provides an overview of each of the tools in its knowledge base to users on the company network. Special emphasis is placed on the human problems that are present in managing teams of knowledge workers (e.g., software engineers). The SkillSet system incorporates knowledge captured from both organizational experiences and external consultants.<<ETX>>", "title": "SkillSet: a knowledge based system for managing projects", "claims": null}], "query": "skill model project manager", "summary_abstract": "The research on the skill model for project managers highlights the critical competencies required for effective project management and the methodologies for skill management in this context. Hiermann (2005) emphasizes the importance of skill management systems that adapt to the dynamic nature of project requirements, proposing a best practice model that incorporates employee experience and competence. This aligns with Anwar et al. (2013), who advocate for an Expertise Based Skill Management System (EBSMS) tailored for software organizations, addressing the challenge of underutilized skills due to rigid job roles.\n\nAkiyoshi et al. (2018) introduce a project manager skill-up simulator designed to enhance problem-solving abilities in a realistic project environment, underscoring the human-centric nature of project management. Prasad et al. (2014) contribute to this discourse by presenting a methodology using supervised learning to analytically determine the required skill levels for project managers, which is particularly beneficial in larger organizations where subjective assessments may fall short.\n\nIacob (2013) further elaborates on the essential skills of project managers, highlighting the need for a diverse knowledge base and strong communication abilities to navigate complex project landscapes. Tan (2007) enriches this understanding by categorizing the skills of IT project managers into nine distinct areas, including leadership and personal integrity, which are crucial for project success.\n\nLastly, Shane and Schumacher (1993) discuss the SkillSet system, an intelligent tool that aids project managers by providing tailored advice and management tools based on collective organizational knowledge, thus facilitating better decision-making in project contexts.\n\nCollectively, these studies underscore the multifaceted nature of project management skills and the necessity for structured skill management systems to optimize resource allocation and enhance project outcomes.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2010}, "authors": ["Daniel  Kirk", "Diane Brook Napier"], "summary": "The United Arab Emirates (UAE) is undergoing rapid modernization in all sectors. This young nation, formed in 1971, united seven sheikdoms (Abu Dhabi, Dubai, Sharjah, Umm al-Quwain, Ajman, Fujairah, and Ras al-Khaimah) and transcended ancient rivalries and disputes to amalgamate the interests of survival and development in a volatile region. Being fortunate to have vast wealth from oil reserves, the UAE subsequently embarked on policies for accelerated modernization in all sectors, and expansion of education at all levels\u2014including higher education, in which the trends mirror many of those in the transformation of higher education worldwide. Investment in higher education is seen as crucial for a number of purposes: for national development and global competitiveness; for the citizenry to attain improved living standards and achieve democratic participation; for meeting labor force needs and internal national development goals; and for the nation to participate in the global arena of education and economic activity. In the UAE, regional influences and development strategies are also important in shaping decisions about higher education. Within the broad landscape of global higher education transformation, the case of the UAE is distinctive as it reflects both global and regional trends and influences, and embodies several internal characteristics that impinge on higher education policy and practice.", "title": "Global Competition, Local Implications: Higher Education Development in the United Arab Emirates", "claims": null}, {"metadata": {"year": 2012}, "authors": ["D. Vandewalle"], "summary": "As exporters of oil in the Middle East and throughout the world, the United Arab Emirates (UAE) must address the following challenges in the area of development and economic growth: (1) an increasingly integrated world economy in which technology and knowledge will be paramount; (2) the need to diversify its economy from a natural resource-based economy that will inevitably decline in the century ahead; and (3) the challenge of keeping the state as a political community relevant to its citizens. An examination of the UAE's situation against the backdrop of economic growth and educational issues in small countries such as Morocco, Indonesia, Malaysia, Yemen, and Singapore reveals that the responses of societies in countries with hydrocarbon-based economies to inflows of revenues are much more complicated and diversified than many authors have previously argued. The examination further reveals that, for states such as the UAE, a viable economic future is a race against time that can only be won with an integrated triangle consisting of educational policies, state actions, and maximization of economic opportunities. The key to the UAE's future depends on continued development and maintenance of a knowledge-based economy and continual reappraisal of the fundamental and traditional relationship between work, learning, and education. (MN) Reproductions supplied by EDRS are the best that can be made from the original document. Monday 10 April, 2000 Dr. Diederik Vandewalle Crossroads of the New Millennium Higher Education And Development In Arab Oil Exporters: The UAE In Comparative Perspective Prepared and Presented", "title": "Higher Education and Development in Arab Oil Exporters : The UAE in Comparative Perspective", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Lynn Nicks-McCaleb"], "summary": "This paper focuses on the provision of higher education in the United Arab Emirates (UAE) and the issues surrounding strategies employed by institutions to prepare tertiary level students for careers in the global economy. Dramatic growth and development in the Arabian Gulf region over the past two decades has made fundamental changes in the education system necessary. There has been a shift in focus from more traditional education and delivery methods, to contemporary approaches to support student learning with the emphasis on preparing students for careers in the knowledge economy. These changes in higher education, and its widespread provision, have impacted on the UAE and its economy in particular. This paper reviews the provision of higher education, its role in the rapidly developing society and economy of the UAE, and how the community is contributing to and benefiting from emerging partnerships.", "title": "The Impact of State Funded Higher Education on Neighbourhood and Community in the United Arab Emirates.", "claims": null}, {"metadata": {"year": 2007}, "authors": ["W. Fox"], "summary": "The United Arab Emirates (UAE) is in the midst of tremendous economic development. With a rapidly changing economy, it is increasingly important for this expatriate dependent country to start training a native, modern workforce with the skills required to enter the workplace. The UAE must continue improving and developing their higher education system if it wants to create graduates with the training and education required to compete with students and workers from over-seas. This paper will describe the UAE\u00e2\u20ac\u2122s higher education system, as well as current and potential obstacles for UAE universities to overcome, focusing on issues such as funding concerns and student access.", "title": "The United Arab Emirates: Policy Choices Shaping the Future of Public Higher Education", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Akram Masoud Haddad"], "summary": "The United Arab Emirates (UAE) is an Asian country that consists of seven emirates. UAE is the eighth largest oil producer in the world which is the main source used for development. UAE is a member of the Gulf Cooperation Countries (GCC) council. UAE is one of the world\u2019s most open and successful economies. UAE maintains a free-market economy and is one of the most politically stable and secure in the region. All together ensures that UAE has a robust competitive edge since it is the region\u2019s premier commercial hub and the second largest economy. In addition, this economic environment encourages the establishment of private universities and branches of well-known foreign universities in UAE. In this study, the impact of education on the sustainable economic development is measured by human development index (HDI) using simultaneous equations approach of two equations. The results show that there are more than one million students enrolled in primary and secondary schools and more than 60 thousand enrolled in territory education in more than 561 programs distributed over 70 institutions, around 40% of them are noncitizens. The regression results show that there are impacts of education on the sustainable economic development at UAE. These results emphasize the role of education in creating sustainable development in UAE, in particular, as well as the rational policy of the UAE and confirm the results of other studies.", "title": "The Impacts of Education on the Sustainable Development in United Arab Emirates: A Simultaneous Equations Approach", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Rafiq H. Hijazi", "T. Zoubeidi", "I. M. Abdalla", "Mohamed Al\u2010Waqfi", "Nasri Harb"], "summary": "We provide an overview of the state of the higher education system in the UAE, and investigate its capacity to supply Dubai\u2019s economy with skilled labor and achieve the emirate\u2019s strategic social development objectives. We examine various characteristics of the higher education sector and discuss their inherent strengths and weaknesses in light of Dubai\u2019s labor market needs. Our findings reveal that the UAE higher education sector has witnessed an impressive growth since 1997 mostly led by a vigorous expansion of the private higher education sector. This situation led to a substantial growth in higher education opportunities for the UAE population. However, the lack of development of graduate studies and the lower quality of graduates, as perceived by both employees and employers, represent the main challenges to the capacity of the sector in fulfilling the needs of Dubai\u2019s economy.", "title": "A Study of the UAE Higher Education Sector in Light of Dubai's Strategic Objectives", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Spranza", "S. John"], "summary": "This article explores the educational history of the United Arab Emirates, a young and vastly wealthy nation located in the Middle East. A nation for just over 40 years, its leaders have struggled with educating and supporting its native population and managing the massive infusion of immigrants created by the progressive job markets established within its borders. With a mix of mandated free education for natives and a lucrative educational investment market from various other nations, the educational structure in the UAE has become a unique system that attracts students and faculty from around the world. The opulent landscapes of the primary cities of Dubai and Abu Dhabi create a truly global educational environment poised to help lead the region into the future. As a country, the UAE population is highly concentrated in its two major cities of Abu Dhabi and Dubai, with 70 percent of the total population living in these locations. The oil industry boom in the late 1970\u2019s drew a large number of immigrant workers (initially primarily from India and Pakistan) and the trend of attracting droves of migrant workers has continued throughout the last 40 years. Today, approximately 88 percent of the total UAE population are expatriates drawn to the country for the labor market, leaving only 12 percent of the population as native Arabs (NQA, 2013). Currently, almost 65 percent of the national population is under the age of 24 so the country\u2019s growth in its native peoples is on the upswing. This presents the opportunity to cultivate a strong educated national workforce that could help balance the UAE\u2019s reliance on expatriates in the labor market. Education has been a priority for the UAE government since the country\u2019s inception in 1971 and the leaders have invested highly in the development and promotion of all levels of education, from nursery schools to doctoral level programs. However, the UAE government had a steep sand dune to climb since the national literacy rates in 1975 were 54 percent for men and only 31 percent for women. The UAE constitution and Federal Law No. 11 of 1972 established compulsory primary education and guaranteed free education at all levels for all UAE nationals. However, secondary education through grade 12 or age 18 was not made compulsory until 2012. The end result of the concentration on education for its citizens was shown in a 2010 report that indicated the national literacy rates were 89 percent for men and 91 percent for women (UNESCO, 2012). The government-sponsored higher education system was started in 1975 when the young nation opened the UAE University in the city of Al Ain near the border with Oman. UAEU now has six campuses across the country. The largest national institution, the Higher Colleges of Technology, was founded in 1988 and now has 17 campuses across the country. The third and newest national institution, Zayed University, was founded in 1998 and has two campus locations. The Ministry of Higher Education and Scientific Research (MHESR) governs and sets policy for all institutions of higher education across the country, but several Emirates also have governing bodies that regulate the education sector in their own territory. When the MHESR was created, it was established on four policy decisions that would guide its efforts. These decisions included: 1. The UAE would build and operate its own universities; 2. Qualified faculty that 17 Spranza: Globalization of Higher Education in United Arab Emirates Published by Digital Commons@Georgia Southern, 2016 meet international standards would be employed; 3. Instruction would be predominantly in English; and 4. Education was to be free for all qualified Emiratis, and would include women (MHESR, 2007). The Ministry also includes the Commission of Academic Accreditation (CAA) which certifies all institutions of higher education much like the SACS organization does here in the southern part of the United States. According to the CAA, there are currently 116 higher education institutions in the UAE with a cumulative enrollment of 116,912 across 644 accredited academic programs (NQA, 2013). Since there are only three federally-sponsored institutions out of the 116 qualified by the CAA, all of the other higher education institutions are international organizations or institutions that have formed partnerships or joint ventures with either the federal or local governments to establish campuses in the UAE. Due to the population concentration in Dubai and Abu Dhabi, most of the foreign institutions can be found in these two cities, with 19 institutions in Abu Dhabi and a mass of 62 institutions in Dubai (MHESR, 2015). Most of the private institutions can be found in one of the 15 economic \u201cfree zones\u201d established by the UAE federal government or local governments of Dubai and Abu Dhabi. The Free Zones are special areas in the city that offer tax exemptions and other incentives to foreign companies to encourage capital investments that benefit both parties, creating a true global market in these cities within the city. The Dubai Knowledge Village and the Dubai International Academic City are the primary locations of nearly all of the private institutions of higher education, and include institutions from the United States, the United Kingdom, France, Canada, Australia, and India. The institutions found in the free zones are governed by the Dubai Knowledge and Human Development Authority (KHDA) and further managed by the University Quality Assurance International Board (UQAIB) to ensure all institutions meet the benchmarks of quality and accreditation established by the UAE government. Given the mandate that all levels of education would be free to its citizens, the government of UAE spends over a quarter of the federal budget on education. In 2009, the education budget was approximately $10 billion, or 28 percent of the federal budget for that year. In contrast, the participation rate for eligible citizens in the higher education system is still considerably lower than that of the U.S. or European countries, with only 25 percent of eligible Emiratis attending as compared to 71 percent of eligible students in North America and Western Europe (UNESCO, 2009). However, the pending \u201cboom\u201d of youth coming-of-age (65 percent of national population is under the age of 24) in the next decade has the government of UAE looking for ways to keep up the funding levels for education to maintain the quality and meet the demand from its citizens. Costs per student has already impacted the funding model given the inflation rate and increased demand for matriculation, and the government has had to freeze acceptance rates in order to limit the volume of new students in an effort to maintain the quality of instruction and services at the government-funded institutions. Projections by the MHESR indicate that over 50,000 students will be knocking at the universities\u2019 door by the beginning of the next decade (MHESR, 2007). Additionally, if the native population of UAE follows the trend of other countries in regards to adult learners returning to college, the MHESR predict that could potentially add another 13,000 students to the 50,000 youths expected to flood the higher education system if only 10 percent of the 25-29 year olds decide to enroll (MHESR, 2007). Some of the demand for education has been met by the private institutions that have set up shop in the various free zones primarily in Dubai, but these are options typically only available to the more affluent upper-middle and upper class of the socioeconomic scale. Faculty teaching in both the federal system and throughout the private institutions are heavily international. Within the UAE federal system, only approximately 10 percent of the 18 Georgia Journal of College Student Affairs, Vol. 32 [2016], Iss. 1, Art. 4 https://digitalcommons.georgiasouthern.edu/gcpa/vol32/iss1/4 DOI: 10.20429/gcpa.2016.010104 2,568 full-time faculty in 2013-2014 were UAE nationals. Similarly, less than 2 percent of the 3,949 faculty teaching in the private institutions within the free zones during 2013-2014 are UAE nationals (MHESR, 2015). The mandate set by the MHESR regarding the international qualifications of its faculty had assisted the recruitment of foreign scholars in the past, however, due to a static funding level the ability for federal institutions to attract and retain top faculty has declined considerably as the salary and benefits packages offered are no longer considered to be competitive (MHESR, 2007). Additionally, studies have shown that the most popular degree offerings and largest faculty expenses are no longer congruent with the needs of the employer market. Business, engineering, and health sciences dominate the course offerings and faculty appointments across all levels of higher education in the UAE, with 50 percent of students at federal institutions pursuing these degrees; 60 percent of students at private institutions enrolled in these majors; and 60 percent of UAE nationals studying abroad studying these same majors (NQA, 2013). Areas of study with huge industry demands that are not currently being met within the UAE educational system include information technology, education, architecture and construction, natural and physical sciences, and tourism and hospitality (Kazim, 2013).", "title": "Globalization of Higher Education in United Arab Emirates", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Stephen Wilkins"], "summary": "During the last decade, several countries across the Middle and Far East have established higher education hubs, some of which have grown rapidly by attracting foreign universities to set up international branch campuses. The United Arab Emirates (UAE) is by far the largest host of international branch campuses globally, having over 40 providers at the end of 2009. The UAE higher education market has become highly competitive and, in the private sector, supply currently exceeds demand. This paper explores and analyses the outcomes and impacts of this market situation on student recruitment, the student experience, quality and institutional strategies. The long-term viability of international branch campuses in the UAE is also considered, in the context of local political and social issues.", "title": "Higher education in the United Arab Emirates: an analysis of the outcomes of significant increases in supply and competition", "claims": null}], "query": "Higher Education and Economic Development in united arab emirates", "summary_abstract": "The relationship between higher education and economic development in the United Arab Emirates (UAE) is characterized by a strategic focus on modernization and diversification, driven by the need to transition from a resource-based economy to a knowledge-based one. The UAE's rapid economic growth, fueled by its oil wealth, has necessitated significant investments in higher education to meet labor market demands and enhance global competitiveness (Kirk & Napier, 2010; Vandewalle, 2012).\n\nThe UAE's higher education system has evolved dramatically over the past few decades, shifting from traditional educational models to contemporary approaches aimed at preparing students for careers in a globalized economy (Nicks-McCaleb, 2005). This transformation is crucial as the country seeks to cultivate a skilled native workforce capable of reducing its reliance on expatriate labor (Fox, 2007). The government has prioritized education, establishing a framework that includes free education for citizens and the establishment of numerous institutions, both local and international (Spranza, 2016).\n\nResearch indicates that education plays a vital role in sustainable economic development in the UAE, with significant enrollment figures in higher education reflecting the country's commitment to fostering a knowledgeable populace (Haddad, 2020). However, challenges remain, including the quality of graduates and the alignment of educational offerings with labor market needs (Hijazi et al., 2008). The influx of international branch campuses has created a competitive educational landscape, but it also raises questions about the long-term viability and quality of these institutions (Wilkins, 2010).\n\nIn summary, the UAE's investment in higher education is seen as a cornerstone for its economic development strategy, aiming to create a robust, knowledge-based economy that can thrive in an increasingly integrated global market. The ongoing evolution of the higher education sector is essential for addressing the challenges of workforce development and ensuring the country's economic sustainability (Vandewalle, 2012; Haddad, 2020).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2005}, "authors": ["H. Stuckenschmidt"], "summary": "We discuss the use of network analysis methods to support the automatic partitioning of large concept hierarchies. Different from other work in the area, we directly apply these methods on the structure of the hierarchy. We show that this way of using network analysis techniques can provide significant results with respect to identifying key concepts and using them to determine subsets of class hierarchies that are related content-wise. We discuss the methods used and evaluate the result on the ACM classification of computer science topis.", "title": "Network Analysis as a Basis for Partitioning Class Hierarchies", "claims": null}, {"metadata": {"year": 2008}, "authors": ["M. Dehmer", "S. Borgert", "F. Emmert-Streib"], "summary": "In this paper, we propose an information-theoretic approach to discriminate graph classes structurally. For this, we use a measure for determining the structural information content of graphs. This complexity measure is based on a special information functional that quantifies certain structural information of a graph. To demonstrate that the complexity measure captures structural information meaningfully, we interpret some numerical results.", "title": "Network Classes and Graph Complexity Measures", "claims": null}, {"metadata": {"year": 2007}, "authors": ["Ernesto Estrada"], "summary": "We use theoretical principles to study how complex networks are topologically organized at large scale. Using spectral graph theory we predict the existence of four different topological structural classes of networks. These classes correspond, respectively, to highly homogenous networks lacking structural bottlenecks, networks organized into highly interconnected modules with low inter-community connectivity, networks with a highly connected central core surrounded by a sparser periphery, and networks displaying a combination of highly connected groups (quasicliques) and groups of nodes partitioned into disjoint subsets (quasibipartites). Here we show by means of the spectral scaling method that these classes really exist in real-world ecological, biological, informational, technological, and social networks. We show that neither of three network growth mechanisms--random with uniform distribution, preferential attachment, and random with the same degree sequence as real network--is able to reproduce the four structural classes of complex networks. These models reproduce two of the network classes as a function of the average degree but completely fail in reproducing the other two classes of networks.", "title": "Topological structural classes of complex networks.", "claims": null}, {"metadata": {"year": 2014}, "authors": ["M. Trovati", "N. Bessis", "Anna Huber", "Asta Zelenkauskaite", "E. Asimakopoulou"], "summary": "Networks are widely used to model a variety of complex, often multi-disciplinary, systems in which the relationships between their sub-parts play a significant role. In particular, there is extensive research on the topological properties associated with their structure as this allows the analysis of the overall behaviour of such networks. However, extracting networks from structured and unstructured data sets raises several challenges, including addressing any inconsistency present in the data, as well as the difficulty in investigating their properties especially when the topological structure is not fully determined or not explicitly defined. In this paper, we propose a novel method to address the automated identification, assessment and ranking of the most likely structure associated with networks extracted from a variety of data sets. More specifically, our approach allows to mine data to assess whether their associated networks exhibit properties comparable to well-known structures, namely scale-free, small world and random networks. The main motivation is to provide a toolbox to classify and analyse real-world networks otherwise difficult to fully assess due to their potential lack of structure. This can be used to investigate their dynamical and statistical behaviour which would potentially lead to a better understanding and prediction of the properties of the system (s) they model. Our initial validation shows the potential of our method providing relevant and accurate results.", "title": "Extraction, Identification, and Ranking of Network Structures from Data Sets", "claims": null}, {"metadata": {"year": 2000}, "authors": ["V. I. Voropajev", "S. M. Ljubkin", "B. P. Titarenko", "D.  Golenko-Ginzburg"], "summary": "Abstract A newly developed structural classification of network models is suggested. It is based on singling out three main groups of characteristics which define both the structure and the parameters of network models. Those groups are (1) network elements (nodes, arrows, terms\u2019 restriction, logical links, etc.), (2) elements\u2019 parameters (number, function, set of variants, random variable, etc.) and (3) degree of alternativity (alternative logical operations at the node\u2019s input and output). The structural classification arranges an order in the variety of all the types of network models by using a three-dimension matrix. Each model can be characterised by a set of cells in the three-dimensional \u2018house\u2019. The classification enables not only a description of the network models, but also a forecast of new models. The latter may be discovered either by filling in \u2018empty places\u2019 in the matrix or by implementing new attributes in the groups\u2019 characteristics.", "title": "Structural classification of network models", "claims": null}, {"metadata": {"year": 2014}, "authors": ["L. Gallos", "N. Fefferman"], "summary": "The ability to compare complex systems can provide new insight into the fundamental nature of the processes captured, in ways that are otherwise inaccessible to observation. Here, we introduce the n-tangle method to directly compare two networks for structural similarity, based on the distribution of edge density in network subgraphs. We demonstrate that this method can efficiently introduce comparative analysis into network science and opens the road for many new applications. For example, we show how the construction of a \u201cphylogenetic tree\u201d across animal taxa according to their social structure can reveal commonalities in the behavioral ecology of the populations, or how students create similar networks according to the University size. Our method can be expanded to study many additional properties, such as network classification, changes during time evolution, convergence of growth models, and detection of structural changes during damage.", "title": "Revealing effective classifiers through network comparison", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Emily Casleton"], "summary": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii CHAPTER", "title": "A local structure graph model for network analysis", "claims": null}, {"metadata": {"year": 2020}, "authors": ["J. Martin", "James P. Murphy"], "summary": "The notion that there is a single class of objects, \u201cnetworks,\u201d has been a great inspiration to new forms of structural thinking. Networks are considered to be a set of largely voluntary ties that often span organizational boundaries. Despite being divorced from formal hierarchies, they make possible other forms of differentiation, such as status. It is common for network data to be used to produce measures of the status of the nodes (individuals, organizations, cultural products, etc.) and the distribution of these statuses to describe a backdrop of inequality that may condition action or other processes. However, it is also important that network researchers understand the backdrop of various forms of potential inequality that may condition the collection of network data.", "title": "Networks, Status, and Inequality", "claims": null}], "query": "network analysis and class structure", "summary_abstract": "The research question regarding network analysis and class structure is addressed through various approaches and methodologies in the provided abstracts. \n\nStuckenschmidt (2005) emphasizes the application of network analysis methods for the automatic partitioning of large concept hierarchies, demonstrating how these techniques can effectively identify key concepts and related subsets within class hierarchies. This highlights the utility of network analysis in understanding and organizing complex structures.\n\nDehmer et al. (2008) introduce an information-theoretic approach to classify graph structures, proposing a complexity measure that captures the structural information of graphs. This method allows for a nuanced understanding of different graph classes, which is essential for analyzing class structures within networks.\n\nEstrada (2007) contributes to the discussion by identifying four distinct topological structural classes of networks, based on their organization and connectivity. This classification is significant for understanding the underlying structure of various real-world networks, including ecological and social systems, and suggests that different growth mechanisms do not uniformly reproduce these classes.\n\nTrovati et al. (2014) propose a method for the automated identification and assessment of network structures from diverse datasets. Their approach aims to classify networks according to well-known structures, such as scale-free and small-world networks, thereby enhancing the understanding of class structures in real-world applications.\n\nVoropajev et al. (2000) present a structural classification of network models based on three main characteristics: network elements, their parameters, and the degree of alternativity. This classification framework aids in organizing and predicting new network models, contributing to the broader understanding of class structures.\n\nGallos and Fefferman (2014) introduce the n-tangle method for comparing networks based on structural similarity, which can facilitate the classification of networks and reveal insights into their underlying properties.\n\nLastly, Martin and Murphy (2020) discuss the implications of network structures on social differentiation and inequality, suggesting that the analysis of network data can uncover various forms of status and inequality that influence class structures.\n\nCollectively, these papers illustrate the multifaceted nature of network analysis in understanding class structures, highlighting various methodologies and frameworks that contribute to the classification and analysis of complex networks across different domains.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2019}, "authors": ["Michael J Wilkinson", "Emily N C Manoogian", "Adena  Zadourian", "Hannah  Lo", "Savannah  Fakhouri", "Azarin  Shoghi", "Xinran  Wang", "Jason G Fleischer", "Saket  Navlakha", "Satchidananda  Panda", "Pam R Taub"], "summary": "In animal models, time-restricted feeding (TRF) can prevent and reverse aspects of metabolic diseases. Time-restricted eating (TRE) in human pilot studies reduces the risks of metabolic diseases in otherwise healthy individuals. However, patients with diagnosed metabolic syndrome often undergo pharmacotherapy, and it has never been tested whether TRE can act synergistically with pharmacotherapy in animal models or humans. In a single-arm, paired-sample trial, 19 participants with metabolic syndrome and a baseline mean daily eating window of \u226514 h, the majority of whom were on a statin and/or antihypertensive therapy, underwent 10\u00a0h of TRE (all dietary intake within a consistent self-selected 10\u00a0h window) for 12\u00a0weeks. We found this TRE intervention improves cardiometabolic health for patients with metabolic syndrome receiving standard medical care including high rates of statin and anti-hypertensive use. TRE is a potentially powerful lifestyle intervention that can be added to standard medical practice to treat metabolic syndrome. VIDEO ABSTRACT.", "title": "Ten-Hour Time-Restricted Eating Reduces Weight, Blood Pressure, and Atherogenic Lipids in Patients with Metabolic Syndrome.", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Michael J. Wilkinson", "Emily  Manoogian", "Adena  Zadourian", "Hannah  Lo", "Satchidananda  Panda", "Pam R. Taub"], "summary": "Metabolic syndrome affects 34% of the U.S. population and increases risk of cardiovascular disease and type 2 diabetes. Lifestyle changes are first-line therapy. In patients with metabolic syndrome who eat during a significant portion of each day (>/=14 hours/day, for example by late-night snacking", "title": "TIME-RESTRICTED EATING PROMOTES WEIGHT LOSS AND LOWERS BLOOD PRESSURE IN PATIENTS WITH METABOLIC SYNDROME", "claims": null}, {"metadata": {"year": 2021}, "authors": ["A. Charlot", "Fanny Hutt", "Eug\u00e9nie Sabatier", "J. Zoll"], "summary": "The importance of metabolic health is a major societal concern due to the increasing prevalence of metabolic diseases such as obesity, diabetes, and various cardiovascular diseases. The circadian clock is clearly implicated in the development of these metabolic diseases. Indeed, it regulates physiological processes by hormone modulation, thus helping the body to perform them at the ideal time of day. Since the industrial revolution, the actions and rhythms of everyday life have been modified and are characterized by changes in sleep pattern, work schedules, and eating habits. These modifications have in turn lead to night shift, social jetlag, late-night eating, and meal skipping, a group of customs that causes circadian rhythm disruption and leads to an increase in metabolic risks. Intermittent fasting, especially the time-restricted eating, proposes a solution: restraining the feeding window from 6 to 10 h per day to match it with the circadian clock. This approach seems to improve metabolic health markers and could be a therapeutic solution to fight against metabolic diseases. This review summarizes the importance of matching life habits with circadian rhythms for metabolic health and assesses the advantages and limits of the application of time-restricted fasting with the objective of treating and preventing metabolic diseases.", "title": "Beneficial Effects of Early Time-Restricted Feeding on Metabolic Diseases: Importance of Aligning Food Habits with the Circadian Clock", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Takahisa Nakamura"], "summary": "Time-restricted feeding without reducing caloric intake prevents metabolic disease in mice fed a high-fat diet.", "title": "You Are What and When You Eat", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Anna  Kriebs"], "summary": "Nature reviews | Endocrinology the metabolic syndrome is a group of concomitant conditions, including abdominal obesity, elevated blood pressure and hyperglycaemia, which increase a person\u2019s risk for cardiovascular diseases and type 2 diabetes mellitus. Lifestyle interventions, such as reducing calorie intake and exercising, are the first treatment for the metabolic syndrome but are often difficult to adhere to. time-restricted eating (tre) is a dietary intervention that aims to ensure people maintain a regular daily cycle of feeding and fasting. studies in rodents suggest that food timing rather than calorie content underlies the beneficial effects of tre. \u201cit is difficult for many people to count calories,\u201d explains Pam taub, one of the corresponding authors of the study. \u201ctre has given new hope to those involved in treating obesity and metabolic diseases, but it is rarely tested on people who have been diagnosed with metabolic diseases.\u201d therefore, it was unclear whether patients that are on medications for one or several symptoms of the metabolic syndrome can safely implement tre and derive additional benefits from it. to address these questions the researchers recruited 19 patients who had at least three symptoms of the metabolic syndrome for a 12-week tre intervention. the participants selected a daily, 10-hour window in which to consume all of their calories and used the myCircadianClock app, developed by co-corresponding author satchin Panda, to log calorie intake. they also wore a wrist activity monitor and a conti nuous glucose monitor for 2 weeks to track activity and intestinal glucose levels, respectively. the researchers observed that study participants adapted to the tre schedule by moderately shifting meal timing rather than skipping meals and that timing of calorie intake became more regular over the course of the study. Participants lost body weight and experienced improvements in LDL-cholesterol levels, blood pressure and blood glucose values. in addition, none of the partici pants reported adverse effects while they reduced calorie intake. \u201ceven patients who were already on statins and antihypertensive medications further reduced LDL-cholesterol and blood pressure,\u201d reports taub. the authors note that the metabolic improvements could not be explained solely by changes in weight. \u201cFuture studies should combine nutritional quality and/or quantity with the timing of eating to test their relative contributions to health,\u201d concludes Panda. Anna Kriebs, Associate Editor, Nature Communications M E TA B O L I C S Y N D R O M E", "title": "The benefits of time-restricted eating", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Marriam Ali", "S. Reutrakul", "Gregory Petersen", "K. Knutson"], "summary": "Diabetes is highly prevalent and is associated with dietary behaviors. Time-restricted eating, which consolidates caloric intake to a shortened eating duration, has demonstrated improvement in metabolic health. Timing of eating could also impact metabolism. Our objective was to examine whether the timing of eating was associated with metabolic health independently of eating duration. Data (n = 7619) are from four cycles (2005\u20132012) of the National Health and Nutrition Examination Survey (NHANES), a nationally representative U.S. survey that included surveys, physical examinations, and dietary recalls. The primary exposures are eating duration and eating start time estimated from two non-consecutive dietary recalls. Primary outcomes were fasting glucose and estimated insulin resistance using the homeostatic model assessment method (HOMA-IR). The mean (95% CI) eating duration was 12.0 h (11.9\u201312.0) and the mean (95% CI) start time was 8:21 (8:15\u20138:26). Earlier eating start time was significantly associated with lower fasting glucose and estimated insulin resistance but eating interval duration was not. Every hour later that eating commenced was associated with approximately 0.6% higher glucose level and 3% higher HOMA-IR (both p < 0.001). In this cross-sectional study, earlier eating start time was associated with more favorable metabolic measures, indicating that meal timing is another important characteristic of dietary patterns that may influence metabolism.", "title": "Associations between Timing and Duration of Eating and Glucose Metabolism: A Nationally Representative Study in the U.S.", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Hannah  Stower"], "summary": "Time-restricted feeding results in caloric-intake reduction, weight loss and improved cardiometabolic measures. Time-restricted feeding, or intermittent fasting, in which a person eats only during a short period of the day, has gained in popularity as a healthy approach to weight loss and results in an improvement in cardiometabolic risk factors, although it is yet to be assessed in a rigorous clinical trial. Cienfuegos and colleagues carry out a randomized clinical trial of 4-hour and 6-hour restricted feeding, and a control diet, over 8 weeks in a group of 58 participants with obesity. They find that restrictive feeding results in a reduction in body weight, insulin resistance and oxidative stress relative to that of control groups, indicative of the potential for the diet to result in improved health outcomes.", "title": "Intermittent fasting passes trial", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Amir  Zarrinpar", "Amandine  Chaix", "Satchidananda  Panda"], "summary": "Cyclical expression of cell-autonomous circadian clock components and key metabolic regulators coordinate often discordant and distant cellular processes for efficient metabolism. Perturbation of these cycles, either by genetic manipulation, disruption of light/dark cycles, or, most relevant to the human population, via eating patterns, contributes to obesity and dysmetabolism. Time-restricted feeding (TRF), during which time of access to food is restricted to a few hours, without caloric restriction, supports robust metabolic cycles and protects against nutritional challenges that predispose to obesity and dysmetabolism. The mechanism by which TRF imparts its benefits is not fully understood but likely involves entrainment of metabolically active organs through gut signaling. Understanding the relationship of feeding pattern and metabolism could yield novel therapies for the obesity pandemic.", "title": "Daily Eating Patterns and Their Impact on Health and Disease", "claims": null}], "query": "How does time-restricted eating contribute to reducing metabolic disease?", "summary_abstract": "The collective findings from the reviewed papers indicate that time-restricted eating (TRE) can significantly contribute to reducing the risk and severity of metabolic diseases. TRE, which involves limiting food intake to a specific time window each day, has shown promising results in both animal models and human studies.\n\nWilkinson et al. (2019) demonstrated that a 10-hour TRE intervention improved cardiometabolic health in individuals with metabolic syndrome, even those on pharmacotherapy for their conditions. Participants experienced reductions in LDL-cholesterol, blood pressure, and blood glucose levels, suggesting that TRE can be an effective adjunct to standard medical treatments for metabolic syndrome.\n\nFurther supporting this, Nakamura (2012) found that time-restricted feeding without caloric restriction prevented metabolic disease in mice, highlighting the potential of food timing over caloric intake alone. Similarly, Ali et al. (2023) reported that earlier eating start times were associated with lower fasting glucose and insulin resistance, emphasizing the importance of meal timing in metabolic health.\n\nCharlot et al. (2021) discussed how aligning eating patterns with the circadian clock through TRE could mitigate the risks associated with modern lifestyle changes that disrupt natural rhythms. This alignment appears to enhance metabolic health markers, making TRE a viable therapeutic strategy against metabolic diseases.\n\nMoreover, Stower (2020) noted that TRE leads to weight loss and improved cardiometabolic measures, reinforcing the idea that this dietary approach can yield significant health benefits. The findings suggest that TRE not only aids in weight management but also improves various metabolic parameters, making it a promising intervention for those at risk of metabolic diseases.\n\nIn summary, the evidence indicates that time-restricted eating can effectively improve metabolic health by enhancing metabolic markers, reducing the risk of metabolic syndrome, and potentially serving as a complementary strategy alongside traditional medical treatments (Wilkinson et al., 2019; Nakamura, 2012; Ali et al., 2023; Charlot et al., 2021; Stower, 2020).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2021}, "authors": ["Maite  Carrass\u00f3n", "Anna  Soler-Membrives", "Mar\u00eda  Constenla", "C.  Escobar", "R.  Flos", "Jose Maria Gil", "V.  Luz\u00f3n", "Francesc  Piferrer", "L.  Reig"], "summary": "Abstract Aquaculture products are commonplace in markets around the world. However, despite efforts to minimize the negative perceptions towards aquaculture, several misbeliefs or myths still persist, and thus globally consumers tend to value wild fish more highly than farmed fish . The lack of information has been shown to be one of the most important causes of this preference, driving buying decisions to be more emotional than rational. The aim of this study was to determine whether scientific-supported information contrasting one myth could contribute to a better perception of farmed products. To that end, consensus on a series of aquaculture-related issues among different scientists, external experts, and aquaculture societies was used to build up the scientific information. This information was provided to 300 Spanish consumers using two different communication tools (150 consumers each tool): an interactive web documentary and a written and printed document, to detect possible differences in the change of consumers' perception. Consumers were asked for their degree of agreement on a set of 14 statements before and after providing the scientific information. A variable collecting the assessment of each of the statements was calculated as the Overall-perception. Possible significant differences between the scores before and after providing the information and for the \u2018overall perception\u2019 were analysed separately for each communication tool as well as for the combined sample. Possible relationship between the consumers' perception with the sociodemographic factors, the consumers' knowledge and the fish consumption habits were also assessed. Results show that consumer's perception of aquaculture before the query were moderate (5.6 average in a 0 to 10 scale) but that it increased slightly but significantly and regardless of the communication tool used. Among sociodemographic factors, age and gender were the ones that most influenced consumer's perceptions, being older people those who exhibited a generally more positive opinion towards aquaculture. The effects of consumption habits and knowledge about aquaculture were also the two most explicative factors for change in perception. Importantly, the opinion of consumers with less knowledge about seafood products in general and production methods or consuming only wild fish products, improved after being exposed to the information. These results demonstrate the utility of science- and fact-based communication campaigns to improve the societal perception of aquaculture practices and products, regardless of the tool used to transmit this information.", "title": "Information impact on consumers' perceptions towards aquaculture: Dismantling the myth about feeds for farmed fish", "claims": null}, {"metadata": {"year": 2020}, "authors": ["S. Leng"], "summary": "High demand of seafood products combined with over-harvesting of wild catch aquatic products have elevated aquaculture as the alternative way to strike a balance between seafood product supply and demand. However, uncontrolled aquaculture activities are found to affect the environment negatively as well as the health of consumers. Better awareness and understanding can regulate aquaculture practices and bring about environmental health through consumers\u2019 pressure on the aquaculture production system. This study aimed to identify consumers\u2019 perception on aquaculture industry, to study consumers\u2019 awareness on sustainability of aquaculture practice and also to find ways to increase consumers\u2019 awareness and acceptance of sustainable aquaculture practice and its products. A mixed research method was used in this study combining quantitative questionnaire survey of respondents who bought aquaculture products at wet markets in Penang, and qualitative in-depth focus group discussions (FGDs) with a number of selected respondents. Research findings show that most of the respondents are aware that aquaculture might give lots of negative effects to human health and the environment but, they still chose to buy aquaculture products due to cheaper price than wild catch seafood products. Hence, from the findings, it is clear that even though people have awareness, they have little choice but to accept the quality of aquaculture products. However, the results also show that enhancing consumer awareness and understanding empowers consumers to put pressures on producers paving the way towards sustainable aquaculture and greater acceptance of aquaculture practices and products. 2357-1330 \u00a9 2020 Published by European Publisher.", "title": "Consumer Awareness And Acceptance Of Aquaculture Practices And Products In Penang, Malaysia", "claims": null}, {"metadata": {"year": 2010}, "authors": ["B. Rasco"], "summary": "Consumer perception affects how food products are positioned in the marketplace. In addition, misperception about product safety influences how products are labeled and marketed. Regulatory compliance issues facing aquaculture producers are driven both by science-based concerns over product safety and politics. Consumer perception of product safety often has little connection with any legitimate public health concern. This is particularly true for aquatic foods which have been politically targeted throughout the years by inaccurate assessment of product safety as part of a larger strategy to discourage purchases or to direct purchase to politically more acceptable product, for example from a \u201cfarmed\u201d fish to a wild harvested one. Activists target the safety of foods harvested from the marine environment to promote an environmental agency, most recently mercury raising content in pelagic species such as tuna to scare women, particularly pregnant women, from consuming this otherwise healthful food. Recent scares have involved colorant and feed additives for farmed salmon, leading to mandatory retail labeling in Western US markets of \u201cadded color\u201d for all farm raised salmonids to increase consumer concern about product safety and discourage purchases. These efforts detract from improving overall food safety, specifically risks of microbial contamination and growth.", "title": "Perceptions of Seafood Safety", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Jos\u00e9 M. Fern\u00e1ndez-Polanco", "L. Luna", "J. L. F. S\u00e1nchez"], "summary": "Attitudes towards production processes increased their importance over food purchase decissions in recent years. This paper aims to test which factors mainly affects attitudes towards aquaculture. Factors are classified into three groups: demographic, situational, and personal and marketing infuences. Using consumer survey data, factor analysis techniques were used to obtain an attitude meassure from a set of scales. Diferent linear models and fit methods were performed with variables of the three groups of factors to test for significant effects over attitude scores. Significant effects were found with education, ocupation and income. Purchase and consumption place were also found to be significant causes of diferencies across attitudes. Finally Consumer involvement with food safety and sustainability and generic advertising credibility have shown significant effects over attitudes towards aquaculture.", "title": "Factors Affecting Consumer's Attitudes Towards Aquaculture", "claims": null}, {"metadata": {"year": 1997}, "authors": ["B.  Paterson", "B.  Goodrick", "S.  Frost"], "summary": "By growing animals under managed conditions, aquaculture facilitates the production of seafood of consistent quality and optimization of marketing opportunities. Fish can be grown economically by using formulated feeds that can optimize cost and/or quality considerations and consumer needs. A general industry trend towards achieving efficient harvest with minimal stress produces flesh of optimum quality, and delays biochemical deterioration, allowing very fresh product to be transported and marketed.", "title": "Controlling the quality of aquacultured food products", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Rungsaran  Wongprawmas", "Giovanni  Sogari", "Francesco  Gai", "Giuliana  Parisi", "Davide  Menozzi", "Cristina  Mora"], "summary": "Abstract Aquaculture has increased significantly in recent decades and, if managed sustainably, it could contribute to food security and safety. However, several studies have found the evidence that European consumers prefer wild fish relative to farmed fish. Hence, it becomes crucial to provide accurate information on aquaculture products. This study aims to examine the influence of consumers' perceptions and knowledge on their intention to purchase farmed and wild fish. A sample of 804 Italian consumers participated in a survey which included an information treatment. Participants perceived that wild fish tastes better and is more nutritious than farmed fish; further, they considered farmed fish to be cheaper, safer, and more ethical than wild fish. A logit regression was conducted whose results show that several factors, such as fish preference, attitude, and subjective knowledge, affect consumers' intention to purchase. In addition, participants in the treatment group were found to have higher intentions to purchase farmed fish than those in the control group. Therefore, information strategy is crucial to promote the consumption of farmed fish.", "title": "How information influences consumers' perception and purchasing intention for farmed and wild fish", "claims": null}, {"metadata": {"year": 2005}, "authors": ["\u0412\u0430\u0441\u0438\u043b\u044c \u041f\u0435\u0442\u0440\u043e\u0432\u0438\u0447 \u0417\u0430\u0433\u0440\u0438\u0439\u0447\u0443\u043a"], "summary": "Regulation subject matter of fish food behaviour is actively being set up in terms of aquaculture demands, great attention being paid to central mechanisms of regulation. Food behaviour is regulated by multichannel control system, including all known mechanisms of nervous and humoral control, integration of which is carried out in hypothalamus with participation of various brain structures.", "title": "\u0418\u0437\u0443\u0447\u0435\u043d\u0438\u0435 \u043f\u0438\u0449\u0435\u0432\u043e\u0433\u043e \u043f\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u044f \u0440\u044b\u0431 \u043d\u0430 \u0444\u043e\u043d\u0435 \u0432\u043e\u0437\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u0442\u0435\u0441\u0442\u043e\u0441\u0442\u0435\u0440\u043e\u043d\u0430", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Lauren Errickson", "Douglas R Zemeckis"], "summary": "\n \n \n Americans consistently fail to meet dietary guidelines for seafood intake. Efforts are needed to increase consumption, especially of sustainable seafood that can be supplied by domestic aquaculture. However, consumer receptivity to aquaculture products is mixed. The objective of this study was to elicit industry perspectives regarding influences on consumer purchases of aquaculture products.\n \n \n \n Key informant interviews (n\u00a0=\u00a012) were conducted in late 2020 with U.S. salmon, shrimp, and oyster producers, marketers, and industry interest groups. Participants were recruited via snowball sampling. Virtual interviews were conducted by a trained moderator and assistant moderator/notetaker using a semi-structured interview guide. Qualitative data analysis included a thematic review of interview recordings and notes, with key concepts coded according to a priori themes derived from the literature.\n \n \n \n Interviews yielded important insights into consumer receptivity to aquaculture products. Participants believe that outdated misperceptions of aquaculture persist, noting that despite advances in domestic aquaculture production practices to comply with U.S. standards, some consumers perceive aquaculture as environmentally detrimental and unsustainable. Further, participants believe negative attitudes toward genetically modified organisms, corn and soy-based feeds, antibiotics, and chemicals are misplaced, yet contribute to hesitancy among some consumers. Industry opinions on what is important to consumers reflect strong valuation of seafood quality, freshness, local harvest, and sustainability. Participants suggest product labeling efforts be developed accordingly, and that innovative marketing strategies be undertaken, such as aquaculture product promotion through \u201cknow your farmer\u201d campaigns, chef education initiatives, and home delivery programs.\n \n \n \n For domestic aquaculture products to have a meaningful impact on U.S. seafood intake, positive consumer receptivity is key. Industry perspectives will inform marketing and educational efforts toward addressing consumer hesitancy to purchase aquaculture products by resolving misguided concerns, with important implications for consumer health and sustainability of the domestic seafood supply.\n \n \n \n United States Department of Agriculture.\n", "title": "Industry Insights on Consumer Receptivity to Aquaculture Products in the Retail Marketplace: Considerations for Increasing Seafood Intake", "claims": null}], "query": "write an essay on how general knowledge and awareness relate perceived behavioral control towards food products especially unfamiliar low trophic level aquaculture products.", "summary_abstract": "The relationship between general knowledge and awareness and perceived behavioral control towards unfamiliar low trophic level aquaculture products is a complex interplay that significantly influences consumer attitudes and purchasing decisions. Several studies highlight how consumer perceptions are shaped by their knowledge and awareness of aquaculture practices, which in turn affects their willingness to purchase these products.\n\nCarrass\u00f3n et al. (2021) emphasize that a lack of information about aquaculture leads to emotional rather than rational buying decisions, with consumers often favoring wild fish over farmed options due to persistent myths and misconceptions. Their research indicates that providing scientifically-supported information can enhance consumer perceptions of aquaculture products, particularly among those with limited knowledge. This suggests that increasing awareness can empower consumers to make more informed choices, thereby improving their perceived behavioral control over their purchasing decisions.\n\nSimilarly, Leng (2020) found that while consumers are aware of the potential negative impacts of aquaculture on health and the environment, they often choose aquaculture products due to their lower prices compared to wild-caught seafood. This highlights a paradox where awareness does not necessarily translate into positive perceptions or purchasing behavior. However, the study also points out that enhancing consumer understanding can lead to greater acceptance of sustainable aquaculture practices, indicating that knowledge can indeed influence perceived behavioral control.\n\nRasco (2010) discusses how consumer perceptions of product safety are often disconnected from actual health concerns, particularly in the context of aquaculture. Misconceptions about safety can deter consumers from purchasing farmed products, suggesting that improving knowledge about safety regulations and practices in aquaculture could enhance consumer confidence and control over their food choices.\n\nWongprawmas et al. (2022) further illustrate that consumers often perceive wild fish as superior in taste and nutrition, which affects their purchasing intentions. However, the study also shows that providing accurate information can shift these perceptions, leading to increased intentions to purchase farmed fish. This reinforces the idea that knowledge and awareness are critical in shaping consumer attitudes and behaviors towards aquaculture products.\n\nErrickson and Zemeckis (2021) highlight industry perspectives on consumer receptivity to aquaculture, noting that outdated misperceptions persist despite advancements in production practices. They argue that addressing these misconceptions through targeted marketing and educational efforts is essential for improving consumer acceptance of aquaculture products. This aligns with the notion that enhancing general knowledge can lead to better perceived behavioral control among consumers.\n\nIn summary, the collective findings from these studies underscore the importance of general knowledge and awareness in shaping consumer perceptions and behaviors towards unfamiliar low trophic level aquaculture products. By addressing misconceptions and providing accurate information, it is possible to enhance perceived behavioral control, leading to more informed and positive purchasing decisions.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2016}, "authors": ["Manuel  Konrath", "Julie  Gorenflo", "Nils  H\u00fcbner", "Hermann  Nirschl"], "summary": "Abstract Centrifuges represent a well-established tool in separation technology to handle high throughputs of suspensions with micron-sized particles. However, extraordinarily high centrifugal accelerations are required for even smaller particles. For this purpose, we make use of magnetic bearing and drive technology. We report the first application of this contactless technique in separation experiments. The presented prototype is based on a semi-continuous principle, where sediment is built up within the rotor while the liquid (which contains the fine fraction in case of classification) is discharged at the overflow weir. The new centrifuge allows rotational speeds of more than 64k\u00a0min \u22121 . A broad variation of parameters with centrifugal accelerations of up to C =100,000 and flow rates of up to 0.4\u00a0l/min provides a sound experimental basis for this study. The cut size and the product loss of three particle systems between 10\u00a0nm and 1\u00a0\u00b5m (silver nanoparticles, silica nanoparticles, and polystyrene) are precisely adjustable via the mentioned parameters. Furthermore, a good comparability with a commercially available device was found regarding the specific efficiency. We consider our approach to be a promising contribution to nanoscale separation technology where scalable high-throughput methods are rare.", "title": "Application of magnetic bearing technology in high-speed centrifugation", "claims": null}, {"metadata": {"year": 2022}, "authors": ["L. Moreno", "Alexander V\u00e1squez", "Elizabeth Pastrana", "Roxana Pastrana"], "summary": "In this work, self-made laboratory centrifuge equipment was developed for the separation of suspensions and immiscible liquids. This centrifuge was based on the Arduino microcontroller and fabricated from simple and recycled mechanical and electronic parts, spending an inexpensive budget suitably. The centrifuge frame and rotors were manufactured of Polylactic Acid (PLA) filaments, which were printed in a 3D printer. Two rotors labeled R50 and R15 were designed and fabricated to use tubes of 50 mL and 15 mL, respectively. The maximum speed reached, without vibration interferences, was 3350 and 3030 rpm for R50 and R15 rotors, respectively. The total fabrication cost of the fabricated centrifuge was less than 100 USD and the assembly process was not complicated. Finally, the centrifuge performance was evaluated through phases of separation of precipitate of CuCO3(s) and clinical blood samples. The obtained results showed achievement in components separation, entirely comparable with those obtained employing a commercial centrifuge.", "title": "DESIGN, DEVELOPMENT, AND IMPLEMENTATION OF 3D-PRINTED POLYLACTIC ACID CENTRIFUGE ROTORS FOR LABORATORY-SCALE APPLICATIONS", "claims": null}, {"metadata": {"year": 2013}, "authors": ["B. Kwon", "Hyung Hoon Kim", "Jae Hyeong Park", "D. Yoon", "Moon-Chan Kim", "S. Sheard", "K. Morten", "J. Go"], "summary": "In this paper, we describe the development of a microfluidic centrifuge with two inlets and two outlets potentially capable of rapidly separating nanoparticles and nanovesicles. Compared with the microfluidic centrifuge with a single inlet and outlet, the 2 \u00d72 microfluidic centrifuge gives improved centrifugation performance by increasing momentum flux transfer, angular velocity, and centrifugal acceleration. The center of flow rotation and the symmetry of the horizontal velocity in the microchamber were examined numerically. On the basis of the determined maximum velocity, the angular velocity and centrifugal acceleration were also evaluated. The centrifugation time of three different nanoparticles was examined by calculating the time when the nanoparticles left the microchamber for the first time. For visual observation and quantitative measurement of nanoparticle centrifugation, a 2 \u00d72 microfluidic centrifuge was fabricated and the experimental results demonstrate similar physical behavior to those of a mechanical centrifuge. On the basis of a comparison of the centrifugation time of two different nanoparticle populations of 300 and 700 nm in diameter, we propose that nanoparticles of different sizes can be physically separated by time under a range of inlet volume flow rates.", "title": "Separation of different sized nanoparticles with time using a rotational flow", "claims": null}, {"metadata": {"year": 2010}, "authors": ["G. Hwang"], "summary": "As a super high speed small-sized centrifugal separator which is operated by the center inputting method has solved two difficulty problems, it can be made use of developing the new composition through the making of Nano particle (50~100nm) by high speed revolution and alleviation of unbalanced shapes inside the disk continuously throwing slurry in the central part. Uses the air spindle and the centrifugal machine production will be able to manufacture the Roh class fine particle under boil.", "title": "A Study on the Centrifuge Fabrication for a Nano Size Particle Having a Central Supply Tube", "claims": null}, {"metadata": {"year": 2013}, "authors": ["B. Kwon", "Hyung Hoon Kim", "J. Park", "D. Yoon", "Moon Chan Kim", "Steve Sheard", "Karl Morten", "J. Go"], "summary": "In this paper, we describe the development of a microfluidic centrifuge with two inlets and two outlets potentially capable of rapidly separating nanoparticles and nanovesicles. Compared with the microfluidic centrifuge with a single inlet and outlet, the 2 \u00d72 microfluidic centrifuge gives improved centrifugation performance by increasing momentum flux transfer, angular velocity, and centrifugal acceleration. The center of flow rotation and the symmetry of the horizontal velocity in the microchamber were examined numerically. On the basis of the determined maximum velocity, the angular velocity and centrifugal acceleration were also evaluated. The centrifugation time of three different nanoparticles was examined by calculating the time when the nanoparticles left the microchamber for the first time. For visual observation and quantitative measurement of nanoparticle centrifugation, a 2 \u00d72 microfluidic centrifuge was fabricated and the experimental results demonstrate similar physical behavior to those of a mechanical centrifuge. On the basis of a comparison of the centrifugation time of two different nanoparticle populations of 300 and 700 nm in diameter, we propose that nanoparticles of different sizes can be physically separated by time under a range of inlet volume flow rates.", "title": "Separation of Different Sized Nanoparticles with Time Using a Rotational Flow", "claims": null}, {"metadata": {"year": 2005}, "authors": ["S. Vatannia", "F. Barez"], "summary": "A centrifuge is used in bio-industry to separate species in blood and other chemicals. Bio-industry requires a temperature of zero degree centigrade in the rotor compartment of a centrifuge where samples are placed. In general, the current portable centrifuge systems generate a temperature of about 22 \u00b0C in the rotor compartment when operating at 3000 RPM. The motor and the electronics are the primary sources of the heat generation in such centrifuge. The aim of this study is to develop an appropriate cooling system for a specific portable centrifuge used in separating bioparticles that generates a total heat of approximately 43 W. Experimental, analytical and computer simulation were employed to achieve the project objective of reducing and maintaining the rotor compartment temperature at zero degree C. The CFD code Simulation model predicted rotor compartment temperatures that were in good agreement with those of the experimental measurements within 3%. Having confidence in the CFD model, simulation was carried out to incorporate four TEC units that are embedded on the surface of the rotor compartment resulting in reduced temperature to zero degree C.Copyright \u00a9 2005 by ASME", "title": "Cooling System for a Portable Bio-Industry Centrifuge", "claims": null}, {"metadata": {"year": 1955}, "authors": ["G. Derge", "J. Shegog"], "summary": "A rotating-type induction furnace with graphite heater was used to demonstrate the application of the principles developed in Part I. Rotation at 1200 to 1300 r.p.m. for 15 minutes at an average temperature of 2560\u00b0F. (1405\u00b0C.) separated a low-melting slag from a silica brick which had been used in an open-hearth roof.", "title": "High\u2010Temperature Centrifuge for Separation of the Liquid Phase from Refractories: II, Centrifuging Tests", "claims": null}, {"metadata": {"year": 1967}, "authors": ["H. Friedman"], "summary": "A centrifuge was developed for investigation of equilibrium phase separations in molten salts. In operation, the apparatus rotates four sealed metal capsules containing 5-10 g specimens at a maximum speed of 2400 rev min-1 and at a maximum temperature of approximately 1000\u00b0C.", "title": "High temperature centrifuge for phase equilibrium studies in molten salts", "claims": null}], "query": "what rpm is needed in centrifuge during nanofibers separation?", "summary_abstract": "The research question regarding the required RPM for centrifuges during nanofiber separation is addressed in several studies, each contributing insights into the necessary rotational speeds and the effectiveness of different centrifuge designs.\n\nKonrath et al. (2016) highlight the use of a novel centrifuge capable of achieving rotational speeds exceeding 64,000 RPM, which is essential for separating particles in the nanometer range (10 nm to 1 \u00b5m). This high-speed centrifuge utilizes magnetic bearing technology to facilitate the separation of fine fractions from suspensions, indicating that extremely high RPMs are crucial for effective separation at the nanoscale.\n\nIn contrast, Moreno et al. (2022) developed a more accessible laboratory centrifuge that reached maximum speeds of 3,350 RPM and 3,030 RPM for different rotor designs. While these speeds are significantly lower than those reported by Konrath et al., the centrifuge still demonstrated effective separation capabilities comparable to commercial devices, suggesting that while high RPMs are beneficial, lower speeds can also achieve satisfactory results depending on the application.\n\nKwon et al. (2013) introduced a microfluidic centrifuge that enhances separation performance through improved angular velocity and centrifugal acceleration. Although specific RPM values were not provided, the study emphasizes the importance of optimizing flow rates and angular velocities for effective nanoparticle separation, indicating that the design and operational parameters can be as critical as the RPM itself.\n\nOverall, the findings suggest that while extremely high RPMs (up to 64,000) are advantageous for separating nanoparticles, effective separation can also be achieved at lower speeds (around 3,000 RPM) with appropriate design considerations. The choice of centrifuge and its operational parameters should be tailored to the specific requirements of the nanofiber separation process.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2014}, "authors": ["Qian Xuefen"], "summary": "The imperfect competition in factor or product markets always fails to allocate resources according to firm's equal marginal products,therefore resulting in resource misallocation. Due to the negative effects of resources misallocation on aggregate total factor productivity and outputs,both cross-country and cross-industry productivity differences become more pronounced.So figuring out the approaches that measure the degree of resource misallocation provides the constructive practical reference to the analysis of the distortion in Chinese capital market and labor market. After a review of relative literature,this paper summarizes and discusses the main approaches to measure resource misallocation,and explores the insufficiency about the existing studies and policy implications.", "title": "A Survey on Measurements of Resource Misallocation", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Konstantins  Benkovskis"], "summary": "This paper evaluates misallocation of resources in Latvia during 2007\u20132014 using firm-level data. I find that misallocation of resources increased before 2010 and declined afterwards. Initially, output distortion was the major source of misallocation, while the importance of capital distortions increased after the financial crisis. Determinants of changes in allocation efficiency may include growing competition in domestic markets, tighter credit supply and legal issues. However, I show that fragmentation of production induces bias to the estimates of firm-specific distortions, leading to the overestimation of gains from reallocation. Thus, in the absence of inter-firm trade data, the conclusions on misallocation should be treated with caution.", "title": "Misallocation, productivity and fragmentation of production: the case of Latvia", "claims": null}, {"metadata": {"year": 2008}, "authors": ["T. Yamada", "I. Cosovic", "K. Maeda", "S. Kaiser"], "summary": "It is becoming increasingly difficult to satisfy the growing demands on spectrum with the conventional policy of fixed spectrum allocation. To overcome this problem, flexible and dynamic spectrum sharing methods that can significantly improve utilization of the spectrum have gained increasing interest recently. In this paper, resource allocation in decentralized spectrum sharing systems applying the existing policy rule referred to as \"inequality-averse policy\" is focused on. The problem related to the impact of inaccurate estimation of future traffic demands on the performance of such systems is addressed. To alleviate the problem, a novel policy termed misallocation-averse policy, which introduces adjusting factors and is resistant to such estimation errors, is proposed. The effectiveness of the proposed policy rule is verified through computer simulations.", "title": "Misallocation-Averse Policy for Decentralized Resource Allocation in Spectrum Sharing Systems", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Ling\u2010Yun He", "Xiaoling Qi"], "summary": "Developing countries face the conflict between economic development and environmental protection. Resource misallocation will not only affect the effectiveness of economic development, but also have environmental impacts. Based on two large-scale enterprise databases in China, this paper measured the level of enterprise resource allocation, and further used empirical research methods to investigate the environmental impact of enterprise resource misallocation and specific mechanisms. The results show that the low efficiency of resource allocation will harm the quality of China\u2019s environment. Further investigation, resource misallocation is accompanied by an increase in total energy input, a decrease in the labor-to-energy ratio and the capital-to-energy ratio, and a loss of energy efficiency, which in turn affects the environmental performance of enterprises. China is the largest developing country in the world, and research on China\u2019s environmental and economic issues is important. The conclusions of this paper can provide experience and suggestions for other developing countries to improve environmental quality and promote sustainable development from the perspective of resource misallocation.", "title": "Resource Misallocation and Energy-Related Pollution", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Alexander  Monge-Naranjo", "Juan M.  S\u00e1nchez", "Ra\u00fcl  Santaeul\u00e0lia-Llopis"], "summary": "Are production factors allocated efficiently across countries? To differentiate misallocation from factor intensity differences, we provide a new methodology to estimate output shares of natural resources based solely on current rent flows data. With this methodology, we construct a new dataset of estimates for the output shares of natural resources for a large panel of countries. In sharp contrast with Caselli and Feyrer (2007), we find a significant and persistent degree of misallocation of physical capital. We also find a remarkable movement toward efficiency during last 35 years, associated with the elimination of interventionist policies and driven by domestic accumulation. Interestingly, when both physical and human capital can be reallocated, capital would often flow from poor to rich countries.", "title": "Natural Resources and Global Misallocation", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Farah Bellaaj Elloumi", "M. Sellami", "Sami Bhiri"], "summary": "This paper presents an approach for avoiding resource misallocations in Business Processes (BPs). We consider a resource misallocation as an improper combination of resources in a BP and we introduce a decision tree\u2010based approach to minimize these misallocations likelihoods. The approach uses a BP's event log that tracks past misallocations along with their undesirable impacts in terms of time, cost, and quality on BP instances. These impacts are identified by mining resource allocation rules, which provide a concise summary of a BP's event log and thus are also used to generate decision trees. A system demonstrating the feasibility and efficiency of the approach through experiments is presented in this paper as well.", "title": "Avoiding resource misallocations in business processes", "claims": null}, {"metadata": {"year": 2011}, "authors": ["W. Block"], "summary": "It is a truism among economists that only if the full costs and benefits of economic activity are taken into account by the entrepreneur, will resources be allocated in a Pareto Optimal manner. Under present economic conditions, however, some manufacturers are able to impose costs, particularly pollution or disposal costs, onto third parties. As a result, the actual social costs of such activities are greater than the recorded private costs. At present, the benefits of these goods need be equal to or greater than only the lower private costs, not the higher social costs, if the enterprise is to be able to at least break even and thus continue in operation. This being the case, it follows that the value of at least some of these presently produced goods and services will fall below the total or social costs, and should not be produced at all, if wealth is to be maximized.", "title": "Resource Misallocations, Externalities, and Environmentalism: A U.S.-Canadian Analysis", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Shao Yi-han"], "summary": "Explaining TFP from the perspective of inter-sectoral resources allocation has become one of the main topics of growth and development research.Following the idea of Hsieh and Klenow(2009),this paper recalculates the extent of misallocation in China from 1998 to 2007 based on Chinese Industrial Enterprises Database.We show that the efficiency of resource allocation in Chinese industrial enterprises improved at first,but then deteriorated in this period.Firm scale had the most significant impact on misallocation.Specifically,the efficiency of resource allocation across large enterprises improved and that across SMEs deteriorated gradually,which made the overall level of distortions of resource allocation in China worse from 2005 to 2007.In addition,we also find that the loss caused by the soft environment such as financial market distortions was probably two to three times of the loss caused by the transportation facilities and other hard environment.But after 2005,for the large and medium-scale enterprises,transportation and other facilities distorting effects were roughly equal to the financial impacts.Finally,we also discuss the policy implications.", "title": "Resource Misallocations and TFP of Chinese Industrial Enterprises\u2014\u2014A Recalculation Based on Chinese Industrial Enterprises Database", "claims": null}], "query": "Misallocation of Resources", "summary_abstract": "The research question regarding the misallocation of resources is addressed through various studies that highlight its causes, effects, and potential solutions across different contexts.\n\nXuefen (2014) emphasizes that imperfect competition in factor and product markets leads to resource misallocation, which negatively impacts total factor productivity and exacerbates productivity differences across countries and industries. This misallocation is particularly relevant in the context of China's capital and labor markets, where understanding measurement approaches can inform policy implications.\n\nBenkovskis (2018) provides a case study of Latvia, revealing that resource misallocation increased prior to 2010 but declined thereafter. The study identifies output distortion as a primary source of misallocation initially, shifting to capital distortions post-financial crisis. The findings suggest that factors such as increased competition and tighter credit supply influence allocation efficiency, although caution is advised in interpreting results due to potential biases from production fragmentation.\n\nHe and Qi (2021) explore the environmental ramifications of resource misallocation in China, demonstrating that inefficient resource allocation not only hampers economic development but also deteriorates environmental quality. Their empirical analysis indicates that misallocation leads to increased energy input and reduced energy efficiency, highlighting the interconnectedness of economic and environmental outcomes.\n\nMonge-Naranjo et al. (2015) investigate the efficiency of production factor allocation across countries, finding significant misallocation of physical capital despite improvements over the past 35 years. Their methodology distinguishes between misallocation and differences in factor intensity, suggesting that capital often flows from poorer to richer countries, which may exacerbate disparities.\n\nIn a more technical approach, Elloumi et al. (2018) propose a decision tree-based method to minimize resource misallocations in business processes, demonstrating the practical application of data mining to improve resource allocation efficiency.\n\nBlock (2011) discusses the broader economic implications of misallocation, particularly in the context of externalities like pollution. He argues that when social costs exceed private costs, it leads to inefficient production decisions that do not maximize societal wealth.\n\nLastly, Yi-han (2013) analyzes the efficiency of resource allocation in Chinese industrial enterprises, revealing a deterioration in allocation efficiency over time, particularly among small and medium-sized enterprises. The study underscores the significant impact of firm scale and the role of financial market distortions in exacerbating misallocation.\n\nCollectively, these studies illustrate that resource misallocation is a multifaceted issue influenced by market conditions, competition, and externalities, with significant implications for both economic performance and environmental sustainability.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2004}, "authors": ["W. Shu"], "summary": "Sulforaphane is an isothiocyanate isolated from cruciferous vegetable broccoli as the major phase \u2161 enzyme inducer. It has shown chemopreventive activities on DNA damage and cancers, leading to cancer cells apoptosis, cell growth arrest, inhibition of inflammation and modulation the metabolism of carcinogens. So it was considered as a potent cancer therapeutic drug. The mechanisms of SFN were presented in this paper.", "title": "Study on the relationship between sulforaphane and cancers", "claims": null}, {"metadata": {"year": 2018}, "authors": ["A. Alyoussef", "M. Taha"], "summary": "Although there are many treatment options for skin cancer, the chemotherapeutic agents for skin cancer are linked with many adverse effects as well as the development of multidrug resistance. Sulforaphane is an isothiocyanate, which is found in cruciferous vegetables. Consumption of sulforaphane\u2010rich diet has been linked to inhibition of UV\u2010exposed skin carcinogenesis. Therefore, the goal of this study was to determine the ability of sulforaphane to reduce skin cancer in mice through inhibition of sulfatase\u20102 enzyme. Epicutaneous application of 7,12\u2010dimethylbenz (a) anthracene was performed on the shaved dorsal skin of mice followed by croton oil. Sulforaphane (9 \u03bcmol/mouse/day) was administered to mice orally. Skin was removed from the dorsal area for assessment of sulfatase\u20102, glypican\u20103, heparan sulphate proteoglycans (HSPGs), nuclear factor (NF)\u03baB, nuclear factor E2\u2010related factor 2 (Nrf2), tumor necrosis factor (TNF)\u2010\u03b1, IL\u20101\u03b2 and caspase\u20103. In addition, skin sections were stained with haematoxylin/eosin, Mallory and cytokeratin immunostaining. We found that, sulforaphane blocked sulfatase\u20102 activity, leading to significant elevation in HSPGs as well as significant reduction in glypican\u20103. In addition, sulforaphane significantly activated Nrf2 and reduced both the gene and protein expression of NF\u03baB, TNF\u2010\u03b1, IL\u20101\u03b2 and caspase\u20103. In parallel, stained sections obtained from skin cancer mice treated with sulforaphane showed significant reduction in hyperkeratosis, acanthosis and epithelial dysplasia. The collective results indicate that sulforaphane suppresses skin cancer via blocking sulfatase\u20102 with subsequent elevation in HSPGs and reduction in glypican\u20103. Moreover, sulforaphane attenuated skin cancer\u2010induced activation of inflammatory and apoptotic pathways.", "title": "Antitumor activity of sulforaphane in mice model of skin cancer via blocking sulfatase\u20102", "claims": null}, {"metadata": {"year": 2018}, "authors": ["M. Russo", "C. Spagnuolo", "G. Russo", "K. Skalicka\u2010Wo\u017aniak", "M. Daglia", "E. Sobarzo-S\u00e1nchez", "S. Nabavi", "S. Nabavi"], "summary": "ABSTRACT In the past decades, extensive studies have reported the potential chemopreventive activity of sulforaphane, an isothiocyanate derived from glucoraphanin, occurring in large amounts in Brassica genus plants. Sulforaphane was found to be active against several forms of cancer. A growing body of data shows that sulforaphane acts against cancer at different levels, from development to progression, through pleiotropic effects. In this review, we discuss the available experimental and clinical data on the potential therapeutic role of sulforaphane against cancer. Its effects range from the protection of cells from DNA damage to the modulation of the cell cycle via pro-apoptotic, anti-angiogenesis and anti-metastasis activities. At molecular level, sulforaphane modulates cellular homeostasis via the activation of the transcription factor Nrf2. Although data from clinical studies are limited, sulforaphane remains a good candidate in the adjuvant therapy based on natural molecules against several types of cancer.", "title": "Nrf2 targeting by sulforaphane: A potential therapy for cancer treatment", "claims": null}, {"metadata": {"year": 2004}, "authors": ["Nicole  Johnston"], "summary": "Sulforaphane is a chemical found in cruciferous vegetables such as broccoli, sprouts and kale. This compound is a potent inducer of protective enzymes that provide defense against cancer-causing chemicals. Researchers have discovered that sulforaphane can halt human breast cancer cells in their tracks and have identified a new mechanism of action for the compound.", "title": "Sulforaphane halts breast cancer cell growth.", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Hao-feng Gu", "X. Mao", "M. Du"], "summary": "Abstract Cancer is one of the most devastating diseases, and recently, a variety of natural compounds with preventive effects on cancer developments have been reported. Sulforaphane (SFN) is a potent anti-cancer isothiocyanate originating from Brassica oleracea (broccoli). SFN, mainly metabolized via mercapturic acid pathway, has high bioavailability and absorption. The present reviews mainly discussed the metabolism and absorption of SFN and newly discovered mechanistic understanding recent years for SFN\u2019s anti-cancer effects including promoting autophagy, inducing epigenetic modifications, suppressing glycolysis and fat metabolism. Moreover, its inhibitory effects on cancer stem cells and synergetic effects with other anti-cancer agents are also reviewed along with the clinical trials in this realm.", "title": "Metabolism, absorption, and anti-cancer effects of sulforaphane: an update", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Iahtisham\u2010Ul\u2010Haq", "Sipper Khan", "K. A. Awan", "M. Iqbal"], "summary": "Sulforaphane belongs to the active class of isothiocyanates capable of delivering various biological benefits for health promotion and disease prevention. This compound is considered vital to curtail numerous metabolic disorders. Various studies have proven its beneficial effects against cancer prevention and its possible utilization as a therapeutic agent in cancer treatment. Understanding the mechanistic pathways and possible interactions at cellular and subcellular levels is key to design and develop cancer therapeutics for humans. In this respect, a number of mechanisms such as modulation of carcinogen metabolism & phase II enzymatic activities, cell cycle arrest, activation of Nrf2, cytotoxic, proapoptotic and apoptotic pathways have been reported to be involved in cancer prevention. This article provides sufficient information by critical analysis to understand the mechanisms involved in cancer prevention attributed to sulforaphane. Furthermore, various clinical studies have also been included for design and development of novel therapies for cancer prevention and cure. PRACTICAL APPLICATIONS: Diet and dietary components are potential tools to address various lifestyle-related disorders. Due to plenty of environmental and cellular toxicants, the chances of cancer prevalence are quite large which are worsen by adopting unhealthy lifestyles. Cancer can be treated with various therapies but those are acquiring side effects causing the patients to suffer the treatment regime. Nutraceuticals and functional foods provide safer options to prevent or delay the onset of cancer. In this regard, sulforaphane is a pivotal compound to be targeted as a potential agent for cancer treatment both in preventive and therapeutic regimes. This article provides sufficient evidence via discussing the underlying mechanisms of positive effects of sulforaphane to further the research for developing anticancer drugs that will help assuage this lethal morbidity.", "title": "Sulforaphane as a potential remedy against cancer: Comprehensive mechanistic review.", "claims": null}, {"metadata": {"year": 2005}, "authors": ["\ubc15\uc131\uc601", "\ubc30\uc1a1\uc790", "\ucd5c\uc601\ud604"], "summary": "Sulforaphane, an isothiocyanate derived from hydrolysis of glucoraphanin in broccoli and other cruciferous vegetables, was shown to induce phase \u2161 detoxification enzymes and inhibit chemically induced mammary tumors in rodents. Recently, sulforaphane is known to induce cell cycle arrest and apoptosis in human cancer cells, however its molecular mechanisms are poorly understood. In the present study, we demonstrated that sulforaphane acted to inhibit proliferation and induce morphological changes of human cervical carcinoma HeLa cells. Treatment of HeLa cells with 10 \u03bcM or 15 \u03bcM sulforaphane resulted in significant G2/M cell cycle arrest as determined by flow cytometry. Moreover, 20 \u03bcM sulforaphane significantly induced the population of sub-G1 cells (9.83 fold of control). This anti-proliferative effect of sulforaphane was accompanied by a marked inhibition of cyclin A and cyclin-dependent kinase (Cdk)4 protein and concomitant induction of Cdc2, Cdk inhibitor p16 and p21. However, sulforaphane did not affect the levels of cyclooxygenases and telomere-regulatory gene products. Although further studies are needed, the present work suggests that sulforaphane may be a potential chemoprevetive/chemotherapeutic agent for the treatment of human cancer cells.", "title": "Sulforaphane\uc5d0 \uc758\ud55c HeLa \uc778\uccb4\uc790\uad81\uacbd\ubd80\ud568\uc138\ud3ec\uc758 \uc99d\uc2dd \uc5b5\uc81c \uae30\uc804 \uc5f0\uad6c", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Hyeon-Gun Jee", "Kyunglim Lee", "Jong Bin Kim", "Hyo-Keun Shin", "Y. Youn"], "summary": "Sulforaphane is a predominant isothiocyanate in Brassica oleracea, a family of cruciferous vegetables, and is known to be inversely related to the risk of various types of human carcinomas. Studies using oral carcinoma cell lines are scarce, however, and the role of sulforaphane on oral carcinoma cell metastasis is yet to be determined. In this study, the growth inhibition of oral carcinoma cell lines by sulforaphane was determined using aqueous soluble tetrazolium salts, and the growth of various oral cancer cell lines was attenuated. The migration and invasion activities of the cells also decreased, as observed in monolayer scratch assays and transwell invasion experiments. The molecular change behind the impairment of the migration and invasion was investigated via secreted metalloprotease level detection using Multiplex protein analysis kits. At the molecular level, the secreted forms of MMP\u20101 and MMP\u20102 were down\u2010regulated. The expressions of MMP\u20101 and MMP\u20102 did not change when a conventional tumoricidal agent paclitaxel was used. These findings indicate that sulforaphane may have therapeutic potential as an inhibitor of metastasis in oral carcinoma patients. Copyright \u00a9 2011 John Wiley & Sons, Ltd.", "title": "Sulforaphane Inhibits Oral Carcinoma Cell Migration and Invasion In Vitro", "claims": null}], "query": "sulforaphane cancer", "summary_abstract": "The research on sulforaphane, a compound derived from cruciferous vegetables, highlights its significant potential as a chemopreventive and therapeutic agent against various types of cancer. Multiple studies have demonstrated that sulforaphane exerts its anti-cancer effects through several mechanisms, including the induction of apoptosis, cell cycle arrest, and modulation of inflammatory pathways.\n\nShu (2004) emphasizes sulforaphane's role as a potent inducer of phase II detoxification enzymes, which helps protect against DNA damage and promotes cancer cell apoptosis. This is further supported by Alyoussef and Taha (2018), who found that sulforaphane effectively inhibited skin cancer development in mice by blocking the sulfatase-2 enzyme, leading to reduced inflammation and apoptosis in cancerous cells.\n\nRusso et al. (2018) provide a broader overview, noting that sulforaphane's chemopreventive activity spans various cancer types, acting at multiple levels from initiation to progression. They highlight its ability to protect cells from DNA damage and its role in modulating the cell cycle, which is crucial for preventing cancer development.\n\nIn addition, Gu et al. (2021) discuss sulforaphane's mechanisms, including promoting autophagy and inhibiting cancer stem cells, which are critical for tumor growth and metastasis. This aligns with findings from Iahtisham-Ul-Haq et al. (2021), who detail sulforaphane's ability to modulate carcinogen metabolism and activate protective pathways, reinforcing its potential as a therapeutic agent.\n\nMoreover, studies on specific cancer types, such as those by Hyeon-Gun Jee et al. (2011), indicate that sulforaphane can inhibit the growth and metastasis of oral carcinoma cells, further underscoring its therapeutic promise.\n\nOverall, the collective evidence suggests that sulforaphane is a promising candidate for cancer prevention and treatment, with mechanisms that include the induction of protective enzymes, modulation of inflammatory responses, and direct effects on cancer cell proliferation and metastasis (Shu, 2004; Alyoussef & Taha, 2018; Russo et al., 2018; Gu et al., 2021; Iahtisham-Ul-Haq et al., 2021; Jee et al., 2011).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2020}, "authors": ["Hu  Peng", "Yinggan  Zhang", "Yanli  Chen", "Jie  Zhang", "Heng  Jiang", "Xin  Chen", "Zhigang  Zhang", "Yibo  Zeng", "Baisheng  Sa", "Qiulong  Wei", "Jie  Lin", "Hang  Guo"], "summary": "Abstract Polarization of lithium-sulfur (Li-S) batteries consumes energy density upon cycling, which precludes their practical application in state-of-the-art devices. Herein a conformal zinc sulfide/reduced graphene oxide/elemental sulfur (ZnS/rGO/S) composite is synthesized and used as cathode materials in Li-S batteries. The synergetic regulation of ohmic, concentration, and electrochemical polarization decreases the overpotential and improves the cycling performance of the Li||ZnS/rGO/S cell. Combining theoretical calculations and experimental results, it is revealed that the ZnS spheres can absorb lithium polysulfide (LPS) intermediates, catalyze LPS conversion, and provide fast electronic/ionic diffusion kinetics. The ZnS/rGO/S cathode is further applied in pouch Li-S cells, demonstrating the practicability of sulfur cathodes with reduced polarization.", "title": "Reducing polarization of lithium-sulfur batteries via ZnS/reduced graphene oxide accelerated lithium polysulfide conversion", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Changsheng  Qiu", "Gang  He", "Wankai  Shi", "Mengjie  Zou", "Chang  Liu"], "summary": "A high-fidelity electrochemical-thermal coupling was established to study the polarization characteristics of power lithium-ion battery under cycle charge and discharge. The lithium manganese oxide lithium-ion battery was selected to study under cyclic conditions including polarization voltage characteristics, and the polarization internal resistance characteristics of the power lithium-ion battery under cyclic conditions were analyzed via the Hybrid Pulse Power Test (HPPC). The results show that for different working conditions, the polarization voltage difference of the power lithium-ion battery is mainly affected by the change in polarization internal resistance. A higher charge-discharge rate, lower ambient temperatures, and more cycles lead to a greater polarization internal resistance of the battery. Meanwhile, the ohmic polarization internal resistance and the concentration polarization internal resistance of the power lithium-ion battery both exhibit different characteristics under different working condition. In addition, the internal resistance of the ohmic polarization is not affected by the change in current, but it is significantly affected by the change in ambient temperature. Therefore, the ohmic polarization voltage reacts rapidly with changes in the charge/discharge state of the battery; the change of the internal resistance of the concentration polarization is relatively slow.", "title": "The polarization characteristics of lithium-ion batteries under cyclic charge and discharge", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Zi-Xian Chen", "Qian Cheng", "Xi\u2010Yao Li", "Zheng Li", "Yun\u2010Wei Song", "Furong Sun", "Meng-Qiang Zhao", "Xue\u2010Qiang Zhang", "Bo\u2010Quan Li", "Jia\u2010Qi Huang"], "summary": "Lithium-sulfur (Li-S) batteries afford great promise on achieving practical high energy density beyond lithium-ion batteries. Lean-electrolyte conditions constitute the prerequisite for achieving high-energy-density Li-S batteries but inevitably deteriorates battery performances, especially the sulfur cathode kinetics. Herein, the polarizations of the sulfur cathode are systematically decoupled to identify the key kinetic limiting factor in lean-electrolyte Li-S batteries. Concretely, an electrochemical impedance spectroscopy combined galvanostatic intermittent titration technique method is developed to decouple the cathodic polarizations into activation, concentration, and ohmic parts. Therein, activation polarization during lithium sulfide nucleation emerges as the dominant polarization as the electrolyte-to-sulfur ratio (E/S ratio) decreases, and the sluggish interfacial charge transfer kinetics is identified as the main reason for degraded cell performances under lean-electrolyte conditions. Accordingly, a lithium bis(fluorosulfonyl)imide electrolyte is proposed to decrease activation polarization, and Li-S batteries adopting this electrolyte provide a discharge capacity of 985 mAh g-1 under a low E/S ratio of 4 \u03bcL mg-1 at 0.2 C. This work identifies the key kinetic limiting factor of lean-electrolyte Li-S batteries and provides guidance on designing rational promotion strategies to achieve advanced Li-S batteries.", "title": "Cathode Kinetics Evaluation in Lean-Electrolyte Lithium-Sulfur Batteries.", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Jiuchun Jiang", "Qiujiang Liu", "Caiping Zhang", "Weige Zhang"], "summary": "Battery polarization voltage characteristics of power Li-ion batteries are investigated at different charging rates. The polarization voltage is small and relatively stable in the middle state of charge (SOC) range, whereas at both ends of SOC, the polarization voltage is enlarged and varies rapidly. Meanwhile, there is an approximate linear relationship between polarization voltage and charging rate. Taking polarization time constant into account, an acceptable charging current curve is developed, which prevents charging current from being out of range. This curve takes polarization voltage and charge cutoff voltage as constraints to describe acceptable charging current value at different SOC. Based on this curve, the battery can be charged from 20% SOC to 80% SOC within 33 min. The average polarization and the maximum temperature rise are approximately equivalent to those of 0.5C constant current-constant voltage charging method. The experimental results demonstrate that the proposed charging mode can balance charging speed, charging polarization, and temperature rise.", "title": "Evaluation of Acceptable Charging Current of Power Li-Ion Batteries Based on Polarization Characteristics", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Fan  Yang", "Yanlong  Qiao", "Bing  Gao", "Pan  Duan", "Yan  Zhu"], "summary": "An electrochemical-thermal Li-ion battery model considering electrochemical reactions and heat transfer was established. Firstly, polarization characteristics were analyzed in principle and polarization voltage was calculated. Then, cell voltage V and polarization voltage Vp versus SOC (state of charge) at different CC (constant-current) charging rates were discussed. Thirdly, average polarization PA and cutoff SOC SOCc were proposed to quantize the polarization level and its influence on charging under different charging conditions. Finally, influence factors on battery polarization including electrolyte salt concentration, electrolyte conductivity, active material particle radius, positive electrode conductivity, and ambient temperature were studied. In addition, influence of the factors on SOCc and heat generation was investigated. The results show that polarization voltage has direct effect on cell voltage and is negatively related with charging performance and positively related with heat generation. Higher electrolyte salt concentration, positive electrode conductivity, lower electrode particle radius, and ambient temperature make for lower polarization voltage.", "title": "Investigation on Li-ion battery charging polarization characteristics and influence factors", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Hsin\u2010Yi Wang", "Jun-Ming Chen", "Yu-Run Lin", "B. Hwang", "Fu-Ming Wang"], "summary": "s The common charging protocol for Li-ion battery still utilizes traditional constant current followed by a constant voltage (CC-CV) charging. Without changing the materials of the Li-ion battery, finding a fast and practical charging method for current battery with enhanced cycle life is an important issue. In our work, the degree of voltage polarization in different SOC stages was employed as mediums to modify the charging protocol, and the commercial (Sam-Sung LGABB41865) battery with theoretical capacity 2600mAh was studied as a witness cell. To satisfy the requirement that demands the charging time should be restricted within 1.5 hours for commercial application, 0.7C is used as a standard current rate to inspect the voltage polarization during charging. Galvanostatic intermittent titration technique (GITT) revealed the voltage drop within the duration after charging per 2% SOC followed a 30 minutes rest as shown in figure 1(a), and the unshown final 8%SOC was simply induced CC-CV charging protocol for completely charging the battery without GITT test. The average polarization voltage (blue line) can guide us to assign different charging stages with ameliorated charging currents: low current rate was employed to erase the severe polarization regions and lenient polarization regions can be charged with high current rate to reduce the charging time. A smoothed voltage polarization extent is shown in Figure 1(b), and the average voltage drop still remains at the same level. We believe that a moderated polarization extent can mollify the aging effect of electrode materials and further improve the cycle life. Figure 1 The degree of polarization (Voltage drop) revealed by GITT of the Li-ion battery charged by (a) constant 0.7C current and (b) multistage constant current Reference (1) Bergveld, H. J.; Danilov, D.; Notten, P. H. L.; Pop, V.; Regtien, P. P. L. BATTERIES | Adaptive State-of-Charge Determination. In Encyclopedia of Electrochemical Power Sources; x00Fc, rgen, G., Eds.; Elsevier: Amsterdam, 2009; pp 459.", "title": "Multistage constant current for fast-charging Li-ion battery with enhanced long cycle life", "claims": null}, {"metadata": {"year": 1990}, "authors": ["M. Morita", "Takeshi Fukumasa", "Makoto Motoda", "H. Tsutsumi", "Y. Matsuda", "Tooru Takahashi", "H. Ashitaka"], "summary": "A complex of poly(ethylene oxide)-grafted poly(methyl)-methacrylate (PEO-PMMA) with a lithium salt has been examined as a solid electrolyte of an ambient-temperature rechargeable lithium battery. The coulombic efficiency of lithium during the charge-discharge cycle depended on the cycling current density. The average efficiency reached 88% at 50 \u03bcA cm \u22122 cycling when an Al plate was used as a substrate", "title": "POLARIZATION BEHAVIOR OF LITHIUM ELECTRODE IN SOLID ELECTROLYTES CONSISTING OF A POLY(ETHYLENE OXIDE)-GRAFTED POLYMER", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Ling Wenli", "S. Haitao", "L. Xinjun", "Zhao Changjiang", "Fu Linchun"], "summary": "LEO Satellites equipped with high-power microwave payloads bring special demands for fast charging of Lithium ion battery. However, charging polarization occurs in traditional constant current-constant voltage strategy for Li-ion battery, following by comparative long charging time. To realize fast charging of batteries, novel Li-ion battery fast charge topology and principle were present based on Mas Laws, which adopted multi-step constant current charging and fixed limit discharging in one charging cycle. The strategy was provided with many advances, such as de-polarization for charging voltage, raising acceptable charging current and no-intervening battery cycle life. Model of S3R (Sequential Switching Shunt Regulator) unregulated primary EPS(electrical power system) was built in MATLAB/SIMULINK, in which simulations and analysis were processed. Results shown using above fast charging strategy could save over 56% charging time, as well as avoiding charging polarization. New in-orbit charging ideas for future space Li-ion battery could be obtained from the paper.", "title": "Multi-step constant current fast charging topology and principle simulation for LEO satellite", "claims": null}], "query": "polarization in the charge cycle of Li-S battery", "summary_abstract": "The research on polarization in the charge cycle of lithium-sulfur (Li-S) batteries reveals several critical insights into the factors affecting battery performance and strategies to mitigate polarization effects. \n\nHu et al. (2020) highlight that polarization in Li-S batteries significantly impacts energy density during cycling, which is a barrier to their practical application. They propose a composite cathode material (ZnS/rGO/S) that effectively reduces polarization by enhancing lithium polysulfide (LPS) conversion and improving ionic and electronic diffusion kinetics. This approach demonstrates a reduction in overpotential and improved cycling performance, indicating a successful strategy to address polarization issues.\n\nIn a more recent study, Chen et al. (2023) systematically decouple the polarization characteristics of sulfur cathodes under lean-electrolyte conditions, identifying activation polarization during lithium sulfide nucleation as the primary limiting factor. They suggest that the sluggish interfacial charge transfer kinetics under these conditions lead to degraded performance. Their work emphasizes the importance of electrolyte composition, proposing a lithium bis(fluorosulfonyl)imide electrolyte to decrease activation polarization, which significantly enhances discharge capacity.\n\nThese findings collectively underscore the multifaceted nature of polarization in Li-S batteries, where both material composition and electrolyte conditions play pivotal roles. The studies suggest that innovative materials and optimized electrolyte formulations are essential for reducing polarization and improving the overall efficiency and practicality of Li-S batteries in real-world applications.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2009}, "authors": ["Gary C. McDonald"], "summary": "Ridge regression is a popular parameter estimation method used to address the collinearity problem frequently arising in multiple linear regression. The formulation of the ridge methodology is reviewed and properties of the ridge estimates capsulated. In particular, four rationales leading to a regression estimator of the ridge form are summarized. Algebraic properties of the ridge regression coefficients are given, which elucidate the behavior of a ridge trace for small values of the ridge parameter (i.e., close to the least squares solution) and for large values of the ridge parameter. Further properties involving coefficient sign changes and rates\u2010of\u2010change, as functions of the ridge parameter, are given for specific correlation structures among the independent variables. These results help relate the visual behavior of a ridge trace to the underlying structure of the data. Copyright \u00a9 2009 John Wiley & Sons, Inc.", "title": "Ridge regression", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Dan Li", "Qifei Ge", "Pengcheng Zhang", "Y. Xing", "Zan Yang", "Wei Nai"], "summary": "Ridge regression is a method for analyzing multicollinear regression data. In reality, multicollinearity often exists among data. Linear regression based on the principle of ordinary least square method has too large variance due to the unbiasedness of least squares estimation, which leads to it being far from the true value. In essence, ridge regression algorithm is an improved least square estimation method. By giving up the unbiasedness of least square method, the result is more practical, and the problem is indirectly transformed into an optimal parameter problem that minimizes the error function. In the field of machine learning, gradient method is usually used to solve the problem, but it is easy to fall into the local minimum trap. In this paper, a ridge regression based on high order truncated gradient descent method is proposed, and the high order truncated gradient descent method is used to replace gradient descent method.", "title": "Ridge Regression with High Order Truncated Gradient Descent Method", "claims": null}, {"metadata": {"year": 1997}, "authors": ["L. Firinguetti"], "summary": "This paper discusses the use of Ridge Regression in the context of a system of Seemingly Unrelatged Regression Equations, when the explanatory variables are affected by multi-collineaarity. The properties of Ridge Regression estimators are derived assuming the shrinkage parameters are nonstochastic. The optimal values of the parameters are found to be dependent on the unknown parameters of the model. Some operational Ridge Regression estimators are proposed and then compared, in a Monte Carlo study, with an operational Generalized Least Squares estimator. It is found that the Ridge Regression estimators can outperform the Operational Generalized Least Squares estimator, when the data are multicollinear and the signal to noise ratio of the equations is not too large.", "title": "Ridge regression in the context of a system of seemingly unrelated regression equations", "claims": null}, {"metadata": {"year": 2011}, "authors": ["C. Goga", "M. Shehzad"], "summary": "The ridge regression is a biased estimation method used to circumvent the instability in the regression estimators obtained by ordinary least squares method in the presence of multicollinearity. This method has been used in survey sampling in order to cope with negative or extremely large weights resulted when a very large number of calibration or balancing constraints was imposed. In this paper, we give a review and some new interpretations of the ridge-type estimators in a survey sampling framework.", "title": "Overview of ridge regression estimators in survey sampling", "claims": null}, {"metadata": {"year": 2013}, "authors": ["G. Khalaf", "K. M\u00e5nsson", "G. Shukur"], "summary": "Ridge regression is a variant of ordinary multiple linear regression whose goal is to circumvent the problem of predictors collinearity. It gives up the Ordinary Least Squares (OLS) estimator as a method for estimating the parameters of the multiple linear regression model . Different methods of specifying the ridge parameter k were proposed and evaluated in terms of Mean Square Error (MSE) by simulation techniques. Comparison is made with other ridge-type estimators evaluated elsewhere. The new estimators of the ridge parameters are shown to have very good MSE properties compared with the other estimators of the ridge parameter and the OLS estimator. Based on our results from the simulation study, we may recommend the new ridge parameters to practitioners.", "title": "Modified Ridge Regression Estimators", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Seung Hoe Choi", "Hye-Young  Jung", "Hyoshin  Kim"], "summary": "Ridge regression model is a widely used model with many successful applications, especially in managing correlated covariates in a multiple regression model. Multicollinearity represents a serious threat in fuzzy regression models as well. We address this issue by combining ridge regression with the fuzzy regression model. Our proposed algorithm uses the \\(\\alpha\\)-level estimation method to evaluate the parameters of the ridge fuzzy regression model. Two examples are given to illustrate the ridge fuzzy regression model with crisp input/fuzzy output and fuzzy coefficients.", "title": "Ridge Fuzzy Regression Model", "claims": null}, {"metadata": {"year": 1970}, "authors": ["A. E. Hoerl", "R. Kennard"], "summary": "This paper is an exposition of the use of ridge regression methods. Two examples from the literature are used as a base. Attention is focused on the RIDGE TRACE which is a two-dimensional graphical procedure for portraying the complex relationships in multifactor data. Recommendations are made for obtaining a better regression equation than that given by ordinary least squares estimation.", "title": "Ridge Regression: Applications to Nonorthogonal Problems", "claims": null}, {"metadata": {"year": 2010}, "authors": ["Gary C. McDonald"], "summary": "Ridge regression is a parameter estimation method used to address the collinearity problem frequently arising in multiple linear regressions. The methodology defines a class of estimators indexed by a non\u2010negative scalar parameter, k. When utilizing ridge regression, the analyst eventually chooses a particular value of k which, in turn, uniquely determines the regression parameter estimates within the ridge class. Many such methods of determination, both deterministic and stochastic, have been proposed and evaluated in the literature. A plot of the ridge regression estimates as a function of k, the so\u2010called ridge trace, is often used in the selection process. In this article, one such criterion, the square of the correlation coefficient of the actual dependent variable and its predicted values, is examined. It can be used, along with a ridge trace and other quantitative measures, as another important tradeoff between using ordinary least squares estimates and estimates from the ridge class. WIREs Comp Stat 2010 2 695\u2013703 DOI: 10.1002/wics.126", "title": "Tracing ridge regression coefficients", "claims": null}], "query": "Ridge Regression", "summary_abstract": "Ridge regression is a robust statistical technique designed to address the challenges posed by multicollinearity in multiple linear regression models. The method modifies the ordinary least squares (OLS) estimation by introducing a penalty term, which helps stabilize the estimates when predictors are highly correlated.\n\nMcDonald (2009) provides a comprehensive overview of ridge regression, detailing its formulation and the properties of ridge estimates. He emphasizes the significance of the ridge parameter, which influences the behavior of the regression coefficients, particularly in relation to the ridge trace\u2014a graphical representation that illustrates how estimates change with varying values of the ridge parameter. This visual tool aids analysts in selecting an appropriate ridge parameter to optimize their model.\n\nLi et al. (2020) further elaborate on the practical applications of ridge regression, highlighting its effectiveness in reducing variance in the presence of multicollinearity. They propose an innovative approach using a high-order truncated gradient descent method to enhance the ridge regression algorithm, thereby improving its performance in machine learning contexts.\n\nFiringuetti (1997) explores the application of ridge regression within systems of seemingly unrelated regression equations, demonstrating that ridge estimators can outperform traditional generalized least squares estimators under certain conditions, particularly when dealing with multicollinear data.\n\nKhalaf et al. (2013) contribute to the discussion by evaluating various methods for specifying the ridge parameter, showing that their proposed estimators exhibit superior mean square error properties compared to both traditional ridge estimators and OLS. This finding underscores the importance of selecting an optimal ridge parameter to enhance model accuracy.\n\nChoi et al. (2019) extend the application of ridge regression to fuzzy regression models, addressing the multicollinearity issue in this context. Their work illustrates the versatility of ridge regression across different modeling frameworks.\n\nOverall, the literature consistently supports the efficacy of ridge regression as a solution to multicollinearity, with various studies proposing enhancements and applications that broaden its utility in statistical analysis and machine learning (McDonald, 2009; Li et al., 2020; Firinguetti, 1997; Khalaf et al., 2013; Choi et al., 2019).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2004}, "authors": ["Dao Trong Thi", "Le Danh Ton"], "summary": "In last several years, Vietnam\u2019s economy has reached significant achievements. Those are clearly seen by analyzing economic growth, economic structure transformation, trade and investment and Vietnam\u2019s economy competitiveness. However, in the process of development, Vietnam\u2019s economy is still facing many difficulties and challenges. Vietnam is accelerating speed of innovation process, is active and quicker in international economic integration in order to complete the economic \u2013 social objectives in the period of 2001\u20132005 with annual average economic growth rate of 7.5%.", "title": "Vietnam\u2019s economy: Achievements and perspective", "claims": null}, {"metadata": {"year": 2018}, "authors": ["N. Luan"], "summary": "2017 is the first time after many years that Vietnam met and exceeded 13 social-economic indicators. Vietnam\u2019s economy experienced a high economic growth rate, stable and sustainable macroeconomic environment. GDP growth is 6.81% (target 6.7%), CPI 3.53% (target (4%), credit growth 18,1%, and FDI of 36 billion USD. Import-export turnovers remain a significant achievement and has been maintained at a high level. Early results from restructuring of state-own enterprises signal a successful restructuring. These positive indicators provide impetus for the Vietnam\u2019s economy in 2018. The targets in 2018 include maintaining a stable macro-economic environment, achieving a 6.7% GDP growth, focusing on growth quality and sustainability, 4% CPI, 3.7% state budget deficit, 63.9% public debt, 8-10% increase in export turnover against 2017, less than 3% trade deficit and a ratio of government investment to GDP at 34%. To successfully achieve 2018 economic indicators, it is necessary to strongly apply policies and solutions for a creative and innovative system, develop science and technology nationwide and in every industry. This is to create breakthrough for the changes of the structure and growth model. \n\u00a0", "title": "Vietnam\u2019s economy in 2017 - 2018: Growth on an efficient and sustainable foundation", "claims": null}, {"metadata": {"year": 2022}, "authors": ["H\u00f9ng V\u0103n Tr\u1ea7n"], "summary": "The article is based on secondary data sources collected from the World Bank and Vietnamese authorities and approaches from the perspective of qualitative research through descriptive and comparative statistical analysis to assess the status of Vietnam's economic growth over the period 2010 - 2020 based on Indicators measuring economic growth rate including real GDP growth rate, GDP per capita growth rate and average real GDP growth rate. Research results show that Vietnam achieves and maintains a relatively high economic growth rate in the period 2010-2020, especially in 2020 when the economy faces many difficulties and challenges posed from the COVID-19 pandemic situation, but Vietnam's economy still maintains a positive growth rate as well as a positive growth of GDP per capita. Despite achieving remarkable economic growth during this period, when comparing real GDP and GDP per capita with other countries in the ASEAN region, these indicators of Vietnam are still quite low and there is a significant difference compared to other countries in the region. At the same time, the article also outlines several reasons for the comparatively low economic growth of Vietnam. On that basis, the article proposes some recommendations to contribute to the growth of Vietnam's economy in the future.", "title": "Vietnam's economic growth in 2010 \u2013 2020", "claims": null}, {"metadata": {"year": 2015}, "authors": ["V. Khuong"], "summary": "This paper conducts a comparative analysis of economic reforms in Vietnam and China to gain insights into critical reform areas that are imperative for Vietnam to achieve more robust economic growth. This study finds that the growth gap between the two countries has been substantial not only for GDP but also for labour productivity and total factor productivity, both of which are essential for sustainable economic performance in the long term. The main causes behind the growth disparity between Vietnam and China are governance-related factors, namely government effectiveness, regulatory quality, administrative reforms, state-owned enterprise reforms and policy experimentation efforts, along with induced elements such as education and technological upgrading. While China\u2019s stronger performance does not imply that Vietnam should embrace the former\u2019s growth model, the considerable performance gap between the two countries suggests that Vietnam has great potential to accelerate its own economic growth. The insights and discussion from this study indicate that Vietnam can achieve far more robust growth if it launches a new wave of reforms to effectively strengthen the aforementioned factors. This paper provides a suite of relevant and actionable policy recommendations for Vietnam to embark on this endeavour.", "title": "Can Vietnam Achieve More Robust Economic Growth?: Insights from a Comparative Analysis of Economic Reforms in Vietnam and China", "claims": null}, {"metadata": {"year": 2006}, "authors": ["D. V. An"], "summary": "In 2005, Vietnams GDP growth rate attained 8.4%, far exceeding the 2004 figure of 7.8%. This is the highest growth rate that Vietnam has experienced since 1997. Vietnam also scored the second highest growth rate (preceded by China) compared to other East Asian countries. High economic growth rate in 2005 enabled Vietnam to fulfill its targeted average growth rate of 7.5% per annum set in the 2001-2005 Five-year Plan.", "title": "Vietnam Economy 2001-2005 and Socio-Economic Development Plan 2006-2010", "claims": null}, {"metadata": {"year": 2004}, "authors": ["T. Larsen", "V. Suri", "V. Dinh"], "summary": "Vietnam's economy grew by 7.2 percent in 2003 despite challenges from Severe Acute Respiratory Syndrome (SARS) and a lackluster global economy. Export growth, in value terms, accelerated to reach 21 percent in 2003. In the first five months of 2004 exports have surpassed expectations by growing at over 17 percent compared with the same period last year. Industrial production remains strong and investment solid. In the first quarter of 2004, gross domestic product (GDP) growth was 7 percent, and the government's growth target for 2004 is around 8 percent. Vietnam continues the implementation of a comprehensive reform agenda. Its main goals are to complete the transition to a market economy, to keep development inclusive and sustainable, and to build modern governance. While there has been progress on all three fronts, there has also been substantial variation within each of them. Considerable progress in the integration with the world economy, promising steps towards curbing policy lending, an acceleration of the State-Owned Enterprise (SOE) transformation process and the apparent resolve to adopt a more systemic approach towards fighting corruption are among the main highlights in recent months. On the other hand, limited progress in banking reform remains a matter of concern.", "title": "Taking stock : an update on recent economic developments and reforms in Vietnam", "claims": null}, {"metadata": {"year": 2011}, "authors": ["B. Herr", "Rainer Stachuletz"], "summary": "Vietnam\u2019s great success should not distract us from a number of problems that have developed over the years. The trade deficit in Vietnam accounts for about 15% of GDP. Due to considerable transfers from Vietnamese living abroad (more than 5% of GDP), current account deficits are far less but still high. Vietnam has experienced a surge in foreign direct investment flows since the country joined the WTO in 2007. Annual net foreign direct investment4 has recently reached 6.8 billion USD, almost one quarter As the result of the \u201cDoi-Moi\u201d reform policy, Vietnam has experienced a remarkable phase of growth since 1986. In this period the Gross Domestic Product (GDP) quadrupled. Since the population of Vietnam (about 88.5 million people in 2010) is currently growing by more than one million people annually, the increase in per capita income lags behind GDP growth, but still reached 2900 U.S. dollars in 2009, which ranks 1652 in the world (see Table 1). In order to prevent its per capita income from falling off because of this huge increase in population growth, Vietnam needs a real GDP growth of over 5%. Although the poverty rate was decreased from 58% to 13% between 2003 and 2008, considerable regional disparities especially between rural and urban areas continue to persist3. vIETNAm AT ThE CROSSROAdS \u2013 AN ANALYSIS ANd dRAFT FOR REFORm OF ThE vIETNAmESE ECONOmY By Hansj\u00f6rg Herr and Rainer Stachuletz1", "title": "vIETNAm AT ThE CROSSROAdS \u2013 AN ANALYSIS ANd dRAFT FOR REFORm OF ThE vIETNAmESE ECONOmY", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Giovanna Dore"], "summary": "Between 1990 and 2005, Vietnam achieved an average annual growth rate of 7.5 percent in gross domestic product (GDP). The industrial sector was the most significant driver of this economic expansion with an annual growth of rate of more than 10 percent, at a time when agriculture, forestry, and fisheries grew at an annual average rate of less than 4 percent. Such structural shift is in line with the National Socioeconomic Development Plan 2006-2010, which calls for an annual growth of 10 percent for the industrial sector (with a particular focus on the manufacturing sector, where a sustained annual growth of 15 percent is expected) and with international good practice in industrialization and modernization policies. This report was originally initiated under the concept of a country environmental analysis for Vietnam; however, during its development, and in consultations with Vietnamese counterparts, it was decided to limit the focus of the report to trends in industrial development, with particular attention to the environmental challenges of manufacturing industries, and the existing regulatory and institutional framework for environmental management including issues relating to decentralization and policy coordination. The environmental challenges deriving from the accession to World Trade Organization (WTO) are also an important focus. As such, this report is intended to be a contribution to a general countrywide environmental analysis but not constituting one of its own.", "title": "Danh gia va phan tich tac dong o nhiem do nganh cong nghiep che bien, che tao o Viet Nam", "claims": null}], "query": "Results: Analysis of Vietnam's economic growth achievements", "summary_abstract": "The analysis of Vietnam's economic growth achievements reveals a trajectory marked by significant progress, particularly in the years following the implementation of economic reforms. Dao Trong Thi and Le Danh Ton (2004) highlight that Vietnam's economy has experienced substantial growth, with an average annual growth rate of 7.5% during the 2001-2005 period, despite facing various challenges. This growth is further corroborated by D. V. An (2006), who notes that Vietnam achieved an impressive GDP growth rate of 8.4% in 2005, the highest since 1997, positioning it as one of the fastest-growing economies in East Asia.\n\nIn more recent years, N. Luan (2018) reports that Vietnam not only met but exceeded 13 socio-economic indicators in 2017, achieving a GDP growth rate of 6.81%. This period also saw a stable macroeconomic environment, with significant foreign direct investment (FDI) inflows and a focus on restructuring state-owned enterprises, which contributed to the overall economic momentum.\n\nH\u00f9ng V\u0103n Tr\u1ea7n (2022) provides a broader perspective by analyzing Vietnam's economic growth from 2010 to 2020, indicating that the country maintained a relatively high growth rate even amidst the challenges posed by the COVID-19 pandemic. However, he also points out that Vietnam's GDP and GDP per capita remain lower compared to other ASEAN nations, suggesting room for improvement.\n\nThe comparative analysis by V. Khuong (2015) emphasizes the growth disparity between Vietnam and China, attributing it to governance-related factors and suggesting that Vietnam has the potential for accelerated growth through further reforms. This sentiment is echoed by Giovanna Dore (2008), who notes that the industrial sector has been a key driver of Vietnam's economic expansion, with a focus on manufacturing as a critical area for future growth.\n\nOverall, the collective findings from these studies illustrate that Vietnam has made remarkable strides in economic growth, characterized by high GDP growth rates and structural transformation, while also highlighting the need for continued reforms and improvements in governance to sustain and enhance this growth trajectory.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2020}, "authors": ["Swarup  Mandal", "Arnab  Deb", "Dipak  Sen"], "summary": "Numerical analysis of mixed convection with surface radiation on a vertical channel is conducted. Five protruding heat sources are mounted on the left wall of the channel, and copper heat spreader is attached upon each heat source. Governing equations are solved using SIMPLER algorithm in ANSYS 16.2 software. Results are presented to depict the effects of parameters like heat spreader width (Ws = W \u2212 2W), emissivity of heat spreader (esp = 0.1\u20130.9) and Reynolds number (Re 250\u2013750) on the rate of heat transfer by fixing emissivity of heat source and substrate. It is found that with increasing spreader width and emissivity, heat transfer performance increases.", "title": "Mixed Convective Heat Transfer with Surface Radiation in a Vertical Channel in Presence of Heat Spreader", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Giovanni  Tanda"], "summary": "Abstract The effect of repeated horizontal protrusions on the free-convection heat transfer in a vertical, asymmetrically heated, channel has been experimentally investigated. The protrusions have a square section and are made of a low-thermal-conductivity material. Experiments were conducted by varying the number of the protrusions over the heated surface (whose height was held fixed) and the aspect ratio of the channel. The convective fluid was air and the wall-to-ambient air temperature difference was set equal to 45\u00a0K. The local heat transfer coefficient was obtained by means of the schlieren optical technique. The protrusions were found to significantly alter the heat transfer distribution along the heated surface of the channel, especially in the vicinity of each obstacle. For the ranges of parameters studied, the addition of low-conductivity protrusions leads to a decrease in the average heat transfer coefficient, as compared to that for the smooth surface, in the 0\u20137% range for the largest channel aspect ratio and in the 18\u201343% for the smallest channel aspect ratio.", "title": "Natural convective heat transfer in vertical channels with low-thermal-conductivity ribs", "claims": null}, {"metadata": {"year": 1990}, "authors": ["Y. Sudo", "T. Usui", "M. Kaminaga"], "summary": "The effects of gap size on heat transfer characteristics were investigated experimentally for a vertical rectangular flow channel with gaps of 2.5 mm, 6 mm and 18 mm", "title": "Heat Transfer Characteristics in Narrow Vertical Rectangular Channels Heated from Both Sides", "claims": null}, {"metadata": {"year": 2010}, "authors": ["L. Qing", "Qiu Sui", "Yepan Zhong"], "summary": "Experimental researches on natural heat transfer of air in three vertical narrow rectangular channels are conducted.These three channels are 800 mm in length,60 mm in width and 1.0 mm, 1.8 mm and 2.5 mm in gap depth,respectively.By electrically heated with stainless heating rod, the natural heat transfer characteristics are investigated with or without an adiabatic chimney at the channel outlet.It is found that the impact of chimney on natural heat transfer characteristics is slight of channel with 1.0 mm and 1.8 mm gap depth,while that of channel with 2.5 mm gap depth depresses the heat transfer.Furthermore,the heat transfer data with or without chimney can be well correlated in two different correlations,respectively.", "title": "EXPERIMENTAL RESEARCH ON NATURAL HEAT TRANSFER OF AIR IN VERTICAL NARROW RECTANGULAR CHANNELS", "claims": null}, {"metadata": {"year": 1992}, "authors": ["R. G. Rahall"], "summary": "Abstract : Natural, mixed and forced convection liquid cooling of discrete heat sources in a vertical channel were investigated experimentally. Ten heat sources were flush mounted to one side of a channel wall, while the opposite wall was unheated. Measurements of heater surface temperatures were made for a channel Reynolds number range of 0 - 1700 and heat flux range of 1050 - 4500 W/m 2. In the mixed convection regime, both aiding and opposed flow were examined. Temperature patterns on the heated surface were visualized using liquid crystal.", "title": "Convective heat transfer from discrete heat sources in a liquid filled vertical channel.", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Tiantian  Zhang", "Hongxing  Yang"], "summary": "Abstract Design and construction of internal ventilated air layers have become a popular way to improve the thermal performance of exterior envelopes in modern buildings. These air layers provide multiple benefits to the building envelopes, including improving the thermal insulation property, as well as achieving the effects of fresh air preheating, space heating, natural ventilation, passive cooling, etc. Obviously, the flow and heat transfer condition of the solar driven natural convection in these air layers can significantly influence the performances of these envelopes. This study numerically investigates the flow and heat transfer process, as well as the influence factors of the temperature and velocity fields, the induced air flowrate and the temperature increase in these air layer structures. The results demonstrate that the flow transition, velocity promotion and temperature increase mainly occur in the near-wall regions. For vertical air layers with the height of 2\u20134\u202fm, the width of 0.1\u20130.8\u202fm, and the input heat flux of 100\u2013400\u202fW/m2, the air flowrate varies between 0.042\u202fkg/s and 0.255\u202fkg/s, and the range of the temperature rise is 0.66\u201314.70\u202f\u00b0C. For air layers intending to improve ventilation capacity, the channel width should not be bigger than 0.6\u202fm, while for those with the purpose of supplying warm air, the width should be lower than 0.2\u202fm.", "title": "Flow and heat transfer characteristics of natural convection in vertical air channels of double-skin solar fa\u00e7ades", "claims": null}, {"metadata": {"year": 1997}, "authors": ["N. DeGuzman"], "summary": "An experimental study of evaporative heat transfer in a vertical channel with obstructions was conducted. Tests that encompass a range of mass fluxes of 37-220 klbm/ft2-s (50-300 kg/m2-s), heat fluxes of 960-8000 Btu/ft2-h (3-25 kW/m2), and inlet qualities of 0.2-0.8 were performed with the working fluid R-134a. For both upward and downward flow, both convective and nucleate boiling were observed. At higher mass fluxes, both flow configurations yield the same heat transfer performance. As mass flux decreases, upward flow has higher heat transfer performance relative to downward flow. The upward flow configuration suppresses nucleate boiling when compared to the downward flow configuration. Upon inspection of the data, there appears to be a discontinuity in heat transfer performance which may be attributed to a flow transition. There are no correlations in the literature that accurately describe the experimental heat transfer data present here.", "title": "Evaporative Heat Transfer Characteristics in a Vertical Channel with Obstructions", "claims": null}, {"metadata": {"year": 2006}, "authors": ["Viktor I. Terekhov", "V. V. Terekhov"], "summary": "A numerical model is developed of heat transfer in a high vertical narrow enclosure in the presence of fins on one of the isothermal side walls. The Navier\u2014Stokes equations were solved numerically in two-dimensional formulation. It is demonstrated that the mean coefficient of heat transfer over the channel surface for adiabatic fins first increases with the number of fins to reach a maximum and then decreases by approximately 30% compared to smooth walls. For heat-conducting fins, the enhancement of heat transfer may be as high as 200% or more.", "title": "Heat transfer in a high vertical enclosure with fins attached to one of the side walls", "claims": null}], "query": "Effect of surface emissivity on heat transfer in vertical channels", "summary_abstract": "The research question regarding the effect of surface emissivity on heat transfer in vertical channels is addressed through various studies that explore the interplay between surface characteristics and heat transfer mechanisms.\n\nMandal et al. (2020) conducted a numerical analysis focusing on mixed convection with surface radiation in a vertical channel. Their findings indicate that increasing the emissivity of heat spreaders significantly enhances heat transfer performance. Specifically, as the emissivity of the heat spreader rises from 0.1 to 0.9, the rate of heat transfer improves, suggesting that higher emissivity surfaces facilitate better thermal exchange.\n\nIn contrast, Tanda (2008) examined the impact of protrusions on heat transfer in a vertical channel. Although this study primarily focused on the geometry of the surface rather than emissivity, it revealed that the addition of low-conductivity protrusions can decrease the average heat transfer coefficient compared to smooth surfaces. This suggests that surface modifications can have complex effects on heat transfer, potentially influenced by emissivity.\n\nFurther insights are provided by Terekhov and Terekhov (2006), who developed a numerical model to analyze heat transfer in a vertical enclosure with fins. Their results showed that the mean coefficient of heat transfer increases with the number of fins up to a certain point, after which it declines. This indicates that while surface modifications can enhance heat transfer, there may be diminishing returns or optimal configurations that need to be considered.\n\nOverall, the collective findings emphasize that surface emissivity plays a crucial role in heat transfer within vertical channels, with higher emissivity generally leading to improved thermal performance (Mandal et al., 2020). However, the effects of surface geometry and configuration, as highlighted by Tanda (2008) and Terekhov and Terekhov (2006), also significantly influence heat transfer characteristics, suggesting a multifaceted relationship that warrants further investigation.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2021}, "authors": ["Hui Luo"], "summary": "There is an increasing consensus among the international community that cooperation and the sharing of knowledge provide a pathway for solving global challenges and achieving sustainable development. As UNESCO has noted: \u2018In the context of pressing planetary and socio-economic challenges, sustainable and innovative solutions require an efficient, transparent and vibrant scientific effort\u2014not only stemming from the scientific community, but from the whole society.\u2019 Thus arises the idea of open science, the principle of which is to allow \u2018scientific information, data and outputs to be more widely accessible (open access) and more reliably harnessed (open data) with the active engagement of all relevant stakeholders (open to society)\u2019 (UNESCO, 2020). Open science calls for international cooperation in science and technology to achieve a win\u2013win result. The world response to the COVID-19 pandemic is a good example that demonstrates that only by strengthening communication and sharing in the scientific field can we further develop the world economy. Recognizing the importance of open science in today\u2019s socio-economic development, UNESCO (2021) has made an effort to promote open science by providing a \u2018UNESCO recommendation on open science\u2019. The recommendation sets international standards for open science. In this context, the China Association for Science and Technology (CAST) organized a session focusing on dialogue between the world\u2019s top scientists and young scientists during its 23th annual meeting in July 2021. The theme of the session was \u2018openness, cooperation and sharing\u2019. Inspired by the views shared by scientists from different countries, and considering the worldwide attention on open science, we have produced six articles exploring the topic of open science, which will be presented in two issues. The current issue includes four contributions from Wei Yang; Dawei Ding and Zhengfeng Li; Yangxu Lu, Yandong Zhao and Huang Lei; and Ke Guan, Danping He and Zhangdui Zhong. Another two contributions, from Ke Gong and from Junmin Wei et al., will be published in the next issue. In this issue, Professor Wei Yang explores the development of open and inclusive science in China in his article \u2018Open and inclusive science: A Chinese perspective\u2019. He analyses the topological and dynamic aspects of open and inclusive science and proposes five major stakeholders in the", "title": "Open science: Pursuing development through communication, cooperation and sharing", "claims": null}, {"metadata": {"year": 2022}, "authors": ["I. Drach", "M. Popovych", "Oleksandr Polishyk", "O. Polishchuk"], "summary": "One of the important mechanisms of world transformation in the context of the formation of sustainable development of society is the achievement and provision of innovative, inclusive, fair and high-quality models of development and institutionalization of science. The process of formation and implementation of a new modern policy in the field of science foresees the implementation of the paradigm of Open Science. The analysis of the key documents of the European Research Area and the European Higher Education Area, which define the policies and procedures for the development of Open Science, allowed to characterize the understanding of its features, goals, characteristics and indicators. The key provisions of the EU policy on the development of Open Science include the following: recognition of the priority of open research; ensuring the earliest possible exchange of knowledge and research data; promoting the involvement of research partners from industry, government and community groups; compliance of publications with the requirements \u201copen as much as possible, closed as necessary\u201d; recognition and reward of participation in research of citizens and end consumers; ensuring compliance with intellectual property rights. Active promotion of the ideas of Open Science through appropriate policies and procedures strengthens global cooperation and corresponds to the principles and goals of sustainable development of society, focused on improving the quality of human life in a favorable socio-economic environment and environmentally friendly, healthy, diverse environment.", "title": "Global cooperation in Open Science policies and procedures to ensure the sustainable development of society", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Adamantia Rachovitsa"], "summary": "Open science - namely more openness with regard to educational resources, access, methodology, reproducibility and data - is fundamental to realising the Sustainable Development Goals (SDGs). Science is a universal public good and it can be a game-changer when addressing global problems. This is exemplified by the critical role that the production and diffusion of science (scientific outputs and process) plays in the transfer of knowledge and technology. Some examples include the need for decentralised ways to acquire, manage and analyse open data in disaster management (resilient &amp; sustainable cities under SDG 11); or the need for new Research &amp; Development models for innovation and knowledge &amp; tech transfer for the production, availability and access to safe, quality and affordable medicines and vaccines (SDG 3); or the need for technologies for providing clean water &amp; sanitation, affordable &amp; clean energy and mitigating (the adverse effects of) climate action (SDGs 6, 7, 13 and 9). The urgent relevance of pursuing open science across scientific disciplines and various industries is further exemplified by the thus far \u201cdormant\u201d right to science. The right to science, as formulated under Article 15 of the International Covenant on Economic, Social and Cultural Rights, provides that everyone has the right to enjoy the benefits of scientific progress and its applications and that States parties shall take steps necessary for the conservation, the development and diffusion of science.", "title": "Blockchain for Open Science and Sustainable Development Goals", "claims": null}, {"metadata": {"year": 2022}, "authors": ["J. Camkin", "S. Neto", "B. Bhattarai", "H. Ojha", "Shahbaz Khan", "A. Sugiura", "Jiaying Lin", "Fitrie Atviana Nurritasari", "Joseph Karanja"], "summary": "Achieving the United Nations Sustainable Development Goals is increasingly challenging due to widening inequality in access to scientific and technological knowledge and resources. With science remaining too discipline based, and policymakers too often science averse, there is a need for greater understanding of the opportunities and challenges of open science for science practitioners, policy makers and communities. Acknowledging that open science can be a powerful tool to reduce inequalities, UNESCO has been supporting the shift to open science. Following global multistakeholder consultation, the UNESCO Recommendation on Open Science was adopted in November 2021, establishing a universal definition, common standards and shared set of values and principles. In 2021, the UNESCO Regional Science Bureau for Asia and the Pacific partnered with the Institute for Study and Development Worldwide on local and national mapping to identify implementation strategies and mechanisms already in place to enable open science in Asia and the Pacific, and to identify what more is needed. Focused on Malaysia, Republic of Korea, Pakistan, Samoa and Uzbekistan, the study showed that while there are many examples of good practice in aspects of open science, none of the focus countries currently has in place all the policies, infrastructure, awareness and capacity building needed. Typically, clear policies on open science have not yet been articulated and funding mechanisms not yet established. Trust is a key pre-requisite for open science. Current inequities in access to open science infrastructure will need to be addressed or implementation may be unbalanced, exacerbating existing inequities at national, regional, and global scales. There are many opportunities to learn from existing efforts toward open science, but there will be no generic model; each country will need to design an open science model and implementation pathway suited to its context. A predominant message from this research was that the convening power of UNESCO should continue to be harnessed to engage countries on open science implementation. Further, those committed to open science will need to work hard, with UNESCO, to democratize science and encourage an ethos of \u201cpolicy for science, science for policy - science for society and society for science\u201d.", "title": "Open Science for Accelerating the Sustainable Development Goals: Status and Prospects in Asia and the Pacific", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Moumita Koley"], "summary": "One of the core principles of science is to aid socio-economic growth. Open science is a movement that reinforces the primacy of science in the direction of economic and social welfare. UNESCO's recommendation on open science aims to provide an international framework for open science policy and practice. It endorses unrestricted access to scholarly publications and data, the use of digital technologies to drive scientific processes, more collaboration and cooperation among the actors in the scientific ecosystem, sharing of research infrastructure, acknowledgment of diverse knowledge systems, and science for society. Open science could enable a productive science ecosystem in global south countries through efficient knowledge circulation, resource sharing, and collaboration. Analysis of open science policy from a global south country can provide valuable insights. India is preparing to adopt an open science framework recommended in the 5th Science, Technology, and Innovation Policy (STIP) draft, released in December 2020. The STIP draft recommends open access to articles and research data from publicly funded projects, access to research infrastructure beyond the boundary of academic and research institutions, strengthening of Indian journals, and open educational resources. However, the draft lacks an exhaustive implementation plan. The draft falls short in devising strategies to foster collaboration between actors of the STI ecosystem, the inclusion of traditional knowledge systems, and society's role in knowledge creation processes. The science policymakers and advisers of the Department of Science and Technology and the government of India should probe these areas to develop a more effective and inclusive open science framework.", "title": "Analysis of Open Science Policy Recommendations Proposed in India\u2019s 5th Science, Technology & Innovation Policy Draft", "claims": null}, {"metadata": {"year": 2022}, "authors": ["I. Drach"], "summary": "Open science, based on high standards of transparency, cooperation and communication, is a priority for the development of research policy in the European Research Area (ERA) and the European Higher Education Area (EHEA). The purpose of the article is to analyse the essence of Open Science, its goals and benefits, as well as the necessary changes for large-scale implementation of Open Science policy in universities. Methods of analysis, synthesis, comparison, systematization of scientific works of foreign and domestic scientists and documents of EHEA and ERA are used. Open science is seen as a new approach to the scientific process based on collaboration and new ways of disseminating knowledge through digital technologies and new tools for collaboration. The main goal of Open Science is to improve the exchange, transfer and access to scientific knowledge. The advantages of Open Science are defined as follows: increasing the efficiency of science by reducing duplication and costs of creating, transmitting and reusing data; increasing transparency and quality in the process of confirming the reliability of research results; accelerating the transfer of knowledge, promoting a faster transition from research to innovation; increasing the impact of research results on the economy; a more effective response to global challenges that require coordinated international action; promoting the involvement of citizens in research, active participation in scientific experiments and data collection. It is concluded that Open Science, by maximizing the benefits of the free movement of knowledge, provides a movement towards the fifth freedom in Europe (along with the freedom to move goods, services, persons and capital). The transition to large-scale implementation of Open Science practice in universities requires changes that cover a wide range of academic activities at the institutional level and include: promoting increased knowledge transfer and cooperation between the academic and non-academic sectors; focus on knowledge transfer in the context of digitalization; optimizing the role of universities in research infrastructures.", "title": "OPEN SCIENCE IN UNIVERSITIES: GOALS AND BENEFITS", "claims": null}, {"metadata": {"year": 2018}, "authors": ["D. Alemneh", "Suliman Hawamdeh", "Abebe Rorissa", "S. Assefa", "Kris Helge"], "summary": "Presentation for the \"Open Science and Open Data for Sustainable Development: A Global View\" panel that discusses the global landscape of open science.", "title": "Open Science and Open Data for Sustainable Development: A Global View", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Y. Murayama"], "summary": "Since the G8 Science Ministers' Meeting in UK 2013, Open Science policy and practice have been intensively discussed in related countries. Many bodies have been committing this activity like academic committees ICSU-World Data System (WDS) and Committee of Science and Technology Data (CODATA), RDA (Research Data Alliance), a data consortium in connection to G8 Group of Senior Officials (GSO)'s data infrastructure working group, and recently OECD (Organization of Economic Cooperation and Development) Global Science Forum, etc. In Japan, Cabinet Office of Japan played a leading role in Japan to publish the first national guiding principle of Open Science for Japan (March 2015). In 2016 G7 Science and Technology Ministers' Meeting was held in Japan, including the Open Science session as one of its six main themes.", "title": "A view of International Landscape of Open Science and Research Data Sharing", "claims": null}], "query": "Releavance of Open science to sustainable development and international cooperation", "summary_abstract": "The relevance of open science to sustainable development and international cooperation is increasingly recognized across various scholarly works. Open science, characterized by transparency, accessibility, and collaboration, is seen as a vital mechanism for addressing global challenges and achieving the Sustainable Development Goals (SDGs).\n\nLuo (2021) emphasizes that open science facilitates the sharing of knowledge and resources, which is essential for tackling pressing socio-economic issues. The COVID-19 pandemic exemplified how international cooperation in science can lead to significant advancements in public health and economic recovery. UNESCO's efforts to promote open science through recommendations and standards further highlight its importance in fostering global collaboration.\n\nDrach et al. (2022) discuss the European Union's commitment to open science, which aims to enhance the exchange of knowledge and research data. They argue that open science aligns with the principles of sustainable development by promoting inclusivity and fairness in scientific practices. This is echoed by Rachovitsa (2018), who notes that open science is fundamental to realizing the SDGs, as it enables decentralized access to scientific knowledge necessary for addressing issues like disaster management and public health.\n\nMoreover, Camkin et al. (2022) point out that open science can help reduce inequalities in access to scientific knowledge, which is crucial for achieving the SDGs. They highlight UNESCO's role in establishing a universal definition and standards for open science, which can guide countries in implementing effective policies.\n\nKoley (2022) adds that open science can significantly benefit countries in the Global South by enhancing knowledge circulation and resource sharing. However, she notes that many countries still lack comprehensive policies and infrastructure to fully embrace open science.\n\nIn summary, the collective findings from these papers underscore that open science is not only relevant but essential for sustainable development and international cooperation. It fosters inclusivity, enhances knowledge sharing, and provides a framework for addressing global challenges collaboratively (Luo, 2021; Drach et al., 2022; Rachovitsa, 2018; Camkin et al., 2022; Koley, 2022).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2017}, "authors": ["Michael  Ungar", "Janice  Ikeda"], "summary": "A qualitative study of 61 youth receiving mandated services (child welfare, mental health, probation) or services where there were no alternatives (residential care for homeless youth) explored worker-client relationships from the perspective of young people themselves. Findings suggest three different but related roles played by workers that successfully engage adolescent clients: (1) \u201cInformal supporters\u201d de-professionalize their role and flatten hierarchies, emphasizing empathy and enforcing few rules; (2) \u201cAdministrators\u201d enforce rules that are in the child\u2019s best interest but do so with little emotional engagement; and (3) \u201cCaregivers\u201d who hold reasonable expectations and impose structures but are flexible in their negotiations with youth when rules were broken. While youth spoke most positively about their workers when they acted as informal supporters, a deeper analysis of the data showed that youth also engaged well with workers who enforced rules when those rules were necessary for the child\u2019s safety, applied flexibly, age-appropriate, and fit with cultural norms. Use of all three approaches to youth engagement may help workers create better therapeutic relationships with youth receiving mandated services.", "title": "Rules or No Rules? Three Strategies for Engagement with Young People in Mandated Services", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Karanpreet  Bath", "Lisa D. Hawke", "Tracey  Skilling", "Gloria  Chaim", "Joanna  Henderson"], "summary": "INTRODUCTION\nThere is paucity of research on treatment-related coercion in youth: most research focuses on adult populations and legally mandated treatment. This study aims to examine the service-seeking profiles of youth with substance misuse issues who report a legal mandate or perceived coercion to enter treatment.\n\n\nMETHODS\nDifferences between youth who were legally mandated and not legally mandated, and differences between youth reporting high and low perceived coercion, were examined for demographic characteristics, mental health and substance use profiles, motivation, and readiness to change.\n\n\nRESULTS\nCompared to participants reporting low perceived coercion, those experiencing high perceived coercion reported more substance use problems, greater mental health needs, and greater external and introjected motivation. Legally mandated youth reported fewer mental health issues, lower identified motivation, and greater readiness to change than those reporting no legal mandate.\n\n\nDISCUSSION\nMany youth who present for substance use services report experiencing a sense of coercion, which suggests the potential importance of considering youth-centered strategies for involving youth in treatment planning and the development of treatment goals. Youth seeking treatment also have multiple intersecting needs which may benefit from a collaborative and integrative approach.", "title": "The service-seeking profiles of youth reporting a legal mandate or perceived coercion for substance use treatment.", "claims": null}, {"metadata": {"year": 2022}, "authors": ["N. Tiburcio", "Scarlett L. Baker", "Kristin S. Kimmell"], "summary": "Objective This article presents an investigation of the defensiveness demonstrated by teens who are mandated to participate in treatment as compared to their non-mandated peers. Methods The data for this study was collected as part of The Substance Abuse Subtle Screening Inventory (SASSI) Institute\u2019s third iteration of the Adolescent SASSI-A3. A total of 164 teenagers in treatment served as the dataset for the present study. All participant cases were provided by clinicians working in service settings throughout all U.S. Census Regions and serving in a variety of venues including substance use treatment, criminal justice programs, community corrections, private clinical practices, behavioral health centers, and social service organizations. Results We present two brief de-identified treatment case studies, aptly demonstrating defensiveness and denial from a clinical standpoint. Additionally, we review cases demonstrating high-levels of defensiveness and denial in mandated teen clients, and ethical ways to break through that barrier to treatment engagement. Conclusion Working with teens can be extremely difficult given their rapid mood changes, intensely felt experiences and shifting states of compliance, openness and defiance. When teens are mandated for treatment, they may often feel their choices have been taken away and the counselor may be viewed as more of a power authority rather than a concerned and helping figure. Contingencies placed on the teen as part of the mandated treatment experience are generally the primary focus, rather than addressing their underlying substance use disorder (SUD). As a result, the teen, as well as the counselor, require greater focus than just making sure those requirements are met", "title": "Mandated Treatment for Troubled Adolescents and Substance Use Disorder: Identifying and Breaking Through Defensiveness and Denial", "claims": null}, {"metadata": {"year": 2001}, "authors": ["Joshua S. Meisel"], "summary": "The purpose of this study was to examine the impact of participation in the Intensive Aftercare Demonstration Project on relationships between youth and their service providers. Successful intensive case supervision often assumes that providers develop positive relationships with their clients. The study employed an experimental research design in which participating youth were randomly assigned to treatment (received intensive aftercare services) or control conditions during the assessment phase of their commitment. A total of 97 youth participating in the pilot study completed survey questionnaires, at up to four separate phases, to assess change over time. Two survey instruments were used to measure youth perceptions of relationships with service providers as well as case manager perceptions of youth strengths. The findings suggest that participation in the experimental group was strongly associated with youth's perceiving positive relationships with their client managers.", "title": "Relationships and Juvenile Offenders: The Effects of Intensive Aftercare Supervision", "claims": null}, {"metadata": {"year": 2006}, "authors": ["Kristin Duppong  Hurley", "Stephanie  Ingram", "J. Douglas  Czyz", "Nicholas  Juliano", "Evelyn  Wilson"], "summary": "We describe a comprehensive program to train emergency shelter staff in effective methods for dealing with youth who have behavioral and emotional problems; assess the degree to which staff implemented the treatment approach; measure the impact of the intervention on shelter-wide incidents such as out-of-control behavior, runaways, and violence against other youth; and examine pre-post changes in staff experiences at the shelter via anonymous surveys. Overall, the short-term shelter staff indicated adequate implementation of the intervention. The rate of youth incidents at the shelter significantly declined from pre to post assessment. Direct-care staff ratings of their satisfaction with their proficiency in behavior management and teaching youth skills increased significantly from pre to post implementation. These findings suggest that it is feasible to deliver an effective staff-training program to improve the behavior management and social skills of youth residing in short-term care facilities within the child welfare system.", "title": "Treatment for Youth in Short-Term Care Facilities: The Impact of a Comprehensive Behavior Management Intervention", "claims": null}, {"metadata": {"year": 2007}, "authors": ["J. Degner", "Ann-Karina Henriksen", "L. Oscarsson"], "summary": "Involvement by parents and other significant social work members (PSMs) in the treatment program is important for positive treatment effects when youths are placed in residential treatment centres ...", "title": "Youths in coercive residential care : Perception of parents and social network involvement in treatment programs", "claims": null}, {"metadata": {"year": 2018}, "authors": ["S. Snyder"], "summary": "Juvenile-justice-involved youth are known to have higher rates of traumatic exposure and traumatic stress symptoms, which increases the necessity for appropriate treatment and targeted case planning for these youth. Traumatic stress symptoms have been shown as a risk factor for delinquent behaviors, though practices for treating juvenile-justice-involved youth who have traumatic stress symptoms is limited. Evidence-based practice dissemination has focused on the use of trauma-focused treatment in residential and community settings. Juvenile-justice-involved youth can be most vulnerable in the secure detention setting. Youth in detention face ongoing stressors in the secure setting, having to witness interpersonal violence by peers, being subject to physical restraints, and attempting to cope in a setting with limited choices. Best practices for the juvenile detention setting are even more limited, despite knowledge of the efficacy of evidence-based trauma interventions in reducing trauma symptoms and restoring healthy family dynamics and relationships that trauma disrupts. This practice note discusses the implementation of trauma-focused cognitive\u2013 behavioral therapy in the detention setting, and the interventions flexibility, appropriateness, and impact for youth in detention.", "title": "Implementation of Trauma-Focused Cognitive\u2013Behavioral Therapy in Juvenile Detention: A Practice Note From the Field", "claims": null}, {"metadata": {"year": 1979}, "authors": ["K. Michaels", "R. Green"], "summary": "A pilot project providing therapy for families of status-offender youths has proved effective in reducing placements and costs.", "title": "A child welfare agency project: therapy for families of status offenders.", "claims": null}], "query": "What are the effects of mandatory therapy on youth participation in youth living in child protective services?", "summary_abstract": "The effects of mandatory therapy on youth in child protective services reveal a complex interplay between coercion, engagement, and therapeutic relationships. Research indicates that youth often experience a sense of coercion when mandated to participate in treatment, which can significantly impact their motivation and readiness to change. For instance, Bath et al. (2019) found that youth who reported high levels of perceived coercion exhibited more substance use problems and greater mental health needs compared to those who felt less coerced. This suggests that the experience of coercion can hinder effective engagement in treatment.\n\nMoreover, Ungar and Ikeda (2017) highlight the importance of the worker-client relationship in fostering positive engagement among youth receiving mandated services. They identified three roles that workers can adopt: informal supporters, administrators, and caregivers. Youth responded most positively to workers who acted as informal supporters, emphasizing empathy and a de-professionalized approach. However, they also engaged well with workers who enforced necessary rules flexibly and in a culturally appropriate manner.\n\nTiburcio et al. (2022) further emphasize the defensiveness often exhibited by mandated teens, which can complicate the therapeutic process. They note that when teens feel their autonomy is compromised, they may view counselors as authority figures rather than supportive allies, which can lead to resistance in treatment.\n\nMeisel (2001) provides evidence that positive relationships with service providers can enhance the therapeutic experience for youth. His study found that participation in intensive aftercare services was associated with youth perceiving stronger relationships with their case managers, which is crucial for effective treatment outcomes.\n\nOverall, the literature suggests that while mandatory therapy can present challenges, particularly in terms of perceived coercion and defensiveness, the quality of the worker-client relationship plays a pivotal role in determining the effectiveness of these interventions. Strategies that prioritize youth engagement and foster supportive relationships may mitigate some of the negative effects associated with mandated treatment (Ungar & Ikeda, 2017; Bath et al., 2019; Meisel, 2001).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2018}, "authors": ["Junfeng Shi", "Galit Fichman", "J. Schneider"], "summary": "Post-translational modification is a common mechanism to affect conformational change in proteins, which in turn, regulates function. Herein, this principle is expanded to instruct the formation of supramolecular assemblies by controlling the conformational bias of self-assembling peptides. Biophysical and mechanical studies show that an engineered phosphorylation/dephosphorylation couple can affectively modulate the folding of amphiphilic peptides into a conformation necessary for the formation of well-defined fibrillar networks. Negative design principles based on the incompatibility of hosting residue side-chain point charge within hydrophobic environments proved key to inhibiting the peptide's ability to adopt its low energy fold in the assembled state. Dephosphorylation relieves this restriction, lowers the energy barrier between unfolded and folded peptide, and allows the formation of self-assembled fibrils that contain the folded conformer, thus ultimately enabling the formation of a cytocompatible hydrogel material.", "title": "Enzymatic Control of the Conformational Landscape of Self-Assembling Peptides.", "claims": null}, {"metadata": {"year": 2000}, "authors": ["J. T. Meijer"], "summary": "Peptide amphiphiles are class of protein mimics and model systems. Especially, the assembly behavior of peptide amphiphiles has attracted much attention. Not only could the self-assembly behavior provide a starting point for peptide based structures, but the peptide amphiphiles could also be co-assembled with other components in order to introduce additional functionality and/or biocompatibility, leading to a bottom-up fabrication of new nano-materials. Tuning of this self-assembly process should be possible by tuning the hydrophobicity of peptide amphiphiles. This thesis describes the influence of added hydrophobicity, achieved by the introduction of alkyl tails of varying lengths, on peptide (self-) assembly and folding. The influence of additional hydrophobic interactions on a peptide that forms amyloid like fibrils, with the sequence Lys Thr Val Ile Ile Glu, was examined. Specifically the effect of the introduction of an alkyl tail at the N-terminus or C-terminus on the stability of fibrils formed by the peptide is investigated. Furthermore, it was examined whether it is also possible to not only stabilize, but also disassemble these fiber assemblies by manipulating the hydrophobic hydrophilic balance. And the possibility of in situ disassembly of peptide fibrils through the introduction of a cleavable hydrophobic moiety is studied. The influence of introduced hydrophobic interactions on the folding and assembly of a non-self assembling peptide, with the sequence Gly Ala Asn Pro Asn Ala Ala Gly (GANPNAAG), was examined to contrast the effect on hydrophobic interactions of the already fibril forming peptide. These alkyl modified peptides were anchored into liposomes. The effect on folding and anchoring is examined, by varying the length of the alkyl tails on the peptide and liposome composition. Finally, the possibilities were investigated to use alkyl modified peptides, either anchored in liposomes or attached to a carrier protein, for the production of vaccines.", "title": "Stabilization of peptide structure by non-covalent interactions", "claims": null}, {"metadata": {"year": 2019}, "authors": ["R. Wakabayashi", "Ayumi Suehiro", "M. Goto", "N. Kamiya"], "summary": "We herein designed bi-functional aromatic peptide amphiphiles both self-assembling to fibrous nanomaterials and working as a substrate of microbial transglutaminase, leading to peptidyl scaffolds with different morphologies that can be enzymatically post-functionalized with proteins.", "title": "Designer aromatic peptide amphiphiles for self-assembly and enzymatic display of proteins with morphology control.", "claims": null}, {"metadata": {"year": 2011}, "authors": ["D. Zanuy", "I. Hamley", "C. Alem\u00e1n"], "summary": "The conformational properties of the hybrid amphiphile formed by the conjugation of a hydrophobic peptide with four phenylalanine (Phe) residues and hydrophilic poly(ethylene glycol), have been investigated using quantum mechanical calculations and atomistic molecular dynamics simulations. The intrinsic conformational preferences of the peptide were examined using the building-up search procedure combined with B3LYP/6-31G(d) geometry optimizations, which led to the identification of 78, 78, and 92 minimum energy structures for the peptides containing one, two, and four Phe residues. These peptides tend to adopt regular organizations involving turn-like motifs that define ribbon or helical-like arrangements. Furthermore, calculations indicate that backbone\u00b7\u00b7\u00b7side chain interactions involving the N-H of the amide groups and the \u03c0 clouds of the aromatic rings play a crucial role in Phe-containing peptides. On the other hand, MD simulations on the complete amphiphile in aqueous solution showed that the polymer fragment rapidly unfolds maximizing the contacts with the polar solvent, even though the hydrophobic peptide reduce the number of waters of hydration with respect to an individual polymer chain of equivalent molecular weight. In spite of the small effect of the peptide in the hydrodynamic properties of the polymer, we conclude that the two counterparts of the amphiphile tend to organize as independent modules.", "title": "Modeling the tetraphenylalanine-PEG hybrid amphiphile: from DFT calculations on the peptide to molecular dynamics simulations on the conjugate.", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Brian K. Chu", "Iris W. Fu", "Cade B. Markegard", "Seong-eun Choi", "H. Nguyen"], "summary": "Peptide amphiphiles (PA) offer the potential of incorporating biological function into synthetic materials for tissue engineering in regenerative medicine. These hybrid conjugates are known to undergo self-assembly starting from single molecules to nanofibers before turning into hydrogel scaffolds-such a process involves conformational changes in secondary structures of peptides. Therefore, insights on the ability of peptide amphiphiles to form secondary structure as single molecules are useful for understanding self-assembly behavior. We report here a molecular simulation study of peptide folding by two PA sequences, each contains an alkyl tail and short peptide segment. The alkyl tail is observed to play two opposing roles in modulating sequence-dependent folding kinetics and thermodynamics. On one hand, it restricts conformational freedom reducing the entropic cost of folding, which is thus promoted. On the other hand, it acts as an interaction site with nonpolar peptide residues, blocking the peptide from helix nucleation, which reduces folding.", "title": "A tail of two peptide amphiphiles: effect of conjugation with hydrophobic polymer on folding of peptide sequences.", "claims": null}, {"metadata": {"year": 1995}, "authors": ["Y. Fukushima"], "summary": "An 18-residue amphiphilic peptide (NH2-GELELELEQQKLKLKLKG-COOH) was designed and prepared, and the helix-sheet conformational transition of the peptide was investigated by circular dichroism in aqueous solution of varying pH, peptide concentration and ionic strength. The conformation of the peptide was converted from a \u03b2-sheet to \u03b1-helix from neutral pH to pH 2.0 or pH 12.0. At acidic condition, an \u03b1-helix to \u03b2-sheet conformational transition was observed at increase of peptide concentration or addition of salt. An anion with higher valency effectively induced the conformational transition. The independency of \u03b1-helical content with peptide concentration indicated that the \u03b1-helix was monomeric structure, whereas the \u03b2-sheet conformation formed an aggregated structure because of increase of the \u03b2-sheet content with peptide concentration. The conformational transition was provided by self-association and dissociation of the peptide. The secondary structure of the peptide was significantly affected by altering hydrophobic as well as electrostatic interactions such as ionic and hydrogen bonds. Because it was also found that conformational transition takes place very rapidly, the peptide may have properties for rapid responsive materials in practical use.", "title": "Self-Induced Helix-Sheet Conformational Transitions of an Amphiphilic Peptide", "claims": null}, {"metadata": {"year": 2001}, "authors": ["Navdeep B. Malkar", "Neal  Niemczyk", "Gregg B. Fields"], "summary": "Controlling cell adhesion to polymer substrates is one of the important issues in tissue engineering, and includes the ability to direct specific cell types to proliferate, migrate, and express normal physiological behaviors, in order to yield a defined cellular architecture [1], Engineering bioactive, conformationally constrained peptides onto biomaterial surfaces could satisfy the above criteria and hence improve biocompatibility. Our laboratory previously demonstrated that linking lipid tails to peptides enables them to self-assemble into biomimetic films with a highly organized interface, and perhaps more importantly, enhances their ability to assume well-defined secondary and tertiary conformations [2\u20134]. These peptide-amhiphiles effectively promote a variety of cell activities [2]. In the present study, novel peptide-amphiphiles that have the potential to form \u03b1-helical structure combined with an amino acid sequence from the angiogenesis-inducing region of SPARC have been constructed. The peptide sequence 21r, Lys-Ala-(Glu-Ile-Glu-Ala-Leu-Lys-Ala)2-Tyr-Lys-Gly-His-Lys, was lipidated with one of the three \u201ctails\u201d listed below and characterized.", "title": "Peptide-Amphiphile Induction of \u03b1-Helical Molecular Architecture and Interaction with Biomaterial Surfaces", "claims": null}, {"metadata": {"year": 1999}, "authors": ["Y. C. Yu", "V. Roontga", "V. Daragan", "K. Mayo", "M. Tirrell", "G. Fields"], "summary": "Organized polymeric assemblies that incorporate bioactive sequences and structures are finding important applications for the study of protein structure-function relationships. We have recently described a heteropolymeric peptide-amphiphile system that forms organized structures in solution and on surfaces. While the overall three-dimensional features of peptide-amphiphiles have been studied previously, the precise environment of specific residues, particularly those within biologically active regions, have not been examined in detail. In the present study, we have used heteronuclear single quantum coherence (HSQC) and inverse-detected 1H-15N NMR spectroscopy to examine the structure and dynamics of a peptide and peptide-amphiphile that incorporate the alpha1(IV)1263-1277 ([IV-H1]) amino acid sequence from type IV collagen. Three variants of the sequence (Gly-Pro-Hyp)4-[IV-H1]-(Gly-Pro-Hyp)4 were constructed with a single 15N-labeled Gly placed in the middle of the N-terminal (Gly-Pro-Hyp)4 region (residue Gly7), in the middle of the [IV-H1] sequence (residue Gly19), or in the middle of the C-terminal (Gly-Pro-Hyp)4 region (residue Gly34). These peptides were also N-terminally acylated with hexanoic acid to create an analogous series of 15N-labeled peptide-amphiphiles. HSQC spectra indicated that both the peptide and the peptide-amphiphile were in triple-helical conformation at low temperature, supporting prior circular dichroism (CD) spectroscopic results. The intensities of the triple-helical cross-peaks were stronger for the peptide-amphiphile, consistent with an enhanced triple-helical thermal stability within the peptide-amphiphile construct compared to that of the peptide alone. Relative relaxation values for the peptide-amphiphile monomeric and trimeric species were consistent with those reported previously for other triple-helical peptides. Relaxation measurements indicated that the triple-helical [IV-H1] region did not appear to be dramatically more flexible than the Gly-Pro-Hyp regions. The angle between Gly N-H bonds and the helix dyad axis, determined from the relaxation data, was within the range expected for triple helices. Overall, the peptide headgroup of the C6-(Gly-Pro-Hyp)4-[IV-H1]-(Gly-Pro-Hyp)4 peptide-amphiphile appears to form a continuous triple helix that behaves similarly, in a dynamic sense, to a triple-helical peptide. The enhanced thermal stability of the peptide-amphiphile compared to the analogous triple-helical peptide, along with the multitude of organized structures formed by lipidlike compounds, suggest that peptide-amphiphiles could be utilized as targeted liposomes, sensors, receptors, or enzymes.", "title": "Structure and dynamics of peptide-amphiphiles incorporating triple-helical proteinlike molecular architecture.", "claims": null}], "query": "peptide amphiphiles as manipulators of protein conformation", "summary_abstract": "The research question regarding peptide amphiphiles as manipulators of protein conformation is addressed through various studies that explore their self-assembly behavior, conformational changes, and potential applications in biomaterials and tissue engineering.\n\nShi et al. (2018) highlight the role of engineered phosphorylation/dephosphorylation in modulating the folding of amphiphilic peptides, which is crucial for forming fibrillar networks. Their findings suggest that such modifications can effectively control the conformational state of peptides, enabling the creation of cytocompatible hydrogels.\n\nMeijer (2000) discusses how the self-assembly of peptide amphiphiles can be influenced by their hydrophobicity, particularly through the introduction of alkyl tails. This manipulation not only stabilizes peptide fibrils but also allows for their disassembly, showcasing the ability to control peptide conformation through hydrophobic interactions.\n\nWakabayashi et al. (2019) present bi-functional aromatic peptide amphiphiles that self-assemble into fibrous nanomaterials and can be enzymatically modified, indicating that these amphiphiles can serve as versatile scaffolds for protein interactions and conformational changes.\n\nZanuy et al. (2011) utilize molecular dynamics simulations to investigate the conformational properties of hybrid amphiphiles, revealing that the interactions between hydrophobic peptides and their environment significantly influence their folding and assembly behavior.\n\nChu et al. (2014) emphasize the importance of alkyl tails in peptide amphiphiles, which can both promote and hinder folding kinetics, thus affecting the overall conformational landscape of the peptides during self-assembly.\n\nFukushima (1995) explores the conformational transitions of an amphiphilic peptide in response to pH and ionic strength, demonstrating that such environmental factors can induce rapid changes between \u03b1-helix and \u03b2-sheet structures, further illustrating the dynamic nature of peptide conformations.\n\nMalkar et al. (2001) and Yu et al. (1999) both discuss the enhancement of conformational stability and bioactivity of peptides when linked to lipid tails, suggesting that peptide amphiphiles can be engineered to adopt specific conformations that promote desired biological functions.\n\nCollectively, these studies underscore the potential of peptide amphiphiles to manipulate protein conformation through various strategies, including chemical modifications, environmental changes, and structural design, making them valuable tools in biomaterials science and regenerative medicine.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2013}, "authors": ["T. Gaziano", "N. Pagidipati"], "summary": "Chronic diseases are increasingly becoming a health burden in lower- and middle-income countries, putting pressure on public health efforts to scale up interventions. This article reviews current efforts in interventions on a population and individual level. Population-level interventions include ongoing efforts to reduce smoking rates, reduce intake of salt and trans-fatty acids, and increase physical activity in increasingly sedentary populations. Individual-level interventions include control and treatment of risk factors for chronic diseases and secondary prevention. This review also discusses the barriers in interventions, particularly those specific to low- and middle-income countries. Continued discussion of proven cost-effective interventions for chronic diseases in the developing world will be useful for improving public health policy.", "title": "Scaling up chronic disease prevention interventions in lower- and middle-income countries.", "claims": null}, {"metadata": {"year": 1999}, "authors": ["C. Lengeler"], "summary": "Preventive interventions in the health sector are an essential component of public health work. The authors of the 1993 World Development Report (World Bank 1993) suggested that in resource-poor countries only US$ 4.2 per capita per year would buy a basic preventive public health package including EPI Plus, school health, micronutrient supplementation, health education, nutrition advice and family planning, tobacco and alcohol prevention programmes, vector control and AIDS prevention. As evidence from research emerges, new interventions are added to the existing ones, such as insecticide-treated nets (ITNs) for malaria control. This basic package consists of feasible and highly cost-effective interventions for a bargain price. Some are one-off expenses (for example childhood vaccines) while others represent recurrent cost (for example the insecticide treatment of nets). Unfortunately even this is well above what a large number of low-income country governments are currently investing in the provision of health services. Where is the difference going to come from? Clearly, when government budgets are tight, it must be provided by either the population or an outside donor.", "title": "Editorial: From Rio to Iragua \u2013 sustainability versus efficiency and equity for preventive health interventions", "claims": null}, {"metadata": {"year": 2007}, "authors": ["Shahid  Yusuf", "Kaoru  Nabeshima", "Wei  Ha"], "summary": "The benefits of good health to individuals and to society are strongly positive, and improving the health of the poor is a key millennium development goal (MDG). A typical health strategy advocated by some calls for increased public spending on health targeted to favor the poor backed by foreign assistance, combined with an international effort to perfect drugs and vaccines to ameliorate the major infectious diseases prevalent in developing nations. However, if the objective is better health outcomes at the least cost and a reduction in urban health inequity, our research suggests that the four most potent policy interventions are: improving access to clean water and sanitation; widely available primary care and health programs aimed at influencing diets and lifestyles; raising the level of education; and better urban land use and transport planning which contains urban sprawl and minimizes the trend towards sedentary living habits. The payoff from these four, in terms of health outcomes especially for those in low-income categories, dwarfs the returns from new drugs and curative hospital-based medicine, although these certainly have their place in a modern urban health system. We find, moreover, that the resource requirements for successful health care policies are likely to depend on an acceleration of economic growth rates, which increase household purchasing power and enlarge the pool of resources available to national and subnational governments to invest in and maintain health-related infrastructure and services. Thus, an acceleration of growth rates may be necessary to sustain a viable urban health strategy, which is equitable, and to ensure steady gains in health outcomes.", "title": "Income and Health in Cities: the Messages from Stylized Facts", "claims": null}, {"metadata": {"year": 2012}, "authors": ["M. Massoud", "Nana Mensah-Abrampah", "P. Barker", "S. Leatherman", "E. Kelley", "B. Agins", "S. Sax", "J. Heiby"], "summary": "Research is needed into creating workable systems that can deliver and sustain interventions", "title": "Improving the delivery of safe and effective healthcare in low and middle income countries", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Tim  Evans", "Ariel  Pablos-M\u00e9ndez"], "summary": "www.thelancet.com Published online April 13, 2016 http://dx.doi.org/10.1016/S0140-6736(16)30238-0 1 At the Annual Universal Health Coverage (UHC) Financing Forum in Washington, DC, USA, on April 14\u201315, 2016, governments and development partners will debate how to raise and organise public and private resources needed for low-income and lower-middle-income countries to assure aff ordable, quality health care to all of their people by 2030. The health fi nancing challenge to reach UHC and the health-related Sustainable Development Goals is daunting. The Lancet Commission on investing in health estimated in 2013 that an additional US$70 billion to $90 billion is needed annually to make basic services universally available, which corresponded to a third of low-income and lower-middle income countries\u2019 total health spending in 2013. But as Joseph Dieleman and colleagues show in The Lancet, health expenditure growth will be insuffi cient to meet this fi nancing gap based on current trend projections of government health expenditure: 27 (79%) of 34 low-income countries will still spend less than $86 per capita (a commonly accepted benchmark for provision of a basic package of services in low-income and lower-middle income countries). Moreover, although development assistance will be crucial to help bridge this gap, it will not be suffi cient. To put these countries on a more ambitious trajectory than at present therefore requires a transformation of domestic and development fi nancing for health in line with the Sustainable Development Goal fi nancing agenda endorsed by the UN member states at the Third International Conference of Financing for Development in Addis Ababa, Ethiopia, in 2015. Domestic fi nancing for health in low-income and lower-middle-income countries requires concerted strategies within strengthened public fi nance systems. Resources for health can be raised by growing of government revenues through eff ective tax collection and combating of tax evasion. Use of indirect or socalled sin taxes on consumables such as cigarettes and high-sugar drinks are also growing, which can help generate revenue and promote healthy behaviours. Prohealth subsidies targeted to the poor, such as conditional cash transfers, should be promoted, whereas ineff ective subsidies (eg, for fuel), which can exceed a country\u2019s spending on health, should be eliminated. Likewise, optimisation of the health-enhancing eff ect of other sectors, such as water and sanitation, education, and transport, makes good health and fi scal policy. But governments should also give health a larger share of the public resource envelope than at present. Twelve (35%) of 34 low-income countries\u2019 governments allocate less than 8% of total spending to health, roughly half of the Abuja target endorsed by many African countries in 2001 (several sub-Saharan governments made a commitment to allocate at least 15% of their budgets to the health sector as signatories of the Abuja Declaration). This spending equates, on average, to $12 per capita\u2014far too little to provide even the most basic services to the 360 million people in these countries. Strategic planning, eff ective budget execution, and demonstrable results help to convince ministers of fi nance to create fi scal space for investment in health. As governments increase health expenditures, they and their partners cannot ignore the large share of private spending and must eff ectively harness it for the needs of the health system. Most importantly, high out-of-pocket spending among those who fall ill continue to make up the largest share of health expenditure in most lowincome and lower-middle income countries, with millions of people falling into or remaining trapped in poverty or foregoing care because of its prohibitive costs. Given the size of the informal economy in low-income and lower-middle-income countries, an urgent need exists for innovations that direct private expenditure into prepaid pools; for example, through expansion of Shaping of a new era for health fi nancing", "title": "Shaping of a new era for health financing", "claims": null}, {"metadata": {"year": 2007}, "authors": ["N. Temple"], "summary": "Many health promotion intervention studies have been conducted with the goal of encouraging people to change their lifestyles. These include interventions in the community, at worksites, in a medical setting, and on patients at high risk of disease. These interventions have had, in general, a limited impact (eg, risk of heart disease is lowered by 5%-15%). These interventions were carried out in developed countries, and little is known as to their effectiveness in developing countries such as South Africa. Numerous barriers impede the success of health promotion interventions, including the cost of healthy food, advertising for unhealthy food, and food labels that are confusing. Policy measures are proposed, including government interventions using taxes and subsidies so as to manipulate prices and bring about desirable changes in eating patterns and other aspects of lifestyle. A new type of healthcare professional is suggested with a specialization in health promotion.", "title": "Strategies for health promotion: facing the challenge in South Africa.", "claims": null}, {"metadata": {"year": 2007}, "authors": ["Srijana M. Bajracharya"], "summary": "Introduction Sustaining a healthy community is the goal of every part of the world. However, achieving this goal requires careful planning and organizing by community members, health organizations, academic institutions, and various government agencies. Although, in terms of education, technology, health resources, and per capita purchasing power, the United States is one of the best countries to live in, it falls short in terms of delivering the best health care at a reasonable cost. About 45 million--15.6% of the total population--are not covered by health insurance (U. S. Census Bureau 2003). Yet, the United States, which spends 16% of its GDP on health care, spends more on health care per capita than any other industrialized country. For example, Switzerland and Germany (which also spend a relatively high percentage of their GDP on health care) each spend 11% of their GDP on health care. In 2004, the cost of health care in the United States rose 7.9% to $1.9 trillion, or an average of $6,280/person (National Center for Health Statistics 2006). Obesity and Obesity-related health problems have become a national crisis. National data indicate that people in the lower economic brackets are most vulnerable to such problems. These statistics show why even a highly industrialized country like the United States is looking for innovative ways to meet the health needs of its communities. Health promotion and disease prevention initiatives Disease prevention has become a well-known verity for the health care profession. However, preventive activities, as they are intangible in nature, must be carefully planned. Several government agencies, private and public academic institutions, and even religious organizations have been responding to the national health care crisis for decades. Health promotion and disease prevention initiatives originated in the '70s. While health promotion programs (which address political, economical, and environmental factors) are a much broader disease prevention strategy than health education, health education is the core of these programs (McKenzie, Neiger, & Smeltzer 2005). Hence, health education has become an important part of the programs of all health service organizations (figure 1). Providing effective health education can be challenging. It can be delivered properly only by professional health educators. The current formal definition of health education includes activities beyond merely classroom lectures about health. The responsibilities of today's health educators include not only teaching health effectively, but also assessing the needs of target populations and planning, implementing, and evaluating programs (NCHC 2007). Additionally, professional health educators should be proficient in collaborating and coordinating programs with others and communicating health information to the public. To ensure the quality of health education preparation programs, the National Commission for Health Education Credentialing, Inc. (NCHEC) created the Certified Health Education Specialist (CHES) exam for academically trained health educators. Those who pass the exam indicate that they fulfill all of the competencies of being professional health educators (NCHEC 2007). Meeting the needs of citizens: a national effort Faculty who are preparing professional health educators should provide students with authentic experiential learning so that they can design programs that are effective, acceptable, and innovative. Many faculty across the United States are using service-learning projects to enhance students' learning through hands-on experience. In 1985, the presidents of Brown, Georgetown, and Stanford universities and the president of the Education Commission of the States founded Campus Compact (Campus Compact b 2007), a coalition of over 1,000 college and university presidents devoted to using university resources (including the talents and skills of faculty, students, and staff) to support service-learning projects and other programs to help fellow citizens in need (Campus Compact a 2007). \u2026", "title": "Community-Based Health Education Intervention: A Service-Learning Approach", "claims": null}, {"metadata": {"year": 1999}, "authors": ["D  Mechanic"], "summary": "Policymakers in many countries seek to contain health care costs over the long range by promoting health and more effective health behavior. Such efforts can be directed at entire populations, at members of a health plan, at defined risk groups or single individuals at risk. Many health risks are associated with socio-economic status and social inequalities but these are often difficult to address because of social and political conflicts. Health, also, is often a product of culture and other social circumstances. Health may be promoted through non-health interventions or through more targeted health efforts seeking to effect behavior change. Preventive screening is of growing importance but such efforts often out-pace evidence of efficacy or cost-effectiveness. Many opportunities exist to build on new clinician-patient partnerships to make patients better informed and to effect positive health behavior. New technologies, and particularly the internet, offer new ways to promote health and more constructive illness behaviors.", "title": "Issues in promoting health.", "claims": null}], "query": "what are the most cost-effective interventions in health promotion for low and middle income countries", "summary_abstract": "The research question regarding the most cost-effective interventions in health promotion for low and middle-income countries is addressed through various studies that highlight a range of strategies and their associated costs. \n\nGaziano and Pagidipati (2013) emphasize the importance of both population-level and individual-level interventions to combat chronic diseases, which are increasingly burdensome in these regions. They suggest that effective public health policies should focus on reducing smoking, lowering salt and trans-fatty acid intake, and promoting physical activity.\n\nLengeler (1999) provides a foundational perspective by outlining a basic preventive public health package that can be implemented in resource-poor countries for as little as $4.2 per capita per year. This package includes essential services such as immunizations, health education, and vector control, which are deemed highly cost-effective.\n\nYusuf et al. (2007) identify four potent policy interventions that can yield significant health benefits at minimal costs: improving access to clean water and sanitation, enhancing primary care availability, promoting better dietary and lifestyle choices, and improving education. They argue that these interventions can lead to better health outcomes, particularly for the poor, and are more impactful than investments in new drugs or hospital-based care.\n\nEvans and Pablos-M\u00e9ndez (2016) discuss the financial challenges faced by low-income countries in achieving universal health coverage. They highlight the need for innovative financing strategies, including the use of taxes on unhealthy products to generate revenue while promoting healthier behaviors.\n\nTemple (2007) notes that while health promotion interventions have been implemented in various settings, their effectiveness in low-income countries remains uncertain. She advocates for policy measures such as taxes and subsidies to encourage healthier eating patterns.\n\nIn summary, the literature suggests that cost-effective health promotion interventions in low and middle-income countries include a combination of preventive health packages, access to clean water and sanitation, primary care improvements, and educational initiatives. These strategies not only address immediate health needs but also promote long-term health equity and sustainability.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2018}, "authors": ["S. Yuan"], "summary": "With the development of economic globalization and regional economic clustering, foreign trade and foreign direct investment have become important economic activities to strengthen international economic exchanges and strengthen international division of labor. This paper makes an empirical analysis of Vietnam\u2019s foreign trade, foreign direct investment, and economic growth. The results show that in the long run, there is a cointegration relationship between GDP, foreign trade, and foreign direct investment, and there is long-term stability between them. The equilibrium relationship between foreign trade and foreign direct investment has a significant impact on GDP. There is no interaction between foreign trade and foreign direct investment, as well as foreign direct investment is the Granger cause of GDP growth.", "title": "Vietnam ' s Economic Growth Influenced by Export-Oriented Economy", "claims": null}, {"metadata": {"year": 2011}, "authors": ["\uc774\uae30\ubc30", "\uc774\ubb34\uc6d0", "\ud568\uc601\uc9c4"], "summary": "Vietnam\u2019s economic growth and household income have increased rapidly with accession of WTO and foreign investment. In addition, the proportion of...", "title": "\uae08\uc735\uc5c5\uacc4\uc758 \ubca0\ud2b8\ub0a8 \uc0ac\uc5c5\ud655\ub300 \uc804\ub7b5\uc5f0\uad6c", "claims": null}, {"metadata": {"year": 2009}, "authors": ["J. Thoburn"], "summary": "Vietnam.s development performance since the early 1990s has been one of the strongest in the world, following the introduction of its doi moi (.renovation.) economic reform programme in 1986. The core of Vietnam.s economic strategy has been rapid integration into the world economy, with a diversified portfolio of exports and the attraction of direct foreign investment. This open approach has been combined with successful domestic agricultural growth and a strong, continued role for state-owned enterprises while encouraging growth of the private sector. Following an .East Asian. model, Vietnam has opened its domestic market only slowly while encouraging export growth.", "title": "Vietnam as a Role Model for Development", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Zheng Guo-fu"], "summary": "Vietnam had the implementation of \"Doi Moi\" in 1986.Vigorously developing foreign trade and attracting foreign direct investment(FDI) have become Vietnam\u2032s strategic choice for participating in economic globalization.More than twenty years,Vietnam\u2032s strategies on foreign trade and economic cooperation have made remarkable achievements.The empirical study shows that the development of foreign trade and attraction of foreign direct investment in Vietnam are an essential \"engine\" of the rapid economic growth.", "title": "An Empirical Study on the Relationship between the Economic Globalization and Economic Growth of Vietnam", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Sajid  Anwar", "Lan Phi Nguyen"], "summary": "By making use of a panel dataset that covers 61 provinces of Vietnam over the period 1997 to 2006, this paper examines the link between financial development and economic growth. Our analysis, which is based on endogenous growth theory, reveals that financial development has contributed to economic growth in Vietnam. We find that high ratio of credit to Gross Provincial Product (GPP) has accelerated economic growth in Vietnam. We also found a strong positive link between financial development and economic growth when alternative measures of financial development were used. The impact of foreign direct investment on economic growth will be stronger if more resources are invested in financial market development.", "title": "Financial development and economic growth in Vietnam", "claims": null}, {"metadata": {"year": 2010}, "authors": ["O. Nakamura"], "summary": "Within the transition period to a market oriented economy, Vietnam attracts foreign capitals to strengthen the supply-side economy. Especially, FDI plays a very important role to be achieving an export-led growth and contributes to the steady economic growth at 6-8% per annum. According to the scenario forecasts by means of the global model system, it is expected that the Vietnamese economy is likely to experience a high growth achieving a take-off stage with an export-led growth in the strong economic growth in Asia. It is noteworthy that the Vietnamese macro imbalances including I-S and trade imbalance, and so on, are also expected to be improved if FDI will be increasing.", "title": "Perspective for the Vietnamese Economy in the Context of Asia and the Paci c: An econometric analysis with a global macro econometric model", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Qin Li-fan"], "summary": "Vietnam\u2019s economy maintained rapid development in 2011 though it faced many difficulties and had high inflation. One of its highlights was that its trade deficit further reduced. Vietnam\u2019s foreign trade and investment in other countries kept accelerating. In particular, it accelerated its market exploration in its neighboring countries and Africa.", "title": "Review of Vietnam\u2019s Economic Development in 2011", "claims": null}, {"metadata": {"year": 2006}, "authors": ["Joseph Wann Shang Jye"], "summary": "\u30d9 \u30c8 \u30ca \u30e0 \u306e \u30dd \u30c6 \u30f3 \u30b7 \u30e3 \u30eb 1. Overview of Vietnam\u2019s economy Vietnam remains one of Asia\u2019s fastest growing economies with 8.4% GDP growth in 2005, the highest level in the past nine years, up from 7.7% in 2004. Vietnam\u2019s economic prospects continue to brighten. The economy is rapidly moving from a planned economy to a market-oriented economy, with several key developments which should boost Vietnam\u2019s long-run sustainable growth rate, while also helping to prepare the country for accession to the World Trade Organization. Vietnam is currently the second fastest growing country in East Asia behind China, ahead of Thailand, Malaysia, Singapore, Indonesia and the Philippines. Vietnam, however, is one of the poorest countries in the region, with a per capita income of just USD640 per annum in 2005. This suggests that Vietnam has the potential to catch up and should continue to enjoy strong growth, following the example of other South East and East Asian countries. WTO accession should also boost foreign investor confidence. An increase in Foreign Direct Investment (FDI) in the first month of 2006 heralds a good start for Vietnam\u2019s effort to lure FDI in the remainder of the year. The total capital of newly licensed and capital-increased projects for January 2006 was estimated at USD444 million, up 12% from the same period last year. As Vietnam starts to sign trade deals and joins the international trading system, even more investments will be expected. Furthermore, government efforts are being made to tackle corruption which foreign investors see as a major hindrance to doing business in the country. Laws planned to simplify foreign investment procedures, regulate the tender process and deal with graft should all boost investment. Although many investment managers have given a positive assessment of Vietnam, investors also point out that Vietnam\u2019s investment environment would be more attractive if the country improves its infrastructure, especially in energy, telecommunication and seaports.", "title": "Vietnamese Port Facilities and Logistics Development Overview (\u7279\u96c6 \u30d9\u30c8\u30ca\u30e0\u306e\u30dd\u30c6\u30f3\u30b7\u30e3\u30eb)", "claims": null}], "query": "Vietnam's economic growth has been driven by various factors, including industrialization, increased foreign investment, a growing services sector, and a relatively young and dynamic workforce. The country's strategic location in Southeast Asia and its efforts to attract foreign investments have also played a role in its economic expansion.", "summary_abstract": "The economic growth of Vietnam has been significantly influenced by several key factors, particularly foreign direct investment (FDI), foreign trade, and strategic economic reforms. The empirical analysis conducted by Yuan (2018) highlights a long-term relationship between GDP, foreign trade, and FDI, indicating that FDI serves as a Granger cause of GDP growth, thereby underscoring its critical role in the economic expansion of Vietnam. This aligns with the findings of Zheng (2009), who emphasizes that the development of foreign trade and the attraction of FDI have been essential engines driving Vietnam's rapid economic growth since the implementation of the \"Doi Moi\" reforms in 1986.\n\nThoburn (2009) further elaborates on Vietnam's economic strategy, noting that the country has successfully integrated into the global economy through a diversified export portfolio and the encouragement of both state-owned and private enterprises. This open approach has facilitated significant foreign investment, which has been crucial for sustaining high growth rates. Additionally, Nakamura (2010) points out that FDI has been instrumental in achieving export-led growth, contributing to a steady economic growth rate of 6-8% per annum.\n\nMoreover, the accession to the World Trade Organization (WTO) has also played a pivotal role in enhancing Vietnam's economic landscape. According to the analysis by Anwar and Nguyen (2011), the influx of foreign investment post-WTO accession has led to rapid increases in household income and overall economic growth. This sentiment is echoed by Qin (2012), who notes that Vietnam's foreign trade and investment activities have continued to accelerate, further solidifying its economic position in the region.\n\nIn summary, Vietnam's economic growth has been propelled by a combination of strategic reforms, a focus on attracting foreign investment, and an emphasis on expanding foreign trade. These elements have collectively fostered a dynamic economic environment that has allowed Vietnam to emerge as one of the fastest-growing economies in Southeast Asia.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2019}, "authors": ["T. Chaplya"], "summary": "In the following article the principle characteristics of a cognitive map within its correlation with the mental map and the cognitive and mental environment are viewed and the conclusion upon the fact that they introduce the objective and subjective ideas about space which find their implementation in social behavior models of the society, in the images and sensual experience is made. In the basis of the analysis the concepts of social geography and the theory of geographical images as well as the theories of perception were laid. The fact that the needs of both, a separate individual and the society, that generate domains of various functional use are the source of forming space understanding is shown.", "title": "COGNITIVE MAP \u2013 SENSITIVE AND IMAGINATIVE VISION OF THE ENVIRONMENT", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Zhong Ye-xun"], "summary": "According to the element of non-degenerate interval being equipollence set,the authors proofed that the mapping and compound mapping from the cartographic region to two-dimensional map plane are bi-jection.It makes map symbol satisfiying visual perception and bi-jection function relation between mapping object A__ and map symbol pgf(A__ ) while the character of mapping object is defined by main body intention.The authors have proofed that the map symbol pgf(A__ ) is the homeomorphism of mapping object A__ according to the theorem of set continuity which gives the mathematical basis for the space cognition of maps.", "title": "The mathematical principle of cognition in map space", "claims": null}, {"metadata": {"year": 2018}, "authors": ["T. Behrens", "T. Muller", "James C. R. Whittington", "Shirley Mark", "A. Baram", "Kimberley L. Stachenfeld", "Z. Kurth-Nelson"], "summary": "It is proposed that a cognitive map encoding the relationships between entities in the world supports flexible behaviour, but the majority of the neural evidence for such a system comes from studies of spatial navigation. Recent work describing neuronal parallels between spatial and non-spatial behaviours has rekindled the notion of a systematic organisation of knowledge across multiple domains. We review experimental evidence and theoretical frameworks that point to principles unifying these apparently disparate functions. These principles describe how to learn and use abstract, generalisable knowledge and suggest map-like representations observed in a spatial context may be an instance of general coding mechanisms capable of organising knowledge of all kinds. We highlight how artificial agents endowed with such principles exhibit flexible behaviour and learn map-like representations observed in the brain. Finally, we speculate on how these principles may offer insight into the extreme generalisations, abstractions and inferences that characterise human cognition.", "title": "What is a cognitive map? Organising knowledge for flexible behaviour", "claims": null}, {"metadata": {"year": 2021}, "authors": ["\u00d0ur\u0111ica \u017divkovi\u0107"], "summary": "This thesis analyses the concept of a cognitive map in the research fields of geography. Cognitive mapping research is essential as it investigates the relations between cognitive maps and external representations of space that people regularly use by acquiring spatial knowledge, such as maps in geographic information systems. Moreover, cognitive maps, when expanded on semantic maps, explain the relations between people and things in a non-physically environment, where the considered space is not spanned by distance but with other non-spatially variables. Nevertheless, cognitive maps are often distorted. Although a good formation of a cognitive map is vital in navigation processes, cognitive distortions are barely investigated in the field of geography. By analyzing the relevant work, especially Tobler\u2019s first law of geography, a new lexical variant of Tobler\u2019s first law could be stated that could presumably describe a specific distortion in the processing of landmarks in cognitive maps.", "title": "Principles of Cognitive Maps", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Timothy E.J. Behrens", "Timothy H. Muller", "James C.R. Whittington", "Shirley  Mark", "Alon B. Baram", "Kimberly L. Stachenfeld", "Zeb  Kurth-Nelson"], "summary": "It is proposed that a cognitive map encoding the relationships between entities in the world supports flexible behavior, but the majority of the neural evidence for such a system comes from studies of spatial navigation. Recent work describing neuronal parallels between spatial and non-spatial behaviors has rekindled the notion of a systematic organization of knowledge across multiple domains. We review experimental evidence and theoretical frameworks that point to principles unifying these apparently disparate functions. These principles describe how to learn and use abstract, generalizable knowledge and suggest that map-like representations observed in a spatial context may be an instance of general coding mechanisms capable of organizing knowledge of all kinds. We highlight how artificial agents endowed with such principles exhibit flexible behavior and learn map-like representations observed in the brain. Finally, we speculate on how these principles may offer insight into the extreme generalizations, abstractions, and inferences that characterize human cognition.", "title": "What Is a Cognitive Map? Organizing Knowledge for Flexible Behavior", "claims": null}, {"metadata": {"year": 2010}, "authors": ["A. Samsonovich", "R. Goldin", "G. Ascoli"], "summary": "The notion of a universal semantic cognitive map is introduced as a general indexing space for semantics, useful to reduce semantic relations to geometric and topological relations. As a first step in designing the concept, the notion of semantics is operationalized in terms of human subjective experience and is related to the concept of spatial position. Then synonymy and antonymy are introduced in geometrical terms. Further analysis building on previous studies of the authors indicates that the universal semantic cognitive map should be locally low-dimensional. This essay ends with a proposal to develop a metric system for subjective experiences based on the outlined approach. We conclude that a computationally defined universal semantic cognitive map is a necessary tool for the emerging new science of the mind: a scientific paradigm that includes subjective experience as an object of study. \u00a9 2009 Wiley Periodicals, Inc. Complexity, 2010", "title": "Toward a semantic general theory of everything", "claims": null}, {"metadata": {"year": 2009}, "authors": ["S. M. Freundschuh"], "summary": "Map perception is the process of using the human senses to acquire spatial information about the surrounding environment from maps. Maps, in this sense, are defined quite broadly as any representation that demonstrates spatial relationships (where things are in relation to each other) in an environment. Map cognition, on the other hand, involves mental (or brain) functions and capabilities that facilitate the acquisition and integration of spatial knowledge from maps through reasoning, intuition, and/or perception. To understand the significant breadth and scope of map perception and cognition, one must consider the historical developments of the four different research areas that have evolved in this field. These four areas were essentially born 60 years ago with the publication of Arthur Robinson\u2019s The Look of Maps in 1952. The Look of Maps focused on issues of map design, spawning a large body of psychophysical studies that explored the perception of \u2018graphic marks\u2019, \u2013 what people see when they look at maps. Born from this early psychophysical work was the idea that maps are devices for communication and that they convey a coherent message set forth by the cartographer. Models for communication were borrowed from psychology and adapted for the perception of maps. Next to develop was a very multidisciplinary branch of research on maps and cognition. This body of work explored how maps facilitate spatial knowledge and behavior, and delved into the underlying cognitive structures and functions that maps effect. The most recent body of work is called geographic visualization (GVis). GVis comprises the techniques and technologies (whose development was informed by previous psychophysical studies, previous models for map communication, and previous cognitive models for spatial knowledge and behavior) that facilitate the understanding of complex spatial phenomena via both traditional and nontraditional maps and other spatial representations.", "title": "Map Perception and Cognition", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Fatemeh  Imani", "Marzieh  Tabaeian"], "summary": "Abstract People have some kind of internal model of a geographical space. This research is concerned with aspects of environmental cognition; that is our ability to imagine about our spatial world. In particular, this study makes an attempt to describe cognitive maps and the individual differences between people's map. Data was gathered from a sample of fifty persons. All of them walked through a historical region in Isfahan and were asked to draw a sketch map; the map was then assessed. The major finding was that our spatial knowledge is \u201cmap-like\u201d rather than, say, a sequence of instructions for various routes.", "title": "Recreating mental image with the aid of cognitive maps and its role in environmental perception", "claims": null}], "query": "Cognitive maps as universal organising principle of perception", "summary_abstract": "The research question regarding cognitive maps as a universal organizing principle of perception is addressed through various studies that explore the nature, function, and implications of cognitive maps in understanding spatial and non-spatial environments.\n\nChaplya (2019) emphasizes the relationship between cognitive maps and mental maps, suggesting that both contribute to our understanding of space and influence social behavior. This highlights the subjective and objective dimensions of spatial perception, indicating that cognitive maps are integral to how individuals and societies navigate their environments.\n\nBehrens et al. (2018) propose that cognitive maps facilitate flexible behavior by encoding relationships between entities in the world. They argue that the principles underlying cognitive maps extend beyond spatial navigation to encompass a broader organization of knowledge across various domains. This suggests that cognitive maps serve as a fundamental mechanism for organizing information, supporting the idea that they are a universal principle of perception.\n\nZhong (2005) provides a mathematical foundation for understanding cognitive maps, demonstrating that the relationship between mapping objects and their representations can be characterized through homeomorphism. This mathematical perspective reinforces the notion that cognitive maps are structured and systematic, further supporting their role as an organizing principle.\n\n\u017divkovi\u0107 (2021) discusses the importance of cognitive maps in geographic information systems, noting that they help individuals acquire spatial knowledge. However, she also points out that cognitive distortions can affect the accuracy of these maps, which is crucial for effective navigation.\n\nSamsonovich et al. (2010) introduce the concept of a universal semantic cognitive map, proposing that it can index semantic relations geometrically. This approach suggests that cognitive maps can also organize non-spatial information, reinforcing their role as a universal cognitive tool.\n\nFreundschuh (2009) differentiates between map perception and map cognition, indicating that maps are not only tools for spatial representation but also facilitate the integration of spatial knowledge through cognitive processes. This distinction underscores the complexity of how cognitive maps function in perception.\n\nLastly, Imani and Tabaeian (2012) highlight individual differences in cognitive maps, showing that people's internal models of geographical spaces are inherently \"map-like.\" This finding supports the idea that cognitive maps are a fundamental aspect of how individuals conceptualize their environments.\n\nCollectively, these studies illustrate that cognitive maps are indeed a universal organizing principle of perception, facilitating the understanding of both spatial and non-spatial relationships, while also being subject to individual variations and cognitive distortions.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2007}, "authors": ["M. Lewrick", "R. Raeside", "T. Peisl"], "summary": "The concept of innovation has evolved significantly over the last decade. It now appears that inherent successful innovation can be explained by the influence of the social networks and social capital. Originally based on tangible forms of capital and the necessity of technology pull and push, innovation management is now integrated in a larger system. The innovator\u2019s social network contains knowledge and information obtained from multiple connections in the social capital embedded in organisational and Inter-organisational network. This study shows the impact of inter-organisational, personal and business network in different sectors. The impact of the different networks on start-ups and mature companies, and how this is related to different performance levels in mature companies, is also outlined.", "title": "THE INNOVATORS'S SOCIAL NETWORK A cross-sector exploration on the influence of social networks and social capital on innovation and success.", "claims": null}, {"metadata": {"year": 2007}, "authors": ["M. Lewrick", "R. Raeside", "T. Peisl"], "summary": "The concept of innovation has evolved significantly over the last decade. It now appears that inherent successful innovation can be explained by the influence of the social networks and social capital. Originally based on tangible forms of capital and the necessity of technology pull and push, innovation management is now integrated in a larger system. The innovator\u2019s social network contains knowledge and information obtained from multiple connections in the social capital embedded in organisational and Inter-organisational network. This study shows the impact of inter-organisational, personal and business network in different sectors. The impact of the different networks on start-ups and mature companies, and how this is related to different performance levels in mature companies, is also outlined.", "title": "The Innovators\u2019s Social Network", "claims": null}, {"metadata": {"year": 2001}, "authors": ["Knut  Koschatzky"], "summary": "Networks deal with people, with firms and other institutions, and with their social and economic interaction. Economic and social networks are, of course, affected by physical networks like the Internet, telecommunication and traffic networks. Although important, the latter are not the subject of further discussion here. Social and economic networking is not a new phenomenon, but the basis of early trade which developed several thousand years ago, and, perhaps even more important, of urbanisation, the root of modern civilisation. In recent years, networking became an important subject in deductive and inductive theoretical disquisitions as well as in policy actions.", "title": "Networks in Innovation Research and Innovation Policy \u2014 An Introduction", "claims": null}, {"metadata": {"year": 2005}, "authors": ["N. Agapitova"], "summary": "The theoretical framework of national innovation systems constitutes an important step forward in understanding the nature of technological change and sources of national competitiveness. We argue, however, that studies of innovative activities of individual actors and related institution-building processes are incomplete without taking into account the social structures that underlie economic actions. In this paper, we develop a conceptual model that introduces the analysis of social networks into the study of technological and institutional evolutions of national innovation systems.", "title": "THE ROLE OF SOCIAL NETWORKS FOR NATIONAL INNOVATION SYSTEMS' DYNAMICS", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Alfonso Badiola-S\u00e1nchez", "P. D. Castro", "P. Coto-Mill\u00e1n", "M. A. Pesquera"], "summary": "This research analyses the relationship between the use of social networks and the generation of innovations in European regions through a production function of innovations using human capital, technological capital and virtual relational capital with the new theoretical approach. For this, it is estimated an econometric model of panel data for the period 2007-2011 and 190 European regions.", "title": "The impact of social networks on innovation: The case of European regions", "claims": null}, {"metadata": {"year": 2012}, "authors": ["J. Steen", "Sam Macaulay"], "summary": "Many years ago Joseph Schumpeter, the founder of the economics of innovation, described innovation as the process of searching for and recombining knowledge. Even this most fundamental model of innovation implies a process of finding the right connections through search and then consolidating those connections.", "title": "The past, present and future of social network analysis in the study of innovation", "claims": null}, {"metadata": {"year": 2011}, "authors": ["S. Keskin"], "summary": "The conception of innovation has evolved quite drastically over the last 60 years. This evolution has generated two results: first, innovation is no longer considered as a discrete occurrence only engaging the development of technical solutions, but as a process also engaging interactions between all parts of society; second, in the social network theories, innovation results from the combinations of tangible forms of capital in conjuction with intangible forms of capital described by disorderly and sustained interactions occurring between economic actors. This study introduces selected concepts from the analysis of social networks and integrates them into National Innovation System (NIS) conceptual framework. The application of social networks into NIS provides a deeper understanding of mechanisms driving industrial development.", "title": "SOCIAL NETWORKS, INNOVATIONS AND NATIONS", "claims": null}, {"metadata": {"year": 2022}, "authors": ["B. Lopez", "Isabel Maria Santos Caetano", "Antonio Vargas Alcaide"], "summary": "In an increasingly digitized world and after the disruption of the covid-19 pandemic, the internet and social networks are becoming the central backbone of interactions between individuals, organizations, and governments worldwide in general and defending the democratic values in Ukraine in particular. The influence capacity of social networks on creativity and innovation has grown in recent years and has been an objective of the study. This paper analyses the interaction between innovation and social networks from a balanced assessment, considering opportunities and risks. The interaction between social networks and innovation could redesign the value creation process. The proliferation of social media platforms coincides with the expansion of the open innovation paradigm and has demonstrated its efficiency in facilitating solutions in different fields such as science, statistics, engineering, production and the generation of social policies. On the other hand, risks have to be analyzed and mitigated. The social networks and search engines could have become a proxy for organizing and accessing information and knowledge on a large scale; however, evidence points out how the fake data and concentration could suffocate innovation. The research develops a theoretical framework to analyze how the organizational structure of social networks could influence the knowledge absorption capability and innovation; what is the influence of the social networks on creativity and innovation; and their role as drivers to create the social value. The results of the research could be practically valuable for many stakeholders: Chief Innovation Officers and Communication Managers, Teams responsible for Stakeholders Engagement and Open Innovation programmes, Policy Makers, and the Scientific Community interested in developing empirical research on the topic, as well as citizens to understand their role as change-makers contributing to developing Innovation and Creativity.", "title": "Innovation And Social Networks For Creating Social Value", "claims": null}], "query": "foundations of social network theory and its impact in innovation", "summary_abstract": "The exploration of social network theory reveals its significant impact on innovation, as evidenced by various studies. Lewrick, Raeside, and Peisl (2007) emphasize that successful innovation is increasingly understood through the lens of social networks and social capital, highlighting how these networks provide essential knowledge and information that drive innovation in both start-ups and established companies. Their research illustrates the varying effects of inter-organizational, personal, and business networks across different sectors, suggesting that the structure and quality of these networks can influence performance levels in mature companies.\n\nKoschatzky (2001) further contextualizes the role of networks, noting that social and economic interactions have been foundational to trade and urbanization, which are critical to modern innovation systems. This historical perspective underscores the enduring relevance of networking in fostering innovation.\n\nAgapitova (2005) introduces a conceptual model that integrates social network analysis into the study of national innovation systems, arguing that understanding the social structures underlying economic actions is crucial for comprehending technological change and national competitiveness. This approach aligns with Keskin's (2011) assertion that innovation is not merely a technical endeavor but a process involving interactions across society, where social networks play a pivotal role in combining various forms of capital.\n\nBadiola-S\u00e1nchez et al. (2014) provide empirical evidence of the relationship between social networks and innovation generation in European regions, reinforcing the idea that social capital is a vital component of the innovation process. Steen and Macaulay (2012) echo this sentiment by referencing Schumpeter's model of innovation, which emphasizes the importance of finding and consolidating connections to facilitate knowledge recombination.\n\nFinally, Lopez et al. (2022) discuss the contemporary relevance of social networks in a digitized world, particularly in the wake of the COVID-19 pandemic. They highlight the dual nature of social networks as both enablers and potential disruptors of innovation, advocating for a balanced assessment of their influence on creativity and the value creation process.\n\nCollectively, these studies illustrate that social network theory is foundational to understanding innovation dynamics, emphasizing the critical role of social capital and interconnections in fostering innovative activities across various contexts.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2017}, "authors": ["A. Levenets", "I. Kolodiy", "N. V. Berezhnaya", "Y. S. Lipovskaya"], "summary": "The structure and mechanical properties of high-entropy CoCrFeMnNi (equiatomic) and Co20Cr26Fe20Mn20Ni14 alloys, which differ significantly in the stacking-fault energy, are studied. The structure of the alloys was investigated in three states \u2013 as-cast, after homogenizing annealing at T = 1000 \u00b0C for 24h and after annealing at T = 850 \u00b0C. It was found that in the cast state and after homogenizing annealing at 1000 \u00b0C both alloys are single-phase solid solutions with a FCC lattice. Annealing at 850 \u00b0C preserves the single-phase state in the equiatomic CoCrFeMnNi alloy, but leads to the appearance of a \u03c3-phase in the Co20Cr26Fe20Mn20Ni14 alloy. The mechanical properties of these alloys were studied in a single-phase state by uniaxial compression tests and hardness measurements. It was discovered, that both alloys in a single-phase state had high plasticity and tendency to force strain hardening. The behavior of hardening coefficients in the area of true strains e = 0.05 \u2013 0.17 vary considerably. This may be due to a significant difference in the stacking-fault energy of the investigated alloys. As a result, in Co20Cr26Fe20Mn20Ni14 alloy along with dislocation deformation mechanism there is a high probability of implementation the twinning mechanism.", "title": "INFLUENCE OF COMPOSITION AND MICROSTRUCTURE ON THE FEATURES OF MECHANICAL PROPERTIES OF Co-Cr-Fe-Mn-Ni HIGH ENTROPY ALLOYS", "claims": null}, {"metadata": {"year": 2017}, "authors": ["A. Levenets", "I. Kolodiy", "N. V. Berezhnaya", "Y. S. Lipovskaya"], "summary": "The structure and mechanical properties of high-entropy CoCrFeMnNi (equiatomic) and Co20Cr26Fe20Mn20Ni14 alloys, which differ significantly in the stacking-fault energy, are studied. The structure of the alloys was investigated in three states \u2013 as-cast, after homogenizing annealing at T = 1000 \u00b0C for 24h and after annealing at T = 850 \u00b0C. It was found that in the cast state and after homogenizing annealing at 1000 \u00b0C both alloys are single-phase solid solutions with a FCC lattice. Annealing at 850 \u00b0C preserves the single-phase state in the equiatomic CoCrFeMnNi alloy, but leads to the appearance of a \u03c3-phase in the Co20Cr26Fe20Mn20Ni14 alloy. The mechanical properties of these alloys were studied in a single-phase state by uniaxial compression tests and hardness measurements. It was discovered, that both alloys in a single-phase state had high plasticity and tendency to force strain hardening. The behavior of hardening coefficients in the area of true strains e = 0.05 \u2013 0.17 vary considerably. This may be due to a significant difference in the stacking-fault energy of the investigated alloys. As a result, in Co20Cr26Fe20Mn20Ni14 alloy along with dislocation deformation mechanism there is a high probability of implementation the twinning mechanism.", "title": "Ng INFLUENCE OF COMPOSITION AND MICROSTRUCTURE ON THE FEATURES OF MECHANICAL PROPERTIES OF Co-Cr-Fe-Mn-Ni HIGH ENTROPY ALLOYS", "claims": null}, {"metadata": {"year": 2020}, "authors": ["M.  Klimova", "D.  Shaysultanov", "A.  Semenyuk", "S.  Zherebtsov", "G.  Salishchev", "N.  Stepanov"], "summary": "Abstract Specimens of a CoCrFeMnNi high entropy alloy with different content of nitrogen (0.5\u20132.0\u00a0at.%) were prepared by induction melting. Microstructure and tensile mechanical behavior of the alloys in the as-cast condition were analyzed. The alloys with a low amount of N (0.5\u20131.0\u00a0at.%) had a single fcc phase coarse-grained microstructure. An increase in the content of N to 2.0\u00a0at.% resulted in the precipitation of a small amount (\u223c1%) of Cr-rich M2N nitride particles. The yield strength of the alloys increased in proportion to the percentage of nitrogen by 117\u00a0MPa/at% N at 293\u00a0K or by 316\u00a0MPa/at% N at 77\u00a0K. The observed increase in strength was attributed to solid solution hardening. Ductility of the alloy with 0.5 or 1.0\u00a0at.% of N increased with a decrease in the testing temperature while ductility of the alloy with 2\u00a0at.% dropped from 67% at 293\u00a0K to 8% at 77 K. The plastic deformation of the alloys at both 77\u00a0K or 293\u00a0K was mostly associated with planar dislocation slip. No signs of mechanical twinning were found even at 77\u00a0K. Thermo-Calc software was used to produce a quasi-binary CoCrFeMnNi\u2013N phase diagram for comparing the experimental and calculated results. The effect of N on strengthening and deformation mechanisms at different temperatures was discussed.", "title": "Effect of nitrogen on mechanical properties of CoCrFeMnNi high entropy alloy at room and cryogenic temperatures", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Wenqiang Feng", "Y. Qi", "Shaoqing Wang"], "summary": "The effects of Mn and Al additions on the structural stability and magnetic properties of FeCoNi-based alloys were investigated by density functional theory. The simulation results show that adding Al into FeCoNi alloy leads to the transformation of alloy structure from FCC to BCC with B2 structure. The doping of non-magnetic Al reduces the saturation magnetization, regardless alloy structure is FCC, BCC or B2 phase. The FeCoNiMn alloy prefers to form L12 structure to diminish the magnetic frustration. In the L12 structure, Mn minority spin peak moves from above to below Fermi level, further increasing Mn magnetic moment. For FeCoNi(MnAl)x alloys, the alloy structures change from FCC to BCC with the increase of Mn and Al content. The Mn atoms are transformed from anti-ferromagnetic to ferromagnetic order and the saturation magnetizations are significantly enhanced due to phase transition. It is found that there is obvious short-range order (SRO) in FeCoNi(MnAl)2 alloy. The SRO parameters are positive between Al\u2013Al, Co-Co, Co-Ni pairs and negative between Ni-Al, Co-Al, Co-Mn, and Mn-Ni pairs. The electronic structure near the Fermi level shows significant difference from random solid structure, which will affect magnetic and electrical properties of FeCoNi(MnAl)2 alloy.", "title": "Effects of Mn and Al addition on structural and magnetic properties of FeCoNi-based high entropy alloys", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Wenqi Luo", "Qin Zou", "Yang Li", "X. Ye", "Xiaowei Yang", "Jintao Song", "Yong\u2019an Luo"], "summary": "Abstract CoCrNiCuX (X = Mn, MnFe, FeAl) high-entropy alloys were prepared by mechanical alloying and spark plasma sintering. The relationship between microstructural and mechanical properties of CoCrNiCuX powders and bulks was investigated. The effects of the addition and substitution of elements on crystal structure transition, microstructural evolution, and mechanical properties were assessed. The results indicated that, with the addition of Fe, the crystal structure of the alloys was transformed from fcc phase to a mix of fcc and bcc phases. By substituting Mn with Fe and Al, the formation of bcc phase was promoted, which caused precipitation hardening that significantly improved the hardness and compression strength of the investigated high-entropy alloys.", "title": "Microstructure and mechanical properties of CoCrNiCuX high-entropy alloys fabricated by spark plasma sintering", "claims": null}, {"metadata": {"year": 2023}, "authors": ["P. Orsatti", "P. Mignanelli", "T. Slater", "H. Stone", "N. Jones"], "summary": ".", "title": "On the in \ufb02 uence of Mn on the phase stability of the CrMn x FeCoNi high entropy alloys", "claims": null}, {"metadata": {"year": 2017}, "authors": ["D. Mitric\u0103", "V. Soare", "I. Constantin", "M. Burada", "M. Olaru", "V. Badilita", "V. Soare", "F. Stoiciu", "G. Popescu", "I. Carcea"], "summary": "In the present paper, high entropy alloys based on an aluminum-chromium-iron-manganese-nickel (Al-Cr-Fe-Mn-Ni) system were prepared by induction melting and annealed in an inert atmosphere. The resulting samples were analyzed by optical microscopy, scanning electron microscopy, and X-ray diffraction to determine the structural characteristics before and after the heat treatment process. Significant phase transformations and changes in the phase distribution were noticed after the heat treatment process. The results were discussed against the thermodynamic criteria calculations for most promising compositions. Hardness tests were provided for the selected samples to indicate the changes in the mechanical properties between various compositions and between as-cast and annealed samples. Results indicated that the heat treatment process determined a significant hardness increase in one of the studied high entropy alloys.", "title": "Influence of the Heat Treatment Processes on the Properties of High Entropy Alloys Based on Al-Cr-Fe-Mn-Ni System", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Hui Jiang", "Li Jiang", "Yiping Lu", "Tongmin Wang", "Z. Cao", "Tingju Li"], "summary": "The elements Mo, Cr and V were added to the W-Ni-Co system high entropy alloys, the effects of these added elements on microstructure and mechanical properties of these alloys were studied. The alloys were produced by vacuum arc melting. The compositions were W0.5Ni2Co2VMo0.5, W0.5Ni2Co2VCr0.5 and W0.5Ni2Co2CrMo0.5 (denoted as Alloy 1, Alloy 2 and Alloy 3) respectively. The theoretical melting temperatures were higher than 2000 K. X-ray diffraction, SEM and energy dispersive spectroscopy (EDS) results indicated that the matrix of the alloys is face-centered cubic (FCC) solid-solution, the alloys showed dendrite crystal structure. Ni, Co elements were enriched in the dendrite areas, the W, Mo were enriched in the inter-dendrite regions ,while V, Cr elements were uniform distribution. The Vickers hardness of these alloys was 376.1 HV, 255.88 HV and 306.8 HV, respectively. The yield strength values (\u03c30.2) of Alloy 1, Alloy 2 and Alloy 3 were approximately 1000MPa, 750MPa, 250MPa, respectively. The alloys show good compression plasticity deformation capacity at RT.", "title": "Microstructure and Mechanical Properties of the W-Ni-Co System Refractory High-Entropy Alloys", "claims": null}], "query": "Influence of Mn on properties of Co-Cr-Fe-Mn-Ni high entropy alloy systems", "summary_abstract": "The influence of manganese (Mn) on the properties of Co-Cr-Fe-Mn-Ni high entropy alloy systems has been explored in several studies, revealing significant insights into their structural and mechanical characteristics.\n\nLevenets et al. (2017) investigated the CoCrFeMnNi and Co20Cr26Fe20Mn20Ni14 alloys, noting that both alloys maintained a single-phase face-centered cubic (FCC) structure in their as-cast and homogenized states. The study highlighted that the stacking-fault energy differences between the alloys affected their mechanical properties, with both exhibiting high plasticity and strain hardening tendencies. The Co20Cr26Fe20Mn20Ni14 alloy, however, showed a transition to a \u03c3-phase upon annealing, which could influence its mechanical behavior.\n\nKlimova et al. (2020) focused on the effect of nitrogen content in CoCrFeMnNi alloys, finding that increasing nitrogen levels led to the formation of Cr-rich M2N nitride particles, which enhanced yield strength through solid solution hardening. This study emphasized the role of alloying elements in modifying mechanical properties, although it primarily concentrated on nitrogen rather than manganese.\n\nFeng et al. (2018) examined the structural stability and magnetic properties of FeCoNi-based alloys with Mn additions. Their findings indicated that Mn contributed to the formation of a stable L12 structure, which enhanced the magnetic moment and influenced the overall magnetic properties of the alloy. This suggests that Mn not only affects mechanical properties but also plays a crucial role in the magnetic characteristics of high entropy alloys.\n\nLuo et al. (2022) explored the CoCrNiCuX (X = Mn, MnFe, FeAl) alloys, noting that the addition of Mn influenced the transition from FCC to a mixture of FCC and body-centered cubic (BCC) phases. This phase transformation was linked to improved hardness and compression strength, indicating that Mn can significantly alter the mechanical properties of these high entropy alloys.\n\nIn summary, manganese plays a pivotal role in determining the structural and mechanical properties of Co-Cr-Fe-Mn-Ni high entropy alloys. Its influence is evident in phase stability, mechanical strength, and even magnetic properties, as highlighted by the studies of Levenets et al. (2017), Klimova et al. (2020), Feng et al. (2018), and Luo et al. (2022). The collective findings underscore the importance of Mn as a key alloying element in enhancing the performance of high entropy alloys.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2013}, "authors": ["S. Colles", "Shweta Singh", "Chhavi Kohli", "A. Mithal"], "summary": "Background: Almost 15% of India's urban adult populace now lives with type 2 diabetes. This study aimed to characterize the eating patterns, knowledge, beliefs, and determinants of food choice, and assess associations with the metabolic health among urban Asian Indians with type 2 diabetes. Materials and Methods: A cross-sectional study of 258 individuals (mean age 55.7 \u00b1 10 years; body mass index 27.1 \u00b1 4.8 kg/m2; diabetes duration 10.1 \u00b1 6.5 years) attending two out-patient clinics in New Delhi, India. Food-related information was collected during a semi-structured interview. Clinical, anthropometric, and biochemical data were recorded. Results: Beliefs related to health and diabetes played a role determining food choice and dietary patterns; erroneous views were associated with the poor food choices and greater metabolic perturbations. Average consumption of fruits/vegetables was low. Intakes were positively associated with intentions to manage diabetes; inversely associated with the waist circumference and negatively correlated with one's degree of personal responsibility for food choice. Household saturated fat usage was common. High fat intakes were positively associated with the taste preference, ratings of perceived \u201chealth-value;\u201d waist circumference, glycosylated haemoglobin percentage (HbA1c%) and lipids. Conclusions: Strategies to enhance diabetes control among Asian Indians are required and should encourage fruit/vegetable intake, personal accountability, and consider individual beliefs and preferences. Greater emphasis and resources directed to regular dietary and behavioral counseling may assist.", "title": "Dietary beliefs and eating patterns influence metabolic health in type 2 diabetes: A clinic-based study in urban North India", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Jeetendra Yadav", "Shaziya Allarakha"], "summary": "The level of morbidity and mortality due to diabetes and its potential complications are enormous and pose significant healthcare burden on both families and society. In India almost 9% people are affected by diabetes mellitus. Furthermore, in recent years, health seeking behaviour of the people has been affected due to increasing Health Care Expenditure. The healthcare burdens in this regard pose a major challenge to the sustainable development goals (SDGs) which are dedicated to decrease one-third of early mortality from noncommunicable diseases by 2030 (UN, 2015). The present study aims to examine the socioeconomic and demographic factors that affect Utilization of Health Services and health care expenditure across socioeconomic backgrounds of the Diabetes Mellitus patients. The purpose is to identify those vulnerable to these costs and implement policies on improving the health systems. The study used data from the 71 st round of the National Sample Survey Organisation (NSSO 2014-2015). Statistical tests of association were conducted to identify the significant variables and bivariate logistic regression was applied to study the effect of these variables on the outcome. The study revealed that there exists a statistically significant difference in utilization of health services and health care expenditure among socioeconomic groups. Rural residents, Scheduled Tribes and poorer sections were the ones who utilize the public health care facilities more. Health care expenditure in private health care facilities was higher as compared to the public health care facilities. It was found that there are considerable Socioeconomic differentials in utilization of health services. Thus, experiencing higher health care expenditure as well as depending more on inefficient mechanism to finance their health expenditure. Considering the Socioeconomic differentials in utilization of health services and health care expenditure, the study recommends that making accessible availability of health care services and implementing policy of public health as first choice for health care, particularly for Rural residents, Scheduled Tribes and poorer sections should be taken as priority. We also suggest reducing health care costs by improving availability of affordable health care facilities for all may go long way in reducing the burden of Diabetes Mellitus epidemic in India. Introduction and review of literature Diabetes mellitus constitutes a group of metabolic disorders characterised by hyperglycaemia arising as a result of defects in insulin action or secretion or both [01]. The disease is very challenging to manage as it affects almost all the body tissues giving rise to several comorbidities and complications [02-03]. The WHO global report on diabetes says that the burden of diabetes has almost doubled since 1980s, the age -standardised prevalence in the adult population being 4.7% in 1980s and 8.5% in 2014. To address the growing burden of diabetes, the \u201eGlobal Action Plan\u201f for the Prevention and Control of NCDs \u201e2013-2020\u201f was sanctioned over 190 countries under the leadership of the WHO with the aim of reducing the diabetes burden [04-05]. Once thought of as a disease of the affluent sections of the society, this disease is now emerging as a pandemic with India being a major epicentre. Studies have suggested that almost 75% of the diabetic population resides in low and middle-income countries which are expected to witness the highest increase in the burden of this disease. Thus, further contributing to the heightened economic crisis situation for these countries [06]. India is emerging as a diabetes capital on the global map. The country stands at second position after china with over 69 million people suffering from diabetes (IDF, 2013 and IDF, 2015). Around 9.3% Indians are affected by diabetes and the burden is increasing at a daunting rate in the first quarter of the 21 st century (IDF, 2013). Hence, it poses a huge challenge to the ambitious target of reducing the premature mortality from NCDs (including diabetes) by 1/3 rd as a part of the Sustainable Development Goals (SDG) by 2030 (UN, 2015). The explosive increase in the burden of Diabetes in the country is to a large extent due to rapid changes in lifestyle associated with increased urbanisation and socioeconomic development. The situation is even more concerning as Indians have a higher likelihood of developing Diabetes, and that too at a lower age and BMI, than rest of the world [07]. Thus, the financial burden of Diabetes, in a developing country like India, is a very crucial subject. Moreover, there are huge differences in both,", "title": "Differentials in Health Care Seeking Behaviour and Health Care Expenditure among Diabetes Mellitus Patient in India", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Aisha Bhimla"], "summary": "Methods: Data was collected during a medical mission trip in eight medical clinics in Nagercoil, Tamil Nadu, India. A crosssectional study was conducted using information from a patient intake form and a diabetes questionnaire that was administered to newly and previously diagnosed DM patients. Body mass index (BMI), systolic and diastolic blood pressure, and waist circumference were measured.Patients were also administered theglycohemoglobin test (HbA1c).", "title": "Examining the prevalence of obesity and cardiovascular disease risk factors in type 2 diabetic patients living in rural South India", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Sarika  Palepu", "Binod Kumar Patro"], "summary": "Chronic diseases are on a constant rise in developing countries. India ranks second globally in terms of diabetes burden. In this case report, an attempt was made to analyse the health care-seeking behaviour of Mrs. SM, receiving treatment at Non-Communicable Diseases Prevention Clinic, All India Institute of Medical Sciences, Bhubaneswar. The patient travels a long distance to seek treatment at AIIMS although she suffers from financial hardships. The pathway of care included referral from Primary Health Centre to tertiary health care facility and back referral. This prompted the authors to study the health care-seeking behaviour with the aid of Health Belief Model. Analysis of the model revealed the enabling factors for treatment adherence and treatment continuity. The present study also analysed the chronic disease care gap from the health system perspective and focused on the need for reformatory measures.", "title": "Management of diabetes mellitus: need for empowering primary health care", "claims": null}, {"metadata": {"year": 2013}, "authors": ["B. Baur", "N. Manna", "Jhuma Sarkar", "G. Basu", "L. Bandyopadhyay", "K. Bhattacharya"], "summary": "The report of World Health Organization (WHO) shows that India tops the world with largest number of diabetic subjects. This increase is attributed to the rapid epidemiological transition accompanied by urbanization and changes in the life style, which is occurring in India. A cross-sectional descriptive study was conducted in the Department of Biochemistry of Calcutta National Medical College among the patients who were doing first time blood examination for diagnosis of diabetes. The objective of this study was to assess socio-demographic profile and risk factors associated with the diabetic patients. Prevalence of diabetes was more (63.6%) among those who had BMI (Body Mass Index) 25 or more, where as the proportion of diabetes was less among the persons having BMI less than 18.5. This association was found to be statistically significant (\u03c72 = 32.78, df = 2 p = 0.0001). Prevalence of diabetes was also high among persons consuming tobacco (34.5%) (\u03c72 = 14.76, df = 1, p = 0.0001) and animal protein (51.5%) (\u03c72 = 55.65, df = 1, p = 0.0001). Lifestyle modifications, reduction of body weight, increasing physical activity are some of the important primary preventive measures to halt the upward trend of diabetes epidemic in India.", "title": "An epidemiological study on risk factors of diabetes mellitus among the patients attending a tertiary care hospital of West Bengal, India", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Shashank R Joshi"], "summary": "BACKGROUND\nDiabetes has become a major health care problem in India with an estimated 66.8 million people suffering from the condition, representing the largest number of any country in the world.\n\n\nOBJECTIVE\nThe rising burden of diabetes has greatly affected the health care sector and economy in India. The goal of health care experts in India is to transform India into a diabetes care capital in the world.\n\n\nMETHODS\nAn expert detailed review of the medical literature with an Asian Indian context was performed.\n\n\nFINDINGS\nRecent epidemiologic studies from India point to a great burden from diabetes. Diabetes control in India is far from ideal with a mean hemoglobin A1c of 9.0%-at least 2.0% higher than suggested by international bodies. Nearly half of people with diabetes remain undetected, accounting for complications at the time of diagnosis. Screening can differentiate an asymptomatic individual at high risk from one at low risk for diabetes. Despite the large number of people with diabetes in India, awareness is low and needs to be addressed. Other challenges include balancing the need for glycemic control with risk reduction due to overly tight control, especially in high-risk groups and taking into account health care professional expertise, attitudes, and perceptions. Pharmacologic care should be individualized with early consideration of combination therapy. Regular exercise, yoga, mindful eating, and stress management form a cornerstone in the management of diabetes.\n\n\nCONCLUSIONS\nConsidering the high cost incurred at various steps of screening, diagnosis, monitoring, and management, it is important to realize the cost-effective measures of diabetes care that are necessary to implement. Result-oriented organized programs involving patient education, as well as updating the medical fraternity on various developments in the management of diabetes, are required to combat the current diabetes epidemic in India.", "title": "Diabetes Care in India.", "claims": null}, {"metadata": {"year": 2017}, "authors": ["Jasmine Hentschel", "S. Sherugar", "Rui Zhou"], "summary": "In India, where diabetes is a growing concern and approximately 69 million are affected, we investigate the factors that influence diet management, a critical component of living with the disease. Taking the middle-income diabetes-affected household as our unit of analysis, we use a combination of semi-structured interviews and a design probe to understand if and how diets are monitored, tailored, and balanced. We research the various information-seeking behaviors of our participants and their culturally situated approaches to food and eating. Our findings illuminate how contextual nuances shape individuals\u2019 beliefs around dealing with diabetes and the ways in which family, friends, and broader social networks influence dietary decisions. We conclude by offering a framework of Learning-Being-Doing to inform the holistic design of technologies for managing diets and diabetes.", "title": "Diets and Diabetes in Urban Indian Households", "claims": null}, {"metadata": {"year": 2019}, "authors": ["G.F.  L\u00f3pez S\u00e1nchez", "L.  Smith", "R.  Raman", "D.  Jaysankar", "S.  Singh", "R.  Sapkota", "A.  D\u00edaz Su\u00e1rez", "S.  Pardhan"], "summary": "Summary Aim To describe and analyse physical activity behaviour and barriers and facilitators, in adults with diabetes residing in India. Methods The sample consisted of 190 (110 men and 80 women) adults with diabetes living in India. The mean age of the sample was 57 years (18 to 83 years), and the mean duration since diagnosis of diabetes was 10.38 years (SD 8.41 years). All participants completed a questionnaire about their physical activity behaviour, importance of, and barriers and facilitators. Results In all, 78.9% of participants thought that physical activity was important in the control of diabetes. In all, 54.7% of respondents did regular physical activity every week, and physical activity was more frequent in men (63.6%), and in those with an intermediate (60%) or advanced (65.2%) level of English. Barriers that discouraged participants from physical activity were lack of time (31.6%), lack of knowledge (23.2%) and health limitations (17.4%). Potential identified facilitators included more information about physical activity (39%) and group exercise with people of the same sex or speaking the same language. Conclusions Most of the sample thought that physical activity is important to control diabetes, although few participated in regular physical activity. Findings also suggest that physical activity had a higher importance for men, for those with an advanced level of English and those under 60 years of age. Men, those with intermediate or advance level of English, and people under 60 years were significantly more active.", "title": "Physical activity behaviour in people with diabetes residing in India: A cross-sectional analysis", "claims": null}], "query": "Preferences of health seeking for patients with diabetes and obesity in India", "summary_abstract": "The research on health-seeking preferences among patients with diabetes and obesity in India reveals several critical insights into the factors influencing their healthcare behaviors and dietary choices. \n\nColles et al. (2013) highlight that beliefs about health and diabetes significantly impact food choices among urban Asian Indians. Their study found that low consumption of fruits and vegetables was linked to erroneous health beliefs, which in turn correlated with poorer metabolic health outcomes. This suggests that enhancing dietary education and personal accountability could improve diabetes management.\n\nYadav and Allarakha (2019) examined the socioeconomic factors affecting healthcare utilization among diabetes patients, revealing that rural residents and economically disadvantaged groups tend to rely more on public healthcare facilities. They emphasized the need for policies that improve access to affordable healthcare, particularly for vulnerable populations, to alleviate the financial burden associated with diabetes management.\n\nBaur et al. (2013) and Joshi (2015) both underscore the rising prevalence of diabetes in India, attributing it to lifestyle changes and urbanization. They advocate for lifestyle modifications, including increased physical activity and dietary changes, as essential strategies to combat the diabetes epidemic. Joshi further notes that awareness of diabetes management is low, indicating a need for organized educational programs.\n\nHentschel et al. (2017) explored dietary management among middle-income households, finding that social networks and cultural contexts significantly influence dietary decisions. This highlights the importance of considering social dynamics when designing interventions for diabetes management.\n\nLastly, L\u00f3pez S\u00e1nchez et al. (2019) focused on physical activity behaviors among adults with diabetes, noting that while many recognize its importance, participation in regular physical activity is low. Barriers such as lack of time and knowledge were identified, suggesting that targeted interventions could help increase physical activity levels.\n\nCollectively, these studies illustrate that health-seeking behaviors among diabetes and obesity patients in India are shaped by a complex interplay of beliefs, socioeconomic factors, and social influences. Addressing these factors through education, policy reform, and community support is crucial for improving health outcomes in this population.", "summary_extract": null}, {"papers": [{"metadata": {"year": 1992}, "authors": ["M. Lejoyeux", "C. Rodi\u00e8re-Rein", "J. Ad\u00e8s"], "summary": "Antidepressant withdrawal symptoms, following abrupt or gradual discontinuation of antidepressants, include general somatic distress (flu-like syndromes, gastro-intestinal disturbances, myalgias, headache, chills, weakness and rhinorrhea), anxiety, agitation, sleep disturbances, movement disorders, cardiac arrhythmias, delirium and manic reactions. Two cases of delirium, an hypomanic reaction and two general distress and movement disorders are reported. Cases 1 and 2 required admission to a general hospital. The etiology of the delirium was difficult to assess as long as the clinicians did not know that patients were taking antidepressants. Case 3 corresponds to the paradoxical activation following antidepressant interruption. Cases 4 and 5 constitutes light withdrawal syndromes. Most of cases are probably unrecognized. These cases reflect the importance in daily practice of the phenomena. It can be concluded from our study that: antidepressants must not be abruptly discontinued when a somatic disease appears. When a patient treated with a psychotropic drug develops delirium, the withdrawal of antidepressant must be suspected and the prescribing physician contacted to know what kind of psychoactive medication was prescribed.", "title": "[Withdrawal syndrome from antidepressive drugs. Report of 5 cases].", "claims": null}, {"metadata": {"year": 2005}, "authors": ["Vlaminck Jj", "van Vliet Im", "Zitman Fg"], "summary": "- Withdrawal symptoms are encountered with both the classical antidepressants, i.e. the tricyclic antidepressants and monoamineoxidase inhibitors, the modern antidepressants, i.e. the selective serotonin re-uptake inhibitors, and the new antidepressants such as venlafaxine and mirtazapine. - The symptoms that are reported following the withdrawal of these drugs can be classified into 8 groups: influenza-like symptoms, psychic symptoms, gastrointestinal symptoms, sleep disorders, equilibrium disorders, sensory disturbances, extrapyramidal symptoms and other symptoms. - It is characteristic of these symptoms that they appear 1-4 days after reduction of the dose or the last administration of the drug. They may also appear in the infants of mothers who used an antidepressant during the last phase of the pregnancy. - It is important that the patient be informed accurately and carefully at the start and termination of any treatment with an antidepressant. - In order to prevent withdrawal symptoms, it is advisable to reduce the dosage slowly if possible when the treatment is to be terminated.", "title": "Withdrawal symptoms of antidepressants", "claims": null}, {"metadata": {"year": 2005}, "authors": ["J. Vlaminck", "I. V. van Vliet", "F. Zitman"], "summary": "Withdrawal symptoms are encountered with both the classical anti-depressants, i.e. the tricyclic antidepressants and monoamineoxidase inhibitors, the modern antidepressants, i.e. the selective serotonin re-uptake inhibitors, and the new antidepressants such as venlafaxine and mirtazapine. The symptoms that are reported following the withdrawal of these drugs can be classified into 8 groups: influenza-like symptoms, psychic symptoms, gastrointestinal symptoms, sleep disorders, equilibrium disorders, sensory disturbances, extrapyramidal symptoms and other symptoms. It is characteristic of these symptoms that they appear 1-4 days after reduction of the dose or the last administration of the drug. They may also appear in the infants of mothers who used an antidepressant during the last phase of the pregnancy. It is important that the patient be informed accurately and carefully at the start and termination of any treatment with an antidepressant. In order to prevent withdrawal symptoms, it is advisable to reduce the dosage slowly if possible when the treatment is to be terminated.", "title": "[Withdrawal symptoms of antidepressants].", "claims": null}, {"metadata": {"year": 1994}, "authors": ["S C Dilsaver"], "summary": "SummaryThe withdrawal of heterocyclic antidepressants and antipsychotic agents can produce nausea, emesis, anorexia, diarrhoea, rhinorrhoea, diaphoresis, myalgias, paraesthesias, anxiety, agitation, restlessness and insomnia. The withdrawal of monoamine oxidase (MAO) inhibitors may result in severe anxiety, agitation, pressured speech, sleeplessness or drowsiness, hallucinations, cognitive impairment, delirium, suicidality and delusions of persecution. The withdrawal of antipsychotic agents may give rise to symptoms preceding the onset of psychosis. These potential harbingers of relapse include anxiety, agitation, restlessness and insomnia. The withdrawal phenomena reviewed are usually prevented by gradually reducing the total daily dosage of the pertinent drug. Antimuscarinic agents often alleviate the distress produced by the withdrawal of tricyclic antidepressants and antipsychotic agents. MAO inhibitor withdrawal syndromes may constitute medical emergencies. The prevention of the evolution of a MAO inhibitor withdrawal-precipitated syndrome is a high priority.", "title": "Withdrawal Phenomena Associated with Antidepressant and Antipsychotic Agents", "claims": null}, {"metadata": {"year": 1998}, "authors": ["I. Mourad", "M. Lejoyeux", "J. Ad\u00e8s"], "summary": "The authors prospectively assessed symptoms induced by the interruption of antidepressants in 16 patients (11 women and 5 men), aged from 33 to 85 years (mean = 52.4 +/- 16.4), treated with antidepressants since at least two weeks. All patients were free of alcohol abuse or dependence disorder and of other dependence to psychoactive substances. None of them presented medical illness. Diagnosis were made by separate evaluations by two authors and confirmed with a semistructered assessment instrument: the Schedule for Affective Disorders and Schizophrenia (Lifetime Version). All patients were submitted to a brutal discontinuation of their antidepressant agent. Patients were assessed twice, before the interruption of the antidepressant, and 72 hours later. Effects of antidepressant interruption were assessed by several means. Modification of anxiety and depression were evaluated using the Montgomery Asberg Depression Rating Scale (MADRS) and the Hamilton Anxiety Scale. Symptoms of withdrawal were assessed with Cassano and al.'s scale SESSH including an evaluation of anxiety, agitation, irritability, anergy, difficulty on concentrating, depersonalization, sleep and appetite disorders, muscle pains, nausea, tremor, sweating, altered taste, hyperosmia, paresthesias, photophobia, motor incoordination, dizziness, hyperacousia pain, delirium. Fourteen of the 16 patients (87.5%) presented modifications of their somatic or psychic state 3 days after the interruption of the antidepressant treatment. Most frequent symptoms were: increase in anxiety (31%), increase in irritability (25%), sleep disorders (19%), decrease of anergia and fatigue (19%). Mean scores of anxiety and depression were not significantly modified by the withdrawal. Following TCAs interruption (7 patients) most frequent symptoms were sleep disorders; increase in anxiety, nausea. Among patients withdrawn from SSRIs (6 patients), most frequent symptoms were increase in anxiety, increase in irritability, headache. Patients also presented a decrease of nausea, and of anorexia.", "title": "[Prospective evaluation of antidepressant discontinuation].", "claims": null}, {"metadata": {"year": 1996}, "authors": ["Michel  Lejoyeux", "Jean  Ad\u00e8s", "Sabelle  Mourad", "Jacquelyn  Solomon", "Steven C. Dilsaver"], "summary": "SummaryWithdrawal from tricyclic antidepressants (TCAs), monoamine oxidase inhibitors (MAOIs) and selective serotonin (5-hydroxytryptamine; 5-HT) reuptake inhibitors (SSRIs) can produce somatic and psychological distress. Influenza-like syndromes, gastrointestinal adverse effects, arrhythmias, anxiety, sleep disturbances, movement disorders, mania or hypomania, panic attacks and delirium may follow antidepressant withdrawal. At present, the aetiology of withdrawal symptoms is not fully known.Withdrawal phenomena are usually prevented by gradually reducing the total daily dosage of the drug in question rather than abruptly discontinuing it. Antimuscarinic agents can be prescribed in order to alleviate the symptoms produced by the withdrawal of TCAs and MAOIs. To date, no drugs have been shown to be useful in the treatment of SSRI-associated withdrawal symptoms. The withdrawal syndrome associated with MAOIs may constitute a medical emergency.", "title": "Antidepressant Withdrawal Syndrome", "claims": null}, {"metadata": {"year": 2019}, "authors": ["J. Henssler", "A. Heinz", "Lasse Brandt", "T. Bschor"], "summary": "BACKGROUND\nAntidepressants are among the most commonly prescribed drugs worldwide. They are often discontinued, frequently without the knowledge of the prescribing physician. It is, therefore, important for physicians to be aware of the withdrawal and rebound phenomena that may arise, in order to prevent these phenomena, treat them when necessary, and counsel patients appropriately.\n\n\nMETHODS\nThis review is based on a comprehensive, structured literature search on antidepressant withdrawal phenomena that we carried out in the CENTRAL, PubMed (Medline), and Embase databases. We classified the relevant publications and reports by their methodological quality.\n\n\nRESULTS\nOut of a total of 2287 hits, there were 40 controlled trials, 38 cohort studies and retrospective analyses, and 271 case reports that met the inclusion criteria. Withdrawal manifestations are usually mild and self-limiting; common ones include dizziness, headache, sleep disturbances, and mood swings. More serious or pro- longed manifestations rarely arise. There is an increased risk with MAO inhibitors, tricyclic antidepressants, venlafaxine, and paroxetine; on the other hand, for agome- latine and fluoxetine, abrupt discontinuation seems to be unproblematic. There is also some evidence of rebound phenomena, i.e., of higher relapse rates or especially severe relapses of depression after the discontinuation of an anti- depressant.\n\n\nCONCLUSION\nA robust evidence base now indicates that there can be acute with- drawal phenomena when antidepressants are discontinued. Putative rebound phenomena have not been adequately studied to date. It is recommended that antidepressants should be tapered off over a period of more than four weeks.", "title": "Antidepressant Withdrawal and Rebound Phenomena.", "claims": null}, {"metadata": {"year": 1983}, "authors": ["S. Dilsaver", "M. Feinberg", "J. Greden"], "summary": "The authors describe three patients who developed withdrawal symptoms after discontinuation of antidepressants. Their symptoms were successfully treated with atropine. Central cholinergic overdrive is implicated in the genesis of the symptoms.", "title": "Antidepressant withdrawal symptoms treated with anticholinergic agents.", "claims": null}], "query": "antidepressant and withdrawal symptoms", "summary_abstract": "The research question regarding antidepressant withdrawal symptoms is addressed in several studies, highlighting the prevalence and nature of these symptoms across various types of antidepressants. \n\nLejoyeux et al. (1992) report a range of withdrawal symptoms that can occur following both abrupt and gradual discontinuation of antidepressants, including somatic distress (such as flu-like symptoms and gastrointestinal disturbances), anxiety, agitation, sleep disturbances, and even severe reactions like delirium and manic episodes. They emphasize the importance of recognizing these symptoms in clinical practice, particularly when patients present with delirium, as it may be linked to antidepressant withdrawal.\n\nVlaminck et al. (2005) classify withdrawal symptoms into eight categories, including influenza-like symptoms, psychic symptoms, and gastrointestinal issues, noting that these symptoms typically manifest 1-4 days after reducing the dosage or stopping the medication. They stress the necessity of careful patient education regarding the potential for withdrawal symptoms and recommend a gradual tapering of the dosage to mitigate these effects.\n\nDilsaver (1994) expands on the types of symptoms associated with withdrawal from various antidepressants, including nausea, anxiety, and cognitive disturbances. He highlights that withdrawal from monoamine oxidase inhibitors (MAOIs) can lead to severe symptoms, which may require urgent medical attention.\n\nMourad et al. (1998) conducted a prospective assessment of withdrawal symptoms in patients, finding that a significant majority experienced changes in their psychological or somatic state shortly after discontinuation. Common symptoms included increased anxiety and sleep disturbances, reinforcing the notion that withdrawal effects are prevalent and can vary based on the type of antidepressant.\n\nHenssler et al. (2019) provide a comprehensive review of the literature on antidepressant withdrawal phenomena, indicating that while most withdrawal symptoms are mild and self-limiting, there is a notable risk associated with certain medications, particularly MAOIs and tricyclic antidepressants. They recommend a gradual tapering process over more than four weeks to minimize withdrawal effects.\n\nOverall, the literature consistently indicates that withdrawal from antidepressants can lead to a variety of symptoms, ranging from mild to severe, and underscores the importance of careful management and patient education during the discontinuation process (Lejoyeux et al., 1992; Vlaminck et al., 2005; Dilsaver, 1994; Mourad et al., 1998; Henssler et al., 2019).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2023}, "authors": ["Alpha Chiemezie Madu", "K. Osborne"], "summary": "A good health financing system is crucial for the effective performance of a country's health system. Many health systems around the world, especially in lower- and middle-income countries such as Nigeria, grapple with perennial challenges such as chronic underfunding, wastefulness, and lack of accountability rendering these systems inefficient. Nigeria's health system faces additional extraneous challenges such as a huge and rapidly growing population, a stagnant economy, and worsening insecurity of lives and property. Furthermore, recent disease outbreaks such as the Ebola epidemic and the COVID-19 pandemic and an evolving disease demography evidenced by an increasing prevalence of chronic, noncommunicable diseases asphyxiate an already floundering health system. To address these challenges and to bolster its efforts in attaining universal health coverage (UHC) and meeting the targets of the Sustainable Development Goals, the Nigerian government launched a new health policy in 2017. A review of the health financing section of this policy shows a focus on improving funding for health care by all levels of government and guaranteeing affordable and equitable access to health services by all Nigerians, albeit with insufficient clarity on how these will be achieved. A more critical review of the country's health financing system also reveals deep-seated systemic issues. Its out-of-pocket payments for funding health care are among the highest in the world, with abysmally low government contributions to health. Successive governments seem to lack the political will to address these shortfalls. Critical gaps exist in the country's health laws, making it difficult to implement the strategies proposed in the new policy. Nigeria must strengthen its health laws to, among other things, mandate health insurance and provide adequate funding for the health system by the government. It should also formulate a dedicated, more precise health financing policy with specific, measurable aims targeting identified problems to enable it to attain universal health coverage.", "title": "Healthcare Financing in Nigeria: A Policy Review.", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Nadine Nicola Akinyemi"], "summary": "* School of Advanced Studies, University of Phoenix Introduction This paper reports a review of empirical literature on factors contributing to mal-distribution of health care services, disparities in health care financing, and government allocations to the health sector. Lack of adequate government commitment to financing health services widens the gap of inequity, fairness, and access to health services. This creates an imperative to design and implement innovative health care models to ensure access to care across the health delivery system. Method The ProQuest, EBCOHOST, Science Direct, Google Scholar, and PuBMed databases were searched from 2005 to 2014 using keywords, financing health care Nigeria, health care spending, government health spending, private sector financing, health care payment, and managing health care services. Results Contributing factors to health care inequity in Nigeria are multifarious, and include lack of government health spending, outdated allocation matrices, failure to leverage effectively the private sector, disparities in payment for health services, and lack of innovation. Discussion It is important that health care decision-makers leverage more government support for health care. Health care organizations need to establish a competitive patient cost-sharing system to decrease risk exposure and implement a preventive health care model that forges an integrative model of care. Such models should examine the organizational structure to determine the best integration approach\u2014be it horizontal or vertical integration of services to optimize the use of resources. Keywords: financing health care Nigeria, managing health care services, government health spending, private sector financing, health care payment Optimizing Health Care Operations in Nigeria/Akinyemi 2", "title": "Optimizing Health Care Operations with Scarce Resources: Nigeria", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Vitalis C Echebiri"], "summary": "This paper examines the challenges facing the Nigerian government toward the implementation of global public health priories. The Nigerian government recognizes the need to implement these priorities by putting in place the necessary policy framework, but political instability, poor infrastructural development and inadequate funding have remained barriers toward the achievement of success in implementing these priorities. The rest of the paper elucidates the fact that despite leadership and influence from the World Health Organization and other United Nations agencies, and some responses from the Nigerian government, tackling these public health problems requires much more fundamental reform to primary health services and a reduction in poverty. Although the government has shown enough political will to tackle these problems, it is expected that a better result will be achieved through injecting more funds into the Nigerian health sector, and deploying astute health administrators to manage the sector rather than pure health professionals without managerial acumen.", "title": "The factors affecting Nigeria\u2019s success toward implementation of global public health priorities", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Ile Ife"], "summary": "The health sector is crucial to growth and development of a nation. Despite sound policies and interventions to develop the Nigerian health sector, it has witnessed several challenges that continue to reduce the progress and achievement of universal access to health care. Some of the factors that affect the overall performance of the health system include; inadequate health facilities/structure, poor human resources and management, poor remuneration and motivation, lack of fair and sustainable health care financing, unequal economic and political relations, the neo-liberal economic policies of the Nigerian state, corruption, illiteracy, very low government spending on health, high out-of-pocket expenditure in health and absence of integrated system for disease prevention, surveillance and treatment, inadequate mechanisms for families to access health care, shortage of essential drugs and supplies and inadequate supervision of health care providers are among some of the persistent problems of the health system in Nigeria. This paper looks at the enormity of the problems and recommends policy options vital to addressing the problems in order to attain the equilibrium in demand, quality and efficiency in the health care delivery system in Nigeria.", "title": "Balancing Demand, Quality and Efficiency in Nigerian Health Care Delivery System", "claims": null}, {"metadata": {"year": 2014}, "authors": ["G. Timothy", "O. Irinoye", "U. Yunusa", "A. Dalhatu", "S. Ahmed", "A. Suberu"], "summary": "The health sector is crucial to growth and development of a nation. Despite sound policies and interventions to develop the Nigerian health sector, it has witnessed several challenges that continue to reduce the progress and achievement of universal access to health care. Some of the factors that affect the overall performance of the health system include; inadequate health facilities/structure, poor human resources and management, poor remuneration and motivation, lack of fair and sustainable health care financing, unequal economic and political relations, the neo-liberal economic policies of the Nigerian state, corruption, illiteracy, very low government spending on health, high out-of-pocket expenditure in health and absence of integrated system for disease prevention, surveillance and treatment, inadequate mechanisms for families to access health care, shortage of essential drugs and supplies and inadequate supervision of health care providers are among some of the persistent problems of the health system in Nigeria . This paper looks at the enormity of the problems and recommends policy options vital to addressing the problems in order to attain the equilibrium in demand, quality and efficiency in the health care delivery system in Nigeria. Keywords : Demand, Quality, efficiency, health care system, Nigeria", "title": "Balancing Demand, Quality and Efficiency in Nigerian Health Care Delivery System", "claims": null}, {"metadata": {"year": 2023}, "authors": ["O. Akpor", "Timothy A. Akingbade", "O. Olorunfemi"], "summary": "The health sector is crucial to the growth and development of a nation. Despite sound policies and interventions to develop the Nigerian health sector, there are challenges that continue to reduce the progress and achievement of universal access to healthcare. The major factors that affect the overall performance of the health system include inadequate health facilities/structure, poor human resources and management, poor remuneration and motivation, lack of fair and sustainable healthcare financing, unequal economic and political relations. Other factors inlucde neo-liberal economic policies of the Nigerian state, corruption, illiteracy, very low government spending on health, high out-of-pocket expenditure in health, and the absence of an integrated system for disease prevention, surveillance and treatment, inadequate mechanisms for families to access healthcare, and the most persistent problem, the shortage of basic and essential equipment in most public health facilities in Nigeria. The agony which patients and nurses are passing through due to the non-availability of equipment needed to render the required services causes inefficient service delivery and increases job stress. This review paper describes the enormity of the problems and recommends options vital to addressing the problem to attain equilibrium in demand, quality and efficiency in healthcare delivery.", "title": "Lack of adequate equipment for healthcare \u2013 The agony of patients and nurses: A review", "claims": null}, {"metadata": {"year": 2023}, "authors": ["Archibong E P", "Ogana J F", "Edet A F", "Enamhe D C"], "summary": "The average annual budgetary allocation for health in Nigeria is about the lowest in Africa at 5.7% as the total health expenditure is only 0.7% of GDP against the WHO recommended 4%-5%. The out-of-pocket spending by Nigerians is more than 60% of total health expenditure instead of WHO recommended 30-40%, while the national coverage of the National Health Insurance is below 5% which poses a serious problem to health care service utilization. In Nigeria, the ever rising socioeconomic costs of healthcare and the limited or complete lack of supportive public financing is a major challenge to the attainment of the sustainable development goals (SDGs) recently adopted to reduce poverty and improve the wellbeing of all citizens of the world. This paper examined health care financing in Cross River State and determined the extent to which it poses problems to services utilization. The paper reviewed literature relevant to the study while the Marxist conflict theory of healthcare utilization was employed. The design for the paper was Expost facto which relied on secondary data from Cross River State Ministry of Health. The study reviewed various sources of health care financing in Cross River State and the impact of these sources on health care system strengthening or quality health care provision. From the study or discourse and review of health expenditure and its related challenges, it was found out that, out-of -pocket expenditure or user-fee charges, community-based financing, private donor agencies financing and tax-based public health financing sources in the state have not significantly impacted on good services provision to individuals. From these findings, it was recommended among others that government should encourage the establishment of village/ward development committees whose responsibilities should include taking initiative to assist government in building health post (in communities where such do not exist), Government should maintain existing health facilities and provide proper logistics during health campaigns and monitoring of health workers activities at the health facilities.", "title": "Health care financing and services utilization in cross river state, nigeria", "claims": null}, {"metadata": {"year": 2014}, "authors": ["D. State", "Osun State"], "summary": "Every nation's dream is for its government to achieve a sustainable improvement in the quality of life of its citizens. This can only be achieved through good governance and planned actions that detail the vision, mission, goals/objectives and strategies to be attained over a given period of time. Nigeria like other countries around the globe has not for once lacked any development plans. Unfortunately, with its current estimated population of 150 million and estimated total of 23,640 health facilities operated via a three - tiered governance structure, it is still ranked by World Health Organization at 187 th position in its health system among 191 member states. This article reviewed related relevant literature which revealed that, for more than two decades ago, African countries including Nigeria have been plunged into economic crisis which seriously affected a large portion of their populations and raised social and political tensions. To achieve sustainability, there is need for transformative plans and solutions that requires cooperation among industries and the government through delivery of minimum set of cost - effective interventions, packages and revitalization of the country's health system. Some of the major challenges of sustaining Nigeria healthcare system are: counterfeit and adulterated drugs, poor healthcare financing and sustainability, increased out-of-pocket expenditure, inadequate basic infrastructure/equipments/drugs and inequitable distribution, poor remuneration and other push factors, bribery and corruption and shortage of staff. This article concluded by recommending the following among others as the way forward; provision of a system of equitable distribution of healthcare facilities and drugs, implementation of cost - effective healthcare interventions plans, establishment of a functional and competent monitoring and evaluation systems.", "title": "Sustainable Healthcare System in Nigeria: Vision, Strategies and Challenges.", "claims": null}], "query": "the cost of implementing advanced health management information systems in Nigeria is high, and funding is scarce amidst widespread government corruption which further hinders progress", "summary_abstract": "The implementation of advanced health management information systems in Nigeria is significantly hindered by high costs and a lack of funding, exacerbated by widespread government corruption. The literature reveals a consistent theme of inadequate government commitment to health financing, which has led to a health system characterized by inefficiency and inequity. Madu and Osborne (2023) highlight that Nigeria's health financing system is plagued by chronic underfunding and high out-of-pocket expenses, which are among the highest globally. This situation is compounded by low government contributions to health, insufficient clarity in health policies, and a lack of political will to address these systemic issues.\n\nAkinyemi (2014) further emphasizes that the mal-distribution of health care services and disparities in health financing stem from inadequate government spending and outdated allocation matrices. The absence of innovative health care models and effective private sector engagement also contributes to the challenges faced in the health sector. Echebiri (2015) notes that political instability and poor infrastructure are significant barriers to implementing global public health priorities, indicating that without substantial funding and effective management, progress will remain stunted.\n\nThe overall performance of Nigeria's health system is severely affected by a combination of factors, including corruption, low government health expenditure, and high out-of-pocket costs (Ile Ife, 2014; Akpor et al., 2023). The average annual budgetary allocation for health is alarmingly low, at about 5.7%, with out-of-pocket spending exceeding 60% of total health expenditure (Archibong et al., 2023). This financial strain not only limits access to health services but also undermines the potential for implementing advanced health management systems.\n\nIn summary, the collective findings from these studies underscore that the high costs associated with advanced health management information systems in Nigeria are deeply intertwined with a broader context of inadequate funding, systemic corruption, and ineffective governance. Addressing these challenges requires a comprehensive approach that includes increased government investment, improved health financing policies, and a commitment to reducing corruption within the health sector.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2020}, "authors": ["Kiersten  LeBar"], "summary": "OBJECTIVES\nThis article aims to identify the steps necessary to evaluate the clinical need for innovative coverage models within the oncology setting to help prevent hospital readmissions.\n\n\nDATA SOURCES\nMultiple published studies suggest alternative methods for patient care delivery that are safe and cost effective.\n\n\nCONCLUSION\nImproving care transitions for the oncology patient is necessary to be able to provide low-cost, high-quality, and patient-centered care. Many of the review studies in this article suggest that emergency room visits and subsequent readmission could be decreased with the use of innovative care models.\n\n\nIMPLICATIONS FOR NURSING PRACTICE\nNurses are critical to the care of medically fragile patients. Nurse-led activities such as telephone triage, post discharge phone calls, or telehealth visits can reduce patient emergency department utilization and readmissions through early recognition of symptoms and oncologic emergencies by prompting timely referrals/consultations and quick interventions.", "title": "Care Models Preventing Readmission.", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Martha Hsueh", "K. Dorcy"], "summary": "Gaps in complex oncology care coordination between inpatient and outpatient settings can result in treatment and monitoring delays and omissions, which can negatively affect patient outcomes. Gaps also exist for patients facing complex treatment modalities and collaborations between multiple care teams working at geographically distant sites. A pilot advanced practice nurse care coordinator \u2028(APNCC) role to coordinate these complex care transitions and implement processes for safer and more efficient care has shown promise.\u2029.", "title": "Improving Transitions of Care With an Advanced Practice Nurse: A Pilot Study.", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Joanna-Grace M. Manzano", "J. Halm", "Jeffrey S. Farroni"], "summary": "250 Background: Policymakers have identified 30-day readmissions as an important quality indicator of poor care or coordination of care. Among cancer centers, there is no benchmark data in terms of readmission rates or recommendations in terms of risk adjustment models.\n\n\nMETHODS\nRetrospective data analysis to estimate baseline readmission rate and identify risk factors.\n\n\nINTERVENTIONS\n(1)admitting to single floor, (2) twice weekly interdisciplinary meetings with using risk assessment tool-Cancer Outcomes Augmented through Safe Transitions (COAST) tool, and (3) re-evaluate readmission rate post intervention at 6 months and 1 year.\n\n\nRESULTS\nUnplanned readmission rate on the Hospitalist Service at MD Anderson was 21.5% at baseline. After 6 months of interventions, our readmission rate over 6 months was 23.3%. Age 45-65, having Medicare insurance, and being discharged to hospice were protective of a readmission. Distant metastases and having more comorbidities were associated with increased risk for readmission. Readmitted patients have a greater length of stay (7 days) and a higher average cost of inpatient stay ($20.3K vs. 17.9K). The median days to readmission was 11 days. Top comorbidities: hypertension, fluid and electrolyte disorders, anemia, diabetes mellitus, and abnormal weight loss. Top reasons: metastatic disease, biliary tract disease, GI hemorrhage, intestinal obstruction, septicemia, renal failure.\n\n\nCONCLUSIONS\nOur project has provided insight into the rates and risk factors for readmission in oncology hospitalist service in a tertiary cancer center. The development of web-based COAST risk assessment tool is expected to give an improved understanding of our patient population. Although our readmission rates have not shown decrease over the 6 months after our interventions, this means that more interventions and more time may be necessary to impact readmission rates of services dealing with complex cancer patients. Additionally, a proportion of these unplanned readmissions in cancer patients may not be preventable. Benchmark data we have presented and that we continue to collect will help inform recommendations for effective transitions of care, patient safety practices as well as strategies for reducing readmission rates in cancer centers.", "title": "Augmentation of cancer outcomes through safe transitions.", "claims": null}, {"metadata": {"year": 2012}, "authors": ["T. Evans", "N. Mitra"], "summary": "48 Background: Reducing the risk of readmissions is an important quality goal for hospitals. Little is known of the perspectives of patients on underlying challenges that may lead to readmission. The objective of this study was to elicit perspectives of readmitted oncology patients on barriers to a successful transition from hospital to home.\n\n\nMETHODS\nAs part of a larger survey of readmitted inpatients, a 36-item survey was administered to 197 oncology patients readmitted to the hospital within 30 days of discharge to home. Surveys were administered at The Hospital of the University of Pennsylvania (an urban tertiary care academic medical center) and Penn Presbyterian Medical Center (an affiliated urban community hospital), both located in Philadelphia. Responses were entered in real-time into the electronic medical record (EMR) and used by the care team to address patient concerns and improve quality.\n\n\nRESULTS\n45.2% of readmitted oncology patients reported challenges during the transition from hospital to home which they perceived as contributing to readmission. The most commonly reported transition challenges within the oncology population included difficulty with activities of daily living (ADLs) (17.8%), feeling unprepared for discharge (14.2%) and difficulty adhering to medications (7.1%). 15.2% of patients could not identify any modifiable factor contributing to readmission and reported returning simply because of symptoms from progressive illness. After adjusting for potential confounders (age, gender and severity of illness) using multivariable logistic regression models, uninsured and Medicaid patients were more likely than other patients to attribute readmission to difficulty accessing medications (OR 4.5, 95%CI 1.0, 19.9) and performing ADLs (OR 2.7, 95%CI 1.18, 6.1).\n\n\nCONCLUSIONS\nUnderstanding challenges reported by readmitted oncology patients may enable inpatient oncologists to tailor transitions interventions to patient needs. Specifically, ensuring patients are able to perform necessary ADLs, are prepared for discharge and have assistance with medication adherence may help prevent unplanned readmissions. Uninsured and Medicaid patients may require additional assistance with accessing medications and ADLs.", "title": "Patient-reported oncology readmission factors.", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Janet Gordils-Perez", "M. Gabel"], "summary": "PROGNOSES OF PATIENTS WITH CANCER HAVE IMPROVED over the years because of advances in oncologic therapies. However, patients with cancer still face a long and arduous treatment journey, one that is often poorly coordinated and challenging to navigate (Thygesen, Pedersen, Kragstrup, Wagner, & Mogensen, 2011). Patients recently diagnosed with cancer face significant barriers to accessing timely and quality cancer services, as well as challenges in navigating the healthcare system (Hunnibell et al., 2012; Lobb, Allen, Emmons, & Ayanian, 2010; Wulff, Thygesen, S\u00f8ndergaard, & Vedsted, 2008). Given the interprofessional nature of cancer care and the realities of the healthcare system, patients are frequently left on their own to coordinate care with numerous treatment providers, balance conflicting information, and make treatment decisions, all the while coping with their disease and treatment side effects (Horner et al., 2013). The Institute of Medicine (2001) report Crossing the Quality Chasm: A New Health System for the 21st Century outlines the six aims for healthcare improvements: providing safe, effective, patient-centered, timely, efficient, and equitable care. Embedded in these aims is the responsibility of healthcare systems to provide prompt patient access to coordinated care. The primary goal of care coordination is improved health through timely support of patients and families regarding diagnosis and therapy options, better access to care, and improved patient satisfaction (Lee et al., 2011). One approach to enhancing coordination of oncology care is the implementation of nurse navigation programs. In this model of care, an oncology nurse navigator coordinates the overall care of patients throughout discrete phases of cancer care by (a) removing barriers to care to address the critical disconnect between time of diagnosis and start of therapy (Freeman & Rodriguez, 2011) and (b) providing timely access to care by supporting them and their families through the many decisions associated with early cancer care (Wagner et al., 2010; Wells et al., 2008). Timely delivery of cancer care services can positively affect patient outcomes. Delays often lead to deferrals in diagnosis, late-stage disease, and poor prognosis (Aiello Bowles et al., 2008). Treatment delays, lack of coordination of care, gaps in patient education, and unaddressed psychosocial issues (Aiello Bowles et al., 2008), as well as financial and transportation KEYWORDS", "title": "Development and implementation of a program at a comprehensive cancer center", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Rupal Mansukhani", "M. B. Bridgeman", "Danielle Candelario", "Laurie J Eckert"], "summary": "The coordination of care as patients move from one health care setting to another is crucial to treatment, but breakdowns in the process lead to poor transitions, fragmented care, hospital readmissions, and increased costs. This article discusses evidence-based strategies for improving communication and reducing readmissions.", "title": "Exploring Transitional Care: Evidence-Based Strategies for Improving Provider Communication and Reducing Readmissions.", "claims": null}, {"metadata": {"year": 2019}, "authors": ["L. Overholser", "C. Callaway"], "summary": "Most cancer survivors will follow-up with primary care at some point in the cancer continuum, but coordination between the oncology and primary care teams remains challenging. According to Drs. Linda Overholser and Carlin Callaway, who discussed the importance of care coordination to improve outcomes in cancer survivors at the NCCN 2019 Annual Conference, effective communication is needed between specialists and primary care providers and, when established, has been shown to improve treatment outcomes. Use of survivorship care plans, as well as adherence to the NCCN Guidelines for Survivorship, can further standardize the provision of quality oncology care.", "title": "Improving Care Coordination to Optimize Health Outcomes in Cancer Survivors.", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Kristin E. Hand", "R. Cunningham"], "summary": "W ith decreasing reimbursement and penalties for 30-day readmissions that particularly affect Medicare patients, healthcare systems worry that they will have to absorb unsustainable costs (Hansen, Young, Hinami, Leung, & Williams, 2011). In response, many hospitals are trying to identify low-cost solutions to minimize 30-day readmissions. The complexity of the discharge process coupled with patients\u2019 stress about leaving the hospital as soon as possible causes a higher likelihood of readmission. Nurses, pharmacists, social workers, and other healthcare providers work to give patients all of the information they need before leaving the hospital. However, patients often remain underinformed, problems arise, and they are then readmitted. Readmissions have been linked to poor communication, failure to coordinate and reconcile medications, lack of postdischarge follow-up, and poor planning for care transitions (Berenson, Paulus, & Kalman, 2012; Harrison, Hara, Pope, Young, & Rula, 2011). With the current focus on preventing readmissions, healthcare providers are trying to identify strategies to support patients during the postdischarge period. Patients with cancer are particularly affected by readmissions because they require inpatient care more often than other patients and have complex needs (e.g., medication, symptom management). The complexity and diversity of care requirements create substantial challenges in planning for appropriate postdischarge support (Mistiaen & Poot, 2006). Implementing postdischarge telephone calls for patients with cancer could serve as a low-cost intervention to address their complex needs during the transition from hospital to home. Although telephone follow-up offers a low-cost strategy to reduce readmissions, the intervention involves many factors (e.g., number of calls, timing, call content). For the current article, the authors conducted a comprehensive review of literature to help understand what is known about follow-up telephone calls in patients with cancer discharged from hospital to home.", "title": "Table of Evidence ( Continued ) Study Objective Design Intervention Results Limitations", "claims": null}], "query": "enhanced care coordination, transitions, and patient education in oncology readmissions", "summary_abstract": "The research question regarding enhanced care coordination, transitions, and patient education in oncology readmissions is addressed through various studies that highlight the importance of effective communication, innovative care models, and patient-centered approaches to reduce hospital readmissions.\n\nLeBar (2020) emphasizes the necessity of improving care transitions for oncology patients to provide high-quality, cost-effective care. The study suggests that innovative care models, including nurse-led activities such as telephone triage and telehealth visits, can significantly reduce emergency department visits and readmissions by facilitating early recognition of symptoms and timely interventions.\n\nHsueh and Dorcy (2016) identify the gaps in oncology care coordination between inpatient and outpatient settings, which can lead to treatment delays and negatively impact patient outcomes. They propose the implementation of an advanced practice nurse care coordinator role to enhance care transitions and improve the efficiency of oncology care.\n\nManzano et al. (2014) provide insights into the readmission rates among cancer patients, noting that while their interventions did not significantly reduce readmission rates within six months, they highlighted the need for ongoing efforts and more comprehensive strategies to address the complexities of cancer care.\n\nEvans and Mitra (2012) explore the perspectives of readmitted oncology patients, revealing that many face challenges during the transition from hospital to home, such as difficulties with daily activities and medication adherence. Addressing these challenges through tailored interventions could help prevent unplanned readmissions.\n\nGordils-Perez and Gabel (2018) discuss the critical role of care coordination in oncology, advocating for nurse navigation programs to help patients navigate the complexities of cancer treatment and improve access to timely care. This approach aims to enhance patient satisfaction and outcomes by addressing barriers to care.\n\nMansukhani et al. (2015) highlight evidence-based strategies for improving communication and reducing readmissions, reinforcing the idea that effective care coordination is essential for successful transitions between healthcare settings.\n\nOverholser and Callaway (2019) stress the importance of communication between oncology and primary care teams to improve outcomes for cancer survivors, suggesting that survivorship care plans can standardize quality oncology care.\n\nLastly, Hand and Cunningham (2019) point out that the complexity of discharge processes and inadequate patient education contribute to higher readmission rates. They advocate for postdischarge follow-up strategies, such as telephone calls, to support patients with cancer during their transition from hospital to home.\n\nCollectively, these studies underscore the critical need for enhanced care coordination, effective communication, and tailored patient education to improve transitions and reduce readmissions in oncology care.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2011}, "authors": ["C. Nguyen", "Son Tran"], "summary": "The purpose of this paper was to give an overview of economic development under reform and world integration and to evaluate the main factors influencing the growth of the Vietnamese economy during the reform period (1990-2008 (2009)). Based on statistical data on the Vietnamese economy in the period of 1990-2008 (2009), this study analyzed the factors affecting economic growth. The policy changes, economic development, poverty rates and living standards of Vietnamese population are analyzed over the reform period using qualitative methods. The results of this study show that economic growth under reform and world integration has reduced the poverty rate and increased living standards of population in Vietnam. An evaluation of the factors influencing economic growth is made using a quantitative model of total factor productivity (TFP) and another econometric model. The findings from this quantitative analysis show that the growth of the Vietnamese economy was determined by two factors: (1) capital investments, including foreign direct investment (FDI) and (2) the growth of exports. The results of these qualitative and quantitative analyses lay the foundation for policy recommendations for Vietnam Government to develop economy in the future", "title": "ECONOMIC DEVELOPMENT UNDER REFORM AND WORLD INTEGRATION AND THE MAIN FACTORS INFLUENCING THE ECONOMIC GROWTH IN VIETNAM", "claims": null}, {"metadata": {"year": 2017}, "authors": ["J. S. Mah"], "summary": "Abstract This paper aims to reveal the sources of the rapid growth of the Vietnamese economy since economic reform over the period 1986\u20132013. It applies the autoregressive distributed lags variance bounds test and the error correction model, focusing on the roles of globalization and aid in economic growth. The empirical evidence supports neither the export-led nor the FDI-led economic growth hypothesis. Rather, the increase in import values is revealed to have caused economic growth. When the import variable is excluded from the estimated model, aid inflows are shown to have caused the economic growth in Vietnam.", "title": "Globalization and Aid Inflows in the Economic Growth of Vietnam", "claims": null}, {"metadata": {"year": 2018}, "authors": ["N. Luan"], "summary": "2017 is the first time after many years that Vietnam met and exceeded 13 social-economic indicators. Vietnam\u2019s economy experienced a high economic growth rate, stable and sustainable macroeconomic environment. GDP growth is 6.81% (target 6.7%), CPI 3.53% (target (4%), credit growth 18,1%, and FDI of 36 billion USD. Import-export turnovers remain a significant achievement and has been maintained at a high level. Early results from restructuring of state-own enterprises signal a successful restructuring. These positive indicators provide impetus for the Vietnam\u2019s economy in 2018. The targets in 2018 include maintaining a stable macro-economic environment, achieving a 6.7% GDP growth, focusing on growth quality and sustainability, 4% CPI, 3.7% state budget deficit, 63.9% public debt, 8-10% increase in export turnover against 2017, less than 3% trade deficit and a ratio of government investment to GDP at 34%. To successfully achieve 2018 economic indicators, it is necessary to strongly apply policies and solutions for a creative and innovative system, develop science and technology nationwide and in every industry. This is to create breakthrough for the changes of the structure and growth model. \n\u00a0", "title": "Vietnam\u2019s economy in 2017 - 2018: Growth on an efficient and sustainable foundation", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Tri-Dung Lam"], "summary": "This thesis studies the role of exports in Vietnam's rapid growth since the country \nimplemented a comprehensive reform (Doi-Moi) in 1986 to transform itself from a \ncentrally command system to a 'socialist-oriented market economy'. One central \nfinding is that Vietnam's growth since Doi-Moi has indeed been export-led as the \nsecond-tier NICs of Malaysia, Thailand and the Philippines, but that many of the \ncharacteristics of Vietnam's exports are different to those of these NICs. Vietnam's \nexports are focused on resource-intensive and low-tech industries, which are growing \nslowly in world trade, and in a constant market share model are explained by rising \nresidual competitiveness rather than by market demand or commodity composition \neffects. This rising residual competitiveness stands at odds with the results of the \nWorld Economic Forum and other sources that Vietnam's competitiveness is low and \nfalling in recent years. Another key finding is that since 2000 Vietnam's growth has \nbeen extensive rather than intensive, with labour productivity both at low levels and \ngrowing slowly outside the agricultural sector. High GDP growth since 2000 has been \ndriven by rapid growth in factor supplies, especially labour, with low growth in nonagricultural \nproductivity. This extensive pattern of growth resolves the \ncompetitiveness paradox, as it suggests that Vietnam is expanding low-cost industries \nrapidly but not building its competitiveness in other areas. Such a development path \nwill not support the rate of long-term growth that Vietnam requires to achieve its \ndevelopment objectives, and major policy changes are necessary.", "title": "Trade and economic growth in Vietnam after Doi-Moi : a comparative study with the ASEAN-4", "claims": null}, {"metadata": {"year": 2019}, "authors": ["T. Nguyen", "P. Ngo", "T. Bui", "V. Mai"], "summary": "Since 1986, after over 30 years of implementation of \"Doi Moi\" policy, Vietnam has become one of the world\u2019s high-growth economies. In the 2001-2010 period, the average GDP growth rate was 7.25%. From 2010 to 2015, the growth rate was slightly reduced to 5.90%, increasing in 2016 to 6.21% and again in 2017 to 6.7%. Vietnam's economic growth target for the period of 2016-2020 is 6.5% -7.0% (other goals include economic restructuring, renewal of the current growth model, sustainable development and reduction of the state budget deficit while not exceeding the public debt ceiling of 65% of GDP). The extent of the impact of monetary and fiscal policy therefore plays a vital role in promoting economic growth in Vietnam. This study will assess the role and impact of monetary and fiscal policy on Vietnam's economic growth from 2001 to 2017, drawing policy implications for reference in planning and policy implementation for the Government of Vietnam in the near future.", "title": "Impact of Fiscal and Monetary Policy on Economic Growth in Vietnam", "claims": null}, {"metadata": {"year": 2004}, "authors": ["T. Larsen", "V. Suri", "V. Dinh"], "summary": "Vietnam's economy grew by 7.2 percent in 2003 despite challenges from Severe Acute Respiratory Syndrome (SARS) and a lackluster global economy. Export growth, in value terms, accelerated to reach 21 percent in 2003. In the first five months of 2004 exports have surpassed expectations by growing at over 17 percent compared with the same period last year. Industrial production remains strong and investment solid. In the first quarter of 2004, gross domestic product (GDP) growth was 7 percent, and the government's growth target for 2004 is around 8 percent. Vietnam continues the implementation of a comprehensive reform agenda. Its main goals are to complete the transition to a market economy, to keep development inclusive and sustainable, and to build modern governance. While there has been progress on all three fronts, there has also been substantial variation within each of them. Considerable progress in the integration with the world economy, promising steps towards curbing policy lending, an acceleration of the State-Owned Enterprise (SOE) transformation process and the apparent resolve to adopt a more systemic approach towards fighting corruption are among the main highlights in recent months. On the other hand, limited progress in banking reform remains a matter of concern.", "title": "Taking stock : an update on recent economic developments and reforms in Vietnam", "claims": null}, {"metadata": {"year": 2007}, "authors": ["H. Ying"], "summary": "Vietnam steps into the openning year of the second Five-Year Plan in the new century in 2006,throughout which the economy sees sustained growth,with about 8.2 percent growth in GDP,ranking the first place in growth rate in Southeast Asia. Vietnam's official entry into the WTO and successful host for the 14th APEC Leaders' Meeting will bring new economic development opportunities in 2007.", "title": "Reviews of Vietnamese Economic Situation in 2006 and the Development Goal for 2007", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Long", "My"], "summary": "Over the last decades, economists have argued about the economic growth of East Asian countries such as Japan, South Korea, Hong Kong (China), and Taipei (China) and Singapore. Although economists agree that these countries have achieved impressive growth, they offer conflicting interpretations in relation to the origins of such growth. The main purpose of this paper is to synthesize and systematize studies and assessments of economists to draw lessons for rapid and sustainable growth in Vietnam. Notably, it is suggested that due to market imperfections, the government should apply incentives policies to encourage areas such as infrastructure, human capital, finance, and technology.", "title": "RAPID AND SUSTAINABLE GROWTH IN VIETNAM LESSONS FROM THE MIRACLE EAST ASIA", "claims": null}], "query": "RESULTS - DRIVING FORCE BEHIND vietnam ECONOMIC GROWTH", "summary_abstract": "The driving forces behind Vietnam's economic growth have been explored through various studies, revealing a complex interplay of factors that have contributed to its rapid development since the implementation of the Doi Moi reforms in 1986.\n\nNguyen and Tran (2011) highlight that capital investments, particularly foreign direct investment (FDI), and the growth of exports have been pivotal in driving Vietnam's economic growth during the reform period from 1990 to 2008. Their analysis indicates that these factors not only spurred economic growth but also led to a reduction in poverty rates and improvements in living standards.\n\nIn contrast, Mah (2017) challenges the notion that export-led or FDI-led growth are the primary drivers. Instead, he posits that the increase in import values has significantly contributed to economic growth, with aid inflows also playing a crucial role when imports are excluded from the model. This suggests a more nuanced understanding of Vietnam's economic dynamics, where imports and external aid are integral to growth.\n\nLam (2009) supports the idea of export-led growth but notes that Vietnam's exports are primarily from resource-intensive and low-tech industries, which differ from those of other newly industrialized countries. He argues that while Vietnam has experienced extensive growth driven by labor supply, the slow growth in productivity outside agriculture poses challenges for sustainable long-term growth.\n\nNguyen et al. (2019) emphasize the importance of monetary and fiscal policies in promoting economic growth, particularly from 2001 to 2017. They argue that effective policy implementation is essential for achieving Vietnam's growth targets and ensuring sustainable development.\n\nLarsen et al. (2004) and Ying (2007) provide evidence of strong export growth and industrial production, which have been crucial for maintaining high GDP growth rates despite external challenges. Their findings underscore the significance of Vietnam's integration into the global economy and the ongoing reforms aimed at transitioning to a market economy.\n\nOverall, the collective findings suggest that Vietnam's economic growth is driven by a combination of capital investments, export growth, and supportive monetary and fiscal policies, alongside the critical roles of imports and foreign aid. The need for continued structural reforms and improvements in productivity remains essential for sustaining this growth trajectory in the future.", "summary_extract": null}, {"papers": [{"metadata": {"year": 1986}, "authors": ["Perry Mehrling"], "summary": "The class struggle is formalized as a differential game in a strictly supply-side model, an approach that synthesizes the models of Lancaster and Goodwin. Four different steady-state equilibria are derived, each corresponding to different assumptions about the degree to which each class is organized to promote its own interests. In particular, the Goodwin growth cycle is shown to emerge from a world characterized by unorganized capitalists and workers, in which individuals ignore the effects of their own actions on economywide variables. More relevant for discussion of modern capitalism are the hierarchical equilibria, especially the codetermination equilibrium in which the existence of a full-employment equilibrium turns out to be problematic. Finally, comparative-statics results suggest that the incentives for technological change differ widely among the four regimes.", "title": "A Classical Model of the Class Struggle: A Game-Theoretic Approach", "claims": null}, {"metadata": {"year": 1977}, "authors": [], "summary": "For any C.S .E . member interested in examining the issues of class struggle in Britain as prompted by l . W ickham's review of Erhard Lucas in Capital and Class 1, Grays study must form a major historical bench-mark . Much of his book suggests the possibility of a fundamental examination of the historical development of the labour market under capitalism, its variant forms and the strategies and tactics employed by different types of workers within this structure, and by implication the political and ideological conditions necessary for its transcendence .", "title": "Science, Class and Society", "claims": null}, {"metadata": {"year": 2018}, "authors": ["H. Heller"], "summary": "This chapter affirms that class and class struggle were fundamental to Marx\u2019s conception of history. His claims are affirmed as it is shown that upper-class demands for surplus and lower-class resistance have driven the evolution of society from the Bronze Age to the present and were critical to the passage from the tributary mode of production to the capitalist mode of production. Class struggle exists in all class-based societies but was particularly acute in China and the West. In Classical Antiquity class antagonism mainly took the form of peasant/landlord struggle but also expressed itself in conflict between slave and master. In modern times the bourgeoisie engaged in a two-sided struggle against both the landlord class and against the working class. Its struggle against the latter is ongoing. The state, culture, and ideology are key components of class struggle.", "title": "Class and Class Struggle", "claims": null}, {"metadata": {"year": 1988}, "authors": ["Rhonda F. Levine"], "summary": "In this reassessment of New Deal policymaking, Rhonda Levine argues that the major constraints upon and catalysts for Roosevelt's policies were rooted in class conflict. Countering neo-Marxist and state-centred theories, which focus on administrative and bureaucratic structures, she contends that too little attention has been paid to the effect of class struggle. Levine analyzes the balance of class forces during the Great Depression and the ways in which they shaped the formulation, implementation and consequences of federal policies.", "title": "Class Struggle and the New Deal: Industrial Labor, Industrial Capital, and the State", "claims": null}, {"metadata": {"year": 1988}, "authors": ["R. Levine"], "summary": "In this reassessment of New Deal policymaking, Rhonda Levine argues that the major constraints upon and catalysts for Roosevelt's policies were rooted in class conflict. Countering neo-Marxist and state-centred theories, which focus on administrative and bureaucratic structures, she contends that too little attention has been paid to the effect of class struggle. Levine analyzes the balance of class forces during the Great Depression and the ways in which they shaped the formulation, implementation and consequences of federal policies.", "title": "Class Struggle and the New Deal: Industrial Labor, Industrial Capital, and the State", "claims": null}, {"metadata": {"year": 0}, "authors": ["A. M. Simons"], "summary": "\u201cIn every historical epoch the prevailing mode of economic production and exchan it, form the e and the social organization necessarilly following from 5\u2019. asar upon which is built up, and from which alone can be explained, the polihcal and intellectual history of that epoch; and conse tri % uently the whole history of mankind (since the dissolution of primitive al society, holding land in common ownership) has been a history of class struggles, contests between exploiting and cxpioited, ruling and oppressed classes; thus the history of these class struggles forms R series of evolution in which, now-a-days. II stage has been reached where the exploited and oppressed ciassthe proletarint-cannot attain its emancipation from the away of the exploiting and ruling class the bour~oise--without, at the same time, and once and for nllz cmancipatin society at large from all exploitation. oppression, class-&stinctions % an dnrr-struggles.\u201c-~~#~~~~~ Marifrslo.", "title": "Class struggles in America", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Bob Jeffery", "P. Thomas", "D. Devine"], "summary": "This article reports findings on class identities amongst a small sample of mainly working-class residents in the City of Salford. We attempt to develop a Ranci\u00e8rian framework for understanding class identities, centred on his key concept of dissensus, and how these ideas have been developed by Imogen Tyler through the notion of \u2018classificatory struggles\u2019. From this, we identify a continuum of responses that are discernible in relation to the neoliberal order of classifications: from those orientated to a \u2018policing\u2019 function, either accepting and internalising dominant discourses or attempting to displace abjection onto others, to those that tend more towards \u2018politics\u2019 in either asserting alternative circuits of value or through an appeal to the name of the proletariat as a political claim to radical equality. In examining our data, we note that although a majority disavowed an explicit working-class identity, they nonetheless engaged in a range of classificatory struggles.", "title": "Classificatory struggles in the midst of austerity: Policing or politics?", "claims": null}, {"metadata": {"year": 1988}, "authors": ["Margo J. Anderson"], "summary": "In June of 1948, during the peak period of strike activity in the United States in the past fifty years, Philip Murray, President of the CIO, the most successful militant labor organization of the twentieth century, published an article giving his views on labor organization, democracy, and \u201cour American system.\u201d He argued that \u201cif we pull together . . . we shall surmount the present crisis.\u201d He felt confident about his prediction because, he said, \u201cWe have no classes in this country; that\u2019s why the Marxist theory of class struggle has gained so few adherents. We\u2019re all workers here. And in the final analysis the interests of farmers, factory hands, business and professional people, and white-collar toilers prove to be the same\u201d (Murray, 1948; quoted in Lipsitz, 1981: 143).", "title": "The Language of Class in Twentieth-Century America", "claims": null}], "query": "contemporary review of goodwin class struggle model", "summary_abstract": "The contemporary review of the Goodwin class struggle model reveals a multifaceted understanding of class dynamics and their implications for economic systems. Mehrling (1986) formalizes class struggle within a differential game framework, highlighting how different levels of organization among capitalists and workers lead to various steady-state equilibria. Notably, the Goodwin growth cycle emerges in a context where both classes are unorganized, suggesting that the dynamics of class struggle are crucial for understanding economic fluctuations.\n\nHeller (2018) reinforces the historical significance of class struggle, asserting that it has been a driving force in societal evolution from ancient times to modern capitalism. This perspective aligns with the notion that class antagonism is not only a contemporary issue but has deep historical roots, influencing the transition from one mode of production to another.\n\nLevine (1988) provides a critical analysis of the New Deal, arguing that class conflict significantly shaped policy decisions during the Great Depression. This underscores the idea that class struggle is not merely a theoretical construct but has tangible effects on political and economic outcomes.\n\nFurthermore, the exploration of class identities by Jeffery et al. (2020) introduces a contemporary lens on how individuals navigate their class positions within a neoliberal framework. Their findings suggest that while many may disavow a working-class identity, they still engage in struggles over classification and value, indicating an ongoing relevance of class dynamics in modern society.\n\nCollectively, these papers illustrate that the Goodwin class struggle model and its implications extend beyond theoretical frameworks, influencing historical developments, policy-making, and contemporary class identities. The interplay of class organization, historical context, and individual identity continues to shape the discourse on class struggle today.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2004}, "authors": ["D. Sonntag"], "summary": "We follow an empirical approach from data quality toward text quality, where the expectations of the consumer, human or machine, take the centre stage. We try to obtain numerical text quality statements which must be interpreted for the expectations of the user and suitability for automatic natural language processing (NLP) separately. We state that apart from text accessibility today only representational text quality metrics can be derived and computed automatically. Interestingly, text quality for NLP traces back to questions of text representation.", "title": "Assessing the Quality of Natural Language Text Data", "claims": null}, {"metadata": {"year": 2019}, "authors": ["E. Timoshchuk", "S. Kuznetsov", "Ziyomukhammad Usmonov", "Utih Amartiwi", "Farah Atif", "Harrif Saliu"], "summary": "The quality of requirements is difficult to measure in an automated way because of need in reviews and subjective opinion of stakeholders. Plenty of attributes can be used to evaluate requirements quality, but most of them have vague meaning and no concrete metrics for measurement. We proposed a model based on a goal-question-metric approach to identify the most important quality attributes and its metrics, which can be calculated in an automated way. Text of requirements can be analyzed by natural language processing techniques to reveal weak words and phrases, which make sentence subjective and ambiguous. We proposed metrics for such quality attributes as unambiguity, subjectivity, singularity, completeness, and calculated indexes based on the number of words and sentences for the read-ability attribute. Analytic hierarchy process for complex decisions was applied to convert calculated metrics of every requirement into overall quality evaluation of requirement document according to customer\u2019s priorities. Model was implemented in a prototype with focusing on adopting NLP techniques for Russian language and supporting external API.", "title": "Application of natural language processing with GQM and AHP approaches for requirements quality assessment", "claims": null}, {"metadata": {"year": 2001}, "authors": ["Chen T. Chen"], "summary": "Today the evaluation of service quality is one of the most important management issues for every organization. An important feature of the service process is the degree of customer influence on the service process. A characteristic of service quality evaluation is that it consists of both tangible and intangible factors. In this paper the customers' subjective opinions and the weights of factors are described by linguistic labels which can be expressed in trapezoidal fuzzy numbers. After aggregating these linguistic labels, a linguistic decision\u2013making method is proposed to evaluate the service quality level of each organization. Finally, a numerical example is shown to highlight the procedure of the proposed method at the end of this paper.", "title": "Applying Linguistic Decision-Making Method to Deal with Service Quality Evaluation Problems", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Malak Mashaabi", "Areej Alotaibi", "Hala Qudaih", "Raghad Alnashwan", "H. Al-Khalifa"], "summary": "Artificial intelligence and natural language processing (NLP) are increasingly being used in customer service to interact with users and answer their questions. The goal of this systematic review is to examine existing research on the use of NLP technology in customer service, including the research domain, applications, datasets used, and evaluation methods. The review also looks at the future direction of the field and any significant limitations. The review covers the time period from 2015 to 2022 and includes papers from five major scientific databases. Chatbots and question-answering systems were found to be used in 10 main fields, with the most common use in general, social networking, and e-commerce areas. Twitter was the second most commonly used dataset, with most research also using their own original datasets. Accuracy, precision, recall, and F1 were the most common evaluation methods. Future work aims to improve the performance and understanding of user behavior and emotions, and address limitations such as the volume, diversity, and quality of datasets. This review includes research on different spoken languages and models and techniques.", "title": "Natural Language Processing in Customer Service: A Systematic Review", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Niloufar Salehi Dastjerdi"], "summary": "Nowadays many customers use online services for buying their desired products and many products and services are sold through the Web. On the one hand, considerable numbers of people are expressing their opinion about a particular product or service by writing comments on them; and on the other hand, collecting user\u2019s opinions and giving feedback to the service or product providers. These are important techniques of increasing the chance of business success and improving service quality. For this, identifying opinion sentences and extracting appropriate features of review sentences can be perceived as a proper task. This paper applies techniques of natural language processing for the detection of sentences expressing an opinion. Subsequently, product features can be identified in each opinion sentence, considering appropriate grammatical concepts in the review sentence. In this instance, the review sentences will be considered as an input and extracted features and opinions as an output. Finally the results and accuracy of the system will be measured by related evaluation metrics.", "title": "Product feature extraction using natural language processing techniques", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Muneera  Bano", "Alessio  Ferrari", "Didar  Zowghi", "Vincenzo  Gervasi", "Stefania  Gnesi"], "summary": "With the huge number of services that are available online, requirements analysts face an overload of choice when they have to select the most suitable service that satisfies a set of customer requirements. Both service descriptions and requirements are often expressed in natural language (NL), and natural language processing (NLP) tools that can match requirements and service descriptions, while filtering out irrelevant options, might alleviate the problem of choice overload faced by analysts. In this paper, we propose a NLP approach based on Knowledge Graphs that automates the process of service selection by ranking the service descriptions depending on their NL similarity with the requirements. To evaluate the approach, we have performed an experiment with 28 customer requirements and 91 service descriptions, previously ranked by a human assessor. We selected the top-15 services, which were ranked with the proposed approach, and found 53% similar results with respect to top-15 services of the manual ranking. The same task, performed with the traditional cosine similarity ranking, produces only 13% similar results. The outcomes of our experiment are promising, and new insights have also emerged for further improvement of the proposed technique.", "title": "Automated Service Selection Using Natural Language Processing", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Natalia  Ponomareva", "Angels  Catena"], "summary": "Automatic analysis of customer conversations would be beneficial for service companies to improve service quality. In this case, such customer characteristics as satisfaction or competence are of the special interest. Unfortunately, their manual estimation is very laborious and it has a high level of subjectivity. In this work, we aim at parameterization of dialogues for formal (automatic) assessment of customer satisfaction. We elaborate a set of linguistic indicators represented both by lexico-syntactic patterns and rules and introduce their classification by kind, location and sign. We propose several linear regression models for satisfaction estimation and check them and their parameters on statistical significance. The best of the models demonstrates rather high level of concordance between automatic and manual assessments.", "title": "Linguistic Models Construction and Analysis for Satisfaction Estimation", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Yolande  Piris", "Anne-C\u00e9cile  Gay"], "summary": "Abstract This study uses natural language processing in order to increase knowledge concerning customer satisfaction. A total of 12,000 customer returns were analyzed, 6,800 of which contained freely expressed qualitative feedback. Eight themes emerge from the analysis and bring to light the factors influencing satisfaction. It is also noted that satisfaction is not vertical or horizontal but can involve a more or less important combination of themes. This study also shows the link between the level of satisfaction and the number of themes addressed, thus challenging traditional approaches that do not seem to distinguish the discursive differences between satisfied and dissatisfied customers. Finally, this investigation lays the foundations for automatic and personalized processing of customer comments.", "title": "Customer satisfaction and natural language processing", "claims": null}], "query": "natural language processing to assess service quality", "summary_abstract": "The research question regarding the application of natural language processing (NLP) to assess service quality is addressed through various studies that highlight the potential of NLP techniques in evaluating customer feedback and service interactions.\n\nSonntag (2004) emphasizes the importance of text quality in NLP, suggesting that understanding user expectations is crucial for deriving meaningful metrics from text data. This foundational perspective sets the stage for further exploration of how NLP can be utilized to assess service quality.\n\nTimoshchuk et al. (2019) propose a model that leverages NLP to analyze the quality of requirements in service contexts. Their approach identifies key quality attributes such as unambiguity and completeness, which can be quantitatively assessed through NLP techniques. This model illustrates how NLP can automate the evaluation of service-related texts, enhancing the understanding of service quality.\n\nMashaabi et al. (2022) conduct a systematic review of NLP applications in customer service, noting that chatbots and question-answering systems are prevalent in various fields. They highlight the importance of evaluating user interactions to improve service quality, indicating that NLP can play a significant role in understanding customer needs and emotions.\n\nDastjerdi (2012) focuses on the extraction of opinions from customer reviews using NLP techniques. By identifying opinion sentences and associated product features, this study demonstrates how NLP can facilitate the collection and analysis of customer feedback, which is vital for assessing service quality.\n\nBano et al. (2015) introduce an NLP approach that utilizes Knowledge Graphs to automate service selection based on customer requirements. Their findings suggest that NLP can effectively match service descriptions with customer needs, thereby improving the decision-making process related to service quality.\n\nPonomareva and Catena (2009) explore the automatic analysis of customer conversations to assess satisfaction levels. They develop linguistic indicators for evaluating customer interactions, which can provide insights into service quality without the labor-intensive manual assessments typically required.\n\nFinally, Piris and Gay (2021) analyze customer feedback using NLP to uncover themes influencing satisfaction. Their study reveals that satisfaction is multifaceted and can be better understood through qualitative analysis of customer comments, laying the groundwork for personalized service improvements.\n\nCollectively, these studies illustrate that NLP is a powerful tool for assessing service quality by automating the analysis of customer feedback, identifying key quality attributes, and enhancing the understanding of customer satisfaction. The integration of NLP techniques into service evaluation processes can lead to more informed decision-making and improved service delivery.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2023}, "authors": ["Z. Jeffries", "J. Bos", "P. McManamon", "C. Kershner", "Akhil M Kurup"], "summary": "Abstract. This paper describes the initial results from the first of 3 years of planned testing aimed at developing methods, metrics, and targets necessary to develop standardized tests for these instruments. Here, we evaluate range error accuracy and precision for eight automotive grade lidars; a survey grade lidar is used as a reference. These lidars are tasked with detecting a static, child-sized, target at ranges between 5 and 200 m. Our target, calibrated to 10% reflectivity and Lambertian, is a unique feature of this test. We find that lidar range precision is in line with the values reported by each manufacturer. However, we find that maximum range and target detection can be negatively affected by presence of an adjacent strong reflector. Finally, we observe that design trade-offs made by each manufacturer lead to important performance differences that can be quantified by tests such as the ones proposed here. This paper also includes some lessons learned, planned improvements, and discussion of future iterations of this activity.", "title": "Toward open benchmark tests for automotive lidars, year 1: static range error, accuracy, and precision", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Z. Fasnacht", "A. Vasilkov", "D. Haffner", "W. Qin", "J. Joiner", "N. Krotkov", "A. Sayer", "R. Spurr"], "summary": "Abstract. Satellite-based cloud, aerosol, and trace gas retrievals from ultraviolet (UV) and visible (Vis) wavelengths depend on the accurate representation of surface reflectivity. Current UV and Vis retrieval algorithms typically use surface reflectivity climatologies that do not account for variation in satellite viewing geometry or surface roughness. The concept of geometry-dependent surface Lambertian-equivalent reflectivity (GLER) is implemented for water surfaces to account for surface anisotropy using a Case 1 water optical model and the Cox-Munk slope distribution for ocean surface roughness. GLER is compared with Lambertian-Equivalent reflectivity (LER) derived from the Ozone Monitoring Instrument (OMI) for clear scenes at 354, 388, 440, and 466\u2009nm. We show that GLER compares well with the measured LER data over the open ocean and captures the directionality effects not accounted for in climatological LER databases. Small biases are seen when GLER and the OMI-derived LER are compared. GLER is biased low by up to 0.01\u20130.02 at Vis wavelengths, and biased high by around 0.01 in the UV, particularly at 354\u2009nm. Our evaluation shows that GLER is an improvement upon climatological LER databases as it compares well with OMI measurements, and captures the directionality effects of surface reflectance.\n", "title": "A geometry-dependent surface Lambertian-equivalent reflectivity product for UV/Vis retrievals: Part II. Evaluation over open ocean", "claims": null}, {"metadata": {"year": 2019}, "authors": ["Z. Fasnacht", "A. Vasilkov", "D. Haffner", "W. Qin", "J. Joiner", "N. Krotkov", "A. Sayer", "R. Spurr"], "summary": "Abstract. Satellite-based cloud, aerosol, and trace-gas retrievals from ultraviolet (UV) and visible (Vis) wavelengths depend on the accurate representation of surface reflectivity. Current UV and Vis retrieval algorithms typically use surface reflectivity climatologies that do not account for variation in satellite viewing geometry or surface roughness. The concept of geometry-dependent surface Lambertian-equivalent reflectivity (GLER) is implemented for water surfaces to account for surface anisotropy using a Case 1 water optical model and the Cox\u2013Munk slope distribution for ocean surface roughness. GLER is compared with Lambertian-Equivalent reflectivity (LER) derived from the Ozone Monitoring Instrument (OMI) for clear scenes at 354, 388, 440, and 466\u2009 nm . We show that GLER compares well with the measured LER data over the open ocean and captures the directionality effects not accounted for in climatological LER databases. Small biases are seen when GLER and the OMI-derived LER are compared. GLER is biased low by up to 0.01\u20130.02 at Vis wavelengths and biased high by around 0.01 in the UV, particularly at 354\u2009 nm . Our evaluation shows that GLER is an improvement upon climatological LER databases as it compares well with OMI measurements and captures the directionality effects of surface reflectance.", "title": "A geometry-dependent surface Lambertian-equivalent reflectivity product for UV\u2013Vis retrievals \u2013 Part 2: Evaluation over open ocean", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Davide Cassanelli", "S. Cattini", "Giorgio Di Loro", "L. D. Cecilia", "L. Ferrari", "L. Rovati"], "summary": "The use of LiDARs in automotive is increasingly widespread. In order to operate in a critical environment such as that of mobility, these systems must offer increasingly high performance. In particular, the ability to estimate the position of objects regardless of their reflectance and presence of diffusing backgrounds is a very sought-after feature by manufacturers. In commercial systems various strategies are used to make the measurement as insensitive as possible to these effects, however, it is not possible to fully compensate for the measurement errors caused by them. In this paper, we propose two simple experimental setups to verify the presence of these measurement errors in two scenarios. Moreover, we report the performance of a commercial LiDAR (MRS 6000 by Sick) using certified reflectance standards (Spectralon\u00ae Diffuse Reflectance Standards, by Labsphere). For this LiDAR, the results obtained show that a logarithmic variation of the reflectance of the target of 1.26-log at a target distance 2.4 m provides incompatible measurements. Furthermore, the presence of a background at a distance shorter than 11 cm, 12 cm and 13 cm respectively with 50 %, 75 % and 99 % reflectance also provides incompatible measurements for a target distance of 1.3 m from the LiDAR.", "title": "LiDARs detected signal and Target distance estimation: measurement errors from Target reflectance and multiple echos", "claims": null}, {"metadata": {"year": 2008}, "authors": ["M. Disney", "Philip Lewis", "M. Bouvet"], "summary": "Spaceborne lidar missions are being studied to estimate atmospheric concentrations of CO2, water vapour and O3, as well as for measuring surface biophysical properties. Lidar instruments typically observe the highest possible surface reflectance due to observing in the retroreflection peak (the so-called 'hotspot'), where shadowing on the surface is minimised. The likely range of observed reflectance will determine the required dynamic range and desired signal-to-noise ratio (SNR) of such an instrument, but it is difficult to predict this range a priori. A method is presented for estimating lidar surface reflectance over a range of vegetated surface types using multi-angle, multi-spectral reflectance data. The approach is validated using radiative transfer simulations of highly detailed 3D vegetation canopy models. The method is particularly useful for testing proposed lidar instrument configurations.", "title": "Quantifying Surface Reflectivity for Spaceborne Lidar Missions", "claims": null}, {"metadata": {"year": 1994}, "authors": ["Michael  Oren", "Shree K. Nayar"], "summary": "Lambert's model for diffuse reflection is extensively used in computational vision. For several real-world objects, the Lambertian model can prove to be a very inaccurate approximation to the diffuse component. While the brightness of a Lambertian surface is independent of viewing direction, the brightness of a rough diffuse surface increases as the viewer approaches the source direction. A comprehensive model is developed that predicts reflectance from rough diffuse surfaces. Experiments have been conducted on real samples, such as, plaster, clay, and sand. The reflectance measurements obtained are in strong agreement with the reflectance predicted by the proposed model.", "title": "Seeing Beyond Lambert's Law", "claims": null}, {"metadata": {"year": 1997}, "authors": ["P. Nostell", "A. Roos", "B. Karlsson"], "summary": "Commercially available anodized aluminum, vacuum evaporated aluminum foil, and roofing aluminum have been optically characterized in the wavelength range 300 - 2500 nm in a single beam Edwards sphere at oblique angle of incidence (60 degrees). The first two materials are long-term stable, which makes them promising to use as external reflectors. The third one is of interest because of its low cost. A model is proposed for the correction factors for the single beam sphere. A comparison between the single beam sphere and a Beckman 5240 standard spectrophotometer equipped with an integrating sphere was made. The agreement between the measurements was excellent. Furthermore, angle resolved measurements were made with a scatterometer at 633 nm at the incidence angle 60 degrees for all three samples. These data were used in a simulation model to calculate the intensity in the collector plane, assuming a semi-infinite collector and dito external reflector. A calculation of a two-dimensional ideal Lambertian sample was also made. It is shown that optically dull materials, such as roofing aluminum, which has rolling grooves that scatter light, perform equally well as optically smooth materials, provided the scatter is confined to low angles. It is also concluded that integrating sphere measurements of the specular component of diffuse sample in standard spectrophotometers are misleading. The low-angle scattering of diffuse samples also contributes to the intensity in the collector plane and should therefore also be included in the measurement. For Lambertian samples the scatter is too high and such a surface is not a feasible choice of material.", "title": "Optical characterization of solar reflecting surfaces", "claims": null}, {"metadata": {"year": 2006}, "authors": ["Yang Ruike"], "summary": "Using a plate whose hemisphere reflectivity is known as a standard plate,the lidar scattering cross section(LRCS) and hemisphere reflectivity of a target plate are measured inversely.Because of the influence of the random fluctuation of the atmospheric attenuation coefficient and the laser\u2032s lasing energy,there are deviations between the scattering voltage angular distribution curves and the lambert cosine curve.Therefore,the two-channel and relative measurement method is adopted to determine the atmospheric attenuation coefficient and the lasing energy.Then the lidar scattering cross section of the target plate can be calculated by comparing with that of the standard plate.The experimental value of the target plate\u2032s hemisphere reflectivity is 24.9%,the real value of it is 25.3%,the relative error is 1.6%,which satisfies the accuracy requirements of outfield test.The experimental results indicate that the influence of the random fluctuation of atmospheric attenuation coefficient and laser\u2032s lasing energy on the test results can been washed away very well by using the two-channel and relative measurement method.", "title": "Measurement and Analysis of the Outfield Target\u2032s Laser Scattering Characteristics", "claims": null}], "query": "lidar measurement error for surfaces with a reflectivity higher than a lambertian surface", "summary_abstract": "The research question regarding lidar measurement error for surfaces with reflectivity higher than a Lambertian surface is addressed in several studies, highlighting the complexities and challenges associated with lidar performance in varying reflectivity conditions.\n\nJeffries et al. (2023) conducted tests on automotive-grade lidars, revealing that while range precision aligns with manufacturer specifications, the presence of strong reflectors can adversely affect maximum range and target detection. This suggests that lidar systems may struggle with surfaces that reflect light more intensely than a Lambertian surface, indicating potential measurement errors in such scenarios.\n\nFasnacht et al. (2019) introduced the concept of geometry-dependent surface Lambertian-equivalent reflectivity (GLER) to account for surface anisotropy, particularly in water surfaces. Their findings indicate that traditional climatological reflectivity databases may not adequately represent the directional effects of surface reflectance, which could lead to inaccuracies in lidar measurements when dealing with highly reflective surfaces.\n\nCassanelli et al. (2022) explored the performance of a commercial lidar system, noting that variations in target reflectance can lead to significant measurement errors. Their experiments demonstrated that even small changes in reflectance could result in incompatible measurements, particularly when backgrounds with varying reflectance are present. This highlights the sensitivity of lidar systems to reflectivity variations beyond the Lambertian model.\n\nOren and Nayar (1994) discussed the limitations of the Lambertian model for diffuse reflection, emphasizing that real-world surfaces often exhibit directional reflectance characteristics that deviate from Lambertian behavior. Their comprehensive model for rough diffuse surfaces suggests that lidar measurements may be inaccurate when applied to such materials, further complicating the assessment of measurement errors.\n\nIn summary, the collective findings from these studies indicate that lidar systems face significant challenges when measuring surfaces with reflectivity higher than that of a Lambertian surface. Factors such as the presence of strong reflectors, surface anisotropy, and the inherent limitations of the Lambertian model contribute to potential measurement errors, necessitating further research and development of improved lidar calibration and measurement techniques.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2021}, "authors": ["Elisangela Gueiber Montes", "Fabiana Postiglione Mansani", "M. Schafranski", "Alceu de Oliveira Toledo J\u00fanior", "Lorena de Freitas Calixto", "R\u00f4mulo Lopes da Costa", "Rodrigo Luiz Staichak", "Francine Marques Pinto", "Juliane de Lara Berso", "Nicole Guimar\u00e3es", "Marcos Noris Bernardes de Souza", "Bruno Queiroz Zardo", "J. Vellosa"], "summary": "Rheumatoid arthritis is an autoimmune inflammatory joint disease with global prevalence of 0.4% to 1.0%. Extra-articular manifestations increase its morbidity and severity, and cardiovascular diseases present the greatest risk. Therapeutic approaches have been used to treat rheumatoid arthritis, often involving the use of multiple classes of drugs with different mechanisms and forms of action. Corticosteroid therapy is widely used in this therapeutic combination; however, its use has been widely questioned because of its high toxicity and some negative effects, including the possibility of increased cardiovascular risk, depending on the dosage. Some studies have provided important insights into how glucocorticoids have an impact on cardiac complications in patients with rheumatoid arthritis. Most of these studies have concluded that exposure to these drugs at high or cumulative doses is associated with increased risk of death, as well as possibly being associated with the presence of a positive rheumatoid factor.", "title": "Relationship between corticotherapy and increased cardiac risk in patients with rheumatoid arthritis", "claims": null}, {"metadata": {"year": 2014}, "authors": ["A. V. van Sijl", "M. Boers", "A. Voskuyl", "M. Nurmohamed"], "summary": "Objective To evaluate the risk of cardiovascular disease in patients with rheumatoid arthritis exposed to glucocorticoids. Methods Retrospective analysis of exposure to glucocorticoids in a prospective cohort of 353 patients with rheumatoid arthritis followed from June 2001 up to November 2011 for incident cardiovascular disease in a hospital-based outpatient cohort in the Netherlands. Hazard ratios with 95%-confidence intervals were calculated for the association between different types of exposure to glucocorticoids and incident cardiovascular disease. Associations were adjusted for demographics, cardiovascular risk factors and disease related parameters. Results Recent and current exposure to glucocorticoids were associated with incident cardiovascular disease, as was a longer duration of exposure and cumulative exposure to glucocorticoids. Adjustment for disease activity and severity negated the association. Conclusion In observational studies the finding of incident cardiovascular disease in patients with rheumatoid arthritis exposed to glucocorticoids is strongly confounded by indication due to high disease activity. The adverse cardiovascular effects of glucocorticoids might be balanced by positive effects working through inflammation control.", "title": "Confounding by Indication Probably Distorts the Relationship between Steroid Use and Cardiovascular Disease in Rheumatoid Arthritis: Results from a Prospective Cohort Study", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Negin  Nassabeh"], "summary": "Despite the widespread availability of gluco corticoids for the treatment of rheuma toid arthritis (ra), their long-term cardiovascular safety has been questioned. Driven by evidence indicating an increased prevalence of cardiovascular disease risk factors in ra, investigators from the uK have now explored a potential link between long-term glucocorticoid use and the metabolic syndrome. although definitive diagnostic criteria have not been developed, the metabolic syndrome is generally characterized as a collection of classic cardiovascular risk factors.", "title": "Rheumatoid arthritis: Do steroids increase metabolic syndrome risk?", "claims": null}, {"metadata": {"year": 2005}, "authors": ["John M. Davis", "H. Maradit-Kremers", "S. Gabriel"], "summary": "People with rheumatoid arthritis (RA) experience an increased burden of cardiovascular disease (CVD) and reduced survival compared to the general population1. Myocardial infarction (MI) and heart failure appear to be more prevalent in RA compared to subjects without RA2. In one study, the risk of congestive heart failure was 2-fold higher among RA subjects compared to non-RA controls3. Much of the best evidence suggests that systemic inflammation plays an important role in the pathogenesis of CVD in RA1. Still, a nagging question persists in the minds of many rheumatologists today: What is the role of glucocorticoids (GC)? A common view is that GC harm the cardiovascular system. We wish to examine an alternative hypothesis: that GC might actually reduce the risk of CVD in patients with RA. Our aim is not to provide a comprehensive review of the cardiovascular effects of GC but rather to bring balance to the debate of how use of GC might affect the development of CVD in patients with RA. A discussion of GC is highly relevant given current rheumatology practice. GC have been used to treat people with RA for the last half-century, and recently, there has been renewed interest in these medications4. Use of GC is highly prevalent among people with RA; for example, these are taken by 30\u201350% of patients enrolled in recent therapeutic trials5. Rheumatologists commonly use GC as \u201cbridge therapy\u201d awaiting efficacy of slow-acting disease modifying antirheumatic drugs (DMARD), and also for treating disease flares. Frequently, delays occur in initiating biologic agents due to time required for insurance company authorization, tuberculin skin testing, and/or patient education visits. For these reasons, GC will likely continue to be an important component of the armamentarium for patients with RA, at least for the foreseeable future. Despite years of use, relatively little is known regarding the longterm effects of GC, particularly on the development of CVD4. In light of the increased burden of CVD in RA, it is critical to better understand how GC affect the development of CVD in this population, whether beneficial or harmful. Inflammation plays a fundamental role in the pathogenesis of CVD in RA. For example, abnormalities in traditional cardiovascular risk factors alone cannot explain the increased risk of CVD in this population6. Also, clinical features in RA that are associated with high inflammatory disease activity such as elevated acute phase markers and extraarticular manifestations are associated with cardiovascular events and mortality3,7-10. Theoretically, GC could modulate the risk of CVD in RA in 2 competing ways. The risk could increase due to potentially deleterious effects of GC on lipids, glucose tolerance, hypertension, or obesity11. Alternatively, by alleviating systemic inflammation, GC may actually decrease the risk of atherosclerosis and CVD1,12. Evidence supporting both directions of effect will be reviewed. Furthermore, there is evidence that effective treatment of systemic inflammation with DMARD is associated with lower risk of cardiovascular mortality13. Treatment with antagonists of tumor necrosis factor-alpha (TNF-\u03b1) may lower the risk of heart failure14. Therefore, it is reasonable to consider the possibility that GC may also reduce the risk of cardiovascular events and mortality. A number of novel findings in epidemiological studies suggest that \u201cflares\u201d of inflammation in RA may be involved in triggering cardiovascular events. First of all, the risk of cardiovascular events is independent of the duration of RA8. Second, the risk of sudden death and silent MI appears to be increased very early, even prior to patients ful-", "title": "Use of low-dose glucocorticoids and the risk of cardiovascular morbidity and mortality in rheumatoid arthritis: what is the true direction of effect?", "claims": null}, {"metadata": {"year": 2011}, "authors": ["M. Nikpour", "S. Brady", "D. Liew"], "summary": "Rheumatoid arthritis (RA) is associated with an approximately twofold increased risk of atherosclerotic cardiovascular disease (CVD) including myocardial infarction and stroke. The increased risk of CVD in RA is due to an interplay between traditional risk factors such as hyperlipidemia, hypertension, and smoking and disease-related variables such as the presence of rheumatoid factor and anticyclic citrullinated peptide antibodies, high erythrocyte sedimentation rate, and joint swelling. Systemic inflammation and immune mechanisms form a pathogenic link between synovitis and atherosclerosis in RA. Indeed, high levels of C-reactive protein, an inflammatory marker, predict cardiovascular mortality in RA. Furthermore, the risk of CVD is greatly diminished among patients who respond to disease modifying antirheumatic drugs and biological therapies such as tumor necrosis factor (TNF) alpha antagonists. Through adverse effects on lipid profile and blood glucose level, long-term use of high-dose glucocorticoids in RA also increases cardiovascular risk. However, through control of active disease, glucocorticoids may also indirectly attenuate cardiovascular risk. Through their lipid-lowering and immunomodulatory effects, statins may have a dual benefit in the treatment of patients with RA. However, data on cardiovascular risk reduction in RA through management of traditional risk factors remain scant. Current research efforts are directed toward elucidating the risk factors for CVD in RA and developing strategies to minimize this risk.", "title": "The epidemiological and pathogenic association of rheumatoid arthritis with atherosclerotic cardiovascular disease.", "claims": null}, {"metadata": {"year": 2019}, "authors": ["N. Rebrova", "O. Sarkisova", "T. Ripp", "I. Bogomolova", "E. Anisimova", "Karpov Rs", "V. Mordovin"], "summary": "Objective:Hypertension (HTN) and rheumatoid arthritis (RA) are considered to be independent predictors of stroke. A long exposure to Glucocorticoids (GCs) (in years) is associated with a high CVD risk. A impaired cerebrovascular reactivity (CVR) is associated with increased risk of stroke. The aim o", "title": "GLUCOCORTICOIDS TREATMENT CAN WORSEN CEREBROVASCULAR REACTIVITY IN HYPERTENSIVE PATIENTS WITH RHEUMATOID ARTHRITIS", "claims": null}, {"metadata": {"year": 2015}, "authors": ["L. Rasch", "I. Bultink", "L. V. van Tuyl", "W. Lems"], "summary": "Introduction: Glucocorticoids (GCs) are often used in the treatment of rheumatoid arthritis and many other inflammatory diseases. Besides strong favorable effects on disease activity, GCs can cause (serious) side effects as well. Areas covered: Side effects of GCs that are ranked as most important by rheumatologists as well as by patients are bone loss and fractures, cardiovascular events, hypertension, and diabetes mellitus. In evaluating these side effects, confounding by indication is a disturbing factor: not only the use of GCs can increase the risk of several side effects, but so can the activity of the underlying disease, which in turn is related to the amount of GCs that is prescribed to the patient. Expert opinion: Generally, side effects predominantly occur in patients with a high disease activity and when used in high doses and for a long period of time. For these patients, caution and monitoring are most warranted. However, monitoring is not only recommended in patients with a high disease activity, and high-dose or long-term use of GCs, but in all GC users, since side effects may also occur in patients treated with low-dose GCs. When detecting possible negative effects in time, they might be managed and serious damage due to side effects might hopefully be prevented.", "title": "Glucocorticoid safety for treating rheumatoid arthritis", "claims": null}, {"metadata": {"year": 2010}, "authors": ["B.  Hansel", "E.  Bruckert"], "summary": "Abstract The increased mortality in patients with rheumathoid arthritis (RA) is mainly due to high incidence of cardiovascular (CV) disease. CV morbidity and mortality in RA can be explained by several mechanisms: (1) chronic inflammation, (2) enhanced prevalence of traditional CV risk factors including atherogenic dyslipoproteinemia, (3) a lower use of evidence-based therapy such as statins and (4) chronic treatment for RA such as glucocorticoids. It is difficult to distinguish between the role of pharmacological treatment per se and the severity or duration of the disease since these two parameters are closely interrelated. RA likely influences lipoprotein metabolism leading to quantitative and qualitative alteration of low-density lipoproteins (LDL) and of high-density lipoproteins. Glucocorticoids alter carbohydrate and lipid metabolism. However, by reducing the inflammation level, the net effect on lipid parameters and on the CV risk may be favorable. Data from open follow-up studies would suggest that methotrexate use is associated with a beneficial effect on lipid parameters and with a reduction in the incidence of CV disease. Anti-TNF agents increase LDL-cholesterol in some but not all studies; however the use of anti-TNF agents likely reduce CV risk in patients with RA. The influence of recently developed compounds, anti-CD20, CTLA-4\u00a0Ig or anti-IL6\u00a0is not well documented. Anti-IL6\u00a0seem to increase total and LDL-cholesterol; however these changes are associated with an improvement in the TC/HDL-C ratio.", "title": "Profil lipidique et risque cardiovasculaire chez les patients atteints de polyarthrite rhumato\u00efde : influence de la maladie et de la th\u00e9rapeutique m\u00e9dicamenteuse", "claims": null}], "query": "do glucocorticoids increase the risk of cardiovascular disease in rheumatoid arthritis?", "summary_abstract": "The question of whether glucocorticoids increase the risk of cardiovascular disease (CVD) in patients with rheumatoid arthritis (RA) has been explored in several studies, yielding a nuanced understanding of the relationship between glucocorticoid use and cardiovascular health.\n\nMontes et al. (2021) highlight that while glucocorticoids are commonly used in RA treatment, their high toxicity and potential negative effects, particularly at high or cumulative doses, may indeed elevate cardiovascular risk. They note that studies have linked high glucocorticoid exposure to increased mortality and cardiovascular complications.\n\nConversely, van Sijl et al. (2014) conducted a retrospective analysis and found that recent and cumulative exposure to glucocorticoids was associated with incident cardiovascular disease. However, they also pointed out that this association could be confounded by the high disease activity typical in patients requiring glucocorticoid therapy, suggesting that the adverse cardiovascular effects might be mitigated by the benefits of controlling inflammation.\n\nDavis et al. (2005) present a more balanced view, proposing that glucocorticoids could potentially reduce cardiovascular risk by alleviating systemic inflammation, which is a significant contributor to CVD in RA. They argue that while glucocorticoids may have deleterious effects on traditional cardiovascular risk factors, their role in managing inflammation could counteract these risks.\n\nNikpour et al. (2011) further elaborate on this duality, indicating that while long-term high-dose glucocorticoid use can increase cardiovascular risk through adverse effects on lipid profiles and glucose metabolism, effective management of RA with these medications may also lower the risk of atherosclerosis and cardiovascular events by controlling disease activity.\n\nRasch et al. (2015) emphasize the importance of monitoring glucocorticoid use, particularly in patients with high disease activity, as both the medication and the underlying disease can contribute to cardiovascular risks. They advocate for careful management to mitigate potential side effects.\n\nIn summary, the evidence suggests that glucocorticoids can both increase and potentially decrease cardiovascular risk in RA patients, largely depending on the dosage, duration of use, and the underlying disease activity. The interplay between managing inflammation and the side effects of glucocorticoids remains a critical area for further research and clinical consideration.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2023}, "authors": ["Harry Cooperman", "J. D. Duffie", "Stephan Luck", "Zachry Wang", "Y. Yang"], "summary": "Pre-committed revolving credit lines tend to be drawn heavily when bank funding markets are stressed, presenting an important funding risk to banks. Until recently, banks have mitigated this funding risk by linking the interest paid on lines to risksensitive reference rates such as the London Interbank Offered Rate (LIBOR). We show that incentives to provide credit lines are dampened by the debt-overhang cost to bank shareholders associated with funding stressed-market drawdowns. However, the associated adverse effect on credit supply is attenuated if: (1) drawdowns are expected be left on deposit at the same bank, which occurred at the largest banks during the COVID shock of March 2020, and (2) revolvers are linked to creditsensitive reference rates that reduce borrower incentives to draw more heavily on their lines during stressed markets. We estimate how the replacement of LIBOR with risk-free alternative reference rates will affect the supply of revolving credit. JEL: G00, G01, G02, G20, G21, E4, E43", "title": "Bank Funding Risk, Reference Rates, and Credit Supply", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Anne Duquerroy", "Adrien Matray", "Farzad Saidi"], "summary": "We quantify how banks\u2019 funding costs affect their lending behavior directly, and indirectly by feeding back to their net worth. For identification, we exploit banks\u2019 heterogeneous liability structure and the existence of regulated deposits in France whose rates are set by the government. Using administrative credit-registry and regulatory bank data, we find that a one-percentage-point increase in funding costs reduces credit by 17%. To insulate their profits, banks reach for yield and rebalance their lending towards smaller and riskier firms. These changes are not compensated for by less affected banks at the aggregate city level, with repercussions for firms\u2019 investment. JEL classification: E23, E32, E44, G20, G21, L14", "title": "Tracing Banks\u2019 Credit Allocation to their Funding Costs", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Asli  Demirg\u00fc\u00e7-Kunt", "Harry  Huizinga"], "summary": "This paper examines the implications of bank activity and short-term funding strategies for bank risk and return using an international sample of 1334 banks in 101 countries leading up to the 2007 financial crisis. Expansion into non-interest income generating activities such as trading increases the rate of return on assets, and it may offer some risk diversification benefits at very low levels. Non-deposit, wholesale funding in contrast lowers the rate of return on assets, while it can offer some risk reduction at commonly observed low levels of non-deposit funding. A sizeable proportion of banks, however, attract most of their short-term funding in the form of non-deposits at a cost of enhanced bank fragility. Overall, banking strategies that rely prominently on generating non-interest income or attracting non-deposit funding are very risky, consistent with the demise of the U.S. investment banking sector.", "title": "Bank Activity and Funding Strategies: The Impact on Risk and Return", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Asli Demirg\u00fc\u00e7-Kunt", "H. Huizinga"], "summary": "This paper examines the implications of bank activity and short-term funding strategies for bank risk and return using an international sample of 1334 banks in 101 countries leading up to the 2007 financial crisis. Expansion into non-interest income generating activities such as trading increases the rate of return on assets, and it may offer some risk diversification benefits at very low levels. Non-deposit, wholesale funding in contrast lowers the rate of return on assets, while it can offer some risk reduction at commonly observed low levels of non-deposit funding. A sizeable proportion of banks, however, attract most of their short-term funding in the form of non-deposits at a cost of enhanced bank fragility. Overall, banking strategies that rely prominently on generating non-interest income or attracting non-deposit funding are very risky, consistent with the demise of the U.S. investment banking sector.", "title": "Bank Activity and Funding Strategies: The Impact on Risk and Return", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Peter Bednarek", "Valeriya Dinger", "Daniel Marcel te Kaat", "Natalja von Westernhagen"], "summary": "This paper examines the relationship between central bank funding and credit risk-taking. Employing comprehensive bank-firm-level data from the German credit registry during 2009:Q1-2014:Q4, we find that borrowing from the central bank is associated with rebalancing of bank portfolios towards ex-ante riskier firms. We further establish that this relationship is associated with the ECB's maturity extensions and that the risk-taking sensitivity of banks borrowing from the ECB is independent of idiosyncratic bank characteristics. Finally, we highlight that these shifts in bank lending might lead to an ex-post deterioration of bank balance sheets, but increase firm-level investment and employment.", "title": "Central Bank Funding and Credit Risk-Taking", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Jeffrey R. Gerlach", "Nada  Mora", "Pinar  Uysal"], "summary": "One key risk to the banking system is how funding costs will change as monetary policy is normalized and interest rates rise after almost a decade of near-zero rates. Our contribution is to develop a model that jointly estimates banks\u2019 balance sheets and retail interest rates to arrive at a consistent estimate of the change in bank funding costs as market rates change. Our estimates imply a 100 basis-point shock to the Federal Funds rate would increase overall deposit funding costs by about $40 billion, which is roughly equal to 25% of aggregate annual net income for commercial banks and savings institutions. We also find that deposit rate responses are largely symmetric, in contrast to some previous research showing deposit rates are less responsive to upward movements in reference rates. We introduce unique and confidential data on bank deposit betas to anchor our results.", "title": "Bank funding costs in a rising interest rate environment", "claims": null}, {"metadata": {"year": 2007}, "authors": ["Guy B. Ford", "Maike Sundmacher"], "summary": "An increase in the credit rating on the debt of an organisation is generally perceived positively, as higher credit ratings are, in the main, associated with lower perceived volatility in the market value of the assets of the entity that has issued debt. Lower asset volatility implies more stable and sustainable cash flows, and thus a lower likelihood of default on debt, and the result is a lower credit spread on the debt. If banks price their assets to realise a target return on economic capital, then a higher credit rating will result in higher loan rates if the fall in the bank's cost of capital, associated with the lower insolvency risk, is insufficient to offset the additional net income that the loan must be priced to cover. In this paper we develop a loan pricing model that assumes that financial institutions price their assets on a risk- and cost-adjusted basis and with the aim of achieving a minimum required return on the bank's economic capital holding. We compare theoretically derived decreases in the bank's cost of funds to actual data on bank credit spreads in order to ascertain the extent to which the increase in credit rating is beneficial to the bank. We find that the minimum decline in the cost of funds in our model generally exceeds the empirical data, meaning that the reduction in funding costs is insufficient to offset the increase on loan rates associated with higher economic capital. The divergence increases as the proportion of retail funds increases. We further find that the hurdle rate on economic capital is a significant factor in determining the value of a bank increasing its solvency standard. If the hurdle rate remains fixed regardless of the capital structure of the bank, then an upward movement in credit rating may have little impact on the value of the bank given the large divergence between the theoretical decline in the cost of wholesale funds and empirical data on bank credit spreads. However, the divergence is considerably less pronounced if the hurdle rate is varied in direct proportion to the leverage of the bank.", "title": "Bank Credit Rating Dynamics", "claims": null}, {"metadata": {"year": 2013}, "authors": ["In\u00eas  Drumond", "Jos\u00e9  Jorge"], "summary": "This paper analyzes how the effects of the introduction of risk-based bank capital requirements on bank loan rates depend on the market structure of the banking industry. We show that, when granting loans to borrowers under Basel II or Basel III capital requirements, banks with market power internalize an additional cost, in terms of regulatory capital, associated with the increase of borrowers' risk of default. As a result, the intermediation margin on bank loans increases with the changeover from non-risk to risk-based capital requirements, thereby making lending more expensive.", "title": "Loan interest rates under risk-based capital requirements: The impact of banking market structure", "claims": null}], "query": "impact of bank funding cost on credit risk", "summary_abstract": "The impact of bank funding costs on credit risk is a multifaceted issue explored in several studies. Cooperman et al. (2023) highlight that during periods of market stress, banks face significant funding risks, particularly when pre-committed credit lines are drawn heavily. They note that linking interest rates on these lines to risk-sensitive reference rates can mitigate some adverse effects on credit supply, especially if drawdowns are expected to remain at the same bank.\n\nDuquerroy et al. (2022) provide empirical evidence showing that a one-percentage-point increase in funding costs can lead to a 17% reduction in credit supply. This reduction is particularly pronounced as banks tend to shift their lending towards smaller and riskier firms to maintain profitability, which can have negative repercussions for overall firm investment.\n\nBednarek et al. (2020) further explore the relationship between central bank funding and credit risk-taking, finding that access to central bank borrowing encourages banks to lend to riskier firms. This behavior, while potentially beneficial for firm-level investment and employment, may lead to a deterioration of bank balance sheets over time.\n\nGerlach et al. (2018) discuss the implications of rising interest rates on bank funding costs, estimating that a 100 basis-point increase in the Federal Funds rate could significantly impact banks' net income. This suggests that as funding costs rise, banks may face increased pressure to adjust their lending practices, potentially leading to higher credit risk.\n\nIn summary, the collective findings indicate that rising bank funding costs can lead to reduced credit supply and a shift towards riskier lending practices, which may ultimately heighten credit risk within the banking sector. The studies underscore the delicate balance banks must maintain between managing funding costs and ensuring credit availability, particularly in times of economic stress.", "summary_extract": null}, {"papers": [{"metadata": {"year": 1972}, "authors": ["M. E. Lepera", "J. G. Sonnenburg"], "summary": "Abstract : The storage stability characteristics of diesel fuels meeting the requirements of Federal Specification VV-F-800a (Fuel Oil, Diesel) were investigated by conducting a two-year storage program involving four different diesel fuels. The fuels, stored in 100 bbl. above-ground steel tanks located at Aberdeen Proving Ground, Maryland, were sampled at three levels each month for the first year and every third month during the second year. Specific tests were performed on each sample to assess the degree of fuel deterioration. At the termination of the two-year program, two of the fuels maintained satisfactory storage stability whereas one fuel exhibited a cyclic pattern typical of unsatisfactory storage stability. The fourth fuel was considered marginal with respect to storage stability since initially low values were evidenced followed by a gradual increase indicative of fuel instability. (Author)", "title": "Storage Stability of Automotive Diesel Fuels.", "claims": null}, {"metadata": {"year": 1995}, "authors": ["Jarrett", "H. Von"], "summary": "1. Purchase good clean fuel from a reliable dealer who handles fuel from a good distillation source. 2. Keep fuel clean and free of water and dirt\u2014beware of fuel that has been stored too long. 3. It takes about 24 hours for water to settle out of diesel fuel. 4. Most fuel contamination occurs during storage after delivery; however, hauling gasoline and diesel fuel without thoroughly cleaning tanks and hoses can also cause problems.", "title": "Fuel System Maintenance", "claims": null}, {"metadata": {"year": 1973}, "authors": ["M. E. Lepera", "J. G. Sonnenburg"], "summary": "The storage stability characteristics of four diesel fuels meeting the requirements of Federal Specification VV-F-800a (Fuel Oil, Diesel) were investigated. Two straight-run fuels and two fuels containing catalytically cracked fractions were stored in 100 bbl above-ground steel tanks for two years. The fuels were sampled initially and periodically during storage; each sample was analyzed for existent gum, particulates, and ASTM D2274 stability. Two of the fuels maintained satisfactory storage stability throughout the entire two-year storage period. One fuel was considered marginal with respect to storage stability since its initially low values for gum and particulate contamination were followed by gradually increasing values. The fourth fuel exhibited a cyclic pattern of gum and particulate values typical of unsatisfactory storage stability. These results indicated that production diesel fuels meeting the requirements of Federal Specification VV-F-800a might not have satisfactory storage stability.", "title": "How stable is diesel in storage", "claims": null}, {"metadata": {"year": 1994}, "authors": ["Wechter", "D. Hardy"], "summary": "It has been previously reported that extraction with methanol will improve storage stability characteristics for mid distillate diesel fuel which is at least 6 months old. Moreover, a potential relationship between the methanol soluble, hexane insoluble extractable material and the thermally induced precipitate (TIP) which forms in some fuels under accelerated aging conditions which simulate long term ambient storage, has been suggested. Fuel stability is currently assessed through the use of an accelerated aging procedure. However, the procedure or procedures used require substantial commitments of laboratory time and resources and they are necessarily limited to predicting only the relatively short term (1-2 years). A predictive stability test which could serve for field use for fuels at least 6 months old, or which would require a sufficiently short turnaround time to make it attractive for procurement use is highly desirable. This is particularly true when circumstances require that the fuels may be stored for very extended periods, such as military fuel reserves.", "title": "Predicting stability at the refinery using SMORS", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Amer Marvan Ammar"], "summary": "0009-3092/08/4404\u20130255 \u00a9 2008 Springer Science+Business Media, Inc. In Syria, diesel fuels are stored in the north, south (marine), and central regions for 2-5 years. They are made by atmospheric distillation of mixed Syrian crudes consisting of 40 wt. % heavy and 60 wt. % light crudes. The yield of diesel fuel in this mixture is up to 35 wt. %. The initial indexes for the physicochemical properties (their guaranteed limits are shown in parentheses) of diesel fuel stored in an underground tank in the southern region are reported below:", "title": "Change in the properties of diesel fuels in storage in underground tanks", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Z. Mu\u017e\u00edkov\u00e1", "P. \u0160im\u00e1\u010dek"], "summary": "Over the last 25 years, the diesel fuel has undergone major changes in its composition, which have fundamentally affected its long-term storage possibilities. An oxidation stability is a main parameter characterising the storage of the diesel fuel and it is strongly affected by a diesel fuel composition. The oxidation stability decreases in a line saturated hydrocarbons \u2013 aromatic hydrocarbons \u2013 unsaturated hydrocarbons. The mandatory use of FAME as a biocomponent of the diesel fuel negatively affect its oxidation stability. The FAME contain unstable double bonds C=C and their mixtures with mineral diesel fuel cannot be storage for a long time. The use of antioxidants with FAME has not effect in the long time storage. A recommended usable life of diesel fuel with FAME accor-ding to \u010cSN 65 6500 is from 1 to 3 months according to the FAME content. However, in some cases it is ne-cessary to store diesel fuel for a long time. State material reserves or back-up diesel generators are examples, when the diesel fuel is stored for the long time.\nThe oxidation stability of the fuel expresses a resistance to an oxygen action. The oxygen, which is dissolved in the fuel, attacts molecules of the fuel and various oxidation products create. Hydroperoxides are the primary oxidation products. Secondary oxidation reactions give aldehydes, ketones, karboxylic acids and insoluble deposits. The oxidation products negatively affect the diesel fuel properties.\nBesides the composition the oxidation stability of the fuel is negatively affected by a high temperature,a high content of dissolved oxygen, an UV radiation and a presence of metals with a catalytic effect.\nAn overview of methods used for the measuring or the observing oxidation stability was prepared in the article. It means not only the oxidation stability measuring but also a measuring of the content of different oxidation products which are related to the various oxidation degree .\nThe aim of the article was to summarize the possibilities of a predicting the storage time of the diesel fuel and to propose a procedure for the monitoring and the predicting its longterm durability. Only one standardized storage test according to the ASTM D4625 can be found in the literature. The test is based on the storage of 400 ml of the diesel fuel at 43 \u00b0C for periods of 4, 8, 12, and 24 weeks. After aging for a selected period, a sample is analyzed for insolubles. The correlation of the test results is: a week at 43 \u00b0C is roughly equivalent to a month of the storage at the temperature of 21 \u00b0C. The test is a time and material consuming and the correaltion was determined for diesel fuels made up to 1990\u00b4s.\nThe new shorter storage test based on the standard test according to ASTM D4625 was proposed to predict diesel storage stability. The temperature was rised and the the time was shortened up to one month. During the test short laboratory analyses can be used for monitoring oxidation of the diesel fuel for example: the oxidation stability by PetroOxy, the peroxide number, the oxidation index by infrared spectroscopy, the acid number, the bromine number or the antioxidant content. The sample consumption is about 100 ml according to the selected analyses.", "title": "Oxidation stability of diesel fuel during storage", "claims": null}, {"metadata": {"year": 1983}, "authors": ["J. Bentley", "H. Schellhase"], "summary": "B.C. Hydro is concerned about the long term storage stability of middle distillate fuels. In the first half of 1982 an extensive literature survey was initiated and various experimental approaches were evaluated. Our laboratory followed other investigators in the field. Fuel quality after aging for 12 weeks at 110/sup 0/F was selected as an accelerated test for the assessment of fuel storage life. Thermal aging for 16 hours at 212/sup 0/F was adopted for fast fuel screening, and the more severe conditions of 212/sup 0/F for 60 to 120 hours for the testing of highly stable fuels and additives. A total of seven test fuels were tested under various conditions. Eight fuel additives were tested, and of these - three additives (so called combination additives) were found to be most effective in controlling thermal aging of (''Keogh'') fuel. Initial results of ultrasound aided thermal aging at 110/sup 0/F seem to indicate that the so called ''induction period'' is accompanied by poor aging statistics.", "title": "FUEL STABILITY AND STORAGE LIFE OF MIDDLE DISTILLATE FUELS", "claims": null}, {"metadata": {"year": 2013}, "authors": ["Michael  Thompson", "B.  Mu", "Christopher M. Ewaschuk", "Y.  Cai", "K. J. Oxby", "John  Vlachopoulos"], "summary": "Abstract This paper explores the long-term effects that an ageing biodiesel fuel blended with low sulfur diesel fuel at concentrations of 0%, 15%, 50%, 85%, and 100% (v/v) can have on polyethylene materials. Rotationally molded fuel tanks made of linear low density polyethylene or crosslinked polyethylene were used to appropriately mimic the storage environment of fuels, though the study also considered immersed polymer specimens as this test approach is more commonly reported on in the literature. The extent of oxidation for the fuels was monitored by determination of its peroxide value and acid number over time, with both values being consistently higher for blends with higher biodiesel content. Storage at 80\u00a0\u00b0C for up to 56\u00a0days resulted in significant oxidation of the fuel while testing up to 380\u00a0days at 23\u00a0\u00b0C produced no change in peroxide or acid concentration. The change in mechanical properties for the polyethylenes due to fuel was determined by tensile testing. Comparing the tensile data to the oxidation species in the stored fuels revealed that noted mechanical losses did not correspond with higher concentration of hydroperoxides or acid species but rather occurred as a result of fuel absorption. No difference was found in this regards between the two polyethylene species.", "title": "Long term storage of biodiesel/petrol diesel blends in polyethylene fuel tanks", "claims": null}], "query": "how long can you store diesel before it goes bad?", "summary_abstract": "The research on the storage stability of diesel fuel indicates that the duration for which diesel can be stored before it deteriorates varies significantly based on several factors, including the type of diesel fuel, storage conditions, and the presence of additives.\n\nLepera and Sonnenburg (1972, 1973) conducted extensive studies over two years on different diesel fuels, revealing that while some fuels maintained satisfactory stability throughout the storage period, others exhibited signs of deterioration, such as increasing gum and particulate contamination. Specifically, two of the fuels tested showed good stability, while one was marginal and another demonstrated unsatisfactory stability.\n\nJarrett and Von (1995) emphasized the importance of purchasing clean fuel and maintaining it free from contaminants, noting that fuel stored for too long can lead to quality issues. They highlighted that water can settle out of diesel fuel within approximately 24 hours, which is crucial for maintaining fuel quality.\n\nAmmar (2008) reported that in Syria, diesel fuels are typically stored for periods ranging from 2 to 5 years, suggesting that under certain conditions, diesel can be stored for extended durations without significant degradation.\n\nMu\u017e\u00edkov\u00e1 and \u0160im\u00e1\u010dek (2022) discussed the impact of biodiesel blends on storage stability, indicating that diesel fuel containing FAME (Fatty Acid Methyl Esters) has a usable life of only 1 to 3 months due to its susceptibility to oxidation. They noted that the oxidation stability of diesel is influenced by its composition and environmental factors such as temperature and exposure to oxygen.\n\nIn summary, while some diesel fuels can be stored for up to two years or more under optimal conditions (Lepera & Sonnenburg, 1972; Ammar, 2008), the introduction of biodiesel can significantly reduce this timeframe to as little as 1 to 3 months (Mu\u017e\u00edkov\u00e1 & \u0160im\u00e1\u010dek, 2022). Proper storage practices, including cleanliness and monitoring for contaminants, are essential for prolonging diesel fuel life (Jarrett & Von, 1995).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2017}, "authors": ["Fabio Moliterni"], "summary": "This study outlines a systemic review of the social and economic transformations that have been inducing the business to reconsider its traditional strategies and innovate to become sustainable. In doing so, the study adopts an original approach, focusing on the evolution in the socio-economic context, imposed by globalisation first and by the Great Recession afterwards. Hence, the review covers a wide range of literature and subjects, including political science, sociology, economics, finance and strategic management. Departing from the contextual picture, the study identifies the main drivers of the innovation of the business models, from the adoption of voluntary standards to the acknowledgement of the need to lengthen strategic time horizons. What emerges from the review is that, if business\u2019 primary concern was initially to safeguard its declining reputation under competitive pressure, it perceives sustainability today as a necessary condition to survive the deep transformation of the economy.", "title": "Sustainability-Oriented Business Model Innovation: Context and Drivers", "claims": null}, {"metadata": {"year": 2014}, "authors": ["A. Dottore"], "summary": "A firm\u2019s business model (BM) is an important driver of its relative performance. Constructive adaptation to elements of the BM can therefore sustain the position in light of changing conditions. This study takes a configurational approach to understanding drivers of business model adaptation (BMA) in new ventures. We investigate the effect of human capital, social capital, and technological environment on BMA. We find that a universal, direct effects, analysis can provide useful information, but also risks painting a distorted picture. Contingent, two-way interactions add further explanatory power, but configurational models combining elements of all three (internal resource, external activities, environment) are superior.", "title": "Business model adaptation in new ventures: are technology-based firms different?", "claims": null}, {"metadata": {"year": 2020}, "authors": ["S. Schneider", "T. Clauss"], "summary": "Business models for sustainability (BMFS) explicitly consider, jointly, economic with social and/or ecological value contributions. Thus, managing BMFS requires that one should combine multiple institutional logics, consider different stakeholders, and integrate distinct value dimensions. While research into BMFS has proposed frameworks and tools to capture this complexity, the mechanisms that underlie the interaction of the economic with social and/or ecological value creation cycles in BMFS remain unknown. We use a longitudinal, multiple-case study approach to identify a framework of fundamental choices and consequences that lead to economic and social/ecological value creation. Our findings highlight the interdependence of a set of three fundamental choices (an ambition to cater for multiple purposes, behavioral consistency, and collaboration) that lead to a virtuous circle of reinforcing consequences and cascaded value creation. Furthermore, we show how acceptance of limitations and restrictions functions as a powerful coping mechanism to deal with paradox tensions.", "title": "Business Models for Sustainability: Choices and Consequences", "claims": null}, {"metadata": {"year": 2020}, "authors": ["A. Krmela", "Mariana Tesa\u0159ov\u00e1"], "summary": "Purpose of the article: The article aims at understanding and description of how and why the business model (BM) of an incumbent manufacturing B2B company adapts, collaboratively with a dominant customer, through measures leading to an improved environmental sustainability performance. The research question was: How is a business model of an incumbent, businessto-business company adapted through the inclusion of environmental sustainability measures, with the customer\u2019s involvement? Methodology/Methods: A qualitative research approach, drawing on a descriptive-explanatory, single case study conducted on a Czech manufacturing company active in the field of steel processing was performed. The study draws on the analysis of the focal company\u2019s internal and external documents related to sustainability, as well as on semi-structured interviews with the examined company\u2019s executives. The analysis also include the externally available sustainability related documents of a dominant customer. Scientific Aim: Understanding the impact of the environment-related sustainability measures on the adaptation of the extant BM of the focal company, and the role a dominant customer plays in the process. Findings: The inclusion of environment-related sustainability measures leads to an adaptation of the process and the product, as well as of the focal company\u2019s BM and most of its key elements. A value capture by the focal company has been identified as the mainly affected element of a BM through both cost savings and a more business gained. The element value proposition adapts towards a \u201cgreener company\u201d. The core logic of the BM remains unchanged. Conclusions: B2B incumbent companies implement environmental sustainability measures into their BMs, aiming to contribute to the preservation of the environment. The economic side continues to be carefully considered. It can even be a trigger of implementation. The large customers get strongly involved in the process, contributing to the adaptation of all four key elements of the BM. The BM\u2019s adaptation happens dynamically, in incremental steps.", "title": "Business Model Adaptation Through Inclusion of Environmental Sustainability Measures", "claims": null}, {"metadata": {"year": 2023}, "authors": ["J. Pinkse", "Florian L\u00fcdeke\u2010Freund", "O. Laasch", "Yuliya Snihur", "Ren\u00e9 Bohnsack"], "summary": "Business models for sustainability (BMfS) enable organizations to create social and environmental value for a wide variety of stakeholders. As BMfS are new for well-established industries, their implementation requires deep organizational change to overcome path dependencies of existing business models. In this article, we present a framework which outlines the organizational change process involved in BMfS development. The framework shows that organizations can experiment with novel configurations of value, resources, and transactions, and follow discursive and cognitive pathways to enable BMfS legitimization and implementation. Although the value, resources, and transactions levers can be used either separately or in concert, discursive and cognitive pathways are most powerful when pursued together. We use our framework to highlight the contributions of the articles in the special issue and to propose new directions for BMfS research. We argue that future research should investigate the impacts of BMfS on the sustainability challenges they seek to address.", "title": "The Organizational Dynamics of Business Models for Sustainability: Discursive and Cognitive Pathways for Change", "claims": null}, {"metadata": {"year": 2018}, "authors": ["N. Rashid", "S. Shami"], "summary": "In the modern economy, doing business is a challenging task to overcome volatile competition from established firms in the local and global market. In order to attain sustainability, firms are advised to be ahead from the competitors in term of business model or in its management. In general, the Malaysian automotive industry performance is on a par with Thailand and holds the top three positions in the ASEAN car market due to the strong dominance of the domestic market. However, in the new millennia, the competition is no longer between the companies but about achieving economy, ecology and social sustainability. Thus, to stay competitive, the strategy shifted from profit orientation to the sustainable development. Therefore, this research aimed to provide empirical evidence about the critical success factors towards the achievement of company sustainability. This study employed a quantitative research methodology and 242 returned questionnaires were then analysed via Co variance-based SEM (CB-SEM). The outcome of this research emphasizes that technology collaboration, green human resource, eco culture and eco product innovation are important factors to meet the sustainability of Malaysian automotive industry, especially in economy, ecology and social development. This research is meaningful to attain the world wide and nation strategy to \u201cGo Green.", "title": "Business Model for Sustainable Development: An Insight from Malaysian Automotive Industry", "claims": null}, {"metadata": {"year": 2017}, "authors": ["C. Fogarassy", "B\u00e1lint Horv\u00e1th", "R. Magda"], "summary": "Abstract Considering the frequently changing business environment, staying competitive on the current markets takes more efforts from companies in the 21st century. Conducting only incremental product innovation does not meet the market criteria anymore. The successful entrepreneurs of the future must focus more on the design of their business models to gain superior economic performance. Besides the market, society has also raised a major requirement towards companies, which is being socially responsible. The word \u2018sustainability\u2019 regarding businesses might confuse a lot of people as several aspects of the term reach beyond its traditional economic interpretation. The present study provides a clear explanation and furthermore examines its application at a corporate level. This paper aims to present how business model innovation does not simply lead to economic success but its generated value can be extended to social and environmental aspects as well.", "title": "Business Model Innovation as a Tool to Establish Corporate Sustainability", "claims": null}, {"metadata": {"year": 2022}, "authors": ["F. Gennari"], "summary": "COVID-19 pandemic crisis threatened the stability of economy and the survival of many firms, but it has been also the chance to challenge the current economic development path and to rethink firms\u2019 business models according to a more sustainable approach. Academic literature states that business model (BM) \u00a0innovation is a driver for the transition to sustainability. Sustainable business models (SBMs) incorporate sustainability vision in the main components of business model, which are value proposition, value creation and value capturing. Nevertheless, the usual approach to business models, based on a positive concept of value, can underestimate some areas of potential opportunities to catch. For this reason, in this paper we suggest to adopt a novelty approach that emphasizes the negative concept of value (value uncaptured) to identify unexploited value opportunities. This approach can help firms innovating their BMs towards SBMs.\u00a0 \u00a0", "title": "Sustainable Business Model Innovation: From Value Uncaptured to Value Opportunities", "claims": null}], "query": "Critical discussion of the influence of technological, environmental, social and economic determinates on business model transformation (BMT) towards sustainable business model.", "summary_abstract": "The collective findings from the reviewed papers highlight the multifaceted influences of technological, environmental, social, and economic determinants on business model transformation (BMT) towards sustainable business models (SBMs). \n\nMoliterni (2017) emphasizes that socio-economic transformations, particularly those driven by globalization and economic downturns, compel businesses to innovate their models to ensure sustainability. This shift reflects a growing recognition that sustainability is essential for survival in a rapidly changing economic landscape. Similarly, Rashid and Shami (2018) point out that in the competitive Malaysian automotive industry, the transition from profit-oriented strategies to sustainable development is crucial for maintaining competitiveness. They identify technology collaboration and eco-innovation as key factors in achieving sustainability.\n\nDottore (2014) explores how human capital, social capital, and the technological environment interact to drive business model adaptation in new ventures. This configurational approach underscores the importance of both internal resources and external conditions in shaping business strategies. Schneider and Clauss (2020) further elaborate on the complexity of business models for sustainability, which require the integration of economic, social, and ecological value dimensions. Their findings suggest that successful SBMs depend on a combination of stakeholder engagement and the alignment of multiple institutional logics.\n\nKrmela and Tesa\u0159ov\u00e1 (2020) provide a case study illustrating how a B2B company adapts its business model through environmental sustainability measures, highlighting the role of dominant customers in this transformation. They note that while the core logic of the business model remains intact, elements such as value proposition and value capture are significantly influenced by sustainability initiatives.\n\nPinkse et al. (2023) discuss the organizational changes necessary for implementing SBMs, emphasizing the need for deep structural shifts to overcome existing path dependencies. Their framework suggests that organizations can leverage novel configurations of value and resources to facilitate this transition.\n\nLastly, Gennari (2022) argues that the COVID-19 pandemic has prompted firms to rethink their business models towards more sustainable approaches, advocating for a focus on both captured and uncaptured value opportunities to drive innovation.\n\nIn summary, the literature indicates that BMT towards sustainability is influenced by a complex interplay of technological advancements, environmental imperatives, social expectations, and economic pressures. Companies must adapt their business models not only to survive but to thrive in a landscape increasingly defined by sustainability considerations.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2012}, "authors": ["Dawne Garrett", "Rose Garrett"], "summary": "The term 'personalisation' has become increasingly common in the context of a movement that recognises the importance of people's individuality and their right to exercise choice in their daily lives. In the fields of health and social care, personalisation focuses on placing the individual at the centre of their care and an understanding that they know best what their needs are and how to meet them ( Carr 2008 ). Personalisation can be as significant as a direct payment or individual budget, or as small as supporting a resident in a care home to decide for themselves when they should go to bed.", "title": "Practice question.", "claims": null}, {"metadata": {"year": 2021}, "authors": ["Dmitri Goldenberg"], "summary": "Personalization is omnipresent in our life, with applications ranging from entertainment and commercial uses to smart devices and medical treatments. The integration of personalization in various products turned rapidly from an unnecessary luxury to a commodity that is expected by customers. While different machine learning fields present state-of-the-art advances and super-human performance, personalization applications are often late-adopters of novel solutions due to their complex framing and multiple stakeholders' with different business goals. The role of personalisation applications is also ambiguous: it is unclear, for instance, whether models just predict a user's next action or proactively affect the user's selections. This talk focuses on examining the role of recommenders and their ability to adapt to customer feedback. Key topics such as causality and active exploration are depicted with real examples and demonstrated alongside business considerations and implementation challenges. It relies on recent advances in the field and on work conducted at Booking.com, where we implement personalization models on one of the world's leading online travel platform.", "title": "Putting the Role of Personalization into Context", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Simone van der Hof", "Corien  Prins"], "summary": "New ICTs support new ways of tailoring services to the individual needs and desires of customers. Increasingly, what is called personalisation of services is implemented particularly in Internet-based e-commerce applications but other kinds of technologies and services, such as location-based services, RFID, smartcards and biometrics are expected to follow closely and offer even better opportunities for personalised services. The growing popularity of personalisation requires an exploration of the broader and fundamental implications of this phenomenon. Issues related to the categorisation of people, the creation of identities, the (re)structuring of behaviour and the shaping of the overall movement of information and expression within society need consideration when implementing such techniques in organisational (e.g., e-Government) and business processes. More specifically, this chapter deals with privacy, transparency and the quality of personalisation processes, as well as inclusion, exclusion and control. It is argued that the phenomenon of personalisation must be deliberated in light of the broader developments in the area of ubiquitous computing.", "title": "Personalisation and its Influence on Identities, Behaviour and Social Values", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Gulden Uchyigit"], "summary": "The advent of the Internet, personal computer networks and interactive television networks has lead to an explosion of information available online from thousands of new sources, a situation which is overwhelming to the end-user and is likely to worsen in the future. Personalization technologies have emerged as specialized tools in assisting users with their information needs. Personalization can be defined as the process of enabling a system to tailor information to its user\u2019s needs and preferences. ABSTRACT", "title": "The Next Generation of Personalization Techniques", "claims": null}, {"metadata": {"year": 2017}, "authors": ["H. Dangi", "Anuradha Malik"], "summary": "Internet has highly transformed contemporary business practices and presented a new paradigm for business relationships and transactions. Mass market is dead and personalisation is the emerging trend, in fact it has become a necessity in e-commerce. Personalisation simply means individualising the shopping experiences for customers based on data collected about them by marketers. Over last few decades, personalisation has become key element in marketing strategy of e-commerce firms. While personalisation is a buzz word today but conceptually it still lacks clarity. Various academicians and practitioners have expressed different viewpoints on personalisation. In this paper, we try to synthesise various viewpoints on personalisation by analysing key themes, components and approaches in literature to describe the concept of personalisation. The paper also highlights customers' attitudes towards personalisation.", "title": "Personalisation in marketing: an exploratory study", "claims": null}, {"metadata": {"year": 2012}, "authors": ["Jeanette Bredsten", "P. Sorce"], "summary": "Personalization is a marketing tactic in which various media channels are used to send a personalized message to a consumer or client based on their interests. Personalization is not a new tactic. The use of personalization has been slowly increasing over the past few decades to become one of the key tactics used capture a customer\u2019s attention. Personalized print\u2014one media channel that can be used in the marketing mix\u2014is divided among different applications such as mail merge, transactional print, versioning, moderate personalized print and highly personalized print.", "title": "Personalization in Europe", "claims": null}, {"metadata": {"year": 2006}, "authors": ["Hyunjin Lee", "Wonil Kim"], "summary": "In this paper, we present personalization that of important research fields recently. As the interest of individual becomes more diverse, more and more digital contents become personalized. Personalization makes users more satisfied by helping them to use the contents effectively. We discuss the type and the substantial interface of personalization that provides satisfaction to user.", "title": "Personalization of Digital Contents", "claims": null}, {"metadata": {"year": 2004}, "authors": ["Maximilian  Teltzrow", "Alfred  Kobsa"], "summary": "Recent developments in privacy awareness and legislation may have a significant impact on the advancement of personalized systems. Though many countries have enacted comprehensive privacy laws, user concerns are still high. We compared 30 opinion surveys on Internet privacy, categorized the responses, and matched them with possible impacts on personalized systems. The analysis of a cross-section of privacy surveys should provide a more objective view of consumer concerns than results from a single study. This research thus represents a first contribution towards the identification of requirements for privacy-preserving personalization, to improve users\u2019 trust when interacting with personalized systems. Introduction Personalized (or \u201cuser-adaptive\u201d) systems have become increasingly popular since the beginning of the 1990\u2019s, and have gained substantial momentum with the rise of the World Wide Web. The market research firm Jupiter defines personalization as predictive analysis of consumer data used to adapt targeted media, advertising, or merchandising to consumer needs (Foster 2000). A more general definition was proposed by Kobsa et al. (2001) who regard a personalized hypermedia application as a hypermedia system which adapts the content, structure and/or presentation of the networked hypermedia objects to each individual user\u2019s characteristics, usage behavior and/or usage environment. In contrast to user-adaptable systems where the user is in control of the initiation, proposal, selection and production of the adaptation, user-adaptive systems perform all steps autonomously. A well-known example of a personalized website is Amazon.com, which generates purchase recommendations based on a user\u2019s purchase and interaction history. Other examples of personalized web sites are listed in (Dean 2000). A categorization of user-adaptive systems according to Kobsa et al. (2001) is depicted in Table 1. The advantages of personalization can be manifold. Online users see the major benefits in sites being able to offer more relevant content and to recall user preferences and interests (Cyber Dialogue 2000). However, personalization of hypermedia presentation is beneficial for several other purposes as well, most notably for improving the learning progress in educational software (Brusilovsky et al., 1998; Eklund & Brusilovsky, 1998; Specht, 1998). In order to deal with impacts of privacy concerns on personalization systematically, it is helpful to identify privacy-critical personalization processes. Personalization can basically be depicted as a cycle of recurring processes consisting of data collection, profiling and matching (Foster 2000). From collected data, user profiles can be created that are the basis for adapting user interfaces to individuals or groups of individuals. We focused on the step of data collection because it is the most privacy-critical in the personalization process. The collection of extensive knowledge about users\u2019 interests, behavior, demographics and actions is necessary for most user-adaptive systems. However, this could provoke privacy fears that limit consumers\u2019 willingness to share information. We looked at privacy issues from a user\u2019s standpoint and not from a legal point of view. While privacy legislation should serve and protect the users\u2019 interests, user concerns should be a central starting point for the legislative process. For a recent discussion of privacy legislation in different countries we refer to the International Survey of Privacy Laws and Developments (Electronic Privacy Information Center 2002). Data Sources The amount of personal data that is available online has rapidly increased over the years. Individuals often transmit personal information online, either actively by submitting data (e.g. a shipping address for books), or passively, by leaving electronic traces in log files both at the server side as well as in the network. Improved accessibility of data \u2013 not only from the World Wide Web but also from multiple user touch points and external data sources \u2013 have further increased the amount of information available about individuals. 1 This does not seem to be very much to be the case at the moment: studies suggest that legislative actions seem to only have a marginal impact on consumer concerns (EU/Interactive Policy Making, 2002) Kobsa (2001) partitions data types into user data, usage data, and environment data. User data denotes information about personal characteristics of the user, while usage data is related to a user\u2019s (interactive) behavior. Usage regularities are based on frequently re-occuring interactions of users. Environment data focuses on the user\u2019s software and hardware and the characteristics of the user\u2019s current locale. Personalization systems often need to acquire a certain amount of data before they can start adapting to the user. Thus, they are often only useful in domains where users engage in extended (and most often repeated) sessions of system use. They may not be appropriate for infrequent users with typically short session. No. Input Data of UserAdaptive Systems Examples of User-Adaptive Systems", "title": "Impacts of User Privacy Preferences on Personalized Systems", "claims": null}], "query": "How is  personalization becoming more ubiquitous?", "summary_abstract": "The concept of personalization is increasingly becoming a fundamental aspect of various sectors, reflecting a shift towards recognizing individual preferences and needs. This trend is particularly evident in health and social care, where personalization emphasizes the importance of placing individuals at the center of their care (Garrett & Garrett, 2012). The rise of personalization is not limited to healthcare; it has permeated numerous domains, including e-commerce, entertainment, and smart technologies, transforming from a luxury to an expected standard among consumers (Goldenberg, 2021).\n\nIn the realm of e-commerce, personalization has emerged as a critical strategy for enhancing customer experiences. Businesses are now focusing on tailoring shopping experiences based on data collected about individual customers, making personalization a necessity rather than an option (Dangi & Malik, 2017). This shift is supported by advancements in information and communication technologies (ICTs), which facilitate the customization of services to meet specific customer desires (van der Hof & Prins, 2008). The integration of personalization technologies, such as location-based services and smart devices, is expected to further enhance the ability to deliver tailored experiences (van der Hof & Prins, 2008).\n\nMoreover, the growing ubiquity of personalization raises important considerations regarding privacy and user trust. As personalization systems rely heavily on data collection and profiling, concerns about privacy have become increasingly prominent (Teltzrow & Kobsa, 2004). Users are often apprehensive about sharing personal information, which can hinder the effectiveness of personalized systems. Therefore, addressing privacy issues is crucial for fostering user confidence in these technologies (Teltzrow & Kobsa, 2004).\n\nIn summary, personalization is becoming more ubiquitous across various sectors, driven by technological advancements and changing consumer expectations. It is reshaping how services are delivered, particularly in e-commerce, while also necessitating careful consideration of privacy and user trust (Garrett & Garrett, 2012; Goldenberg, 2021; Dangi & Malik, 2017; van der Hof & Prins, 2008; Teltzrow & Kobsa, 2004).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2018}, "authors": ["J. R. Larson", "Amanda C. Egan"], "summary": "Information sharing is essential for learning and performance in groups and organizations. This chapter examines factors that either encourage or discourage information sharing, particularly during group meetings convened for the purpose of solving a problem or making a decision. Such purposes are usually best served when members share with one another the task-relevant information they hold that others in the group do not possess (i.e., uniquely held information). Yet meetings are often dominated by discussion of information that most members already know (i.e., commonly held information), to the relative exclusion of their uniquely held information. We examine in some detail the information sampling process that gives rise to this phenomenon, and we consider a range of situational and motivational factors that moderate it, including features of the information held, the task performed, and the group performing it. Finally, we offer recommendations for how information sharing during group meetings might be improved.", "title": "Information Sharing Within Groups in Organizations", "claims": null}, {"metadata": {"year": 2015}, "authors": ["A. Cairns"], "summary": "BACKGROUND. \nThe need for public sector agencies to work together to deal with complex issues which overlap agencies spheres of work has been well established. Cases such as the Soham murders in 2002 (BBC News, 2003), the Climbie child abuse case in 2003 (Health Committee, 2003) and the Pilkington Anti-social Behaviour case in 2007 (Telegraph, 2009) each highlighted the need for public sector agencies to work more closely and share information with each other to more effectively serve their public. \nMETHODOLOGY. \nA three year period of participant observation in the implementation of a real time information sharing system used by multiple agencies to jointly manage anti-social behaviour was undertaken. An information sharing framework was produced detailing the factors which impact an information sharing project, classified into six categories; External Environment, Organisation, Process, Project, Technology and Individual. \nFINDINGS. \nFirstly bureaucracy is not always a barrier to information sharing. At times bureaucracy was found to enable information sharing by providing documentary evidence to decisions made throughout the system implementation. Secondly an agency\u2019s level of buy in and involvement with the information sharing project was shown to be a key indicator of their motivation to share information, correlating with the level of case recording on the system. Thirdly whilst technology enables the process of information sharing the research showed people had a much greater impact on whether information sharing took place. Finally whilst the UK public sector encourages public sector agencies to share information a wide scale review and approach to IT infrastructure would better enable future information sharing projects. \nCONCLUSIONS. \nThe research identified there are many factors which impact an agency\u2019s ability/motivation to share information. It is the level of motivation an agency has minus the cost of sharing which ultimately determines whether information sharing occurs. In order to further encourage information sharing there is a recommendation that the UK government look at ways to make integrating the disparate data sources easier to decrease the cost of sharing and thus improve the likelihood information sharing will occur.", "title": "Multi-agency information sharing in the public sector", "claims": null}, {"metadata": {"year": 2011}, "authors": ["M. Jamali", "Ehsan Shafiezadeh", "M. Mohammadi"], "summary": "Indeed, one of the effective technological factors leading to an organization's successful deployment of knowledge management strategy is the development of proper IT infrastructure. Yet it seems IT management has substantively surpassed individuals' ability to mutually understand and agree upon their desired information and its sharing. This is so, because, despite the amazing improvements of information technology in recent decades, and its omnipresence in organizations, the improvements in the field of knowledge sharing were insignificant. In the present paper, it is discussed that despite the claimed knowledge sharing resources and the certain and ever-growing capabilities of ICT for sharing knowledge in organizations, still knowledge sharing is extremely difficult. The motives for sharing, or maybe more importantly, the motives behind refusing to share knowledge should be understood on the basis of correct organizational image. Thus, the article reviews the political and cultural perspectives of organizational functions and compares these two. In addition, the roles of informal networks in knowledge sharing are examined.", "title": "Information Technology and its Deficiencies in Sharing Organizational Knowledge", "claims": null}, {"metadata": {"year": 2004}, "authors": ["Dennis Hart", "Sigi Goode"], "summary": "The Information Systems literature observes that, while there are ostensibly benefits to sharing of data and information, barriers to organisational data sharing appear significant. Managers may be understandably concerned that the sharing activity is adversely affecting their own organisations. This paper develops a model of data and information sharing based on the traditional system model, and proposes a theory of the sharing activity in organisations. The paper theorises that employees may engage in or oppose sharing based on the assessment of perceived benefits accruing to themselves from the activity. In particular, the paper highlights the contention that data and information sharing are likely to decrease as organisations grow in size, and also offers an explanation for why the sharing activity is so poorly undertaken in modern organisations even though the technological capability to support it may be there.", "title": "Understanding Data and Information Sharing in Organizations: A Value-Based Approach", "claims": null}, {"metadata": {"year": 2004}, "authors": ["Dennis Hart", "Sigi Goode"], "summary": "The literature observes that, while there are ostensibly benefits to sharing of data, barriers to organisational data sharing appear significant. Managers may be understandably concerned that the sharing activity is adversely affecting their own organisations. This paper develops a model of data sharing based on the traditional system model, and proposes a theory of the sharing activity in organisations. The paper theorises that employees may engage in or oppose sharing based on the assessment of perceived benefits accruing to themselves from the activity. In particular, the paper highlights the contention that data sharing decreases as organisations grow, and also offers an explanation for why the sharing activity is so poorly undertaken in modern organisations.", "title": "A Value-Based Analysis of Data Sharing Behaviour in Organizations", "claims": null}, {"metadata": {"year": 2011}, "authors": ["A. Cairns", "Thomas W. Jackson", "Louise Cooke"], "summary": "With the increasing move to partnership working in the public sector this paper looks at the main barriers in place which reduce the chances of Public Agencies working together. Agencies such as the Police, Local Councils, Youth Services and Health Services would like to work closer to improve their ability to serve the public whilst reducing the costs associated with this. A review of the literature along with personal experience from talking to and working with these agencies has identified the key elements affecting data and information sharing. The paper has found that whilst the agencies themselves are able to work on many of the barriers to data and information sharing the Data Protection Act 1998 continues to act as a deterrent.", "title": "The factors involved in sharing information between public agencies", "claims": null}, {"metadata": {"year": 2011}, "authors": ["Tung-Mou  Yang", "Terrence A. Maxwell"], "summary": "Abstract Information sharing is considered an important approach to increasing organizational efficiency and performance. With advances in information and communication technology, sharing information across organizations has become more feasible. In the public sector, government agencies are also aware of the importance of information sharing for addressing policy issues such as anti-terrorism and public health. However, information sharing can be a complex task. Identifying factors that influence information sharing is critical. In the literature, research in information sharing focuses on the interpersonal, intra-organizational, and inter-organizational levels. This paper reviews the current information-sharing research, discusses the factors affecting information sharing at the three levels, and provides summative frameworks. These frameworks provide a means to discover future research opportunities, and a systematic way for practitioners to identify key factors involved in successful information sharing.", "title": "Information-sharing in public organizations: A literature review of interpersonal, intra-organizational and inter-organizational success factors", "claims": null}, {"metadata": {"year": 2020}, "authors": ["A. Alrawi"], "summary": "The economic importance of sharing information and its roles within an established enterprise are stems from its guides of every decision that an enterprise or expects to make, or already taken.Therefore, information is guided the management in taking the decision-making process or problem-solving, and with this right information, enterprise are avoiding making mistakes in these key processes. In this research the researcher introduced the concept of information sharing between enterprises in a particular industry of the economy for an expecting success. The aim of this research is to shed light on information sharing importance \u00a0as an essential component of keeping the enterprise\u2019s\u00a0 knowledge dynamics, and \u00a0also, the enterprise\u2019s barriers for implementation such concept in the economic environment. This study was accomplished through questionnaires that surveyed (51) enterprise\u2019s owners/CEOs that are located in the capital Baghdad. Mathematical methods/Equations were used. Findings revealed that barriers for information sharing are: Employees' commitment, Employees' understanding organizational goals, Employee\u2019s trust in their organizations and, and Owner\u2019s education. The main conclusions are sharing information between enterprises in the industries is crucial and affecting employees commitments, their trust and understanding organizational goals.", "title": "The Economic Effects of Information Sharing on the Enterprise\u2019s Achievement in Industrial Sustained Success", "claims": null}], "query": "factors hindering information sharing in organisations", "summary_abstract": "The research question regarding the factors hindering information sharing in organizations is addressed through various studies that highlight a range of barriers, both technological and human, that impede effective communication and collaboration.\n\nOne significant factor identified is the organizational culture and individual motivations. Larson and Egan (2018) emphasize that during group meetings, discussions often revolve around commonly held information rather than uniquely held insights, which can stifle the sharing of critical knowledge. This phenomenon is influenced by situational and motivational factors, suggesting that a lack of encouragement or incentive to share information can lead to underutilization of valuable insights.\n\nCairns (2015) further explores the public sector context, revealing that while bureaucracy can sometimes facilitate information sharing by providing necessary documentation, it can also act as a barrier. The level of agency buy-in and involvement in information-sharing initiatives is crucial; agencies that are more engaged tend to share information more effectively. Additionally, the study indicates that the perceived costs of sharing information, both in terms of time and resources, can deter agencies from participating in collaborative efforts.\n\nJamali et al. (2011) highlight the disconnect between technological advancements in information sharing and the actual practice within organizations. Despite the availability of sophisticated IT infrastructure, the reluctance to share knowledge often stems from political and cultural dynamics within the organization. This suggests that addressing interpersonal relationships and fostering a culture of trust is essential for enhancing information sharing.\n\nHart and Goode (2004) propose a model that illustrates how perceived personal benefits influence employees' willingness to share information. They argue that as organizations grow, the likelihood of sharing diminishes, indicating that larger organizations may face unique challenges in fostering a collaborative environment.\n\nMoreover, Yang and Maxwell (2011) categorize the factors affecting information sharing into interpersonal, intra-organizational, and inter-organizational levels, underscoring the complexity of the issue. They advocate for a systematic approach to identify and address these barriers to improve information sharing practices.\n\nLastly, Alrawi (2020) identifies specific barriers such as employee commitment, understanding of organizational goals, and trust in the organization as critical to successful information sharing. These findings suggest that enhancing employee engagement and fostering a supportive organizational culture are vital for overcoming obstacles to information sharing.\n\nIn summary, the literature collectively points to a multifaceted array of factors hindering information sharing in organizations, including cultural dynamics, individual motivations, bureaucratic structures, and the perceived costs associated with sharing. Addressing these barriers requires a comprehensive approach that considers both technological and human elements to foster a more collaborative environment.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2006}, "authors": ["J. Gonz\u00e1lez-Chapela"], "summary": "This paper discusses methods that permit evaluating the degree of heterogeneity in the use of time. In particular, it is argue that scalar measures of multivariate scatter constitute a general means to assess the degree of withingroup time-use heterogeneity. The techniques are then illustrated by inquiring into the heterogeneity in the allocation of time of retired and employed older adults.", "title": "On measuring heterogeneity in the use of time", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Caroline Datchary", "G\u00e9rald Gaglio"], "summary": "In productive organizations, times, their measurement and their representations, form a cognitive framework fraught with issues, for individuals and for work collectives. The present article therefore looks at the knowledge of times, their equipping with technologies that help shape them, their polychronicity, heterogeneity and incompatibilities. Time management involves questions of production, circulation and crystallization of knowledge. Thinking about time involves singular relationships to the world and movement. This raises the question of how to study temporal heterogeneity in organizations.", "title": "Temporal heterogeneity and work activity: Between conflicts and articulations", "claims": null}, {"metadata": {"year": 2009}, "authors": ["W. Kamakura"], "summary": "Time-use has already been the subject of numerous studies across multiple disciplines such as economics, marketing, sociology, transportation and urban planning. However, most of this research has focused on comparing demographic groups on a few broadly defined activities (e.g., work for pay, leisure, housework, etc.). In this study we take a holistic perspective, identifying a typology of latent \u201ctime-styles,\u201d that defines the different ways people allocate the 24 hr in a day across multiple competing daily activities. We propose a finite-mixture time-allocation model that accounts for differences in life priorities across individuals, taking into consideration the fact that we all have the same \u201cbudget\u201d of 24 hr to spend every day and that this allocation leads to highly sparse, truncated data. This model is then applied to time-use data from the American Time Use Survey collected by the U.S. Bureau of Labor Statistics in 2006.", "title": "American Time-Styles: A Finite-Mixture Allocation Model for Time-Use Analysis", "claims": null}, {"metadata": {"year": 1983}, "authors": ["M. M. Sanik"], "summary": "Time use studies frequently involve collecting data for more than one day. A repeated measure design considers the non-independence of the days' time data in the analysis. The significant difference between the days' time use was found not to be due to day of the week alone, but attributable to the difference between the \u201cyesterday\u201d or recall approach and the \u201ctomorrow\u201d or record approach to collecting time data.", "title": "Repeated Measure Design: A Time Use Application", "claims": null}, {"metadata": {"year": 2009}, "authors": ["A. Wawer", "R. Nielek", "R. Kotowski"], "summary": "A theoretical account aimed at modeling time usage founded on social network-based multiagent system is proposed. The model introduces mutual, dynamic relationships between the network and schedules: on the one hand, a network is generated by schedules, on the other, schedules are shaped by the network. Time allocation is driven by globally distributed semantic resources. Social network structure and agent time usage have been adjusted by applying evolutionary algorithm to match empirically observed values. The resulting allocation reveals an emergent property which ties the number of agents with activity type priorities. Using the adjusted social network, simulations have been run to investigate time patterns. For that purpose, a time cycle similarity measure has been proposed and computed, unveiling a high amount of regularity, which indirectly supports the view of culture's function as a latent pattern maintenance.", "title": "Patterns in Time: Modelling Schedules in Social Networks", "claims": null}, {"metadata": {"year": 2018}, "authors": ["A. Roeters"], "summary": "This publication investigates whether these changes are reflected in the time use of the Dutch", "title": "Background of the Time Use Survey", "claims": null}, {"metadata": {"year": 2004}, "authors": ["F. Duch\u00eane", "C. Garbay", "V. Rialle"], "summary": "Defining the similarity of objects is crucial in any data analysis and decision-making process. For those which effectively deal with moving objects, the main issue becomes the comparison of trajectories, also referred to as time-series. Moreover, complex applications may require an object to be a multidimensional vector of heterogeneous parameters. In that paper, we propose a similarity measure for heterogeneous multivariate time-series using a non-metric distance based on the Longest Common Subsequence (LCSS). The proposed definition allows for imprecise matches, outliers, stretching and global translating of the sequences in time. We demonstrate the relevance of our approach in the context of identifying similar behaviors of a person at home.", "title": "Similarity measure for heterogeneous multivariate time-series", "claims": null}, {"metadata": {"year": 2003}, "authors": ["Risha W. Levinson"], "summary": "THE CONCEPT of \"time\" has a wide range of meanings and is applied to highly diverse settings. For example, prisoners serve time, musicians mark time, idlers pass time, referees call time, historians record time, and score keepers keep time (Greenberg, 1990). The Scriptures maintain that \"to everything there is a season - a time to be born, and a time to die, a time to kill and a time to heal, a time to love and a time to hate ... \"(Ecclesiastes, 3). Thus, time is assigned to a specific task or a predetermined event in accordance with a specific purpose. This article will explore different ideas related to how seniors can improve the way they spend time and maximize their positive impact on the next generation -- in other words, how seniors can become better time-binders. An Aging Society -- Socio-Demographic Trends and Time-Binding Opportunities The general semantics formulation, time-binding, concerns our human ability to use language and other symbols to store and pass on knowledge, so that each new generation can benefit from earlier discoveries and start from where the previous generation left off. The concept of time-binding has assumed special significance in view of the unprecedented expansion of the aging population. Demographic forecasts predict that the aging population will continue to increase significantly in the twenty-first century. America is steadily growing older as more people are living longer and more are celebrating their centenarian birthdays. It is predicted that by 2030, one out of every five persons (20%) will be over 60 years old (Hooyman and Kiyak, 1996). Moreover, as the 75 million \"baby boomers\" who were born between 1946 and 1964 \"come of age,\" the number of older persons will significantly increase into what has been described as an \"age wave\" and a \"gerontocracy.\" Another significant demographic trend is the increase of multi-generational families, which may include four- and five-generation families. This growing phenomenon is a consequence of extended longevity, in addition to higher divorce rates and remarriages, since the longer the members of a particular generation live on, the more likely it is that they will be living among subsequent generations. Consequently, opportunities for time-binding will multiply as younger and older generations engage in time-sharing. In our post-industrial (or post-modern) society research no longer focuses exclusively on the pathology associated with aging, nor exclusively on what is possible despite aging. It also investigates what is possible because of aging, given the potential and opportunity for creativity during the advanced years. There is no denial of the \"problems\" that occur in later life. However, research is focusing more on the possibilities, the strengths, and the opportunities of inter-generational sharing. Longevity has heralded a new era of shared opportunities for intergenerational time-binding. Older volunteers are engaged as readers and storytellers in elementary schools, as career counselors and tutors in high schools, and as associates in colleges. Older and younger volunteers share hours of community service in local neighborhoods as well as in health facilities. Children are teaching English to their newly arrived ethnic grandparents and they provide instruction in computer science to their parents. Grandparents are increasingly assuming responsibility for childcare in working families as well as in homes of divorced parents. The proverbial \"empty nest\" is intermittently filled with adult children returning to live with aging parents or grandparents. These time-sharing experiences provide transmission of inter-generational values and culture that extend beyond the lifetime of individual family members. Cultural Attitudes Toward Aging In various traditional societies such as Asian and Biblical cultures, older persons tend to be regarded with reverence, and are often respected as \"visionaries\" and \"persons of wisdom\" in the transmission of culture from generation to generation. \u2026", "title": "Aging and Time-Binding in the Twenty-First Century", "claims": null}], "query": "Time-use homogeneity", "summary_abstract": "The research question regarding time-use homogeneity is addressed through various studies that explore the heterogeneity in time allocation across different demographics and contexts. Gonz\u00e1lez-Chapela (2006) emphasizes the importance of scalar measures of multivariate scatter to evaluate time-use heterogeneity, particularly among retired and employed older adults. This suggests that even within specific groups, there can be significant variability in how time is allocated.\n\nDatchary and Gaglio (2014) delve into the complexities of time management within organizations, highlighting the cognitive frameworks that shape individuals' relationships with time. They point out that understanding temporal heterogeneity is crucial for effective time management in productive settings, indicating that time-use is not uniform even within organizational contexts.\n\nKamakura (2009) takes a broader approach by identifying different \"time-styles\" that reflect how individuals allocate their 24-hour day across various activities. This study underscores the diversity in time allocation based on personal life priorities, suggesting that time-use is inherently heterogeneous across individuals.\n\nWawer et al. (2009) propose a model that links social networks and time usage, revealing that time allocation is influenced by both individual schedules and the social context. Their findings indicate that cultural factors play a significant role in shaping time-use patterns, further supporting the notion of heterogeneity.\n\nIn summary, the collective findings from these studies illustrate that time-use is characterized by significant heterogeneity, influenced by demographic factors, organizational contexts, personal priorities, and social networks. This complexity challenges the notion of time-use homogeneity, suggesting that a nuanced understanding of time allocation is essential for both research and practical applications in various fields.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2017}, "authors": ["J. Rachlinski", "Andrew J. Wistrich", "C. Guthrie"], "summary": "IntroductionPerhaps no topic at the intersection of law and social science has generated as much research as the influence of political attitudes on judicial decisionmaking.1 One would think that summarizing it would be a nearly Herculean task, but it is actually straightforward: judicial politics matters.2 From Stuart Nagel's well-known comprehensive study of the effect of politics on state and federal supreme court justices conducted over a half century ago3 right up to the present day, study after study finds that the political orientation of judges influences their decisions.4 This finding is somewhat remarkable, given that many studies use the political party of the appointing president as the measure of political attitudes.5 The underlying effect of politics on judges must be potent indeed if such a blunt and unreliable measure of political attitudes can generate meaningful effects.Despite the consistent finding that political attitudes influence judicial decisionmaking, there is a robust and heated debate on the topic that can nonetheless be simply summarized as follows: academics assert that attitudes influence judicial decisionmaking, but judges usually deny that politics matters.6 Consider the recent testimony by then-judge Gorsuch in his confirmation hearings. He insisted that the law will drive his decisions, not his politics.7 This position is hardly unusual. When they comment at all on the social science evidence suggesting that they are influenced by politics, judges tend to dismiss it.8 Political scientists continue to pile on evidence that politics influences judges, even though judges deny it.What explains the divide between the evidence and the judges? Several reasons suggest themselves. First, judges might possibly be disingenuously denying the influence of their political attitudes. Like most public officials (and perhaps more so than most given the way they are selected and their lack of enforcement power), judges are highly protective of the legitimacy of their institution.9 The assertion that judging is politics by another means threatens the legitimacy of the judiciary. Second, maybe judges are reacting to the more numerous, mostly mundane legal issues that they must decide. As then-judge Gorsuch asserted at his recent nomination hearing, he agreed with his colleagues on the U.S. Court of Appeals for the Tenth Circuit in 97% of the cases they decided.10 Perhaps the law is determinate enough that most of the time, it leaves little room for political judgment.11 Judges also might incorrectly assume that political attitudes do not influence fact finding.12 After all, the day-to-day experience of judges is that the law is clear in most cases and they easily find consensus with colleagues possessing different political perspectives.The explanation that most judicial decisionmaking is determinate applies perhaps the least to the U.S. Supreme Court.13 Judges' political attitudes might be less significant in lower court cases, in which judges often are constrained by precedent and address less politically contentious issues. Political scientists tend to emphasize those close, politically charged cases decided by the U.S. Supreme Court, rather than the ordinary disputes that occupy most judges, thereby producing different conclusions about the influence of political attitudes on judicial decisionmaking.Third, perhaps the most intriguing account of the divergence between judges and the academy, is that judges might be oblivious to the role that politics plays in their decisionmaking processes. Judges perhaps feel that they \"call them like they seem them,\" to use Chief Justice Roberts' umpire metaphor.14 Their attitudes and beliefs affect how they see facts, respond to arguments, and understand the law, but perhaps all of that operates in a way that is invisible to them. In effect, judges are what psychologists call naive realists,15 who believe they see the world through a clear lens, unaffected by political beliefs. \u2026", "title": "Judicial Politics and Decisionmaking: A New Approach", "claims": null}, {"metadata": {"year": 2018}, "authors": ["Elliott Ash", "Daniel L. Chen", "Sergio Galletta"], "summary": "This paper provides an empirical analysis of how the sentiments expressed in judicial rulings affect social attitudes in the local jurisdiction. We use natural language processing methodologies to extrapolate judges\u2019 preferences towards specific target groups from the text of U.S. appellate court opinions. To provide causal estimates we use an instrumental variables strategy by exploiting the random assignment of judges to cases and the evidence that judges\u2019 characteristics are good predictors of their expressed sentiments. In reduced form estimates, we find that judges born in the 1940s (who came of age in the 1960s) have a significant suasive impact on social attitudes toward courts, congress, police, and labor unions. In 2SLS estimates, we find that judge writing style has heterogeneous impacts. When judges write approvingly of whites, police, and congress, there is a backlash effect. When judges write approvingly of the federal government, conservatives, or the Supreme Court, there is a suasive effect.", "title": "Judicial Sentiments and Social Attitudes : Evidence from U . S . Circuit Courts", "claims": null}, {"metadata": {"year": 1979}, "authors": ["S. Goldman"], "summary": "The proposition that an individual's past behavior provides the best guide to predicting subsequent behavior given similar circumstances or stimuli is well accepted by social psychologists and by most students of politics. However, traditional students of the law, insofar as judicial decision-making is concerned, never accepted this. For them, it was court precedent that guided individual behavior. More sophisticated legal scholars recognized the importance of individual contributions to the development of the law but political, social, and economic stimuli were considered outside the realm of decision-making. It remained for the judicial behavior school to provide a wealth of studies to demonstrate that judges respond not only to legal stimuli but also to a wide range of political or public policy stimuli. The inference was irresistible that a judge's attitudes and values play a major role in judicial decision-making. Other studies have tested and uncovered some relationships between judges' attributes and their judicial behavior.1 Thus it is now generally accepted by political scientists (although still meeting skepticism from some legal circles) that judges respond to political stimuli, that certain of their attributes are linked in some ways to their decision-making, and that knowledge of past behavior is the best predictor of future behavior, given similar political stimuli. However, the literature is surprisingly barren of precise tests that suggest how much of the variance of subse", "title": "THE EFFECT OF PAST JUDICIAL BEHAVIOR ON SUBSEQUENT DECISION-MAKING", "claims": null}, {"metadata": {"year": 1989}, "authors": ["Craig R. Ducat", "R. Dudley"], "summary": "Contemporary research in judicial politics has shown that political attitudes and values significantly influence the votes cast by judges in the decision of concrete cases.' There has also been a convincing demonstration of the linkage between the political values of various presidential administrations and the appointment of federal judges who then have effected basic policy changes through their decisions,2 although presidents have shown differences in interest and commitment to judicial policymaking premised on ideological conviction. 3 However, despite the fact that more than a decade has now elapsed since the various legal controversies included under the generic title of Watergate, there remains a complete absence of any empirical examination of federal judicial decisionmaking on the issues of presidential power, as distinguished from inquiries about federal judicial support for policy items comprising the president's agenda.", "title": "Federal Judges and Presidential Power: Truman to Reagan", "claims": null}, {"metadata": {"year": 1978}, "authors": ["R. Lehne", "John Reynolds"], "summary": "This research note explores the impact of judicial activism on public opinion by examining citizen evaluations of a state court system deeply involved in a highly political controversy. Data are derived from four surveys of New Jersey citizens conducted at different stages of the controversy. The note first appraises the prominence of the state supreme court and gauges the degree of public approval, and then it identifies and analyzes the correlates of the affective attitudes. The study is guided by familiar hypotheses from the empirical analysis of public support for the United States Supreme Court.", "title": "The Impact of Judicial Activism on Public Opinion", "claims": null}, {"metadata": {"year": 2008}, "authors": ["Jeff Yates", "Elizabeth Coggins"], "summary": "Two prominent theories of legal decision making provide seemingly contradictory explanations for judicial outcomes. In political science, the Attitudinal Model suggests that judicial outcomes are driven by judges' sincere policy preferences - judges bring their ideological inclinations to the decision making process and their case outcome choices largely reflect these policy preferences. In contrast, in the law and economics literature, Priest and Klein's well-known Selection Hypothesis posits that court outcomes are largely driven by the litigants' strategic choices in the selection of cases for formal dispute or adjudication - forward thinking litigants settle cases where potential judicial outcomes are readily discernable (e.g. judicial attitudes are known), hence nullifying the impact of judicial ideological preferences on case outcomes. We believe that the strategic case sorting process proposed in the law and economics literature does, in fact, affect the influence of judge ideology or attitudes on judicial outcomes. However, these two perspectives can be effectively wed to provide an integrated model of judicial decision making that accounts for the influences of both the strategic behavior of litigants and the attitudinal preferences of judges. We test this integrated model of decision making on case outcomes in the U.S. Supreme Court and employ an interactive specification to assess the influence of judicial ideology on Supreme Court outcomes while simultaneously accounting for litigants' (and justices) strategic case sorting behavior.", "title": "The Intersection of Judicial Attitudes and Litigant Selection Theories: Explaining U.S. Supreme Court Decision Making", "claims": null}, {"metadata": {"year": 2010}, "authors": ["A. Fernandez"], "summary": "There is a wealth of scholarship examining how judicial attitudes affect appellate decision-making. Less attention, however, has been paid to the attitudes of trial judges. While there is some scholarship examining the objective actions of trial judges, there is very little examining how their subjective motivations affect their performance. Trial judges merit more attention because they are the best known and most accessible judges to the American people. Moreover, just as with appellate judges, more ethnographic research is needed to investigate the connections between attitudes and actions. Therefore, in this paper, I use qualitative interviews to investigate the motivations of five state and federal trial court judges. These interviews show how judges\uff92 perceptions of law and politics influence how they view their role as a judge. This alternative way of investigating trial court judicial decision-making reveals that, at least in the minds of trial judges, legal factors have greater influence than social or political ones. This exploratory study of judicial decision-making may generate interesting projects for future scholars. Specifically, it helps us understand how judges see law as a check on their personal and political motivations. Even in political cases, judges see themselves as good faith interpreters of law because of their sense of professional obligation and the influence of jurisprudential schools of thought, which they see as outweighing political ideology.", "title": "Judges on Trial: Law and Politics in the Life of Trial Judges", "claims": null}, {"metadata": {"year": 1977}, "authors": ["V. Flango", "Craig R. Ducat"], "summary": "THE VIEW OF JUDGES as political actors whose attitudes toward subtantive policy issues often influence their decisions is the cornerstone of judicial behavioralism. The assumption is that there is \". . . a kind of stare deciks underlying the Supreme Court's decisions but that it is based on personal rather than institutional precedents.\"' Voting records of individual justices are examined to uncover regularities in behavior and to provide the basis for inferring the attitudes of judges toward the substantive issues raised. Of course, if each justice were merely following his conscience on a case by case basis, a random, rather that a persistent, coalitional pattern of voting would emerge. Evidence derived both from observations of voting patterns and from interviews with judges seems", "title": "Toward an Integration of Public Law and Judicial Behavior", "claims": null}], "query": "What is the causal effect of judicial decisions on political attitudes?", "summary_abstract": "The causal effect of judicial decisions on political attitudes is a complex interplay that has garnered significant scholarly attention. The research indicates that judicial rulings can indeed shape public sentiment and political attitudes, particularly within local jurisdictions. For instance, Ash, Chen, and Galletta (2018) utilized natural language processing to analyze U.S. appellate court opinions, revealing that judges' sentiments expressed in their rulings can have a significant impact on social attitudes towards various institutions, such as courts and police. Their findings suggest that the writing style of judges can lead to both suasive and backlash effects, depending on the context of their approval or disapproval of certain groups or entities.\n\nRachlinski, Wistrich, and Guthrie (2017) highlight a broader debate regarding the influence of political attitudes on judicial decision-making. They argue that while political scientists consistently find that judges' political orientations affect their decisions, judges themselves often deny this influence, potentially to maintain the legitimacy of the judiciary. This disconnect suggests that judges may be unaware of how their political beliefs shape their interpretations and rulings, a phenomenon described as \"naive realism.\"\n\nGoldman (1979) supports the notion that judges respond to a variety of stimuli, including political and public policy factors, indicating that their attitudes and values significantly influence their decisions. This aligns with Ducat and Dudley (1989), who assert that political attitudes and values are crucial in determining judicial votes, particularly in cases that involve significant policy implications.\n\nFurthermore, Yates and Coggins (2008) propose an integrated model of judicial decision-making that reconciles the Attitudinal Model\u2014which posits that judges' ideological preferences drive outcomes\u2014with the Selection Hypothesis, which suggests that litigants' strategic choices influence case outcomes. This model underscores the dual role of both judicial attitudes and litigant strategies in shaping judicial decisions.\n\nIn summary, the collective findings from these studies indicate that judicial decisions do have a causal effect on political attitudes, with judges' sentiments and political orientations playing a significant role in shaping public perceptions and institutional trust. The interplay between judicial behavior and political attitudes remains a rich area for further exploration, particularly in understanding how judges perceive their roles and the implications of their decisions on broader societal attitudes.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2010}, "authors": ["Camilla Elg", "Hilda R\u00f8mer Christensen", "Birte Siim"], "summary": "is an open and contested concept, which analyses the intersections of social categories creating social differences. The intersectionality approach thus addresses the complexities, interferences and interweavings of social differences, which often create inequalities and hierarchies, as they are involved in power relations. Intersectionality is a travelling concept, which has become influential in Nordic gender research (see Kvinder K\u00f8n & Forskning 2006). Kimberly Crenshaw\u2019s conceptualization of intersectionality articulated intersections between the systems of capitalism, racism and patriarchy (Crenshaw 1991). She, thus, inspired intersectionality research that was mainly structuralist in tendency. Since then the perspective has been taken up and reworked in poststructuralist approaches emphasising intersections in discourses and social positions. The intersectionality debate is still developing in dialogues between different feminist ap3", "title": "Intersectionality at Work: Concepts and Cases", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Stefan Wallaschek"], "summary": "Postcolonial theory and intersectionality are the most prominent approaches in current critical social sciences. Surprisingly, both approaches have hardly talked to each other. Influenced by the German academic debate on these, I illustrate the (dis-)connecting relations of both approaches and initiate a dialogue. In a first step, I focus on the work of Chandra T. Mohanty and of Kimberl\u00e9 Crenshaw. While Mohanty bases her criticism on a discourse analysis of writings about \u2018Third-World-Women\u2019 and develops a postcolonial feminist approach, Crenshaw analyzes the intersection of gender and \u2018race\u2019 and criticizes the blindness of the interconnection in court decisions in the US. I demonstrate the differences and resemblances of both approaches and argue in particular that both approaches follow a constructivist methodology which creates four nodal points for the fruitful dialogue. I illustrate this by analyzing the works of Sojourner Truth and Clara Zetkin. Both feminists focus on different categories in the triangle of \u2018race\u2019, class and gender. Through a close reading of crucial selected works, I show that Truth perceives gender and \u2018race\u2019 as equivalent forms of subordination. Zetkin, however, emphasizes the hierarchical subordination by the specific class position of women. I conclude that a closer dialogue between both theoretical perspectives can help to sharpen the view on multiple discrimination and injustice in times of neoliberal capitalist hegemony.", "title": "In Dialogue : Postcolonial Theory and Intersectionality", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Brenda Sims Blackwell", "Jennifer  Cruze"], "summary": "Intersectionality, coined by Crenshaw (1989), is a framework utilized across a broad range of social sciences to explore how individuals within multiple systems of oppression have unique experiences and responses to those experiences. Within the study of crime, intersectionality has primarily been utilized by feminist criminologists to identify how and why women with different social statuses or from different racial or ethnic backgrounds have different levels of criminal involvement and experiences with victimization. This article briefly reviews the state of research utilizing intersectionality frameworks to examine crime and victimization.", "title": "Intersectionality and Crime", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Jocelyn M. Boryczka", "J. Disney"], "summary": "Intersectionality, a term coined by Kimberl\u00e9 Crenshaw just over twenty-five years ago,1 appears frequently in mainstream media discourse and represents a well-recognized body of work that ranges from legal scholarship and policy analysis to social justice organizing, transnational movements, and feminist, critical, and queer theorizing.2 Intersectionality in the twenty-first century seems to provide an analytic method and epistemological perspective that resonates with our attempts, as scholars and everyday people, to understand the rapidly changing and increasingly complex contemporary world. Movement characterizes this Global Age as people travel, physically and virtually, across geographic space. Migration typifies much of human experience, as people regularly cross borders with the number of international migrants worldwide reaching 232 million in 2013.3 Immigration, as a result, represents a central issue in many Global North nations, often generating nativist backlashes that claim to protect the jobs and ways of life of the nonimmigrant population. Mapping how power shifts in relation to people\u2019s identities in order to make visible often invisible marginalized communities such as immigrants, as Crenshaw shows us in her second groundbreaking article \u201cMapping the Margins\u201d becomes a central feature of intersectionality that further strengthens its capacity to help us make sense out of the Global Age.4 While drawing our attention to the structural, representational, and political types of intersectionality, Crenshaw keeps the focus on how our identities, especially race, class, and gender, circumscribe the power that we can exercise in relation to these broader global and national systems. Identity then remains salient to those engaged in intersectional work despite claims that we live in a post-feminist, post-race, and post-identity world.5 These claims", "title": "Intersectionality for the Global Age", "claims": null}, {"metadata": {"year": 2016}, "authors": ["Jenny K. Rodriguez", "E. Holvino", "Joyce K. Fletcher", "S. Nkomo"], "summary": "Intersectionality is understood as a metaphor (Cuadraz and Uttal, 1999; Acker, 2011), a concept (Knapp, 2005; Styhre and Ericksson-Zetterquist, 2008), a research paradigm (Hancock, 2007a; Dhamoon, 2011), an ideograph (Alexander-Floyd, 2012), a broad-based knowledge project (Collins, 2015), and an analytical sensibility (Crenshaw, 2015). In spite of these diverse definitions, intersectionality has been central to the study of inequality, identity and power relations in recent history (Cho et al., 2013), highlighting the inseparability of categories of social differences such as race, gender, class, ethnicity, sexuality and nation, and calling attention to the systemic power dynamics that arise as multiple dimensions of social difference interact across individual, institutional, cultural and societal spheres of influence (Collins, 2000; McCall, 2005; Yuval-Davis, 2006, 2011; Weber, 2010). Coined as a term by Kimberl\u00e9 Crenshaw in 1989 to \u201ccounter the disembodiment of Black women from Law\u201d (Crenshaw, 2014), intersectionality captured the", "title": "The Theory and Praxis of Intersectionality in Work and Organisations: Where Do We Go From Here?", "claims": null}, {"metadata": {"year": 2017}, "authors": ["T. Meer", "A. M\u00fcller"], "summary": "The concept of intersectionality, first conceived by Kimberle Crenshaw (1989), has widely been acknowledged as one of the most significant contributions of feminist scholarship. Broadly speaking, intersectionality posits that different social identities like race, gender, class and sexuality intersect and inform each other in what Patricia Hill Collins (1993) calls \u201cinterlocking systems of oppression\u201d. Consequently, it is the intersection of multiple identities that influences individual worldviews and lifechances differently than would any single form of social stratification.", "title": "Considering intersectionality in Africa", "claims": null}, {"metadata": {"year": 2015}, "authors": ["C\u00e9cile Coquet-Mokoko"], "summary": "This collective work is informed by the project of keeping the political dimension at the core of the theory of intersectionality\u2014a concept coined by Kimberle Crenshaw in two founding texts, \u201cDemarginalizing the Intersection of Race and Sex\u201d (1989) and \u201cMapping the Margins\u201d (1991) in which she critiqued the limitations of both feminist and antiracist activists and academics in pushing for equal rights and fair representation for Black women across racial, class, and gender lines. As Crenshaw ...", "title": "Monica Michlin and Jean-Paul Rocchi (eds), Black Intersectionalities: A Critique for the 21st Century. Liverpool, Liverpool University Press, FORECAAST, (Forum for European Contributions to African American Studies) 1, 2013, 258 p. ISBN 978-1-84631-938-9. \u00a370", "claims": null}, {"metadata": {"year": 2018}, "authors": ["B. Foley"], "summary": "Intersectionality, a way of thinking about the nature and causes of social inequality, proposes that the effects of multiple forms of oppression are cumulative and, as the term suggests, interwoven. Not only do racism, sexism, homophobia, disablism, religious bigotry, and so-called \u201cclassism\u201d wreak pain and harm in the lives of many people, but any two or more of these types of oppression can be experienced simultaneously in the lives of given individuals or demographic sectors. According to the intersectional model, it is only by taking into account the complex experiences of many people who are pressed to the margins of mainstream society that matters of social justice can be effectively addressed. In order to assess the usefulness of intersectionality as an analytical model and practical program, however\u2014and, indeed, to decide whether or not it can actually be said to be a \u201ctheory,\u201d as a number of its proponents insist\u2014we need to ask not only what kinds of questions it encourages and remedies, but also what kinds of questions it discourages and what kinds of remedies it forecloses. It is standard procedure in discussions of intersectionality to cite important forebears\u2014 from Sojourner Truth to Anna Julia Cooper, from Alexandra Kollontai to Claudia Jones to the Combahee River Collective\u2014but then to zero in on the work of the legal theorist Kimberl\u00e9 Crenshaw, who first coined and explicated the term in the late 1980s. Concerned with overcoming the discriminatory situation faced by African American women workers at General Motors (GM), Crenshaw demonstrated the inadequacy of existing categories denoting gender and race as grounds for legal action, since these could not be mobilized simultaneously in the case of a given individual: you had to be either a woman or nonwhite, but not both at the same time. Crenshaw famously developed the metaphor of a crossroads of two avenues, one denoting race, the other gender, to make the point that accidents occurring at the intersection could not be attributed to solely one cause; it took motion along two crossing roads to make an accident happen. While Crenshaw\u2019s model ably describes the workings of what the African American feminist writer Patricia Hill Collins has termed a \u201cmatrix of oppressions,\u201d the model\u2019s spatial two-dimensionality points to its inadequacy as an explanation of why this \u201cmatrix\u201d exists in the first place. Who created these avenues? Why would certain people be traveling down them? Where were they constructed, and when? The spatial model 867944 NLFXXX10.1177/1095796019867944New Labor ForumFoley research-article2019", "title": "Intersectionality: A Marxist Critique", "claims": null}], "query": "List me the works of Crenshaw on intersectionality.", "summary_abstract": "The works of Kimberl\u00e9 Crenshaw on intersectionality have significantly shaped the discourse surrounding social identities and systemic oppression. Crenshaw first coined the term \"intersectionality\" in her foundational texts, \"Demarginalizing the Intersection of Race and Sex\" (1989) and \"Mapping the Margins\" (1991), where she critiqued the limitations of both feminist and antiracist movements in addressing the unique challenges faced by Black women (Coquet-Mokoko, 2015). Her analysis highlighted how the intersections of race, gender, and class create distinct experiences of discrimination that cannot be understood through a singular lens (Elg et al., 2010; Meer & M\u00fcller, 2017).\n\nCrenshaw's work emphasizes the importance of recognizing the interconnectedness of various social categories, arguing that these intersections inform individual experiences and responses to oppression (Blackwell & Cruze, 2015; Rodriguez et al., 2016). This perspective has been influential across multiple disciplines, including legal scholarship, feminist theory, and social justice movements, as it provides a framework for understanding the complexities of identity and power dynamics in contemporary society (Boryczka & Disney, 2015; Foley, 2018).\n\nMoreover, Crenshaw's contributions have sparked dialogues with other critical theories, such as postcolonialism, further enriching the understanding of intersectionality and its implications for addressing multiple forms of discrimination (Wallaschek, 2016). Her work remains a cornerstone in the study of inequality, urging scholars and activists alike to consider the cumulative effects of overlapping identities in their analyses and advocacy efforts (Sims Blackwell & Cruze, 2015; Meer & M\u00fcller, 2017). \n\nIn summary, Crenshaw's intersectionality framework serves as a vital tool for unpacking the complexities of social injustice, advocating for a nuanced approach to understanding how various forms of oppression intersect and impact marginalized communities.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2007}, "authors": ["N. Cheeseman", "R. Ford"], "summary": "Afrobarometer publications report the results of national sample surveys on the attitudes of citizens in selected African countries towards democracy, markets, civil society, and other aspects of development. The Afrobarometer is a collaborative enterprise of Michigan State University (MSU), the Institute for Democracy in South Africa (IDASA), and the Centre for Democratic Development (CDD, Ghana). Afrobarometer papers are simultaneously co-published by these partner institutions and the Globalbarometer. Abstract This paper examines the significance of ethnicity as a political cleavage across African nations over time. While scholars have studied the influence of ethnicity in structuring party politics in Africa, those studies have largely been limited to an examination of ruling party support. This work develops a measure of \u2018ethnic voting\u2019 that is reflective of all significant parties and ethnic groups. This measure of \u2018ethnic voting\u2019 allows us to compare reliably across countries within the Afrobarometer sample. Adopting measures that have been employed in class analyses in developed countries over the past twenty years, we construct two measures,\u2018ethnic polarization\u2019 and \u2018ethnic diversity\u2019. The former captures the importance of ethnicity in determining party support levels while the latter captures variations in the ethnic diversity of the support base of different parties. Together, these two measures show variation in the salience of ethnicity as a political cleavage across African countries over time.", "title": "Ethnicity as a political cleavage", "claims": null}, {"metadata": {"year": 1983}, "authors": ["H. V. D. Merwe"], "summary": "Abstract The process of urbanization has caused a massive influx of Africans to South African cities. In contrast to the traditional government policy that our cities are the domain of whites, the whites are in fact a minority part of an integrated multi-racial society marked by inequality and racial tension. Excessive emphasis on the maintenance of law and order as applied in the influx control system contributes to further racial polarization and animosity. Verligte and pragmatic elements in the Establishment which recognize integration and power-sharing as inevitable provide a base for eventual settlement with genuine black leaders, including the African National Congress.", "title": "Urbanization and the political position of Africans in South Africa", "claims": null}, {"metadata": {"year": 1983}, "authors": ["P. Smit"], "summary": "Abstract The socio-economic and political position of the urban Black population. Suprisingly little is known about the urban Black population in South Africa. Few in-depth studies of Black urbanization have been made and very little is known about the pace of Black urbanization, the aspirations, perceptions and preferences of urban Blacks. There is overwhelming evidence that the urban Black population is gradually taking a normal demographic profile. The levels of personal income, literacy, health services and general well-being between urban and rural Blacks differ considerably. But urban Blacks still have strong links with the national states. Due to increasing inter-ethnic marriages geographic pluralism is decreasing in the urban areas. For the foreseeable future, the process of Black urbanization will not only continue but also accelerate. As in other countries the influx of rural people could unleash tremendous socio-economic and political problems. In view of this, imaginative planning will be need...", "title": "Die sosio-ekonomiese en politieke posisie van die stedelike Swart bevolking", "claims": null}, {"metadata": {"year": 2015}, "authors": ["Un-Habitat"], "summary": "Profound sociocultural, economic and political changes accompany urbanization in Africa which, around 2035, for the first time will cause the larger share of political constituency to reside in cities. Broad themes identified on African urbanism relate to aspects of colonial aftermath; increasing informality; socio-political exclusion; urban governance and service provision; warfare, violence and disease; connectivity; and urban culture. 109", "title": "Re-imagining African urbanis", "claims": null}, {"metadata": {"year": 2009}, "authors": ["Gladys Mitchell-Walthour"], "summary": "Despite that nearly half of Brazil's citizens are of African descent, less than 2% of politicians in federally elected positions are Afro-Brazilian. Most political scientists attribute this to a lack of racial identity and ethnic voting. Considering the changes in racial politics such as university affirmative action, the recognition of racism in Brazilian society, and continuous Black movement activity, it is important to examine notions of black racial group identity. This paper employs qualitative and quantitative methods to examine black linked fate in the Brazilian context in the cities Salvador and Sao Paulo.", "title": "Assessing the Impact of Afro-Brazilian Activists on notions of Blackness and Black group Identity in Salvador and S\u00e3o Paulo, Brazil", "claims": null}, {"metadata": {"year": 1996}, "authors": ["N\u00e9stor P. Rodr\u00edguez"], "summary": "Introduction Various macro-structural forces are transforming the social composition of large U.S. urban areas in the late 20th century. As in other advanced Western societies, global economic restructuring and international migration are dramatically altering socio-cultural and demographic landscapes in the urban United States. In the short historical span since the 1970s, these processes have produced substantial growth of ethnic/racial populations in large U.S. urban areas, e.g., New York, Los Angeles, and Houston, with strong ties to the global economy. Large-scale immigration from Asia, Latin America, and other world regions has intensified urban change in these settings by creating new culturally distinct communities (Lamphere, 1992). Socially and culturally, and in terms of new patterns of intergroup relations, the large U.S. urban centers of the 1990s are not the same settings of 10 or 20 years ago. This large-scale social and demographic change has produced new interrelational matrices in U.S. urban areas (Bach, 1993). With the expansion and diversification of Asian, Latino, and other ethnic/racial communities, the poles of urban race relations have been transformed from a mainly binary plane of black-white relations into multidimensional axes of ethnicity, immigrant status, nationality, race, and other social identifies. Especially after the social eruptions of African Americans and Latinos in Los Angeles in the spring of 1992, this social recomposition has created concerns among mainstream institutional leaders about the interethnic/racial future of their localities. African American and Latino communities are prominent players in this future, since in many of the largest urban areas they form a collective majority of the population. Among the five largest cities in the country, this is true in New York, Los Angeles, Chicago, and Houston (U.S. Bureau of the Census, 1994). In the fifth largest city, Philadelphia, African Americans and Latinos compose 45.2% of the population. In the South, Houston is a critical case of emerging intergroup relations between African Americans and Latinos in the context of high immigration levels. The 1990 census found that Houston has more black residents than any other southern city, and has the second largest Latino population in the South (if not the largest by 1995). Over 40% of Houston's 450,000 Latino population consists of first-generation immigrants (U.S. Bureau of the Census, 1993). In this article, I focus on the arena of intergroup relations between African Americans and Latinos from the perspective of Latino immigration. In the first part of the discussion, I attempt to relate the arena of intergroup relations to larger structural processes with reference to global change and immigration. These processes are important for relations between African Americans and Latinos because they greatly affect the social geographies and related opportunity structures of intergroup interaction. In the second part of the discussion, using findings from recent intergroup surveys and ongoing ethnographic research in the Houston area, I argue that contrary to some expectations (see Johnson et al., 1995) tensions, conflict, and community instability are not the only resulting relations between African Americans and Latinos in contexts of high immigration. Indeed, I attempt to make the case for varied modes of intergroup reactions in such settings, sometimes varying by social identities other than ethnicity or race, and sometimes forming collaborative relations based precisely on identities of minority status. Using the prominent Houston case, my purpose in this discussion also is to suggest that the highly publicized intergroup patterns of Los Angeles do not necessarily represent the future of the U.S. urban system. Macro-Structural Contexts of Immigration And African American-Latino Relations In the late 20th century, urban intergroup relations have become substantially affected by underlying structural processes whose reach transcends not only specific urban settings, but also the very nation-state. \u2026", "title": "U.S. Immigration and Intergroup Relations in the Late 20th Century: African Americans and Latinos", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Fabio Macedo Velame", "T. A. G. D. Costa"], "summary": "The aim of this article is to present an observation on the structural phenomena that culminated in socio-spatial and ethnic-racial segregation, which currently occurs in megacities, large cities and global cities in Africa. To this end, a presentation of futuristic issues that cross contemporary African urbanization discussions will be made. Then, historical issues and discourses present in some African countries, which help to explain ethnic, racial, social and spatial aspects perpetuated in urban space, will be addressed. These aspects will include examples from cities in sub-Saharan African countries, where overpopulation and high foreign investment in the real estate market aggravate the segregationist effects.", "title": "SOCIO-SPATIAL AND ETHNIC-RACIAL SEGREGATION IN MEGACITIES, LARGE CITIES AND GLOBAL CITIES IN AFRICA", "claims": null}, {"metadata": {"year": 2021}, "authors": ["M. Fortner"], "summary": "Racism has profoundly shaped American political development. The postwar restructuring of the American party system around race (Frymer 2010; Carmines and Stimson 2020) overwhelmed intra-racial socioeconomic heterogeneity and generated seemingly endemic racial divides in national American political behavior and attitudes (Kinder and Sanders 1996; Tate 1993; Tesler 2016; Jardina 2019). While early students of African American society encountered significant class cleavages (Drake and Cayton 1945; Frazier 1957), many contemporary observers emphasize \u201clinked fate\u201d\u2014 the theory that African Americans believe their life chances are tied to the fate of their racial group and, as a result, rely on group-based cues when formulating political preferences (Dawson 1993). Local patterns have mirrored, if not help generate, these outcomes. Rich urban histories exhumed a grassroots White backlash to Black migration and civil rights and its impact on the rightward turn in American politics (Sugrue 1996; Kruse 2005; Self 2005). Key texts in the urban politics literature documented how race divided urban Democratic organizations and local electorates and frustrated the emergence of progressive cross-racial coalitions in the post-civil rights era (Pinderhughes 1987; Browning, Marshall and Tabb 1990; Sonenshein 1993; Delorenzo 1997). Analyzing a diverse array of data on urban elections from a limited sample of American cities, Zoltan Hajnal", "title": "Racial Capitalism and City Politics: Toward a Theoretical Synthesis", "claims": null}], "query": "urban ethnicity african politics afrobarometer data", "summary_abstract": "The research question regarding urban ethnicity, African politics, and Afrobarometer data reveals significant insights into the interplay between ethnicity and political dynamics in African urban settings. Cheeseman and Ford (2007) highlight the importance of ethnicity as a political cleavage across various African nations, developing measures of 'ethnic voting' that allow for comparative analysis across the Afrobarometer sample. Their findings indicate that the salience of ethnicity in determining party support varies significantly over time and across countries, suggesting that ethnic identities play a crucial role in shaping political affiliations and electoral outcomes.\n\nMoreover, the process of urbanization in Africa has led to profound socio-political changes, as noted by Un-Habitat (2015). The increasing urban population is expected to shift political constituencies towards cities, which may further complicate ethnic dynamics in urban governance and service provision. This urbanization is accompanied by challenges such as socio-political exclusion and the need for effective governance, which are critical for understanding the political landscape in urban Africa.\n\nSmit (1983) emphasizes the socio-economic and political position of the urban Black population in South Africa, noting that urbanization has resulted in a demographic profile that increasingly resembles that of the general population. This demographic shift, coupled with inter-ethnic marriages, suggests a gradual decrease in geographic pluralism, which may influence political affiliations and ethnic identities in urban areas.\n\nIn summary, the collective findings from these papers underscore the complex relationship between urban ethnicity and politics in Africa. Ethnic identities significantly influence political behavior, and the ongoing urbanization process is reshaping these dynamics, necessitating further research and imaginative planning to address the socio-political challenges that arise in increasingly diverse urban environments (Cheeseman & Ford, 2007; Un-Habitat, 2015; Smit, 1983).", "summary_extract": null}, {"papers": [{"metadata": {"year": 2004}, "authors": ["\uad8c\uc131\uc544"], "summary": "\u2018Hongik Ideology\u2019(HI), ideal of making the world peaceful and in harmony by a man who is helpful universally, has been born with the foundation of Chosun by Dahngoon and reborn with the departure of Korea Provisional Government established in Sanghai. But it became disregarded in political and educational area since Liberalization occurred and military administration ruled in Korea. \nIt was not considered when the constitution established in 1948. Later as the educational law which was set-up in 1949 covered it, HI became to be regarded as the combination of Democracy and Nationalism. After the 6\u00b725 war, HI seemed abused as Conservatism or anti-Communism by the government and the educational world. Consequently, its practical object to educate disappeared when the \u2018basic educational law\u2019 replaced the educational law in 1997. \nHI that is based on everlasting love for a human being and a nature and life-respect ideology suggests following to current educational environment: \nFirst, against the flood tide of Individualism, it emphasizes that we should have community consciousness based on mutual help spirit to survive. It recommends a mankind to form \u2018peace community\u2019 regardless of race, religion and culture ultimately through the education based on HI. \nSecond, regarding the tendency of focusing on passing examination, it suggests school should pursue \u2018the harmony of virtue, wisdom and power\u2019 in every curriculum. \nThird, to balance and overcome egoism, education should be made to value all the living things and to love and take care them. \nFourth, the coincidence of knowing and living in one and healthiness of body and mind is the object of education in school that HI suggests. \nFifth, to improve current teaching method that induces confrontation and competition, HI emphasizes that to live is the process to seek the spirituality given from the heaven in harmony, unity and mutual help spirit. \nAs briefed above, the ideology of \u2018Hongikinghan\u2019 is the humanistic theory in whith contains korean cultural identity. Therefore, under its guidance and direction, we can go forward to meet new world with our tradition and culture confidently.", "title": "\ud64d\uc778\uc778\uac04\uc758 \uc774\uc0c1\uc5d0\uc11c \ubcf8 \ud55c\uad6d\uad50\uc721", "claims": null}, {"metadata": {"year": 2023}, "authors": ["N. Tan", "Y. Yusoff", "D. Koot", "Q. C. Lau", "H. Lim", "T. F. Hui", "H. Y. Cher", "P. Tan", "Y. Koh"], "summary": "A Healthcare-assistive Infection-control RObot (HIRO) is a healthcare-assistive robot that is deployed in an outpatient primary care clinic to sanitise the premises, monitor people in its proximity for their temperature and donning of masks, and usher them to service points. This study aimed to determine the acceptability, perceptions of safety, and concerns among the patients, visitors, and polyclinic healthcare workers (HCWs) regarding the HIRO. A cross-sectional questionnaire survey was conducted from March to April 2022 when the HIRO was at Tampines Polyclinic in eastern Singapore. A total of 170 multidisciplinary HCWs serve approximately 1,000 patients and visitors daily at this polyclinic. The sample size of 385 was computed using a proportion of 0.5, 5% precision, and 95% confidence interval. Research assistants administered an e-survey to gather demographic data and feedback from 300 patients/visitors and 85 HCWs on their perceptions of the HIRO using Likert scales. The participants watched a video on the HIRO\u2019s functionalities and were given the opportunity to directly interact with it. Descriptive statistics was performed and figures were presented in frequencies and percentages. The majority of the participants viewed the HIRO\u2019s functionalities favourably: sanitising (96.7%/91.2%); checking proper mask donning (97%/89.4%); temperature monitoring (97%/91.7%); ushering (91.7%/81.1%); perceived user friendliness (93%/88.3%), and improvement in the clinic experience (96%/94.2%). A minority of the participants perceived harm from the HIRO\u2019s liquid disinfectant (29.6%/31.5%) and that its voice-annotated instructions may be upsetting (14%/24.8%). Most of the participants accepted the HIRO\u2019s deployment at the polyclinic and perceived it to be safe. The HIRO used ultraviolet irradiation for sanitisation during after-clinic hours instead of disinfectants due to the perceived harm.", "title": "Introducing a healthcare-assistive robot in primary care: a preliminary questionnaire survey", "claims": null}, {"metadata": {"year": 2020}, "authors": ["Steve Mann"], "summary": "Over the past 20 years Wearable Computing has emerged as the perfect tool for embodying Humanistic Intelligence (HI). HI is defined as intelligence that arises from the human being in the feedback loop of a computational process in which the human and computer are inextricably intertwined. When a wearable computer functions in a successful embodiment of HI, the computer uses the human\u2019s mind and body as one of its peripherals, just as the human uses the computer as a peripheral. This reciprocal relationship, where each uses the other in its feedback loop, is at the heart of HI.", "title": "Wearable Computing: Towards Humanistic Intelligence", "claims": null}, {"metadata": {"year": 1983}, "authors": ["N. Stevens"], "summary": "Suddenly high technology, or hi tech (which is not to be confused with tech high), is upon us in both our professional and personal lives. This wonderful new complicated, but at the same time simple, technology holds out to us the promise of handling all of our routine tasks, providing instant access to many new forms of information, solving most of our problems, and otherwise revolutionizing our existence.", "title": "Hi(gh) Tech(xpectations)", "claims": null}, {"metadata": {"year": 1993}, "authors": ["J. Gardner", "Robin Wilson"], "summary": "Yesterday evening about 30 members of the Halifax Mechanics Institute and Mutual Improvement Society took tea together at Stott's Temperance Hotel, Broad Street, for the purpose of presenting a testimonial of respect to Mr Thomas A. Hirst, assistant to Mr Carter, land surveyor, who is about leaving the town. Mr Hirst has been an active voluntary teacher in the above society for upwards of 32 years, and has won the esteem and respect both of the directors and members, especially those of his own class, having taught the higher branches of mathematics with great ability.", "title": "Thomas Archer Hirst\u2014Mathematician Xtravagant II. Student Days in Germany", "claims": null}, {"metadata": {"year": 2022}, "authors": ["Tu\u011fba Ta\u015fdemir"], "summary": "John Hick, New York\u2019taki Union Theological Seminary\u2019de verilen Auburn Konferanslar\u0131n\u0131n geni\u015fletilmi\u015f bir versiyonu olan bu \u00e7al\u0131\u015fmas\u0131nda, dinsel \u00e7o\u011fulculukla ilgili ileri s\u00fcrd\u00fc\u011f\u00fc arg\u00fcmanlar\u0131n yol a\u00e7t\u0131\u011f\u0131 yakla\u015f\u0131k 30 y\u0131ld\u0131r yap\u0131lan ele\u015ftirileri, felsefi ve teolojik itirazlara de\u011finen filozof Phil ve teolog Grace ile kar\u015f\u0131l\u0131kl\u0131 diyaloglar \u015feklinde yan\u0131tlar. Kitab\u0131n kapa\u011f\u0131, b\u00fct\u00fcn d\u00fcnya dinlerini A\u015fk\u0131n Ger\u00e7ekli\u011fe y\u00f6nelik onu kavramada, deneyimlemede ve O\u2019na kar\u015f\u0131l\u0131k vermede e\u015fit ge\u00e7erli yollar olarak kabul eden dinsel \u00e7o\u011fulculuk anlay\u0131\u015f\u0131yla ilgili ipu\u00e7lar\u0131 ta\u015f\u0131r. Hick\u2019in ifadesiyle, g\u00fcne\u015fin \u0131\u015f\u0131klar\u0131n\u0131n yery\u00fcz\u00fc atmosferi taraf\u0131ndan muhte\u015fem bir tayfa d\u00f6n\u00fc\u015ft\u00fcr\u00fclmesi gibi g\u00f6kku\u015fa\u011f\u0131 da \u0130lahi \u0131\u015f\u0131\u011f\u0131n bizim be\u015feri k\u00fclt\u00fcrlerimiz taraf\u0131ndan yans\u0131t\u0131lmas\u0131n\u0131 ifade eden bir metafordur.", "title": "\u0130nan\u00e7lar\u0131n G\u00f6kku\u015fa\u011f\u0131 Dinsel \u00c7o\u011fulculuk \u00dczerine Ele\u015ftirel Diyaloglar", "claims": null}, {"metadata": {"year": 2006}, "authors": ["W. Anggraeni"], "summary": "Hubungan hukum antara pemberi hibah dan penerima hibah adalah hubungan hukum karena adanya perjanjian dimana pemberi hibah sebagai debitor dan penerima hibah sebagai kreditor. Hibah adalah hubungan hukum yang sepihak. Artinya, pemberi hibah memberikan hibah pada penerima hibah secara cuma-cuma tanpa ada imbalan apapun dari penerima hibah. Penerima hibah bisa berasal dari pars waris/ waris itu sendiri. \n \nPenerima hibah dapat mengajukan gugatannya akibat pembatalan hibah yang dilakukan oleh si pemberi hibah apabila pemberi hibah wanprestasi yaitu menarik hibah secara sepihak dan hibah yang dibuat antara kedua belah pihak mengikat dan berlaku sebagai Undang-Undang bagi kedua pihak. Kecuali bila si penerima hibah wanprestasi yaitu dengan menelantarkan si pemberi hibah dan dapat dibuktikan di pengadilan, maka yang mengajukan permohonan pembatalan hibah adalah si pemberi hibah dan si penerima hibah tidak bisa menggugatnya karena walupun pasal 1666 BW menyebutkan bahwa hibah tidak dapat ditarik kembali, tetapi pengaturan tentang hibah ada dalam buku III BW yang sifatnya mengatur, sehingga kedua pihak boleh menyimpanginya misalnya si penerima hibah harus memelihara pemberi hibah selama hidupnya, bila tidak maka hibah dapat dibatalkan. \n \nDengan adanya hibah, maka akan timbul hubungan hukum antara pemberi hibah dan penerima hibah walaupun hubungan hukum tersebut sifatnya sepihak yang artinya si pemberi hibah hanya punya kewajiban saja tanpa mempunyai hak, hendaknya dalam memberikan hibah pada seseorang dilihat terlebih dahulu kepatutan dan kepantasan dari si penerima hibah untuk menerima hibah tersebut, sehingga tidak timbul pembatalan hibah yang menyebabkan hubungan hukum antara kedua pihak bermasalah. \n \nGugatan dari si penerima hibah ke pemberi hibah dapat dihindari dengan jalan penyelesaian sengketa secara musyawarah atau kekeluargaan yang akan mempertemukan kepentingan kedua belah pihak daripada melalui jalan pengadilan yang akan memakan waktu lama dan belum tentu kepentingan masing-masing pihak dapat terpenuhi. Hendaknya masing-masing pihak melaksanakan perjanjian hibah itu dengan benar sehingga salah satu pihak tidak ada yang dirugikan. Misalnya penerima hibah harus dengan baik memelihara si pemberi hibah karena si pemberi hibah memberikan hibah secara ikhlas. Sehingga kedua pihak tidak ada yang merasa dirugikan yang pada akhirnya akan mengajukan gugatan kepada masing-masing pihak.", "title": "TANGGUNG GUGAT PEMBERI HIBAH AKIBAT PEMBATALAN HIBAH", "claims": null}, {"metadata": {"year": 2014}, "authors": ["Oskar Rydblom"], "summary": "This study examines the usage of the three greetings words hi, hey and hello incombination with greeting questions, such as how are you, and terms of address, such asBill, Mr Jones or Dad, in spoke ...", "title": "You still say hello! : A corpus study of the greeting words hi, hey and hello in spoken American English", "claims": null}], "query": "HI", "summary_abstract": "The research question \"HI\" encompasses various interpretations and applications of the term \"HI,\" ranging from cultural ideologies to technological advancements. \n\nFirstly, \uad8c\uc131\uc544 (2004) discusses \"Hongik Ideology\" (HI), which emphasizes the importance of community consciousness and harmony in education. This ideology, rooted in Korean cultural identity, advocates for a balanced educational approach that values mutual support, respect for all living beings, and the integration of virtue, wisdom, and power in curricula. The author argues that HI can guide contemporary education towards fostering a peaceful and harmonious society, countering individualism and competition.\n\nIn a different context, N. Tan et al. (2023) explore the deployment of a Healthcare-assistive Infection-control Robot (HIRO) in a clinical setting. Their study reveals that patients and healthcare workers generally perceive HIRO positively, appreciating its functionalities such as sanitization and temperature monitoring. This indicates a growing acceptance of technology in healthcare, aligning with the broader theme of how HI can enhance human experiences through innovative solutions.\n\nSteve Mann (2020) introduces the concept of Humanistic Intelligence (HI) through wearable computing, highlighting the reciprocal relationship between humans and technology. This embodiment of HI suggests that technology can enhance human capabilities, creating a feedback loop that benefits both parties.\n\nCollectively, these papers illustrate that \"HI\" can refer to a cultural ideology promoting community and harmony, a technological advancement improving healthcare experiences, and a conceptual framework for understanding the interplay between human intelligence and technology. Each perspective contributes to a broader understanding of how HI can influence various aspects of society and individual lives.", "summary_extract": null}, {"papers": [{"metadata": {"year": 2019}, "authors": ["Lars Guenther"], "summary": "Science journalism is a specialized form of journalism predominantly covering issues such as science, medicine, and technology; it only professionalized in the second half of the 20th century. For many people, print, audiovisual and online media are the main source they use to get to know something about these fields; hence, science journalism has an important role for the society. However, when looking at science journalism and the research in that area more closely, then a variety of different approaches paint a rather dark picture. Firstly, there is a lot of research criticizing science journalism and science journalists. What these studies focus on is science journalists\u2019 work and role for the society, their routines and practices, their reporting on specific scientific issues, as well as the relationship between journalists and scientists. Secondly, in some countries, science journalism seems to be in a crisis due to increasing digitization and changing media landscapes. Science journalism is declining in these countries and many journalists have lost their jobs. However, assessing the quality and appropriateness of science journalism should be based on journalistic and not on scientific criteria, and these criteria should be used when trying to describe what science journalism is and what not, how science journalism operates and how not, and how best to describe the role science journalism has for the society. In addition, although increasing digitization may change routines and practices of science journalists, these specialized journalists may be able to adapt to new media landscapes and still maintain their important role for the society as the most disinterested source that informs about science, medicine, and technology.", "title": "Science Journalism", "claims": null}, {"metadata": {"year": 2015}, "authors": ["M. Olvera-Lobo", "L. L\u00f3pez"], "summary": "Introduction For more than a century science journalists have acted as mediators between science and the general public, thus taking on the task of conveying the progress of research and contributing, with their educational role, to shaping the critical opinion on scientific and technological development [Treise and Weigold, 2002]. Hence, it is possible to say that they have been the main creators of the social image of scientific research that has been projected.", "title": "Science journalism: the standardisation of information from the press to the internet", "claims": null}, {"metadata": {"year": 2018}, "authors": ["C. El\u00edas"], "summary": "Science Journalism addresses the intersection of two major spheres of Western culture: natural sciences and mass media. And both can be said to share the same ultimate goal: to seek the truth and make it public. On the other hand, Science Journalism is a creative writing between Natural and Social Sciences and, as a profession, is the perfect bridge between the two cultures \u2013scientific and literary- defined by C.P. Snow. It is therefore, a rich discipline in every aspect, but also one that involves a great deal of conceptual and procedural complexity. Journalism is the craft of creative writing, and initially, science also adapted a literary style. Scientific Journalism deals exclusively with Natural Science, but with a Social Sciences point of view. It requires similar standards and guidelines, such as those used by scientists -physicists, chemists, biologists and geologists- for a journalist to approach the facts. In scientific journalism it is important to define what is theory in Natural Science as opposed to the Social Sciences. \"Science\" journalism deals with information that comes exclusively from discoveries and facts. Science journalism, as a profession, is the perfect bridge between these two cultures: scientific and literary. A science journalist has more close contact with scientists -and their scientific results- than a sociologist or philosopher of science. But at the same time, journalism is a literary genre itself.", "title": "Science Journalism as an Academic Discipline: the Fusion of Western Media and Science seen from a Literary and Social Perspective", "claims": null}, {"metadata": {"year": 2011}, "authors": ["P. Munsch"], "summary": "Science journalists are not quite like other journalists. Faced with the scientific field as with the media field, they are constrained from all sides. Their practices and ideology are constantly shifting and their role becomes increasingly inaccurate. This document proposes to re-emphasize the position of science writer, actor of scientific communication through an approach in connection with recent events and through the Information and communication sciences theories. Thus it offers a challenge to the relationship between science, media and society, in our contemporary democracies.", "title": "Le journaliste scientifique en temps de crise", "claims": null}, {"metadata": {"year": 2018}, "authors": ["A. Ridgway"], "summary": "This book is a beginners\u2019 guide to science journalism, explaining the 21st century journalistic process, from generating story ideas to creating multimedia content when the story\u2019s written, taking in research and writing structures along the way. While many of the chapters are introductory, the book also covers topics also likely to be of interest to more experienced writers, such as storytelling techniques and investigative journalism. Readers are introduced to important debates in the field, including the role that science journalism plays; whether it is a form of \u2018infotainment\u2019, or whether its primary role is to hold scientists and the science industry to account. Taken as a whole, what the book does particularly well is to \nintroduce prospective science writers to the judgements they need to make as reflective practitioners.", "title": "Science journalism by a journalist for journalists", "claims": null}, {"metadata": {"year": 2017}, "authors": ["H. Pearson"], "summary": "Science journalists cover some of the most complex, exciting, and important issues of our day, ranging from the impacts of climate change to emerging infectious diseases. They use words, sounds, images, and graphics to create compelling stories about science that appear in newspapers and magazines, in print and online, on the radio and TV, and in podcasts and videos. The field is undergoing rapid change, which presents both challenges and opportunities. The migration of readers and advertising to the internet and digital platforms has led to declining sales of print publications. This means that jobs are now scarce in traditional print media, but that there are growing opportunities to produce digital content.", "title": "Careers in Science Journalism and Writing.", "claims": null}, {"metadata": {"year": 2011}, "authors": ["S. Allan"], "summary": "The people who run the media are humanities graduates with little understanding of science, who wear their ignorance as a badge of honour. Secretly, deep down, perhaps they resent the fact that they have denied themselves access to the most significant developments in the history of Western thought from the past 200 years; but there is an attack implicit in all media coverage of science: in their choice of stories, and the way they cover them, the media create a parody of science. On this template, science is portrayed as groundless, incomprehensible, didactic truth statements from scientists, who themselves are socially powerful, arbitrary, unelected authority figures. They are detached from reality; they do work that is either whacky or dangerous, but either way, everything in science is tenuous, contradictory, probably going to change soon and, most ridiculously, \u2018hard to understand.\u2019 Having created this parody, the commentariat then attack it, as if they were genuinely critiquing what science is all about. (2009: 207\u20138)", "title": "Introduction: Science Journalism in a Digital Age", "claims": null}, {"metadata": {"year": 1996}, "authors": ["Elfriede F\u00fcrsich", "E. Lester"], "summary": "In this study, a new cultural framework for the analysis of science popularization is proposed. This concept is utilized for a textual analysis of the science section of The New York Times focusing on the column \u2018Scientist at Work.\u2019 Findings show that although the journalists try to demystify the scientific enterprise by placing it within a broader context, they juxtapose the notion of a pure science against the other\u2010social, political, and individual\u2010dimensions of scientific work. In the column, scientists do not achieve authority through an arcane stardom, but through their participation in the presumably pristine aspects of the scientific endeavor.", "title": "Science journalism under scrutiny: A textual analysis of \u201cscience times\u201d", "claims": null}], "query": "Form of Science Journalism", "summary_abstract": "The abstracts collectively highlight the multifaceted nature of science journalism, emphasizing its critical role in bridging the gap between scientific knowledge and public understanding. Guenther (2019) notes that science journalism has only recently professionalized and faces challenges due to digitization and changing media landscapes, which have led to job losses and a perceived decline in quality. However, he argues that science journalists can adapt to these changes and continue to serve as vital sources of information on science, medicine, and technology.\n\nOlvera-Lobo and L\u00f3pez (2015) reinforce the idea that science journalists have historically acted as mediators between the scientific community and the public, shaping societal perceptions of scientific research. El\u00edas (2018) further elaborates on this by describing science journalism as a creative discipline that merges natural and social sciences, requiring journalists to navigate complex standards and practices to effectively communicate scientific truths.\n\nMunsch (2011) emphasizes the unique challenges faced by science journalists, who must balance the demands of both the scientific and media fields, while Pearson (2017) discusses the evolving landscape of science journalism, highlighting the shift towards digital platforms and the opportunities this presents for storytelling. Ridgway (2018) provides a practical perspective, offering insights into the journalistic process and the ethical considerations that science writers must navigate.\n\nHowever, the field is not without its criticisms. Allan (2011) argues that media portrayals of science often misrepresent its complexities, leading to a public perception that undermines the credibility of scientific endeavors. F\u00fcrsich and Lester (1996) also point out that while journalists strive to demystify science, they often juxtapose it with social and political dimensions, complicating the public's understanding of scientific authority.\n\nIn summary, science journalism is portrayed as a vital yet challenging field that plays a crucial role in informing the public about scientific issues. It faces significant challenges due to technological changes and media perceptions, but it also holds the potential for adaptation and growth in the digital age.", "summary_extract": null}]